,title,question,response,id,qas,dblp_id,dblp_title,year,ground_truth,bert metrics,bleu,sem_sim
0,Minimally Supervised Learning of Affective Events Using Discourse Relations,What is the seed lexicon?,A set of words with known polarity scores (+1 for positive events and -1 for negative events) that are used to assign scores to other events through label propagation.,1909.00694,"[{'question': 'What is the seed lexicon?', 'question_id': '753990d0b621d390ed58f20c4d9e4f065f0dc672', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}, {'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}]}, {'question': 'What are the results?', 'question_id': '9d578ddccc27dd849244d632dd0f6bf27348ad81', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.']}]}, {'question': 'How are relations used to propagate polarity?', 'question_id': '02e4bf719b1a504e385c35c6186742e720bcb281', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ', 'evidence': [""In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.""], 'highlighted_evidence': [""As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.""]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity', 'evidence': [""In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event."", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': [""As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event."", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.']}]}, {'question': 'How big is the Japanese data?', 'question_id': '44c4bd6decc86f1091b5fc0728873d9324cdde4e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well.', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.']}]}, {'question': 'What are labels available in dataset for supervision?', 'question_id': '86abeff85f3db79cf87a8c993e5e5aa61226dc98', 'answers': [{'unanswerable': False, 'extractive_spans': ['negative', 'positive'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).""], 'highlighted_evidence': ['In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).']}]}, {'question': 'How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?', 'question_id': 'c029deb7f99756d2669abad0a349d917428e9c12', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '3%', 'evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.']}]}, {'question': 'How does their model learn using mostly raw data?', 'question_id': '39f8db10d949c6b477fa4b51e7c184016505884f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity', 'evidence': [""In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.""], 'highlighted_evidence': [""In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive).""]}]}, {'question': 'How big is seed lexicon used for training?', 'question_id': 'd0bc782961567dc1dd7e074b621a6d6be44bb5b4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '30 words', 'evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. ']}]}, {'question': 'How large is raw corpus used for training?', 'question_id': 'a592498ba2fac994cd6fad7372836f0adb37e22a', 'answers': [{'unanswerable': False, 'extractive_spans': ['100 million sentences'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.']}]}]",conf/emnlp/SaitoMK19,Minimally Supervised Learning of Affective Events Using Discourse Relations.,2019,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"(0.8851531744003296, 0.9208178520202637, 0.9026333689689636)",1.2287022022568943e-231,0.75308758020401
1,Community Identity and User Engagement in a Multi-Community Landscape,Do they report results only on English data?,"No. They manually remove communities where the bulk of the contributions are in a foreign language, but it is not specified that all remaining data is in English.",1705.09665,"[{'question': 'Do they report results only on English data?', 'question_id': '003f884d3893532f8c302431c9f70be6f64d9be8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).""], 'highlighted_evidence': ['We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. ']}, {'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How do the various social phenomena examined manifest in different types of communities?', 'question_id': 'bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n"", 'evidence': [""We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right)."", ""As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content.""], 'highlighted_evidence': [""We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right)."", ""As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content.""]}]}, {'question': 'What patterns do they observe about how user engagement varies with the characteristics of a community?', 'question_id': 'eea089baedc0ce80731c8fdcb064b82f584f483a', 'answers': [{'unanswerable': False, 'extractive_spans': ['communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members', ""within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ""], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Engagement and community identity. We apply our framework to understand how two important aspects of user engagement in a community—the community's propensity to retain its users (Section SECREF3 ), and its permeability to new members (Section SECREF4 )—vary according to the type of collective identity it fosters. We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members."", ""More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities.""], 'highlighted_evidence': ['We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members.', ""More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). ""]}]}, {'question': 'How did the select the 300 Reddit communities for comparison?', 'question_id': 'edb2d24d6d10af13931b3a47a6543bd469752f0c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language."", 'evidence': [""Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).""], 'highlighted_evidence': [""Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. ""]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'They collect subreddits from January 2013 to December 2014,2 for which there are at\nleast 500 words in the vocabulary used to estimate the measures,\nin at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.', 'evidence': [""Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).""], 'highlighted_evidence': [""Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).""]}]}, {'question': 'How do the authors measure how temporally dynamic a community is?', 'question_id': '938cf30c4f1d14fa182e82919e16072fdbcf2a82', 'answers': [{'unanswerable': False, 'extractive_spans': ['the average volatility of all utterances'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Dynamicity. A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . We refer to a community whose language is relatively consistent throughout time as being stable.'], 'highlighted_evidence': ['. A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . ']}]}, {'question': 'How do the authors measure how distinctive a community is?', 'question_id': '93f4ad6568207c9bd10d712a52f8de25b3ebadd4', 'answers': [{'unanswerable': False, 'extractive_spans': [' the average specificity of all utterances'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Distinctiveness. A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 . We refer to a community with a less distinctive identity as being generic.'], 'highlighted_evidence': ['A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 ']}]}]",conf/icwsm/ZhangHDJL17,Community Identity and User Engagement in a Multi-Community Landscape.,2017,"We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. ","(0.9128440618515015, 0.8788207769393921, 0.8955093622207642)",0.30811407056068785,0.5140314698219299
2,Question Answering based Clinical Text Structuring Using Pre-trained Language Model,What data is the language model pretrained on?,Chinese general corpus.,1908.06606,"[{'question': 'What data is the language model pretrained on?', 'question_id': '71a7153e12879defa186bfb6dbafe79c74265e10', 'answers': [{'unanswerable': False, 'extractive_spans': ['Chinese general corpus'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.'], 'highlighted_evidence': ['Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.']}, {'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What baselines is the proposed model compared against?', 'question_id': '85d1831c28d3c19c84472589a252e28e9884500f', 'answers': [{'unanswerable': False, 'extractive_spans': ['BERT-Base', 'QANet'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Experimental Studies ::: Comparison with State-of-the-art Methods', 'Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.'], 'highlighted_evidence': ['Experimental Studies ::: Comparison with State-of-the-art Methods\nSince BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.']}, {'unanswerable': False, 'extractive_spans': ['QANet BIBREF39', 'BERT-Base BIBREF26'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.', 'FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL'], 'highlighted_evidence': ['Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. ', 'FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL']}]}, {'question': 'How is the clinical text structuring task defined?', 'question_id': '1959e0ebc21fafdf1dd20c6ea054161ba7446f61', 'answers': [{'unanswerable': False, 'extractive_spans': ['Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained.', 'Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task.', 'Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.', 'To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.'], 'highlighted_evidence': ['FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task.', 'Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.', 'Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. ']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.', 'evidence': ['Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.', 'However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost.', 'To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.'], 'highlighted_evidence': ['Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly.', 'However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size).', 'To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text.']}]}, {'question': 'What are the specific tasks being unified?', 'question_id': '77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8', 'answers': [{'unanswerable': False, 'extractive_spans': [' three types of questions, namely tumor size, proximal resection margin and distal resection margin'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.', 'Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.', 'In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different datasets. A novel model is also proposed to integrate named entity information into a pre-trained language model and adapt it to QA-CTS task. Initially, sequential results of named entity recognition on both paragraph and query texts are integrated together. Contextualized representation on both paragraph and query texts are transformed by a pre-trained language model. Then, the integrated named entity information and contextualized representation are integrated together and fed into a feed forward network for final prediction. Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. The shared task and shared model introduced by QA-CTS task has also been proved to be useful for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset.'], 'highlighted_evidence': ['Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data.', 'All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. ', 'Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks.']}, {'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Is all text in this dataset a question, or are there unrelated sentences in between questions?', 'question_id': '06095a4dee77e9a570837b35fc38e77228664f91', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ', 'evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.'], 'highlighted_evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. ']}]}, {'question': 'How many questions are in the dataset?', 'question_id': '19c9cfbc4f29104200393e848b7b9be41913a7ac', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '2,714 ', 'evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.'], 'highlighted_evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs.']}]}, {'question': 'What is the perWhat are the tasks evaluated?', 'question_id': '6743c1dd7764fc652cfe2ea29097ea09b5544bc3', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Are there privacy concerns with clinical data?', 'question_id': '14323046220b2aea8f15fba86819cbccc389ed8b', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How they introduce domain-specific features into pre-trained language model?', 'question_id': '08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4', 'answers': [{'unanswerable': False, 'extractive_spans': ['integrate clinical named entity information into pre-trained language model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model.', 'In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word.'], 'highlighted_evidence': ['We also propose an effective model to integrate clinical named entity information into pre-trained language model.', 'In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text.']}]}, {'question': 'How big is QA-CTS task dataset?', 'question_id': '975a4ac9773a4af551142c324b64a0858670d06e', 'answers': [{'unanswerable': False, 'extractive_spans': ['17,833 sentences, 826,987 characters and 2,714 question-answer pairs'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.'], 'highlighted_evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. ']}]}, {'question': 'How big is dataset of pathology reports collected from Ruijing Hospital?', 'question_id': '326e08a0f5753b90622902bd4a9c94849a24b773', 'answers': [{'unanswerable': False, 'extractive_spans': ['17,833 sentences, 826,987 characters and 2,714 question-answer pairs'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.'], 'highlighted_evidence': ['Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs.']}]}, {'question': 'What are strong baseline models in specific tasks?', 'question_id': 'bd78483a746fda4805a7678286f82d9621bc45cf', 'answers': [{'unanswerable': False, 'extractive_spans': ['state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.'], 'highlighted_evidence': ['Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.']}]}]",conf/bibm/QiuZMRLS19,Question Answering based Clinical Text Structuring Using Pre-trained Language Model.,2019,"Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.","(0.922122597694397, 0.8281550407409668, 0.8726164102554321)",2.8468306583110405e-82,0.5157392024993896
3,LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,Do they evaluate only on English datasets?,No,2003.07433,"[{'question': 'Do they evaluate only on English datasets?', 'question_id': '53712f0ce764633dbb034e550bb6604f15c0cacd', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do the authors mention any possible confounds in this study?', 'question_id': '0bffc3d82d02910d4816c16b390125e5df55fd01', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How is the intensity of the PTSD established?', 'question_id': 'bdd8368debcb1bdad14c454aaf96695ac5186b09', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.', 'evidence': [""To provide an initial results, we take 50% of users' last week's (the week they responded of having PTSD) data to develop PTSD Linguistic dictionary and apply LAXARY framework to fill up surveys on rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition. In terms of intensity, Coppersmith et. al. totally fails to provide any idea however LAXARY provides extremely accurate measures of intensity estimation for PTSD sufferers (as shown in Fig FIGREF31) which can be explained simply providing LAXARY model filled out survey details. Table TABREF29 shows the details of accuracies of both PTSD detection and intensity estimation. Fig FIGREF32 shows the classification accuracy changes over the training sample sizes for each survey which shows that DOSPERT scale outperform other surveys. Fig FIGREF33 shows that if we take previous weeks (instead of only the week diagnosis of PTSD was taken), there are no significant patterns of PTSD detection.""], 'highlighted_evidence': [' Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. ']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'defined into four categories from high risk, moderate risk, to low risk', 'evidence': [""There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15. This is a psychometric scale that assesses risk taking in five content domains: financial decisions (separately for investing versus gambling), health/safety, recreational, ethical, and social decisions. Respondents rate the likelihood that they would engage in domain-specific risky activities (Part I). An optional Part II assesses respondents' perceptions of the magnitude of the risks and expected benefits of the activities judged in Part I. There are more scales that are used in risky behavior analysis of individual's daily activities such as, The Berlin Social Support Scales (BSSS) BIBREF16 and Values In Action Scale (VIAS) BIBREF17. Dryhootch America BIBREF18, BIBREF19, a veteran peer support community organization, chooses 5, 6 and 5 questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample DOSPERT scale demographic chosen by dryhootch. The threshold (in Table TABREF13) is used to calculate the risky behavior limits. For example, if one individual's weekly DOSPERT score goes over 28, he is in critical situation in terms of risk taking symptoms of PTSD. Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )"", ""High risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life."", ""Moderate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression."", ""Low risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD."", ""No PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD.""], 'highlighted_evidence': [""Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )\n\nHigh risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life.\n\nModerate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression.\n\nLow risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD.\n\nNo PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD.""]}]}, {'question': 'How is LIWC incorporated into this system?', 'question_id': '3334f50fe1796ce0df9dd58540e9c08be5856c23', 'answers': [{'unanswerable': False, 'extractive_spans': [' For each user, we calculate the proportion of tweets scored positively by each LIWC category.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['A threshold of 1 for $s-score$ divides scores into positive and negative classes. In a multi-class setting, the algorithm minimizes the cross entropy, selecting the model with the highest probability. For each user, we calculate the proportion of tweets scored positively by each LIWC category. These proportions are used as a feature vector in a loglinear regression model BIBREF20. Prior to training, we preprocess the text of each tweet: we replace all usernames with a single token (USER), lowercase all text, and remove extraneous whitespace. We also exclude any tweet that contained a URL, as these often pertain to events external to the user.'], 'highlighted_evidence': ['For each user, we calculate the proportion of tweets scored positively by each LIWC category. ']}, {'unanswerable': False, 'extractive_spans': ['to calculate the possible scores of each survey question using PTSD Linguistic Dictionary '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['LAXARY includes a modified LIWC model to calculate the possible scores of each survey question using PTSD Linguistic Dictionary to fill out the PTSD assessment surveys which provides a practical way not only to determine fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment.'], 'highlighted_evidence': ['LAXARY includes a modified LIWC model to calculate the possible scores of each survey question using PTSD Linguistic Dictionary to fill out the PTSD assessment surveys which provides a practical way not only to determine fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment.']}]}, {'question': 'How many twitter users are surveyed using the clinically validated survey?', 'question_id': '7081b6909cb87b58a7b85017a2278275be58bf60', 'answers': [{'unanswerable': False, 'extractive_spans': ['210'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets. Fig FIGREF16 shows each of the 210 veteran twitter users' monthly average tweets. We categorize these Tweets into two groups: Tweets related to work and Tweets not related to work. That is, only the Tweets that use a form of the word “work*” (e.g. work,worked, working, worker, etc.) or “job*” (e.g. job, jobs, jobless, etc.) are identified as work-related Tweets, with the remaining categorized as non-work-related Tweets. This categorization method increases the likelihood that most Tweets in the work group are indeed talking about work or job; for instance, “Back to work. Projects are firing back up and moving ahead now that baseball is done.” This categorization results in 456 work-related Tweets, about 5.4% of all Tweets written in English (and 75 unique Twitter users). To conduct weekly-level analysis, we consider three categorizations of Tweets (i.e. overall Tweets, work-related Tweets, and non work-related Tweets) on a daily basis, and create a text file for each week for each group.""], 'highlighted_evidence': [""We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets. ""]}]}, {'question': 'Which clinically validated survey tools are used?', 'question_id': '1870f871a5bcea418c44f81f352897a2f53d0971', 'answers': [{'unanswerable': False, 'extractive_spans': ['DOSPERT, BSSS and VIAS'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use an automated regular expression based searching to find potential veterans with PTSD in twitter, and then refine the list manually. First, we select different keywords to search twitter users of different categories. For example, to search self-claimed diagnosed PTSD sufferers, we select keywords related to PTSD for example, post trauma, post traumatic disorder, PTSD etc. We use a regular expression to search for statements where the user self-identifies as being diagnosed with PTSD. For example, Table TABREF27 shows a self-identified tweet posts. To search veterans, we mostly visit to different twitter accounts of veterans organizations such as ""MA Women Veterans @WomenVeterans"", ""Illinois Veterans @ILVetsAffairs"", ""Veterans Benefits @VAVetBenefits"" etc. We define an inclusion criteria as follows: one twitter user will be part of this study if he/she describes himself/herself as a veteran in the introduction and have at least 25 tweets in last week. After choosing the initial twitter users, we search for self-identified PTSD sufferers who claim to be diagnosed with PTSD in their twitter posts. We find 685 matching tweets which are manually reviewed to determine if they indicate a genuine statement of a diagnosis for PTSD. Next, we select the username that authored each of these tweets and retrieve last week\'s tweets via the Twitter API. We then filtered out users with less than 25 tweets and those whose tweets were not at least 75% in English (measured using an automated language ID system.) This filtering left us with 305 users as positive examples. We repeated this process for a group of randomly selected users. We randomly selected 3,000 twitter users who are veterans as per their introduction and have at least 25 tweets in last one week. After filtering (as above) in total 2,423 users remain, whose tweets are used as negative examples developing a 2,728 user\'s entire weeks\' twitter posts where 305 users are self-claimed PTSD sufferers. We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses. Among these responses, 92 users were diagnosed as PTSD by any of the three surveys and rest of the 118 users are diagnosed with NO PTSD. Among the clinically diagnosed PTSD sufferers, 17 of them were not self-identified before. However, 7 of the self-identified PTSD sufferers are assessed with no PTSD by PTSD assessment tools. The response rates of PTSD and NO PTSD users are 27% and 12%. In summary, we have collected one week of tweets from 2,728 veterans where 305 users claimed to have diagnosed with PTSD. After distributing Dryhootch surveys, we have a dataset of 210 veteran twitter users among them 92 users are assessed with PTSD and 118 users are diagnosed with no PTSD using clinically validated surveys. The severity of the PTSD are estimated as Non-existent, light, moderate and high PTSD based on how many surveys support the existence of PTSD among the participants according to dryhootch manual BIBREF18, BIBREF19.', ""There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15. This is a psychometric scale that assesses risk taking in five content domains: financial decisions (separately for investing versus gambling), health/safety, recreational, ethical, and social decisions. Respondents rate the likelihood that they would engage in domain-specific risky activities (Part I). An optional Part II assesses respondents' perceptions of the magnitude of the risks and expected benefits of the activities judged in Part I. There are more scales that are used in risky behavior analysis of individual's daily activities such as, The Berlin Social Support Scales (BSSS) BIBREF16 and Values In Action Scale (VIAS) BIBREF17. Dryhootch America BIBREF18, BIBREF19, a veteran peer support community organization, chooses 5, 6 and 5 questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample DOSPERT scale demographic chosen by dryhootch. The threshold (in Table TABREF13) is used to calculate the risky behavior limits. For example, if one individual's weekly DOSPERT score goes over 28, he is in critical situation in terms of risk taking symptoms of PTSD. Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )""], 'highlighted_evidence': ['We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses. ', 'Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )']}]}]",conf/smartcomp/AlamK20,LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment.,2020,,"(0.0, 0.0, 0.0)",0.0,0.5032033920288086
4,Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data,Which vision-based approaches does this approach outperform?,"Bergsma and Van Durme 2011; Kiela, Vulic, and Clark 2015; Hewitt et al. 2018",1906.00378,"[{'question': 'Which vision-based approaches does this approach outperform?', 'question_id': '591231d75ff492160958f8aa1e6bfcbbcd85a776', 'answers': [{'unanswerable': False, 'extractive_spans': ['CNN-mean', 'CNN-avgmax'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:', 'CNN-mean: taking the similarity score of the averaged feature of the two image sets.', 'CNN-avgmax: taking the average of the maximum similarity scores of two image sets.'], 'highlighted_evidence': ['We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets.']}]}, {'question': 'What baseline is used for the experimental setup?', 'question_id': '9e805020132d950b54531b1a2620f61552f06114', 'answers': [{'unanswerable': False, 'extractive_spans': ['CNN-mean', 'CNN-avgmax'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:', 'CNN-mean: taking the similarity score of the averaged feature of the two image sets.', 'CNN-avgmax: taking the average of the maximum similarity scores of two image sets.'], 'highlighted_evidence': ['We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets.']}]}, {'question': 'Which languages are used in the multi-lingual caption model?', 'question_id': '95abda842c4df95b4c5e84ac7d04942f1250b571', 'answers': [{'unanswerable': False, 'extractive_spans': ['German-English, French-English, and Japanese-English'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:'], 'highlighted_evidence': ['We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. ']}, {'unanswerable': False, 'extractive_spans': ['multiple language pairs including German-English, French-English, and Japanese-English.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:'], 'highlighted_evidence': ['We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces.']}]}]",conf/aaai/ChenJH19,Unsupervised Bilingual Lexicon Induction from Mono-Lingual Multimodal Data.,2019,"We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:

CNN-mean: taking the similarity score of the averaged feature of the two image sets.

CNN-avgmax: taking the average of the maximum similarity scores of two image sets.","(0.7627716064453125, 0.7984700202941895, 0.7802127003669739)",0.0,0.09117888659238815
5,Generative Adversarial Nets for Multiple Text Corpora,Which GAN do they use?,"Energy-based GAN (Zhao et al., 2017)",1712.09127,"[{'question': 'Which GAN do they use?', 'question_id': '6548db45fc28e8a8b51f114635bad14a13eaec5b', 'answers': [{'unanswerable': False, 'extractive_spans': ['We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 .'], 'highlighted_evidence': ['We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 .']}, {'unanswerable': False, 'extractive_spans': ['weGAN', 'deGAN'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Suppose we have a number of different corpora INLINEFORM0 , which for example can be based on different categories or sentiments of text documents. We suppose that INLINEFORM1 , INLINEFORM2 , where each INLINEFORM3 represents a document. The words in all corpora are collected in a dictionary, and indexed from 1 to INLINEFORM4 . We name the GAN model to train cross-corpus word embeddings as “weGAN,” where “we” stands for “word embeddings,” and the GAN model to generate document embeddings for multiple corpora as “deGAN,” where “de” stands for “document embeddings.”'], 'highlighted_evidence': ['We name the GAN model to train cross-corpus word embeddings as “weGAN,” where “we” stands for “word embeddings,” and the GAN model to generate document embeddings for multiple corpora as “deGAN,” where “de” stands for “document embeddings.”']}]}, {'question': 'Do they evaluate grammaticality of generated text?', 'question_id': '4c4f76837d1329835df88b0921f4fe8bda26606f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""We hypothesize that because weGAN takes into account document labels in a semi-supervised way, the embeddings trained from weGAN can better incorporate the labeling information and therefore, produce document embeddings which are better separated. The results are shown in Table 1 and averaged over 5 randomized runs. Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. Because the Rand index captures matching accuracy, we observe from the Table 1 that weGAN tends to improve both metrics.""], 'highlighted_evidence': [""Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. ""]}]}, {'question': 'Which corpora do they use?', 'question_id': '819d2e97f54afcc7cdb3d894a072bcadfba9b747', 'answers': [{'unanswerable': False, 'extractive_spans': ['CNN, TIME, 20 Newsgroups, and Reuters-21578'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In the experiments, we consider four data sets, two of them newly created and the remaining two already public: CNN, TIME, 20 Newsgroups, and Reuters-21578. The code and the two new data sets are available at github.com/baiyangwang/emgan. For the pre-processing of all the documents, we transformed all characters to lower case, stemmed the documents, and ran the word2vec model on each corpora to obtain word embeddings with a size of 300. In all subsequent models, we only consider the most frequent INLINEFORM0 words across all corpora in a data set.'], 'highlighted_evidence': ['In the experiments, we consider four data sets, two of them newly created and the remaining two already public: CNN, TIME, 20 Newsgroups, and Reuters-21578. The code and the two new data sets are available at github.com/baiyangwang/emgan. For the pre-processing of all the documents, we transformed all characters to lower case, stemmed the documents, and ran the word2vec model on each corpora to obtain word embeddings with a size of 300. In all subsequent models, we only consider the most frequent INLINEFORM0 words across all corpora in a data set.']}]}]",conf/ijcnn/EbrahimiKW21,Generative Adversarial Nets for Multiple Text Corpora.,2021,"We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 .","(0.8342747092247009, 0.7344344854354858, 0.7811774611473083)",9.10207022952547e-240,0.3214198350906372
6,Stacked DeBERT: All Attention in Incomplete Data for Text Classification,Do they report results only on English datasets?,No.,2001.00137,"[{'question': 'Do they report results only on English datasets?', 'question_id': '637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.', ""The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names.""], 'highlighted_evidence': ['Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.', ""The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names.""]}]}, {'question': ""How do the authors define or exemplify 'incorrect words'?"", 'question_id': '4b8257cdd9a60087fa901da1f4250e7d910896df', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'typos in spellings or ungrammatical words', 'evidence': [""Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3.""], 'highlighted_evidence': [""Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. ""]}]}, {'question': 'How many vanilla transformers do they use after applying an embedding layer?', 'question_id': '7e161d9facd100544fa339b06f656eb2fc64ed28', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do they test their approach on a dataset without incomplete data?', 'question_id': 'abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF22. The first step is to apply a TTS module to the available complete sentence. Here, we apply gtts , a Google Text-to-Speech python library, and macsay , a terminal command available in Mac OS as say. The second step consists of applying an STT module to the obtained audio files in order to obtain text containing STT errors. The STT module used here was witai , freely available and maintained by Wit.ai. The mentioned TTS and STT modules were chosen according to code availability and whether it's freely available or has high daily usage limitations.""], 'highlighted_evidence': ['The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11."", ""In the intent classification task, we are presented with a corpus that suffers from the opposite problem of the Twitter sentiment classification corpus. In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words. Analysis on this dataset adds value to our work by enabling evaluation of our model's robustness to different rates of data incompleteness.""], 'highlighted_evidence': [""For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11."", 'In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words.']}]}, {'question': 'Should their approach be applied only when dealing with incomplete data?', 'question_id': '4debd7926941f1a02266b1a7be2df8ba6e79311a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences.""], 'highlighted_evidence': [""We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. ""]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We propose Stacked Denoising BERT (DeBERT) as a novel encoding scheming for the task of incomplete intent classification and sentiment classification from incorrect sentences, such as tweets and text with STT error. The proposed model, illustrated in Fig. FIGREF4, is structured as a stacking of embedding layers and vanilla transformer layers, similarly to the conventional BERT BIBREF10, followed by layers of novel denoising transformers. The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. By reconstructing these hidden embeddings, we are able to improve the encoding scheme in BERT.'], 'highlighted_evidence': ['The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. ']}]}, {'question': 'By how much do they outperform other models in the sentiment in intent classification tasks?', 'question_id': '3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average', 'evidence': [""Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences."", 'Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.', 'FLOAT SELECTED: Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5)).'], 'highlighted_evidence': [""Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. "", 'Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.', 'FLOAT SELECTED: Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5)).']}]}]",journals/nn/SergioL21,Stacked DeBERT: All attention in incomplete data for text classification.,2021,"Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.","(0.9122917652130127, 0.7925746440887451, 0.8482298851013184)",0.0,0.15631097555160522
7,Gunrock: A Social Bot for Complex and Engaging Long Conversations,What is the sample size of people used to measure user satisfaction?,Not explicitly stated in the snippet.,1910.03042,"[{'question': 'What is the sample size of people used to measure user satisfaction?', 'question_id': '44c7c1fbac80eaea736622913d65fe6453d72828', 'answers': [{'unanswerable': False, 'extractive_spans': ['34,432 user conversations'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?""). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users\' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets).'], 'highlighted_evidence': [' Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?"").']}, {'unanswerable': False, 'extractive_spans': ['34,432 '], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Amazon Alexa Prize BIBREF0 provides a platform to collect real human-machine conversation data and evaluate performance on speech-based social conversational systems. Our system, Gunrock BIBREF1 addresses several limitations of prior chatbots BIBREF2, BIBREF3, BIBREF4 including inconsistency and difficulty in complex sentence understanding (e.g., long utterances) and provides several contributions: First, Gunrock's multi-step language understanding modules enable the system to provide more useful information to the dialog manager, including a novel dialog act scheme. Additionally, the natural language understanding (NLU) module can handle more complex sentences, including those with coreference. Second, Gunrock interleaves actions to elicit users' opinions and provide responses to create an in-depth, engaging conversation; while a related strategy to interleave task- and non-task functions in chatbots has been proposed BIBREF5, no chatbots to our knowledge have employed a fact/opinion interleaving strategy. Finally, we use an extensive persona database to provide coherent profile information, a critical challenge in building social chatbots BIBREF3. Compared to previous systems BIBREF4, Gunrock generates more balanced conversations between human and machine by encouraging and understanding more human inputs (see Table TABREF2 for an example)."", 'From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?""). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users\' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets).'], 'highlighted_evidence': [""Amazon Alexa Prize BIBREF0 provides a platform to collect real human-machine conversation data and evaluate performance on speech-based social conversational systems. Our system, Gunrock BIBREF1 addresses several limitations of prior chatbots BIBREF2, BIBREF3, BIBREF4 including inconsistency and difficulty in complex sentence understanding (e.g., long utterances) and provides several contributions: First, Gunrock's multi-step language understanding modules enable the system to provide more useful information to the dialog manager, including a novel dialog act scheme. "", 'From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock.', 'We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations.']}]}, {'question': 'What are all the metrics to measure user engagement?', 'question_id': '3e0c9469821cb01a75e1818f2acb668d071fcf40', 'answers': [{'unanswerable': False, 'extractive_spans': ['overall rating', 'mean number of turns'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.""], 'highlighted_evidence': [' We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.']}, {'unanswerable': False, 'extractive_spans': ['overall rating', 'mean number of turns'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.""], 'highlighted_evidence': [""We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.""]}]}, {'question': 'What the system designs introduced?', 'question_id': 'a725246bac4625e6fe99ea236a96ccb21b5f30c6', 'answers': [{'unanswerable': False, 'extractive_spans': ['Amazon Conversational Bot Toolkit', 'natural language understanding (NLU) (nlu) module', 'dialog manager', 'knowledge bases', 'natural language generation (NLG) (nlg) module', 'text to speech (TTS) (tts)'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1.""], 'highlighted_evidence': ['We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts).']}]}, {'question': 'Do they specify the model they use for Gunrock?', 'question_id': '516626825e51ca1e8a3e0ac896c538c9d8a747c8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1.""], 'highlighted_evidence': ['We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response.', 'While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1.']}]}, {'question': 'Do they gather explicit user satisfaction data on Gunrock?', 'question_id': '77af93200138f46bb178c02f710944a01ed86481', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?""). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users\' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets).'], 'highlighted_evidence': ['Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?"").']}]}, {'question': 'How do they correlate user backstory queries to user satisfaction?', 'question_id': '71538776757a32eee930d297f6667cd0ec2e9231', 'answers': [{'unanswerable': False, 'extractive_spans': ['modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions."", ""Results showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts.""], 'highlighted_evidence': [""We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.\n\nResults showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts.""]}]}]",conf/emnlp/YuCYCWZZJCBISDB19,Gunrock: A Social Bot for Complex and Engaging Long Conversations.,2019," Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?"").","(0.847624659538269, 0.7937681674957275, 0.8198128342628479)",1.0685238564498298e-234,0.09449232369661331
8,Towards Detection of Subjective Bias using Contextualized Word Embeddings,Do the authors report only on English?,No,2002.06644,"[{'question': 'Do the authors report only on English?', 'question_id': '830de0bd007c4135302138ffa8f4843e4915e440', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the baseline for the experiments?', 'question_id': '680dc3e56d1dc4af46512284b9996a1056f89ded', 'answers': [{'unanswerable': False, 'extractive_spans': ['FastText', 'BiLSTM', 'BERT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.', 'BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.', 'BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.'], 'highlighted_evidence': ['FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.', 'BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.', 'BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.']}, {'unanswerable': False, 'extractive_spans': ['FastText', 'BERT ', 'two-layer BiLSTM architecture with GloVe word embeddings'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Baselines and Approach', 'In this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection.', 'Baselines and Approach ::: Baselines', 'FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.', 'BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.', 'BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.', 'FLOAT SELECTED: Table 1: Experimental Results for the Subjectivity Detection Task'], 'highlighted_evidence': ['Baselines and Approach\nIn this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection.\n\n', 'Baselines and Approach ::: Baselines\nFastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.\n\nBiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.\n\nBERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.', 'FLOAT SELECTED: Table 1: Experimental Results for the Subjectivity Detection Task']}]}, {'question': 'Which experiments are perfomed?', 'question_id': 'bd5379047c2cf090bea838c67b6ed44773bcd56f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'They used BERT-based models to detect subjective language in the WNC corpus', 'evidence': [""In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints. Writers and editors of texts like news and textbooks try to avoid the use of biased language, yet subjective bias is pervasive in these texts. More than $56\\%$ of Americans believe that news sources do not report the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language."", 'In this work, we investigate the application of BERT-based models for the task of subjective language detection. We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy.', 'Experiments ::: Dataset and Experimental Settings', 'We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019. We randomly shuffled these sentences and split this dataset into two parts in a $90:10$ Train-Test split and perform the evaluation on the held-out test dataset.'], 'highlighted_evidence': [""In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints."", 'In this work, we investigate the application of BERT-based models for the task of subjective language detection.', 'Experiments ::: Dataset and Experimental Settings\nWe perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019']}]}]",conf/www/PantDM20,Towards Detection of Subjective Bias using Contextualized Word Embeddings.,2020,,"(0.0, 0.0, 0.0)",0.0,0.5032033920288086
9,"Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",Is ROUGE their only baseline?,No,1809.08731,"[{'question': 'Is ROUGE their only baseline?', 'question_id': '7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.', 'We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.', 'We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as', 'Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:', 'Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments.'], 'highlighted_evidence': ['Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.', 'We compare to the best n-gram-overlap metrics from toutanova2016dataset;', 'We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length.', 'Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:', 'Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . ']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.', 'evidence': ['Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.', 'We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.', 'We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as', 'Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:', 'Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments.'], 'highlighted_evidence': ['Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.', 'We compare to the best n-gram-overlap metrics from toutanova2016dataset;', 'We further compare to the negative LM cross-entropy', 'Our next baseline is perplexity, ', 'Due to its popularity, we also performed initial experiments with BLEU BIBREF17 ']}]}, {'question': 'what language models do they use?', 'question_id': '3ac30bd7476d759ea5d9a5abf696d4dfc480175b', 'answers': [{'unanswerable': False, 'extractive_spans': ['LSTM LMs'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.', 'We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data.'], 'highlighted_evidence': ['We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus.', 'We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data.']}]}, {'question': 'what questions do they ask human judges?', 'question_id': '0e57a0983b4731eba9470ba964d131045c8c7ea7', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/conll/KannRF18,"Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",2018,"Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.","(0.8350042700767517, 0.7840574383735657, 0.808729350566864)",0.0,-0.0366874635219574
10,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,What misbehavior is identified?,None.,1707.00995,"[{'question': 'What misbehavior is identified?', 'question_id': 'f0317e48dafe117829e88e54ed2edab24b86edb1', 'answers': [{'unanswerable': False, 'extractive_spans': ['if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:'], 'highlighted_evidence': ['Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. ']}, {'unanswerable': False, 'extractive_spans': ['if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:'], 'highlighted_evidence': ['Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations.']}]}, {'question': 'What is the baseline used?', 'question_id': 'ec91b87c3f45df050e4e16018d2bf5b62e4ca298', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Which attention mechanisms do they compare?', 'question_id': 'f129c97a81d81d32633c94111018880a7ffe16d1', 'answers': [{'unanswerable': False, 'extractive_spans': ['Soft attention', 'Hard Stochastic attention', 'Local Attention'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image.', 'Soft attention', 'Hard Stochastic attention', 'Local Attention'], 'highlighted_evidence': ['We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 .', 'Soft attention', 'Hard Stochastic attention', 'Local Attention']}]}]",conf/emnlp/DelbrouckD17,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.,2017,"Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. ","(0.9397252202033997, 0.8158978223800659, 0.8734446167945862)",0.0,0.13122962415218353
11,What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016,What are the country-specific drivers of international development rhetoric?,The paper examines the country-speciﬁc drivers of international development rhetoric.,1708.05873,"[{'question': 'What are the country-specific drivers of international development rhetoric?', 'question_id': 'a2103e7fe613549a9db5e65008f33cf2ee0403bd', 'answers': [{'unanswerable': False, 'extractive_spans': ['wealth ', 'democracy ', 'population', 'levels of ODA', 'conflict '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Yet surprisingly little is known about the agenda-setting process for international development in global governance institutions. This is perhaps best demonstrated by the lack of information on how the different goals and targets of the MDGs were decided, which led to much criticism and concern about the global governance of development BIBREF1 . More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.', 'The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). However, we find that the extent to which countries discuss sustainable development (Topic 7) in their annual GD statements varies considerably according to these different structural factors. The results suggest that broadly-speaking we do not observe linear trends in the relationship between these country-specific factors and discussion of Topic 7. Instead, we find that there are significant fluctuations in the relationship between factors such as wealth, democracy, etc., and the extent to which these states discuss sustainable development in their GD statements. These relationships require further analysis and exploration.'], 'highlighted_evidence': [' More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.', ' We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). ']}]}, {'question': 'Is the dataset multilingual?', 'question_id': '13b36644357870008d70e5601f394ec3c6c07048', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements.'], 'highlighted_evidence': ['We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . ']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements.', 'FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model.'], 'highlighted_evidence': ['We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . ', 'FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model.']}]}, {'question': 'How are the main international development topics that states raise identified?', 'question_id': 'e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ' They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.', 'evidence': [""We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in Amazon's Mechanical Turk experiments."", 'Exclusivity scores for each topic follows BIBREF7 . Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. Cohesive and exclusive topics are more semantically useful. Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 .'], 'highlighted_evidence': ['We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures.', 'Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. ', 'Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 .']}]}]",journals/corr/abs-1708-05873,What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016.,2017," More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.","(0.8199812173843384, 0.8576674461364746, 0.8384010791778564)",5.495994572225064e-233,0.532869279384613
12,QnAMaker: Data to Bot in 2 Minutes,What experiments do the authors present to validate their system?,None.,2003.08553,"[{'question': 'What experiments do the authors present to validate their system?', 'question_id': 'fd0ef5a7b6f62d07776bf672579a99c67e61a568', 'answers': [{'unanswerable': False, 'extractive_spans': ["" we measure our system's performance for datasets across various domains"", 'evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""QnAMaker is not domain-specific and can be used for any type of data. To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). Hybrid of deep learning(CDSSM) and machine learning features give our ranking model low computation cost, high explainability and significant F1/AUC score. Based on QnAMaker usage, we observed these trends:""], 'highlighted_evidence': ["" To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19).""]}]}, {'question': 'How does the conversation layer work?', 'question_id': '071bcb4b054215054f17db64bfd21f17fd9e1a80', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What components is the QnAMaker composed of?', 'question_id': 'f399d5a8dbeec777a858f81dc4dd33a83ba341a2', 'answers': [{'unanswerable': False, 'extractive_spans': ['QnAMaker Portal', 'QnaMaker Management APIs', 'Azure Search Index', 'QnaMaker WebApp', 'Bot'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['System description ::: Architecture', 'As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:', 'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.', 'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.', 'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.', 'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.', ""Bot: Calls the WebApp with the User's query to get results.""], 'highlighted_evidence': ['System description ::: Architecture', 'The components involved in the process are:', 'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. ', 'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. ', 'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.', 'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. ', ""Bot: Calls the WebApp with the User's query to get results.""]}, {'unanswerable': False, 'extractive_spans': ['QnAMaker Portal', 'QnaMaker Management APIs', 'Azure Search Index', 'QnaMaker WebApp', 'Bot'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:', 'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.', 'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.', 'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.', 'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.', ""Bot: Calls the WebApp with the User's query to get results.""], 'highlighted_evidence': [""The components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.\n\nBot: Calls the WebApp with the User's query to get results.""]}]}]",conf/www/AgrawalMKNCSKSK20,QnAMaker: Data to Bot in 2 Minutes.,2020," To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19).","(0.9115259647369385, 0.7923140525817871, 0.8477495908737183)",0.0,0.1047292947769165
13,Procedural Reasoning Networks for Understanding Multimodal Procedures,What multimodality is available in the dataset?,Multimodal neural machine translation with images.,1909.08859,"[{'question': 'What multimodality is available in the dataset?', 'question_id': 'a883bb41449794e0a63b716d9766faea034eb359', 'answers': [{'unanswerable': False, 'extractive_spans': ['context is a procedural text, the question and the multiple choice answers are composed of images'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In the following, we explain our Procedural Reasoning Networks model. Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images.'], 'highlighted_evidence': ['Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images.']}, {'unanswerable': False, 'extractive_spans': ['images and text'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. To this end, inspired from BIBREF5, we propose Procedural Reasoning Networks (PRN) that incorporates entities into the comprehension process and allows to keep track of entities, understand their interactions and accordingly update their states across time. We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. We further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps.'], 'highlighted_evidence': ['In particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. ', 'We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. ']}]}, {'question': 'What are previously reported models?', 'question_id': '5d83b073635f5fd8cd1bdb1895d3f13406583fbd', 'answers': [{'unanswerable': False, 'extractive_spans': ['Hasty Student', 'Impatient Reader', 'BiDAF', 'BiDAF w/ static memory'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.', 'Hasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.', 'Impatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.', 'BiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context. Here, we adapt it to work in a multimodal setting and answer multiple choice questions instead.', 'BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities. However, it does not make any updates on the memory cells. That is, it uses the static entity embeeddings initialized with GloVe word vectors. We propose this baseline to test the significance of the use of relational memory updates.'], 'highlighted_evidence': ['We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.\n\nHasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.\n\nImpatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.\n\nBiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context.', 'BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities.']}]}, {'question': 'How better is accuracy of new model compared to previously reported models?', 'question_id': '171ebfdc9b3a98e4cdee8f8715003285caeb2f39', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59', 'evidence': ['Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models. Moreover, it achieves the best performance on average. These results demonstrate the importance of having a dynamic memory and keeping track of entities extracted from the recipe. In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20.', 'FLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines.'], 'highlighted_evidence': ['Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.', 'In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF.', 'FLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines.']}]}]",conf/conll/AmacYEE19,Procedural Reasoning Networks for Understanding Multimodal Procedures.,2019,"Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images.","(0.8563753366470337, 0.8279873132705688, 0.8419421315193176)",1.8047214064961165e-233,0.4905528128147125
14,Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications,What datasets are used to evaluate this approach?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,1905.00563,"[{'question': 'What datasets are used to evaluate this approach?', 'question_id': 'bc9c31b3ce8126d1d148b1025c66f270581fde10', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ' Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ', 'evidence': ['FLOAT SELECTED: Table 2: Data Statistics of the benchmarks.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Data Statistics of the benchmarks.']}, {'unanswerable': False, 'extractive_spans': ['WN18 and YAGO3-10'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Since the setting is quite different from traditional adversarial attacks, search for link prediction adversaries brings up unique challenges. To find these minimal changes for a target link, we need to identify the fact that, when added into or removed from the graph, will have the biggest impact on the predicted score of the target fact. Unfortunately, computing this change in the score is expensive since it involves retraining the model to recompute the embeddings. We propose an efficient estimate of this score change by approximating the change in the embeddings using Taylor expansion. The other challenge in identifying adversarial modifications for link prediction, especially when considering addition of fake facts, is the combinatorial search space over possible facts, which is intractable to enumerate. We introduce an inverter of the original embedding model, to decode the embeddings to their corresponding graph components, making the search of facts tractable by performing efficient gradient-based continuous optimization. We evaluate our proposed methods through following experiments. First, on relatively small KGs, we show that our approximations are accurate compared to the true change in the score. Second, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10. We also explore the utility of adversarial modifications in explaining the model predictions by presenting rule-like descriptions of the most influential neighbors. Finally, we use adversaries to detect errors in the KG, obtaining up to $55\\%$ accuracy in detecting errors.'], 'highlighted_evidence': ['WN18 and YAGO3-10', 'Second, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10. ']}]}, {'question': 'How is this approach used to detect incorrect facts?', 'question_id': '185841e979373808d99dccdade5272af02b98774', 'answers': [{'unanswerable': False, 'extractive_spans': [""if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. ""], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph.""], 'highlighted_evidence': [""if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data."", ""Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph.""]}]}, {'question': 'Can this adversarial approach be used to directly improve model accuracy?', 'question_id': 'd427e3d41c4c9391192e249493be23926fc5d2e9', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['To evaluate this application, we inject random triples into the graph, and measure the ability of to detect the errors using our optimization. We consider two types of incorrect triples: 1) incorrect triples in the form of $\\langle s^{\\prime }, r, o\\rangle $ where $s^{\\prime }$ is chosen randomly from all of the entities, and 2) incorrect triples in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ where $s^{\\prime }$ and $r^{\\prime }$ are chosen randomly. We choose 100 random triples from the observed graph, and for each of them, add an incorrect triple (in each of the two scenarios) to its neighborhood. Then, after retraining DistMult on this noisy training data, we identify error triples through a search over the neighbors of the 100 facts. The result of choosing the neighbor with the least influence on the target is provided in the Table 7 . When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors.'], 'highlighted_evidence': ['When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors.']}]}]",conf/akbc/PezeshkpourTS19,Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.,2019," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","(0.8201615214347839, 0.8095090985298157, 0.814800500869751)",0.0,0.2093929648399353
15,Learning Supervised Topic Models for Classification and Regression from Crowds,what are the advantages of the proposed model?,(i) it does not rely on references; (ii) can naturally be applied at the sentence level; and (iii) does not need human fluency annotations of any kind.,1808.05902,"[{'question': 'what are the advantages of the proposed model?', 'question_id': '330f2cdeab689670b68583fc4125f5c0b26615a8', 'answers': [{'unanswerable': False, 'extractive_spans': ['he proposed model outperforms all the baselines, being the svi version the one that performs best.', 'the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For all the experiments the hyper-parameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 were set using a simple grid search in the collection INLINEFORM3 . The same approach was used to optimize the hyper-parameters of the all the baselines. For the svi algorithm, different mini-batch sizes and forgetting rates INLINEFORM4 were tested. For the 20-Newsgroup dataset, the best results were obtained with a mini-batch size of 500 and INLINEFORM5 . The INLINEFORM6 was kept at 1. The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.', 'In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.'], 'highlighted_evidence': ['The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.', 'In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.']}]}, {'question': 'what are the state of the art approaches?', 'question_id': 'c87b2dd5c439d5e68841a705dd81323ec0d64c97', 'answers': [{'unanswerable': False, 'extractive_spans': ['Bosch 2006 (mv)', 'LDA + LogReg (mv)', 'LDA + Raykar', 'LDA + Rodrigues', 'Blei 2003 (mv)', 'sLDA (mv)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:', ""Bosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv)."", 'The results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.', 'Analyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.', 'Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:', '[itemsep=0.02cm]', ""LDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 ."", ""LDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features."", 'LDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.', ""Blei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv)."", ""sLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers.""], 'highlighted_evidence': [""With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators."", ""Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers.""]}]}, {'question': 'what datasets were used?', 'question_id': 'f7789313a804e41fcbca906a4e5cf69039eeef9f', 'answers': [{'unanswerable': False, 'extractive_spans': ['Reuters-21578 BIBREF30', ' LabelMe BIBREF31', '20-Newsgroups benchmark corpus BIBREF29 '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .', 'In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: “computers"", “science"", “politics"" and “recreative"". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing.'], 'highlighted_evidence': ['In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .', 'In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. ']}, {'unanswerable': False, 'extractive_spans': [' 20-Newsgroups benchmark corpus ', 'Reuters-21578', 'LabelMe'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: “computers"", “science"", “politics"" and “recreative"". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing.', 'In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .'], 'highlighted_evidence': ['In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. ', 'In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .']}]}]",journals/corr/abs-1808-05902,Learning Supervised Topic Models for Classification and Regression from Crowds.,2018,"The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.","(0.8093377351760864, 0.8297198414802551, 0.8194020390510559)",8.422437779564611e-232,0.1573876440525055
16,BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,What models other than standalone BERT is new model compared to?,RoBERTa,1910.07181,"[{'question': 'What models other than standalone BERT is new model compared to?', 'question_id': 'd6e2b276390bdc957dfa7e878de80cee1f41fbca', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Only Bert base and Bert large are compared to proposed approach.', 'evidence': ['Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin.'], 'highlighted_evidence': ['Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin.']}]}, {'question': 'How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?', 'question_id': '32537fdf0d4f76f641086944b413b2f756097e5e', 'answers': [{'unanswerable': False, 'extractive_spans': ['improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin.'], 'highlighted_evidence': ['Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking.']}]}, {'question': 'What are three downstream task datasets?', 'question_id': 'ef081d78be17ef2af792e7e919d15a235b8d7275', 'answers': [{'unanswerable': False, 'extractive_spans': ['MNLI BIBREF21', ""AG's News BIBREF22"", 'DBPedia BIBREF23'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35.""], 'highlighted_evidence': [""To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. ""]}, {'unanswerable': False, 'extractive_spans': ['MNLI', ""AG's News"", 'DBPedia'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35.""], 'highlighted_evidence': [""To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23.""]}]}, {'question': 'What is dataset for word probing task?', 'question_id': '537b2d7799124d633892a1ef1a485b3b071b303d', 'answers': [{'unanswerable': False, 'extractive_spans': ['WNLaMPro dataset'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evalute Bertram on the WNLaMPro dataset of BIBREF0. This dataset consists of cloze-style phrases like'], 'highlighted_evidence': ['We evalute Bertram on the WNLaMPro dataset of BIBREF0.']}]}]",conf/acl/SchickS20,BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance.,2020,Only Bert base and Bert large are compared to proposed approach.,"(0.8209856748580933, 0.8200667500495911, 0.8205259442329407)",0.0,0.06776181608438492
17,Joint Entity Linking with Deep Reinforcement Learning,How fast is the model compared to baselines?,WPSLOR drastically reduces model size and training time.,1902.00330,"[{'question': 'How fast is the model compared to baselines?', 'question_id': '9aca4b89e18ce659c905eccc78eda76af9f0072a', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How big is the performance difference between this method and the baseline?', 'question_id': 'b0376a7f67f1568a7926eff8ff557a93f434a253', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.', 'evidence': ['FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.']}]}, {'question': 'What datasets used for evaluation?', 'question_id': 'dad8cc543a87534751f9f9e308787e1af06f0627', 'answers': [{'unanswerable': False, 'extractive_spans': ['AIDA-B', 'ACE2004', 'MSNBC', 'AQUAINT', 'WNED-CWEB', 'WNED-WIKI'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.', 'AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.', 'ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.', 'MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)', 'AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.', 'WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.', 'WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.'], 'highlighted_evidence': ['In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. ', 'AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.']}, {'unanswerable': False, 'extractive_spans': ['AIDA-CoNLL', 'ACE2004', 'MSNBC', 'AQUAINT', 'WNED-CWEB', 'WNED-WIKI', 'OURSELF-WIKI'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.', 'AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.', 'ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.', 'MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)', 'AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.', 'WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.', 'WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.', 'OURSELF-WIKI is crawled by ourselves from Wikipedia pages.'], 'highlighted_evidence': ['We conduct experiments on several different types of public datasets including news and encyclopedia corpus. ', 'AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nOURSELF-WIKI is crawled by ourselves from Wikipedia pages.']}]}, {'question': 'what are the mentioned cues?', 'question_id': '0481a8edf795768d062c156875d20b8fb656432c', 'answers': [{'unanswerable': False, 'extractive_spans': ['output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Figure 2: The overall structure of our RLEL model. It contains three parts: Local Encoder, Global Encoder and Entity Selector. In this framework, (Vmt ,Vekt ) denotes the concatenation of the mention context vector Vmt and one candidate entity vector Vekt . The policy network selects one entity from the candidate set, and Vat denotes the concatenation of the mention context vector Vmt and the selected entity vector Ve∗t . ht represents the hidden status of Vat , and it will be fed into St+1.', 'Where $\\oplus $ indicates vector concatenation. The $V_{m_i}^t$ and $V_{e_i}^t$ respectively denote the vector of $m_i$ and $e_i$ at time $t$ . For each mention, there are multiple candidate entities correspond to it. With the purpose of comparing the semantic relevance between the mention and each candidate entity at the same time, we copy multiple copies of the mention vector. Formally, we extend $V_{m_i}^t \\in \\mathbb {R}^{1\\times {n}}$ to $V_{m_i}^t{^{\\prime }} \\in \\mathbb {R}^{k\\times {n}}$ and then combine it with $V_{e_i}^t \\in \\mathbb {R}^{k\\times {n}}$ . Since $V_{m_i}^t$ and $V_{m_i}^t$0 are mainly to represent semantic information, we add feature vector $V_{m_i}^t$1 to enrich lexical and statistical features. These features mainly include the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc. After getting these feature values, we combine them into a vector and add it to the current state. In addition, the global vector $V_{m_i}^t$2 is also added to $V_{m_i}^t$3 . As mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 . Thus, the state $V_{m_i}^t$8 contains current information and previous decisions, while also covering the semantic representations and a variety of statistical features. Next, the concatenated vector will be fed into the policy network to generate action.'], 'highlighted_evidence': ['FLOAT SELECTED: Figure 2: The overall structure of our RLEL model. It contains three parts: Local Encoder, Global Encoder and Entity Selector. In this framework, (Vmt ,Vekt ) denotes the concatenation of the mention context vector Vmt and one candidate entity vector Vekt . The policy network selects one entity from the candidate set, and Vat denotes the concatenation of the mention context vector Vmt and the selected entity vector Ve∗t . ht represents the hidden status of Vat , and it will be fed into St+1.', 'As mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 .']}]}]",conf/www/FangC0ZZL19,Joint Entity Linking with Deep Reinforcement Learning.,2019,,"(0.0, 0.0, 0.0)",0.0,0.003899756819009781
18,Marrying Universal Dependencies and Universal Morphology,What are the main sources of recall errors in the mapping?,"Spelling mistakes, casual pronunciation, abbreviations, repeated letters, onomatopoeia, and others (e.g. ""im"" instead of ""I'm"", ""your/ur"" instead of ""you're"", etc.)",1810.06743,"[{'question': 'What are the main sources of recall errors in the mapping?', 'question_id': '642c4704a71fd01b922a0ef003f234dcc7b223cd', 'answers': [{'unanswerable': False, 'extractive_spans': ['irremediable annotation discrepancies', 'differences in choice of attributes to annotate', 'The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them', 'the two annotations encode distinct information', 'incorrectly applied UniMorph annotation', 'cross-lingual inconsistency in both resources'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We present the intrinsic task's recall scores in tab:recall. Bear in mind that due to annotation errors in the original corpora (like the vas example from sec:resources), the optimal score is not always $100\\%$ . Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase."", 'Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation.'], 'highlighted_evidence': ['irremediable annotation discrepancies', ""Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase."", 'We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. ']}]}, {'question': ""Do they look for inconsistencies between different languages' annotations in UniMorph?"", 'question_id': 'e477e494fe15a978ff9c0a5f1c88712cdaec0c5c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation.'], 'highlighted_evidence': ['Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources.']}]}, {'question': 'Do they look for inconsistencies between different UD treebanks?', 'question_id': '04495845251b387335bf2e77e2c423130f43c7d9', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['The contributions of this work are:'], 'highlighted_evidence': ['The contributions of this work are:']}]}, {'question': 'Which languages do they validate on?', 'question_id': '564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur', 'evidence': ['FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.']}, {'unanswerable': False, 'extractive_spans': ['We apply this conversion to the 31 languages', 'Arabic, Hindi, Lithuanian, Persian, and Russian. ', 'Dutch', 'Spanish'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Table 4: Tagging F1 using UD sentences annotated with either original UD MSDs or UniMorph-converted MSDs', ""A dataset-by-dataset problem demands a dataset-by-dataset solution; our task is not to translate a schema, but to translate a resource. Starting from the idealized schema, we create a rule-based tool for converting UD-schema annotations to UniMorph annotations, incorporating language-specific post-edits that both correct infelicities and also increase harmony between the datasets themselves (rather than the schemata). We apply this conversion to the 31 languages with both UD and UniMorph data, and we report our method's recall, showing an improvement over the strategy which just maps corresponding schematic features to each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging."", 'FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.', 'There are three other transformations for which we note no improvement here. Because of the problem in Basque argument encoding in the UniMorph dataset—which only contains verbs—we note no improvement in recall on Basque. Irish also does not improve: UD marks gender on nouns, while UniMorph marks case. Adjectives in UD are also underspecified. The verbs, though, are already correct with the simple mapping. Finally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge.', ""We present the intrinsic task's recall scores in tab:recall. Bear in mind that due to annotation errors in the original corpora (like the vas example from sec:resources), the optimal score is not always $100\\%$ . Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase."", ""For the extrinsic task, the performance is reasonably similar whether UniMorph or UD; see tab:tagging. A large fluctuation would suggest that the two annotations encode distinct information. On the contrary, the similarities suggest that the UniMorph-mapped MSDs have similar content. We recognize that in every case, tagging F1 increased—albeit by amounts as small as $0.16$ points. This is in part due to the information that is lost in the conversion. UniMorph's schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance.""], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Tagging F1 using UD sentences annotated with either original UD MSDs or UniMorph-converted MSDs', 'We apply this conversion to the 31 languages', 'FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.', 'Finally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge.', 'Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.', ""UniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance.""]}]}]",conf/acludw/McCarthySCHY18,Marrying Universal Dependencies and Universal Morphology.,2018,irremediable annotation discrepancies,"(0.7824957370758057, 0.8104230761528015, 0.7962145209312439)",0.0,0.15257447957992554
19,Revisiting Low-Resource Neural Machine Translation: A Case Study,what amounts of size were used on german-english?,20,1905.11901,"[{'question': 'what amounts of size were used on german-english?', 'question_id': '4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development', 'evidence': ['We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.', 'To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see SECREF7 ). Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary.', 'FLOAT SELECTED: Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE→EN data, and for KO→EN data.'], 'highlighted_evidence': ['We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.', 'Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary.', 'FLOAT SELECTED: Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE→EN data, and for KO→EN data.']}, {'unanswerable': False, 'extractive_spans': ['ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our ""mainstream improvements"" add around 6–7 BLEU in both data conditions.', 'In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU). Reducing the batch size to 1000 token results in a BLEU gain of 0.3, and the lexical model yields an additional +0.6 BLEU. However, aggressive (word) dropout (+3.4 BLEU) and tuning other hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2 INLINEFORM2 16.6). The model trained on full IWSLT data is less sensitive to our changes (31.9 INLINEFORM3 32.8 BLEU), and optimal hyperparameters differ depending on the data condition. Subsequently, we still apply the hyperparameters that were optimized to the ultra-low data condition (8) to other data conditions, and Korean INLINEFORM4 English, for simplicity.', 'FLOAT SELECTED: Table 2: German→English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported.'], 'highlighted_evidence': ['Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our ""mainstream improvements"" add around 6–7 BLEU in both data conditions.\n\nIn the ultra-low data condition, reducing the BPE vocabulary size is very effecti', 'FLOAT SELECTED: Table 2: German→English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported.']}]}, {'question': 'what were their experimental results in the low-resource dataset?', 'question_id': '07d7652ad4a0ec92e6b44847a17c378b0d9f57f5', 'answers': [{'unanswerable': False, 'extractive_spans': ['10.37 BLEU'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.'], 'highlighted_evidence': ['Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.']}]}, {'question': 'what are the methods they compare with in the korean-english dataset?', 'question_id': '9f3444c9fb2e144465d63abf58520cddd4165a01', 'answers': [{'unanswerable': False, 'extractive_spans': ['gu-EtAl:2018:EMNLP1'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.'], 'highlighted_evidence': ['Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.']}]}, {'question': 'what pitfalls are mentioned in the paper?', 'question_id': '2348d68e065443f701d8052018c18daa4ecc120e', 'answers': [{'unanswerable': False, 'extractive_spans': ['highly data-inefficient', 'underperform phrase-based statistical machine translation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows:'], 'highlighted_evidence': ['While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . ']}]}]",conf/acl/SennrichZ19,Revisiting Low-Resource Neural Machine Translation: A Case Study.,2019,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","(0.8767521977424622, 0.7179743647575378, 0.7894589900970459)",0.0,0.1847979724407196
20,CAiRE: An End-to-End Empathetic Chatbot,What is the performance of their system?,Their system achieves an F1-score of 0.83 for the classification task.,1907.12108,"[{'question': 'What is the performance of their system?', 'question_id': 'b1ced2d6dcd1d7549be2594396cbda34da6c3bca', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What evaluation metrics are used?', 'question_id': 'f3be1a27df2e6ad12eed886a8cd2dfe09b9e2b30', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the source of the dialogues?', 'question_id': 'a45a86b6a02a98d3ab11f1d04acd3446e95f5a16', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What pretrained LM is used?', 'question_id': '1f1a9f2dd8c4c10b671cb8affe56e181948e229e', 'answers': [{'unanswerable': False, 'extractive_spans': ['Generative Pre-trained Transformer (GPT)'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model. GPT is a multi-layer Transformer decoder with a causal self-attention which is pre-trained, unsupervised, on the BooksCorpus dataset. BooksCorpus dataset contains over 7,000 unique unpublished books from a variety of genres. Pre-training on such large contiguous text corpus enables the model to capture long-range dialogue context information. Furthermore, as existing EmpatheticDialogue dataset BIBREF4 is relatively small, fine-tuning only on such dataset will limit the chitchat topic of the model. Hence, we first integrate persona into CAiRE, and pre-train the model on PersonaChat BIBREF3 , following a previous transfer-learning strategy BIBREF1 . This pre-training procedure allows CAiRE to have a more consistent persona, thus improving the engagement and consistency of the model. We refer interested readers to the code repository recently released by HuggingFace. Finally, in order to optimize empathy in CAiRE, we fine-tune this pre-trained model using EmpatheticDialogue dataset to help CAiRE understand users' feeling.""], 'highlighted_evidence': ['We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model. ']}, {'unanswerable': False, 'extractive_spans': ['Generative Pre-trained Transformer (GPT)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In contrast to such modularized dialogue system, end-to-end systems learn all components as a single model in a fully data-driven manner, and mitigate the lack of labeled data by sharing representations among different modules. In this paper, we build an end-to-end empathetic chatbot by fine-tuning BIBREF1 the Generative Pre-trained Transformer (GPT) BIBREF2 on the PersonaChat dataset BIBREF3 and the Empathetic-Dialogue dataset BIBREF4 . We establish a web-based user interface which allows multiple users to asynchronously chat with CAiRE online. CAiRE can also collect user feedback and continuously improve its response quality and discard undesirable generation behaviors (e.g. unethical responses) via active learning and negative training.'], 'highlighted_evidence': ['In this paper, we build an end-to-end empathetic chatbot by fine-tuning BIBREF1 the Generative Pre-trained Transformer (GPT) BIBREF2 on the PersonaChat dataset BIBREF3 and the Empathetic-Dialogue dataset BIBREF4 .']}]}]",conf/aaai/LinXWSLSF20,CAiRE: An End-to-End Empathetic Chatbot.,2020,,"(0.0, 0.0, 0.0)",0.0,0.05747899413108826
21,Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?,What approaches they propose?,"They argue for a more practical view of faithfulness, calling for a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.",2004.03685,"[{'question': 'What approaches they propose?', 'question_id': 'eeaceee98ef1f6c971dac7b0b8930ee8060d71c2', 'answers': [{'unanswerable': False, 'extractive_spans': ['Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks.', 'Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We argue that a way out of this standstill is in a more practical and nuanced methodology for defining and evaluating faithfulness. We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.', 'We note two possible approaches to this end:', 'Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.', 'For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.', ""Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only.""], 'highlighted_evidence': [""We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only.""]}]}, {'question': 'What faithfulness criteria does they propose?', 'question_id': '3371d586a3a81de1552d90459709c57c0b1a2594', 'answers': [{'unanswerable': False, 'extractive_spans': ['Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks.', 'Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We argue that a way out of this standstill is in a more practical and nuanced methodology for defining and evaluating faithfulness. We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.', 'We note two possible approaches to this end:', 'Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.', 'For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.', ""Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only.""], 'highlighted_evidence': [""We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only.""]}]}, {'question': 'Which are three assumptions in current approaches for defining faithfulness?', 'question_id': 'd4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e', 'answers': [{'unanswerable': False, 'extractive_spans': ['Two models will make the same predictions if and only if they use the same reasoning process.', 'On similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Defining Faithfulness ::: Assumption 1 (The Model Assumption).', 'Two models will make the same predictions if and only if they use the same reasoning process.', 'Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).', 'On similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).', 'Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.'], 'highlighted_evidence': ['Defining Faithfulness ::: Assumption 1 (The Model Assumption).\nTwo models will make the same predictions if and only if they use the same reasoning process.', 'Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).\nOn similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).\nCertain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.']}, {'unanswerable': False, 'extractive_spans': ['Two models will make the same predictions if and only if they use the same reasoning process.', 'On similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Defining Faithfulness ::: Assumption 1 (The Model Assumption).', 'Two models will make the same predictions if and only if they use the same reasoning process.', 'Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).', 'On similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).', 'Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.'], 'highlighted_evidence': ['Defining Faithfulness ::: Assumption 1 (The Model Assumption).\nTwo models will make the same predictions if and only if they use the same reasoning process.', 'Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).\nOn similar inputs, the model makes similar decisions if and only if its reasoning is similar.', 'Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).\nCertain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.']}]}, {'question': 'Which are key points in guidelines for faithfulness evaluation?', 'question_id': '2a859e80d8647923181cb2d8f9a2c67b1c3f4608', 'answers': [{'unanswerable': False, 'extractive_spans': ['Be explicit in what you evaluate.', 'Faithfulness evaluation should not involve human-judgement on the quality of interpretation.', 'Faithfulness evaluation should not involve human-provided gold labels.', 'Do not trust “inherent interpretability” claims.', 'Faithfulness evaluation of IUI systems should not rely on user performance.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Guidelines for Evaluating Faithfulness', 'We propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.', 'Guidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.', 'Conflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques—be clear about which properties are being prioritized.', 'Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.', 'We note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.', 'Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.', 'We should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.', 'Guidelines for Evaluating Faithfulness ::: Do not trust “inherent interpretability” claims.', 'Inherent interpretability is a claim until proven otherwise. Explanations provided by “inherently interpretable” models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.', 'Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.', 'End-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness.'], 'highlighted_evidence': ['Guidelines for Evaluating Faithfulness\nWe propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.\n\nGuidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.\nConflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques—be clear about which properties are being prioritized.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.\nWe note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.\nWe should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.\n\nGuidelines for Evaluating Faithfulness ::: Do not trust “inherent interpretability” claims.\nInherent interpretability is a claim until proven otherwise. Explanations provided by “inherently interpretable” models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.\nEnd-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness.']}]}]",conf/acl/JacoviG20,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,2020,"We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.

We note two possible approaches to this end:

Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.

For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.

Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only.","(0.8839936852455139, 0.8107891082763672, 0.8458104133605957)",3.138661634983361e-157,0.6153614521026611
22,Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference,Did they use the state-of-the-art model to analyze the attention?,"No, they used ESIM-50.",1808.03894,"[{'question': 'Did they use the state-of-the-art model to analyze the attention?', 'question_id': 'aceac4ad16ffe1af0f01b465919b1d4422941a6b', 'answers': [{'unanswerable': False, 'extractive_spans': ['we provide an extensive analysis of the state-of-the-art model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We make two main contributions. First, we introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency.'], 'highlighted_evidence': ['Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency.']}]}, {'question': 'What is the performance of their model?', 'question_id': 'f7070b2e258beac9b09514be2bfcc5a528cc3a0e', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}, {'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How many layers are there in their model?', 'question_id': '2efdcebebeb970021233553104553205ce5d6567', 'answers': [{'unanswerable': False, 'extractive_spans': ['two LSTM layers'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Instead of considering individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference.'], 'highlighted_evidence': ['Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference.']}]}, {'question': 'Did they compare with gradient-based methods?', 'question_id': '4fa851d91388f0803e33f6cfae519548598cd37c', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/emnlp/GhaeiniFT18,Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.,2018,"Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency.","(0.8762868642807007, 0.8355116248130798, 0.8554136753082275)",0.0,-0.000992974266409874
23,SUM-QE: a BERT-based Summary Quality Estimation Model,What are their correlation results?,Pearson correlation; Mean squared error.,1909.00578,"[{'question': 'What are their correlation results?', 'question_id': 'ff28d34d1aaa57e7ad553dba09fc924dc21dd728', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'High correlation results range from 0.472 to 0.936', 'evidence': ['FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.']}]}, {'question': 'What dataset do they use?', 'question_id': 'ae8354e67978b7c333094c36bf9d561ca0c2d286', 'answers': [{'unanswerable': False, 'extractive_spans': ['datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems).'], 'highlighted_evidence': ['We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems).']}]}, {'question': 'What simpler models do they look at?', 'question_id': '02348ab62957cb82067c589769c14d798b1ceec7', 'answers': [{'unanswerable': False, 'extractive_spans': ['BiGRU s with attention', 'ROUGE', 'Language model (LM)', 'Next sentence prediction'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Methods ::: Baselines ::: BiGRU s with attention:', 'This is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).', 'Methods ::: Baselines ::: ROUGE:', 'This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.', 'Methods ::: Baselines ::: Language model (LM):', 'For a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.', 'Methods ::: Baselines ::: Next sentence prediction:', 'BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:', 'where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary.'], 'highlighted_evidence': ['Methods ::: Baselines ::: BiGRU s with attention:\nThis is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n\nMethods ::: Baselines ::: ROUGE:\nThis baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.\n\nMethods ::: Baselines ::: Language model (LM):\nFor a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.\n\nMethods ::: Baselines ::: Next sentence prediction:\nBERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:\n\nwhere $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'BiGRUs with attention, ROUGE, Language model, and next sentence prediction ', 'evidence': ['Methods ::: Baselines ::: BiGRU s with attention:', 'This is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).', 'Methods ::: Baselines ::: ROUGE:', 'This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.', 'Methods ::: Baselines ::: Language model (LM):', 'For a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.', 'Methods ::: Baselines ::: Next sentence prediction:', 'BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:', 'where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary.'], 'highlighted_evidence': ['Methods ::: Baselines ::: BiGRU s with attention:\nThis is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n\nMethods ::: Baselines ::: ROUGE:\nThis baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.\n\nMethods ::: Baselines ::: Language model (LM):\nFor a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.\n\nMethods ::: Baselines ::: Next sentence prediction:\nBERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:\n\nwhere $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary.']}]}, {'question': 'What linguistic quality aspects are addressed?', 'question_id': '3748787379b3a7d222c3a6254def3f5bfb93a60e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Grammaticality, non-redundancy, referential clarity, focus, structure & coherence', 'evidence': ['FLOAT SELECTED: Figure 1: SUM-QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories.'], 'highlighted_evidence': ['FLOAT SELECTED: Figure 1: SUM-QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories.']}]}]",conf/emnlp/XenouleasMAA19,SUM-QE: a BERT-based Summary Quality Estimation Model.,2019,High correlation results range from 0.472 to 0.936,"(0.870187520980835, 0.840761125087738, 0.8552212119102478)",0.0,0.4577708840370178
24,Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction,What benchmark datasets are used for the link prediction task?,WN18 and YAGO3-10.,1911.09419,"[{'question': 'What benchmark datasets are used for the link prediction task?', 'question_id': '6852217163ea678f2009d4726cb6bd03cf6a8f78', 'answers': [{'unanswerable': False, 'extractive_spans': ['WN18RR', 'FB15k-237', 'YAGO3-10'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18.'], 'highlighted_evidence': ['We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27.']}, {'unanswerable': False, 'extractive_spans': ['WN18RR BIBREF26', 'FB15k-237 BIBREF18', 'YAGO3-10 BIBREF27'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18.', 'WN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 BIBREF8, FB15k BIBREF8, and YAGO3 BIBREF27, respectively. As pointed out by BIBREF26 and BIBREF18, WN18 and FB15k suffer from the test set leakage problem. One can attain the state-of-the-art results even using a simple rule based model. Therefore, we use WN18RR and FB15k-237 as the benchmark datasets.'], 'highlighted_evidence': ['We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18.\n\nWN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 BIBREF8, FB15k BIBREF8, and YAGO3 BIBREF27, respectively.']}]}, {'question': 'What are state-of-the art models for this task?', 'question_id': 'cd1ad7e18d8eef8f67224ce47f3feec02718ea1a', 'answers': [{'unanswerable': False, 'extractive_spans': ['TransE', 'DistMult', 'ComplEx', 'ConvE', 'RotatE'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7.'], 'highlighted_evidence': ['In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7.']}]}, {'question': 'How better does HAKE model peform than state-of-the-art methods?', 'question_id': '9c9e90ceaba33242342a5ae7568e89fe660270d5', 'answers': [{'unanswerable': False, 'extractive_spans': ['0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively', ""doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10"", 'HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively. The superior performance of HAKE compared with RotatE implies that our proposed model can better model different levels in the hierarchy.', ""FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets. However, the results also show that our models can gain better performance so long as there exists semantic hierarchies in knowledge graphs. As almost all knowledge graphs have such hierarchy structures, our model is widely applicable."", 'YAGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively.'], 'highlighted_evidence': ['WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively.', ""FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets."", 'AGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively.']}]}, {'question': 'How are entities mapped onto polar coordinate system?', 'question_id': '2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8', 'answers': [{'unanswerable': False, 'extractive_spans': ['radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively. That is, HAKE maps an entity $h$ to $[\\textbf {h}_m;\\textbf {h}_p]$, where $\\textbf {h}_m$ and $\\textbf {h}_p$ are generated by the modulus part and the phase part, respectively, and $[\\,\\cdot \\,; \\,\\cdot \\,]$ denotes the concatenation of two vectors. Obviously, $([\\textbf {h}_m]_i,[\\textbf {h}_p]_i)$ is a 2D point in the polar coordinate system. Specifically, we formulate HAKE as follows:'], 'highlighted_evidence': ['Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively.']}]}]",journals/nca/ZhangSZ24a,Learning hierarchy-aware complex knowledge graph embeddings for link prediction.,2024,"We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27.","(0.8845943808555603, 0.8312051296234131, 0.8570690751075745)",9.327292587886388e-234,0.3396691083908081
25,Machine Translation from Natural Language to Code using Long-Short Term Memory,What additional techniques are incorporated?,Phrase-based word embedding can be incorporated for improved vocabulary mapping. Abstract Syntax Tree (AST) can also be beneficial to get more accurate target code for each line.,1910.11471,"[{'question': 'What additional techniques are incorporated?', 'question_id': 'db9021ddd4593f6fadf172710468e2fdcea99674', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}, {'unanswerable': False, 'extractive_spans': ['incorporating coding syntax tree model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–', '""define the method tzname with 2 arguments: self and dt.""', 'is translated into–', 'def __init__ ( self , regex ) :.', 'The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax.'], 'highlighted_evidence': ['Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–\n\n""define the method tzname with 2 arguments: self and dt.""\n\nis translated into–\n\ndef __init__ ( self , regex ) :.\n\nThe translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax.']}]}, {'question': 'What dataset do they use?', 'question_id': '8ea4bd4c1d8a466da386d16e4844ea932c44a412', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'A parallel corpus where the source is an English expression of code and the target is Python code.', 'evidence': ['SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.'], 'highlighted_evidence': ['SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.']}, {'unanswerable': False, 'extractive_spans': [' text-code parallel corpus'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.'], 'highlighted_evidence': ['A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it .']}]}, {'question': 'Do they compare to other models?', 'question_id': '92240eeab107a4f636705b88f00cefc4f0782846', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the architecture of the system?', 'question_id': '4196d329061f5a9d147e1e77aeed6a6bd9b35d18', 'answers': [{'unanswerable': False, 'extractive_spans': ['seq2seq translation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to train the translation model between text-to-code an open source Neural Machine Translation (NMT) - OpenNMT implementation is utilized BIBREF11. PyTorch is used as Neural Network coding framework. For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation.'], 'highlighted_evidence': ['For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation.']}]}, {'question': ""How long are expressions in layman's language?"", 'question_id': 'a37e4a21ba98b0259c36deca0d298194fa611d2f', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What additional techniques could be incorporated to further improve accuracy?', 'question_id': '321429282557e79061fe2fe02a9467f3d0118cdd', 'answers': [{'unanswerable': False, 'extractive_spans': ['phrase-based word embedding', 'Abstract Syntax Tree(AST)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial.'], 'highlighted_evidence': ['In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial.']}]}, {'question': 'What programming language is target language?', 'question_id': '891cab2e41d6ba962778bda297592c916b432226', 'answers': [{'unanswerable': False, 'extractive_spans': ['Python'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.'], 'highlighted_evidence': ['In target data, the code is written in Python programming language.']}]}, {'question': 'What dataset is used to measure accuracy?', 'question_id': '1eeabfde99594b8d9c6a007f50b97f7f527b0a17', 'answers': [{'unanswerable': False, 'extractive_spans': ['validation data'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Training parallel corpus had 18805 lines of annotated code in it. The training model is executed several times with different training parameters. During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. We run the training with epoch value of 10 with a batch size of 64. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% (Fig. FIGREF17).'], 'highlighted_evidence': ['During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data.', 'After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40%']}]}]",journals/corr/abs-1910-11471,Machine Translation from Natural Language to Code using Long-Short Term Memory.,2019,,"(0.0, 0.0, 0.0)",0.0,-0.021215930581092834
26,A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,Is text-to-image synthesis trained is suppervized or unsuppervized manner?,Text-to-image synthesis was mainly carried out through a search and supervised learning combined process.,1910.09399,"[{'question': 'Is text-to-image synthesis trained is suppervized or unsuppervized manner?', 'question_id': 'e96adf8466e67bd19f345578d5a6dc68fd0279a1', 'answers': [{'unanswerable': False, 'extractive_spans': ['unsupervised '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Following the above definition, the $\\min \\max $ objective function in Eq. (DISPLAY_FORM10) aims to learn parameters for the discriminator ($\\theta _d$) and generator ($\\theta _g$) to reach an optimization goal: The discriminator intends to differentiate true vs. fake images with maximum capability $\\max _{\\theta _d}$ whereas the generator intends to minimize the difference between a fake image vs. a true image $\\min _{\\theta _g}$. In other words, the discriminator sets the characteristics and the generator produces elements, often images, iteratively until it meets the attributes set forth by the discriminator. GANs are often used with images and other visual elements and are notoriously efficient in generating compelling and convincing photorealistic images. Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. The following sections go into further detail regarding how the generator and discriminator are trained in GANs.'], 'highlighted_evidence': ['Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. ']}, {'unanswerable': False, 'extractive_spans': ['Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""black Deep learning shed some light to some of the most sophisticated advances in natural language representation, image synthesis BIBREF7, BIBREF8, BIBREF43, BIBREF35, and classification of generic data BIBREF44. However, a bulk of the latest breakthroughs in deep learning and computer vision were related to supervised learning BIBREF8. Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis BIBREF45, BIBREF14, BIBREF8, BIBREF46, BIBREF47. These subproblems are typically subdivided as focused research areas. DC-GAN's contributions are mainly driven by these two research areas. In order to generate plausible images from natural language, DC-GAN contributions revolve around developing a straightforward yet effective GAN architecture and training strategy that allows natural text to image synthesis. These contributions are primarily tested on the Caltech-UCSD Birds and Oxford-102 Flowers datasets. Each image in these datasets carry five text descriptions. These text descriptions were created by the research team when setting up the evaluation environment. The DC-GANs model is subsequently trained on several subcategories. Subcategories in this research represent the training and testing sub datasets. The performance shown by these experiments display a promising yet effective way to generate images from textual natural language descriptions BIBREF8.""], 'highlighted_evidence': ['Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis BIBREF45, BIBREF14, BIBREF8, BIBREF46, BIBREF47.']}]}, {'question': 'What challenges remain unresolved?', 'question_id': 'c1477a6c86bd1670dd17407590948000c9a6b7c6', 'answers': [{'unanswerable': False, 'extractive_spans': ['give more independence to the several learning methods (e.g. less human intervention) involved in the studies', 'increasing the size of the output images'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples. In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images.'], 'highlighted_evidence': ['Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images.']}]}, {'question': 'What is the conclusion of comparison of proposed solution?', 'question_id': 'e020677261d739c35c6f075cde6937d0098ace7f', 'answers': [{'unanswerable': False, 'extractive_spans': ['HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset', 'In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor', 'text to image synthesis is continuously improving the results for better visual perception and interception'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['While we gathered all the data we could find on scores for each model on the CUB, Oxford, and COCO datasets using IS, FID, FCN, and human classifiers, we unfortunately were unable to find certain data for AttnGAN and HDGAN (missing in Table TABREF48). The best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers. In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset. This is evidence that the attentional model and DAMSM introduced by AttnGAN are very effective in producing high-quality images. Examples of the best results of birds and plates of vegetables generated by each model are presented in Figures FIGREF50 and FIGREF51, respectively.', 'blackIn terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis. However, StackGAN++ did introduce a very worthy enhancement for unconditional image generation by organizing the generators and discriminators in a “tree-like” structure. This indicates that revising the structures of the discriminators and/or generators can bring a moderate level of improvement in text-to-image synthesis.', 'blackIn addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception. Technical wise, DM-GAN BIBREF53 is a model using dynamic memory to refine fuzzy image contents initially generated from the GAN networks. A memory writing gate is used for DM-GAN to select important text information and generate images based on he selected text accordingly. On the other hand, Obj-GAN BIBREF81 focuses on object centered text-to-image synthesis. The proposed framework of Obj-GAN consists of a layout generation, including a bounding box generator and a shape generator, and an object-driven attentive image generator. The designs and advancement of DM-GAN and Obj-GAN indicate that research in text-to-image synthesis is advancing to put more emphasis on the image details and text semantics for better understanding and perception.'], 'highlighted_evidence': ['In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset.', 'In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis.', 'In addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception.']}]}, {'question': 'What is typical GAN architecture for each text-to-image synhesis group?', 'question_id': '6389d5a152151fb05aae00b53b521c117d7b5e54', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Semantic Enhancement GANs: DC-GANs, MC-GAN\nResolution Enhancement GANs: StackGANs, AttnGAN, HDGAN\nDiversity Enhancement GANs: AC-GAN, TAC-GAN etc.\nMotion Enhancement GAGs: T2S, T2V, StoryGAN', 'evidence': ['In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24. The taxonomy organizes GAN frameworks into four categories, including Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. Following the proposed taxonomy, each subsection will introduce several typical frameworks and address their techniques of using GANS to solve certain aspects of the text-to-mage synthesis challenges.', 'FLOAT SELECTED: Figure 9. A Taxonomy and categorization of advanced GAN frameworks for Text-to-Image Synthesis. We categorize advanced GAN frameworks into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. The relationship between relevant frameworks and their publication date are also outlined as a reference.'], 'highlighted_evidence': ['In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24.', 'FLOAT SELECTED: Figure 9. A Taxonomy and categorization of advanced GAN frameworks for Text-to-Image Synthesis. We categorize advanced GAN frameworks into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. The relationship between relevant frameworks and their publication date are also outlined as a reference.']}]}]",journals/corr/abs-1910-09399,A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis.,2019,"Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. ","(0.8597712516784668, 0.8488138914108276, 0.8542574644088745)",0.0,0.37941256165504456
27,Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,Where do they employ feature-wise sigmoid gating?,Vector gate (vg),1904.05584,"[{'question': 'Where do they employ feature-wise sigmoid gating?', 'question_id': '7fe48939ce341212c1d801095517dc552b98e7b3', 'answers': [{'unanswerable': False, 'extractive_spans': ['gating mechanism acts upon each dimension of the word and character-level vectors'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism.'], 'highlighted_evidence': ['The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism.']}]}, {'question': 'Which model architecture do they use to obtain representations?', 'question_id': '65ad17f614b7345f0077424c04c94971c831585b', 'answers': [{'unanswerable': False, 'extractive_spans': ['BiLSTM with max pooling'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 .'], 'highlighted_evidence': ['To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 .']}]}, {'question': 'Which downstream sentence-level tasks do they evaluate on?', 'question_id': '323e100a6c92d3fe503f7a93b96d821408f92109', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF13 , BIBREF18'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .', 'table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not necessarily to the URLs where SentEval got the data from.', 'FLOAT SELECTED: Table B.2: Sentence representation evaluation datasets. SST5 was obtained from a GitHub repository with no associated peer-reviewed work.'], 'highlighted_evidence': ['Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .', 'table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper.', 'FLOAT SELECTED: Table B.2: Sentence representation evaluation datasets. SST5 was obtained from a GitHub repository with no associated peer-reviewed work.']}]}, {'question': 'Which similarity datasets do they use?', 'question_id': '9f89bff89cea722debc991363f0826de945bc582', 'answers': [{'unanswerable': False, 'extractive_spans': ['MEN', 'MTurk287', 'MTurk771', 'RG', 'RW', 'SimLex999', 'SimVerb3500', 'WS353', 'WS353R', 'WS353S'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['table:word-similarity-dataset lists the word-similarity datasets and their corresponding reference. As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each.', 'MEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets.', 'MTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times.', 'MTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 .', 'RG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”.', 'RW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words.', 'SimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs.', 'SimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness.', 'WS353 contains 353 word pairs annotated with similarity scores from 0 to 10.', 'WS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs.', 'WS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously.'], 'highlighted_evidence': ['As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each.\n\nMEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets.\n\nMTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times.\n\nMTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 .\n\nRG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”.\n\nRW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words.\n\nSimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs.\n\nSimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness.\n\nWS353 contains 353 word pairs annotated with similarity scores from 0 to 10.\n\nWS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs.\n\nWS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously.']}, {'unanswerable': False, 'extractive_spans': ['WS353S', 'SimLex999', 'SimVerb3500'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). We also included the RareWords (RW) dataset for evaluating the quality of rare word representations. See appendix:datasets for a more complete description of the datasets we used.'], 'highlighted_evidence': ['To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). ']}]}]",conf/naacl/BalazsM19,Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study.,2019,"The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism.","(0.8769822120666504, 0.7867723703384399, 0.8294317126274109)",1.1162242982778349e-238,0.47437575459480286
28,Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,"Are there datasets with relation tuples annotated, how big are datasets available?","Yes, there is at least one dataset (NYT) available. The size of this dataset is not specified in the given text snippet.",1911.09886,"[{'question': 'Are there datasets with relation tuples annotated, how big are datasets available?', 'question_id': '735f58e28d84ee92024a36bc348cfac2ee114409', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11.'], 'highlighted_evidence': ['This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations.']}]}, {'question': 'Which one of two proposed approaches performed better in experiments?', 'question_id': '710fa8b3e74ee63d2acc20af19f95f7702b7ce5e', 'answers': [{'unanswerable': False, 'extractive_spans': ['WordDecoding (WDec) model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.'], 'highlighted_evidence': ['Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.']}]}, {'question': 'What is previous work authors reffer to?', 'question_id': '56123dd42cf5c77fc9a88fc311ed2e1eb672126e', 'answers': [{'unanswerable': False, 'extractive_spans': ['SPTree', 'Tagging', 'CopyR', 'HRL', 'GraphR', 'N-gram Attention'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare our model with the following state-of-the-art joint entity and relation extraction models:', '(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities.', '(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities.', '(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple.', '(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities.', '(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations.', '(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17).'], 'highlighted_evidence': ['We compare our model with the following state-of-the-art joint entity and relation extraction models:\n\n(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM.', '(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder.', '(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations.', '(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction.', '(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them.', '(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data.']}]}, {'question': 'How higher are F1 scores compared to previous work?', 'question_id': '1898f999626f9a6da637bd8b4857e5eddf2fc729', 'answers': [{'unanswerable': False, 'extractive_spans': ['WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively', 'PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.'], 'highlighted_evidence': ['Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.']}, {'unanswerable': False, 'extractive_spans': ['Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively', 'In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.'], 'highlighted_evidence': ['Among the baselines, HRL achieves significantly higher F1 scores on the two datasets.', 'Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.', 'In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.']}]}]",conf/aaai/NayakN20,Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction.,2020,"This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations.","(0.8568127751350403, 0.8084127902984619, 0.8319094777107239)",1.3928411158501393e-232,0.3271101713180542
29,Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,How much better is performance of proposed method than state-of-the-art methods in experiments?,9.4 BLEU (7.2 →16.6),1910.03891,"[{'question': 'How much better is performance of proposed method than state-of-the-art methods in experiments?', 'question_id': '52f7e42fe8f27d800d1189251dfec7446f0e1d3b', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.', 'evidence': ['Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power.', 'FLOAT SELECTED: Table 2: Entity classification results in accuracy. We run all models 10 times and report mean ± standard deviation. KANE significantly outperforms baselines on FB24K, DBP24K and Game30K.'], 'highlighted_evidence': ['Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets.', 'FLOAT SELECTED: Table 2: Entity classification results in accuracy. We run all models 10 times and report mean ± standard deviation. KANE significantly outperforms baselines on FB24K, DBP24K and Game30K.']}]}, {'question': 'What further analysis is done?', 'question_id': '00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2', 'answers': [{'unanswerable': False, 'extractive_spans': ['we use t-SNE tool BIBREF27 to visualize the learned embedding'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Figure FIGREF30 shows the test accuracy with increasing epoch on DBP24K and Game30K. We can see that test accuracy first rapidly increased in the first ten iterations, but reaches a stable stages when epoch is larger than 40. Figure FIGREF31 shows test accuracy with different embedding size and training data proportions. We can note that too small embedding size or training data proportions can not generate sufficient global information. In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding. Figure FIGREF32 shows the visualization of 256 dimensional entity's embedding on Game30K learned by KANE, R-GCN, PransE and TransE. We observe that our method can learn more discriminative entity's embedding than other other methods.""], 'highlighted_evidence': ['In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding.']}]}, {'question': 'What seven state-of-the-art methods are used for comparison?', 'question_id': 'aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc', 'answers': [{'unanswerable': False, 'extractive_spans': ['TransE, TransR and TransH', 'PTransE, and ALL-PATHS', 'R-GCN BIBREF24 and KR-EAR BIBREF26'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines. For TransE, the dissimilarity measure is implemented with L1-norm, and relation as well as entity are replaced during negative sampling. For TransR, we directly use the source codes released in BIBREF9. In order for better performance, the replacement of relation in negative sampling is utilized according to the suggestion of author.', '2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18. PTransE is the first method to model relation path in KG embedding task, and ALL-PATHS improve the PTransE through a dynamic programming algorithm which can incorporate all relation paths of bounded length.', '3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets.'], 'highlighted_evidence': ['1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines.', '2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18.', '3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets.']}]}, {'question': 'What three datasets are used to measure performance?', 'question_id': 'cf0085c1d7bd9bc9932424e4aba4e6812d27f727', 'answers': [{'unanswerable': False, 'extractive_spans': ['FB24K', 'DBP24K', 'Game30K'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24.'], 'highlighted_evidence': ['First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K.']}, {'unanswerable': False, 'extractive_spans': ['Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24.'], 'highlighted_evidence': ['In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24.']}]}, {'question': 'How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?', 'question_id': '586b7470be91efe246c3507b05e30651ea6b9832', 'answers': [{'unanswerable': False, 'extractive_spans': ['To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""The process of KANE is illustrated in Figure FIGREF2. We introduce the architecture of KANE from left to right. As shown in Figure FIGREF2, the whole triples of knowledge graph as input. The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method. This method can recursively propagate the embeddings of entities from an entity's neighbors, and aggregate the neighbors with different weights. The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification.""], 'highlighted_evidence': ['The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.', 'The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification.']}]}, {'question': 'What are recent works on knowedge graph embeddings authors mention?', 'question_id': '31b20a4bab09450267dfa42884227103743e3426', 'answers': [{'unanswerable': False, 'extractive_spans': ['entity types or concepts BIBREF13', 'relations paths BIBREF17', ' textual descriptions BIBREF11, BIBREF12', 'logical rules BIBREF23', 'deep neural network models BIBREF24'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to address this issue, TransH BIBREF8 models a relation as a relation-specific hyperplane together with a translation on it, allowing entities to have distinct representation in different relations. TransR BIBREF9 models entities and relations in separate spaces, i.e., entity space and relation spaces, and performs translation from entity spaces to relation spaces. TransD BIBREF22 captures the diversity of relations and entities simultaneously by defining dynamic mapping matrix. Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24.'], 'highlighted_evidence': ['Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24.']}]}]",journals/corr/abs-1910-03891,Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding.,2019,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","(0.8464370965957642, 0.8090053200721741, 0.8272979855537415)",0.0,0.1894705444574356
30,A Computational Approach to Automatic Prediction of Drunk Texting,Do they report results only on English data?,No.,1610.00879,"[{'question': 'Do they report results only on English data?', 'question_id': '45306b26447ea4b120655d6bb2e3636079d3d6e0', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Figure 1: Word cloud for drunk tweets'], 'highlighted_evidence': ['FLOAT SELECTED: Figure 1: Word cloud for drunk tweets']}]}, {'question': 'Do the authors mention any confounds to their study?', 'question_id': '0c08af6e4feaf801185f2ec97c4da04c8b767ad6', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['A key challenge is to obtain an annotated dataset. We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet. We create three datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction.'], 'highlighted_evidence': ['Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work.']}]}, {'question': 'What baseline model is used?', 'question_id': '6412e97373e8e9ae3aa20aa17abef8326dc05450', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Human evaluators', 'evidence': ['FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set'], 'highlighted_evidence': ['FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set']}]}, {'question': 'What stylistic features are used to detect drunk texts?', 'question_id': '957bda6b421ef7d2839c3cec083404ac77721f14', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio', 'evidence': ['FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.', 'evidence': ['FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction']}]}, {'question': 'Is the data acquired under distant supervision verified by humans at any stage?', 'question_id': '368317b4fd049511e00b441c2e9550ded6607c37', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0.42 for A1 and A2). Table TABREF20 compares our classifier with humans. Our human annotators perform the task with an average accuracy of 68.8%, while our classifier (with all features) trained on Dataset 2 reaches 64%. The classifier trained on Dataset 2 is better than which is trained on Dataset 1.'], 'highlighted_evidence': ['Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober.']}]}, {'question': 'What hashtags are used for distant supervision?', 'question_id': 'b3ec918827cd22b16212265fcdd5b3eadee654ae', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do the authors equate drunk tweeting with drunk texting? ', 'question_id': '387970ebc7ef99f302f318d047f708274c0e8f21', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [""The ubiquity of communication devices has made social media highly accessible. The content on these media reflects a user's day-to-day activities. This includes content created under the influence of alcohol. In popular culture, this has been referred to as `drunk-texting'. In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. We refer to tweets written under the influence of alcohol as `drunk tweets', and the opposite as `sober tweets'.""], 'highlighted_evidence': [""In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. ""]}]}]",journals/corr/JoshiMABC16,A Computational Approach to Automatic Prediction of Drunk Texting.,2016,FLOAT SELECTED: Figure 1: Word cloud for drunk tweets,"(0.9304408431053162, 0.7957742214202881, 0.8578546047210693)",0.0,0.11733561754226685
31,Answering Complex Questions Using Open Information Extraction,What corpus was the source of the OpenIE extractions?,English Gigaword fifth edition,1704.05572,"[{'question': 'What corpus was the source of the OpenIE extractions?', 'question_id': '2fffff59e57b8dbcaefb437a6b3434fc137f813b', 'answers': [{'unanswerable': False, 'extractive_spans': ['domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).', 'We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams.'], 'highlighted_evidence': ['We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. ', 'Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).', 'The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. ']}, {'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the accuracy of the proposed technique?', 'question_id': 'eb95af36347ed0e0808e19963fe4d058e2ce3c9f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge', 'evidence': ['FLOAT SELECTED: Table 2: TUPLEINF is significantly better at structured reasoning than TABLEILP.9'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: TUPLEINF is significantly better at structured reasoning than TABLEILP.9']}]}, {'question': 'Is an entity linking process used?', 'question_id': 'cd1792929b9fa5dd5b1df0ae06fc6aece4c97424', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.', 'Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $ &\\textit {tf}(x, q)=1\\; \\textmd {if x} \\in q ; \\textit {idf}(x) = log(1 + N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $'], 'highlighted_evidence': ['Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.', 'Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ .']}]}, {'question': 'Are the OpenIE extractions all triples?', 'question_id': '65d34041ffa4564385361979a08706b10b92ebc7', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We create an additional table in TableILP with all the tuples in $T$ . Since TableILP uses fixed-length $(subject; predicate; object)$ triples, we need to map tuples with multiple objects to this format. For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table.'], 'highlighted_evidence': ['For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table.']}]}, {'question': 'What method was used to generate the OpenIE extractions?', 'question_id': 'e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed', 'answers': [{'unanswerable': False, 'extractive_spans': ['for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S', 'take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).'], 'highlighted_evidence': ['For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).']}]}, {'question': 'Can the method answer multi-hop questions?', 'question_id': 'a8545f145d5ea2202cb321c8f93e75ad26fcf4aa', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work.'], 'highlighted_evidence': ['For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work.']}]}, {'question': 'What was the textual source to which OpenIE was applied?', 'question_id': '417dabd43d6266044d38ed88dbcb5fdd7a426b22', 'answers': [{'unanswerable': False, 'extractive_spans': ['domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams.', 'We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).'], 'highlighted_evidence': ['The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining.', 'We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. ', 'We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).']}]}, {'question': 'What OpenIE method was used to generate the extractions?', 'question_id': 'fed230cef7c130f6040fb04304a33bbc17ca3a36', 'answers': [{'unanswerable': False, 'extractive_spans': ['for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S', 'take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).'], 'highlighted_evidence': ['Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).']}]}, {'question': 'Is their method capable of multi-hop reasoning?', 'question_id': '7917d44e952b58ea066dc0b485d605c9a1fe3dda', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work.'], 'highlighted_evidence': ['For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work.']}]}]",conf/acl/KhotSC17,Answering Complex Questions Using Open Information Extraction.,2017,We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. ,"(0.8282384872436523, 0.802888035774231, 0.815366268157959)",0.0,0.16683299839496613
32,ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples,What datasets were used?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,1904.03339,"[{'question': 'What datasets were used?', 'question_id': '01d91d356568fca79e47873bd0541bd22ba66ec0', 'answers': [{'unanswerable': False, 'extractive_spans': ['datasets given on the shared task, without using any additional external data'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['JESSI is trained using only the datasets given on the shared task, without using any additional external data. Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%.'], 'highlighted_evidence': ['JESSI is trained using only the datasets given on the shared task, without using any additional external data.']}]}, {'question': 'How did they do compared to other teams?', 'question_id': '37e45a3439b048a80c762418099a183b05772e6a', 'answers': [{'unanswerable': False, 'extractive_spans': ['second on Subtask A with an F1 score of 77.78% among 33 other team submissions', 'performs well on Subtask B with an F1 score of 79.59%'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['JESSI is trained using only the datasets given on the shared task, without using any additional external data. Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%.'], 'highlighted_evidence': ['Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%.']}]}]",conf/semeval/ParkKLAKSL19,ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples.,2019,"JESSI is trained using only the datasets given on the shared task, without using any additional external data.","(0.8191721439361572, 0.8264447450637817, 0.8227924108505249)",0.0,0.23253943026065826
33,DENS: A Dataset for Multi-class Emotion Analysis,Which tested technique was the worst performer?,Skip-bigram statistics.,1910.11769,"[{'question': 'Which tested technique was the worst performer?', 'question_id': 'a4e66e842be1438e5cd8d7cb2a2c589f494aee27', 'answers': [{'unanswerable': False, 'extractive_spans': ['Depeche + SVM'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation)', 'We computed bag-of-words-based benchmarks using the following methods:', 'Classification with TF-IDF + Linear SVM (TF-IDF + SVM)', 'Classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)', 'Classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)', 'Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation)', 'We computed bag-of-words-based benchmarks using the following methods:\n\nClassification with TF-IDF + Linear SVM (TF-IDF + SVM)\n\nClassification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)\n\nClassification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)\n\nCombination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)']}]}, {'question': 'How many emotions do they look at?', 'question_id': 'cb78e280e3340b786e81636431834b75824568c3', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '9', 'evidence': ['The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral.'], 'highlighted_evidence': ['The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral']}]}, {'question': 'What are the baseline benchmarks?', 'question_id': '2941874356e98eb2832ba22eae9cb08ec8ce0308', 'answers': [{'unanswerable': False, 'extractive_spans': ['TF-IDF + SVM', 'Depeche + SVM', 'NRC + SVM', 'TF-NRC + SVM', 'Doc2Vec + SVM', ' Hierarchical RNN', 'BiRNN + Self-Attention', 'ELMo + BiRNN', ' Fine-tuned BERT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We computed bag-of-words-based benchmarks using the following methods:', 'Classification with TF-IDF + Linear SVM (TF-IDF + SVM)', 'Classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)', 'Classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)', 'Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)', 'Benchmarks ::: Doc2Vec + SVM', 'We also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier.', 'Benchmarks ::: Hierarchical RNN', 'For this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs.', 'The outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training.', 'Benchmarks ::: Bi-directional RNN and Self-Attention (BiRNN + Self-Attention)', 'One challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations.', 'Self-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark.', 'The benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function.', 'Note that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments.', 'Benchmarks ::: ELMo embedding and Bi-directional RNN (ELMo + BiRNN)', 'Deep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words.', 'We used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function.', 'Benchmarks ::: Fine-tuned BERT', 'Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification.', 'We used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%.'], 'highlighted_evidence': ['We computed bag-of-words-based benchmarks using the following methods:\n\nClassification with TF-IDF + Linear SVM (TF-IDF + SVM)\n\nClassification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)\n\nClassification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)\n\nCombination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)\n\nBenchmarks ::: Doc2Vec + SVM\nWe also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier.\n\nBenchmarks ::: Hierarchical RNN\nFor this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs.\n\nThe outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training.\n\nBenchmarks ::: Bi-directional RNN and Self-Attention (BiRNN + Self-Attention)\nOne challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations.\n\nSelf-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark.\n\nThe benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function.\n\nNote that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments.\n\nBenchmarks ::: ELMo embedding and Bi-directional RNN (ELMo + BiRNN)\nDeep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words.\n\nWe used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function.\n\nBenchmarks ::: Fine-tuned BERT\nBidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification.\n\nWe used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%.']}]}, {'question': 'What is the size of this dataset?', 'question_id': '4e50e9965059899d15d3c3a0c0a2d73e0c5802a0', 'answers': [{'unanswerable': False, 'extractive_spans': ['9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words.'], 'highlighted_evidence': ['The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words.']}]}, {'question': 'How many annotators were there?', 'question_id': '67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6', 'answers': [{'unanswerable': False, 'extractive_spans': ['3 '], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We required all annotators have a `master' MTurk qualification. Each passage was labelled by 3 unique annotators. Only passages with a majority agreement between annotators were accepted as valid. This is equivalent to a Fleiss's $\\kappa $ score of greater than $0.4$.""], 'highlighted_evidence': [' Each passage was labelled by 3 unique annotators.']}]}]",conf/emnlp/LiuOA19,DENS: A Dataset for Multi-class Emotion Analysis.,2019,FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation),"(0.86759352684021, 0.8243942260742188, 0.8454424142837524)",0.0,0.18756811320781708
34,Multitask Learning with CTC and Segmental CRF for Speech Recognition,Can SCRF be used to pretrain the model?,No,1702.06378,"[{'question': 'Can SCRF be used to pretrain the model?', 'question_id': 'aecb485ea7d501094e50ad022ade4f0c93088d80', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder. The joint model by multitask learning is slightly more expensive than the stand-alone SRNN model. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model. This is analogous to sequence training of HMM acoustic models, where the network is usually pretrained by the frame-level CE criterion. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model.'], 'highlighted_evidence': ['One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder.', 'To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model.', 'Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model.']}]}]",conf/interspeech/LuKDS17,Multitask Learning with CTC and Segmental CRF for Speech Recognition.,2017,"One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder.","(0.8139395117759705, 0.7838789224624634, 0.7986264824867249)",0.0,0.07811681926250458
35,Transfer Learning Between Related Tasks Using Expected Label Proportions,"How much more data does the model trained using XR loss have access to, compared to the fully supervised model?",Plentiful data.,1909.00430,"[{'question': 'How much more data does the model trained using XR loss have access to, compared to the fully supervised model?', 'question_id': '547be35cff38028648d199ad39fb48236cfb99ee', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Does the system trained only using XR loss outperform the fully supervised neural system?', 'question_id': '47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).']}]}, {'question': 'How accurate is the aspect based sentiment classifier trained only using the XR loss?', 'question_id': 'e42fbf6c183abf1c6c2321957359c7683122b48e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n', 'evidence': ['FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).']}]}, {'question': 'How is the expectation regularization loss defined?', 'question_id': 'e574f0f733fb98ecef3c64044004aa7a320439be', 'answers': [{'unanswerable': False, 'extractive_spans': ['DISPLAYFORM0'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0'], 'highlighted_evidence': ['Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0']}]}]",conf/emnlp/NoachG19,Transfer Learning Between Related Tasks Using Expected Label Proportions.,2019,,"(0.0, 0.0, 0.0)",0.0,0.11490394920110703
36,The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection,What were the non-neural baselines used for the task?,None.,1910.11493,"[{'question': 'What were the non-neural baselines used for the task?', 'question_id': 'b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The Lemming model in BIBREF17', 'evidence': ['BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. The model is globally normalized with the use of a second order linear-chain CRF. To efficiently calculate the partition function, the choice of lemmata are pruned with the use of pre-extracted edit trees.'], 'highlighted_evidence': ['BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. ']}]}]",journals/corr/abs-1910-11493,The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection.,2019,The Lemming model in BIBREF17,"(0.9390981793403625, 0.8183566331863403, 0.8745797276496887)",0.0,0.1352912336587906
37,Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU,Which publicly available NLU dataset is used?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,1910.00912,"[{'question': 'Which publicly available NLU dataset is used?', 'question_id': 'bd3ccb63fd8ce5575338d7332e96def7a3fabad6', 'answers': [{'unanswerable': False, 'extractive_spans': ['ROMULUS dataset', 'NLU-Benchmark dataset'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We tested the system on two datasets, different in size and complexity of the addressed language.', 'Experimental Evaluation ::: Datasets ::: NLU-Benchmark dataset', 'The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. For example, “schedule a call with Lisa on Monday morning” is labelled to contain a calendar scenario, where the set_event action is instantiated through the entities [event_name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and action labels (e.g., calendar_set_event). This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot BIBREF7.', 'Experimental Evaluation ::: Datasets ::: ROMULUS dataset', ""The second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., “enter the bedroom slowly, turn left and turn the lights off ”) to complex requests for information (e.g., “good morning I want to buy a new mobile phone is there any shop nearby?”) or open-domain chit-chat (e.g., “nope thanks let's talk about cinema”). A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain ($\\approx $$70\\%$), though a small portion has been synthetically generated for balancing the frame distribution.""], 'highlighted_evidence': ['We tested the system on two datasets, different in size and complexity of the addressed language.\n\nExperimental Evaluation ::: Datasets ::: NLU-Benchmark dataset\nThe first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities.', 'Experimental Evaluation ::: Datasets ::: ROMULUS dataset\nThe second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation.']}]}, {'question': 'What metrics other than entity tagging are compared?', 'question_id': '7c794fa0b2818d354ca666969107818a2ffdda0c', 'answers': [{'unanswerable': False, 'extractive_spans': ['We also report the metrics in BIBREF7 for consistency', 'we report the span F1', ' Exact Match (EM) accuracy of the entire sequence of labels', 'metric that combines intent and entities'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems. The statistical analysis shows a significant improvement over Rasa $[Z=-2.803, p = .005]$, Dialogflow $[Z=-2.803, p = .005]$, LUIS $[Z=-2.803, p = .005]$ and Watson $[Z=-2.803, p = .005]$.', 'FLOAT SELECTED: Table 4: Comparison of HERMIT with the results in (Liu et al., 2019) by combining Intent and Entity.', 'In this section we report the experiments performed on the ROMULUS dataset (Table TABREF27). Together with the evaluation metrics used in BIBREF7, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions – e.g., a match is when all the three sequences are correct.', 'Results in terms of EM reflect the complexity of the different tasks, motivating their position within the hierarchy. Specifically, dialogue act identification is the easiest task ($89.31\\%$) with respect to frame ($82.60\\%$) and frame element ($79.73\\%$), due to the shallow semantics it aims to catch. However, when looking at the span F1, its score ($89.42\\%$) is lower than the frame element identification task ($92.26\\%$). What happens is that even though the label set is smaller, dialogue act spans are supposed to be longer than frame element ones, sometimes covering the whole sentence. Frame elements, instead, are often one or two tokens long, that contribute in increasing span based metrics. Frame identification is the most complex task for several reasons. First, lots of frame spans are interlaced or even nested; this contributes to increasing the network entropy. Second, while the dialogue act label is highly related to syntactic structures, frame identification is often subject to the inherent ambiguity of language (e.g., get can evoke both Commerce_buy and Arriving). We also report the metrics in BIBREF7 for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks. However, the frame element scores are comparable to the benchmark, since the task is very similar.'], 'highlighted_evidence': ['Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems.', 'FLOAT SELECTED: Table 4: Comparison of HERMIT with the results in (Liu et al., 2019) by combining Intent and Entity.', 'Together with the evaluation metrics used in BIBREF7, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions – e.g., a match is when all the three sequences are correct.', 'We also report the metrics in BIBREF7 for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks.']}]}]",conf/sigdial/VanzoBL19,Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU.,2019,"We tested the system on two datasets, different in size and complexity of the addressed language.

Experimental Evaluation ::: Datasets ::: NLU-Benchmark dataset
The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities.","(0.8448778986930847, 0.8025231957435608, 0.8231561183929443)",8.705695503153126e-234,0.5716046094894409
38,Exploring Hate Speech Detection in Multimodal Publications,What models do they propose?,"They propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.",1910.03814,"[{'question': 'What models do they propose?', 'question_id': '6976296126e4a5c518e6b57de70f8dc8d8fde292', 'answers': [{'unanswerable': False, 'extractive_spans': ['Feature Concatenation Model (FCM)', 'Spatial Concatenation Model (SCM)', 'Textual Kernels Model (TKM)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any).'], 'highlighted_evidence': ['To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM)']}]}, {'question': 'Are all tweets in English?', 'question_id': '53640834d68cf3b86cf735ca31f1c70aa0006b72', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How large is the dataset?', 'question_id': 'b2b0321b0aaf58c3aa9050906ade6ef35874c5c1', 'answers': [{'unanswerable': False, 'extractive_spans': [' $150,000$ tweets'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.'], 'highlighted_evidence': ['We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. ']}]}, {'question': 'What is the results of multimodal compared to unimodal models?', 'question_id': '4e9684fd68a242cb354fa6961b0e3b5c35aae4b6', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ', 'evidence': ['Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.', 'FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.'], 'highlighted_evidence': ['Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.', 'FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.']}]}, {'question': ""What is author's opinion on why current multimodal models cannot outperform models analyzing only text?"", 'question_id': '2e632eb5ad611bbd16174824de0ae5efe4892daf', 'answers': [{'unanswerable': False, 'extractive_spans': ['Noisy data', 'Complexity and diversity of multimodal relations', 'Small set of multimodal examples'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Despite the model trained only with images proves that they are useful for hate speech detection, the proposed multimodal models are not able to improve the detection compared to the textual models. Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data:', '[noitemsep,leftmargin=*]', 'Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.', 'Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.', 'Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate.'], 'highlighted_evidence': ['Next, we analyze why they do not perform well in this task and with this data:\n\n[noitemsep,leftmargin=*]\n\nNoisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.\n\nComplexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.\n\nSmall set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate.']}]}, {'question': 'What metrics are used to benchmark the results?', 'question_id': 'd1ff6cba8c37e25ac6b261a25ea804d8e58e09c0', 'answers': [{'unanswerable': False, 'extractive_spans': ['F-score', 'Area Under the ROC Curve (AUC)', 'mean accuracy (ACC)', 'Precision vs Recall plot', 'ROC curve (which plots the True Positive Rate vs the False Positive Rate)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.'], 'highlighted_evidence': ['Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.', 'Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.']}]}, {'question': 'How is data collected, manual collection or Twitter api?', 'question_id': '24c0f3d6170623385283dfda7f2b6ca2c7169238', 'answers': [{'unanswerable': False, 'extractive_spans': ['Twitter API'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter.""], 'highlighted_evidence': ['We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9.']}]}, {'question': 'How many tweats does MMHS150k contains, 150000?', 'question_id': '21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e', 'answers': [{'unanswerable': False, 'extractive_spans': ['$150,000$ tweets'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.'], 'highlighted_evidence': ['We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.']}]}, {'question': 'What unimodal detection models were used?', 'question_id': 'a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e', 'answers': [{'unanswerable': False, 'extractive_spans': [' single layer LSTM with a 150-dimensional hidden state for hate / not hate classification'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. Since our dataset is not big enough to train a GloVe word embedding model, we used a pre-trained model that has been trained in two billion tweets. This ensures that the model will be able to produce word embeddings for slang and other words typically used in Twitter. To process the tweets text before generating the word embeddings, we use the same pipeline as the model authors, which includes generating symbols to encode Twitter special interactions such as user mentions (@user) or hashtags (#hashtag). To encode the tweet text and input it later to multimodal models, we use the LSTM hidden state after processing the last tweet word. Since the LSTM has been trained for hate speech classification, it extracts the most useful information for this task from the text, which is encoded in the hidden state after inputting the last tweet word.'], 'highlighted_evidence': ['We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations.']}]}, {'question': 'What different models for multimodal detection were proposed?', 'question_id': 'b0799e26152197aeb3aa3b11687a6cc9f6c31011', 'answers': [{'unanswerable': False, 'extractive_spans': ['Feature Concatenation Model (FCM)', 'Spatial Concatenation Model (SCM)', 'Textual Kernels Model (TKM)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any).'], 'highlighted_evidence': ['To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM).']}]}, {'question': 'What annotations are available in the dataset - tweat used hate speach or not?', 'question_id': '4ce4db7f277a06595014db181342f8cb5cb94626', 'answers': [{'unanswerable': False, 'extractive_spans': ['No attacks to any community', ' racist', 'sexist', 'homophobic', 'religion based attacks', 'attacks to other communities'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We annotate the gathered tweets using the crowdsourcing platform Amazon Mechanical Turk. There, we give the workers the definition of hate speech and show some examples to make the task clearer. We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. Each one of the $150,000$ tweets is labeled by 3 different workers to palliate discrepancies among workers.'], 'highlighted_evidence': ['We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities.']}]}]",journals/corr/abs-1910-03814,Exploring Hate Speech Detection in Multimodal Publications.,2019,"To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM)","(0.8560991287231445, 0.7887374758720398, 0.8210389614105225)",5.621351669526789e-232,0.13419978320598602
39,Self-Taught Convolutional Neural Networks for Short Text Clustering,What were the evaluation metrics used?,"SLOR, WP-SLOR, ROUGE-LM, and traditional word-overlap metrics.",1701.00185,"[{'question': 'What were the evaluation metrics used?', 'question_id': '62a6382157d5f9c1dce6e6c24ac5994442053002', 'answers': [{'unanswerable': False, 'extractive_spans': ['accuracy', 'normalized mutual information'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0'], 'highlighted_evidence': ['Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . ']}]}, {'question': 'What were their performance results?', 'question_id': '9e04730907ad728d62049f49ac828acb4e0a1a2a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%', 'evidence': ['FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.', 'FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.', 'FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.']}]}, {'question': 'By how much did they outperform the other methods?', 'question_id': '5a0841cc0628e872fe473874694f4ab9411a1d10', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI', 'evidence': ['FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.', 'FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.', 'FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.']}]}, {'question': 'Which popular clustering methods did they experiment with?', 'question_id': 'a5dd569e6d641efa86d2c2b2e970ce5871e0963f', 'answers': [{'unanswerable': False, 'extractive_spans': ['K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows:'], 'highlighted_evidence': ['In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . ']}]}, {'question': 'What datasets did they use?', 'question_id': '785c054f6ea04701f4ab260d064af7d124260ccc', 'answers': [{'unanswerable': False, 'extractive_spans': ['SearchSnippets', 'StackOverflow', 'Biomedical'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We test our proposed approach on three public short text datasets. The summary statistics and semantic topics of these datasets are described in Table TABREF24 and Table TABREF25 .', 'SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 .', 'StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 .', ""Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different MeSH major topics as in Table TABREF25 . As described in Table TABREF24 , the max length of selected paper titles is 53.""], 'highlighted_evidence': ['We test our proposed approach on three public short text datasets. ', 'SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 .', 'StackOverflow. We use the challenge data published in Kaggle.com. ', ""Biomedical. We use the challenge data published in BioASQ's official website. ""]}]}]",journals/nn/XuXWZTZX17,Self-Taught convolutional neural networks for short text clustering.,2017,"Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . ","(0.8212578296661377, 0.8096972107887268, 0.8154365420341492)",1.1390970104572432e-232,0.34495681524276733
40,Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations,Does pre-training on general text corpus improve performance?,No.,1912.00871,"[{'question': 'Does pre-training on general text corpus improve performance?', 'question_id': '3f6610d1d68c62eddc2150c460bf1b48a064e5e6', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability.', 'Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation.'], 'highlighted_evidence': ['Our attempt at language pre-training fell short of our expectations in all but one tested dataset.', 'Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported.']}]}, {'question': 'What neural configurations are explored?', 'question_id': '4c854d33a832f3f729ce73b206ff90677e131e48', 'answers': [{'unanswerable': False, 'extractive_spans': ['tried many configurations of our network models, but report results with only three configurations', 'Transformer Type 1', 'Transformer Type 2', 'Transformer Type 3'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare medium-sized, small, and minimal networks to show if network size can be reduced to increase training and testing efficiency while retaining high accuracy. Networks over six layers have shown to be non-effective for this task. We tried many configurations of our network models, but report results with only three configurations of Transformers.', 'Transformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024.', 'Transformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024.', 'Transformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512.'], 'highlighted_evidence': ['We tried many configurations of our network models, but report results with only three configurations of Transformers.\n\nTransformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024.\n\nTransformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024.\n\nTransformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512.']}]}, {'question': 'Are the Transformers masked?', 'question_id': '163c15da1aa0ba370a00c5a09294cd2ccdb4b96d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [""We calculate the loss in training according to a mean of the sparse categorical cross-entropy formula. Sparse categorical cross-entropy BIBREF23 is used for identifying classes from a feature set, which assumes a large target classification set. Evaluation between the possible translation classes (all vocabulary subword tokens) and the produced class (predicted token) is the metric of performance here. During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value. We adjust the model's loss according to the mean of the translation accuracy after predicting every determined subword in a translation.""], 'highlighted_evidence': ['During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value.']}]}, {'question': 'How is this problem evaluated?', 'question_id': '90dd5c0f5084a045fd6346469bc853c33622908f', 'answers': [{'unanswerable': False, 'extractive_spans': ['BLEU-2', 'average accuracies over 3 test trials on different randomly sampled test sets'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Approach ::: Method: Training and Testing ::: Experiment 1: Representation', 'Some of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability. Traditionally, a BLEU score is a metric of translation quality BIBREF24. Our presented BLEU scores represent an average of scores a given model received over each of the target test sets. We use a standard bi-gram weight to show how accurate translations are within a window of two adjacent terms. After testing translations, we calculate an average BLEU-2 score per test set, which is related to the success over that data. An average of the scores for each dataset become the presented value.', 'Approach ::: Method: Training and Testing ::: Experiment 2: State-of-the-art', 'This experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect"" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset. This calculation more accurately depicts the generalization of our networks.'], 'highlighted_evidence': ['Approach ::: Method: Training and Testing ::: Experiment 1: Representation\nSome of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability.', 'Approach ::: Method: Training and Testing ::: Experiment 2: State-of-the-art\nThis experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect"" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset.']}]}, {'question': 'What datasets do they use?', 'question_id': '095888f6e10080a958d9cd3f779a339498f3a109', 'answers': [{'unanswerable': False, 'extractive_spans': ['AI2 BIBREF2', 'CC BIBREF19', 'IL BIBREF4', 'MAWPS BIBREF20'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.', 'AI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.', 'CC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.', 'IL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.', 'MAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. The problems not used were more complex problems that generate systems of equations. We exclude such problems because generating systems of equations is not our focus.'], 'highlighted_evidence': ['We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.\n\nAI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.\n\nCC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.\n\nIL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.\n\nMAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set.']}]}]",journals/corr/abs-1912-00871,Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations.,2019,Our attempt at language pre-training fell short of our expectations in all but one tested dataset.,"(0.9279952049255371, 0.835727870464325, 0.8794480562210083)",0.0,0.11852826178073883
41,CamemBERT: a Tasty French Language Model,What is CamemBERT trained on?,Masked Language Modeling (MLM) task.,1911.03894,"[{'question': 'What is CamemBERT trained on?', 'question_id': '71f2b368228a748fd348f1abf540236568a61b07', 'answers': [{'unanswerable': False, 'extractive_spans': ['unshuffled version of the French OSCAR corpus'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.'], 'highlighted_evidence': ['We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.']}]}, {'question': 'Which tasks does CamemBERT not improve on?', 'question_id': 'd3d4eef047aa01391e3e5d613a0f1f786ae7cfc7', 'answers': [{'unanswerable': False, 'extractive_spans': ['its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Experiments ::: Results ::: Natural Language Inference: XNLI', 'On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters).'], 'highlighted_evidence': ['Experiments ::: Results ::: Natural Language Inference: XNLI\nOn the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters).']}]}, {'question': 'What is the state of the art?', 'question_id': '63723c6b398100bba5dc21754451f503cb91c9b8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)', 'evidence': ['We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.', 'In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings.', 'In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50.'], 'highlighted_evidence': ['We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.', 'In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings.', 'In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50.']}]}, {'question': 'How much better was results of CamemBERT than previous results on these tasks?', 'question_id': '5471766ca7c995dd7f0f449407902b32ac9db269', 'answers': [{'unanswerable': False, 'extractive_spans': ['2.36 point increase in the F1 score with respect to the best SEM architecture', 'on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM)', 'lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa', 'For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT', 'For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['CamemBERT also demonstrates higher performances than mBERT on those tasks. We observe a larger error reduction for parsing than for tagging. For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT.', 'On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters).', 'For named entity recognition, our experiments show that CamemBERT achieves a slightly better precision than the traditional CRF-based SEM architectures described above in Section SECREF25 (CRF and Bi-LSTM+CRF), but shows a dramatic improvement in finding entity mentions, raising the recall score by 3.5 points. Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB. One other important finding is the results obtained by mBERT. Previous work with this model showed increased performance in NER for German, Dutch and Spanish when mBERT is used as contextualised word embedding for an NER-specific model BIBREF48, but our results suggest that the multilingual setting in which mBERT was trained is simply not enough to use it alone and fine-tune it for French NER, as it shows worse performance than even simple CRF models, suggesting that monolingual models could be better at NER.'], 'highlighted_evidence': ['For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT.', 'On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M).', 'However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa.', 'Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB.']}]}, {'question': 'Was CamemBERT compared against multilingual BERT on these tasks?', 'question_id': 'dc49746fc98647445599da9d17bc004bafdc4579', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT). We then compare our models to UDify BIBREF36. UDify is a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS tagging and dependency parsing.'], 'highlighted_evidence': ['To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT).']}]}, {'question': 'How long was CamemBERT trained?', 'question_id': '8720c096c8b990c7b19f956ee4930d5f2c019e2b', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What data is used for training CamemBERT?', 'question_id': 'b573b36936ffdf1d70e66f9b5567511c989b46b2', 'answers': [{'unanswerable': False, 'extractive_spans': ['unshuffled version of the French OSCAR corpus'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.'], 'highlighted_evidence': ['We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.']}]}]",journals/corr/abs-2306-15550,CamemBERT-bio: a Tasty French Language Model Better for your Health.,2023,"We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.","(0.8147163391113281, 0.8082141876220703, 0.8114522695541382)",0.0,0.33860719203948975
42,Semantic Sentiment Analysis of Twitter Data,What is the current SOTA for sentiment analysis on Twitter at the time of writing?,"Kouloumpis, E., Wilson, T ., Moore, J.: T witter sentimen t analysis: The good the bad and the OMG! In: Proceedings of the Fifth International Conference on W eblogs and Social Media, ICWSM ’11, pp. 538–541.",1710.01492,"[{'question': 'What is the current SOTA for sentiment analysis on Twitter at the time of writing?', 'question_id': 'fa3663567c48c27703e09c42930e51bacfa54905', 'answers': [{'unanswerable': False, 'extractive_spans': ['deep convolutional networks BIBREF53 , BIBREF54'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Supervised learning. Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data.'], 'highlighted_evidence': [' In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 ']}]}, {'question': 'What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?', 'question_id': '7997b9971f864a504014110a708f215c84815941', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text', 'evidence': ['Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 .', 'Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .'], 'highlighted_evidence': [' Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc.', 'That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document']}]}, {'question': 'What are the metrics to evaluate sentiment analysis on Twitter?', 'question_id': '0d1408744651c3847469c4a005e4a9dccbd89cf1', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/kse/TalpadaHV19,An Analysis on Use of Deep Learning and Lexical-Semantic Based Sentiment Analysis Method on Twitter Data to Understand the Demographic Trend of Telemedicine.,2019," In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 ","(0.7620639801025391, 0.8185170888900757, 0.7892823219299316)",8.844844403089351e-232,0.11638326197862625
43,Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization,How big is dataset domain-specific embedding are trained on?,64 words,1909.12231,"[{'question': 'How big is dataset domain-specific embedding are trained on?', 'question_id': '1a7d28c25bb7e7202230e1b70a885a46dac8a384', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How big is unrelated corpus universal embedding is traned on?', 'question_id': '6bc45d4f908672945192390642da5a2760971c40', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How better are state-of-the-art results than this model? ', 'question_id': '48cc41c372d44b69a477998be449f8b81384786b', 'answers': [{'unanswerable': False, 'extractive_spans': ['we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features', ' RegSum achieves a similar ROUGE-2 score'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF32 depicts models producing 100 words summaries, all depending on hand-crafted features. We use as baselines FreqSum BIBREF22 ; TsSum BIBREF23 ; traditional graph-based approaches such as Cont. LexRank BIBREF9 ; Centroid BIBREF24 ; CLASSY04 BIBREF25 ; its improved version CLASSY11 BIBREF26 and the greedy model GreedyKL BIBREF27. All of these models are significantly underperforming compared to SemSentSum. In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features. Nonetheless, our model is competitive and does not depend on hand-crafted features due to its full data-driven nature and thus, it is not limited to a single domain.'], 'highlighted_evidence': ['In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features.']}]}]",journals/corr/abs-1909-12231,Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization.,2019,,"(0.0, 0.0, 0.0)",0.0,0.257286012172699
44,A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,What were their results on the three datasets?,Their results are presented in Table 3.,1706.08032,"[{'question': 'What were their results on the three datasets?', 'question_id': 'efb3a87845460655c53bd7365bcb8393c99358ec', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR', 'evidence': ['FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION'], 'highlighted_evidence': ['FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION']}]}, {'question': 'What was the baseline?', 'question_id': '0619fc797730a3e59ac146a5a4575c81517cc618', 'answers': [{'unanswerable': False, 'extractive_spans': ['We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN.', 'we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.', 'For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.'], 'highlighted_evidence': ['Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.\n\nFor Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.\n\n']}]}, {'question': 'Which datasets did they use?', 'question_id': '846a1992d66d955fa1747bca9a139141c19908e8', 'answers': [{'unanswerable': False, 'extractive_spans': ['Stanford - Twitter Sentiment Corpus (STS Corpus)', 'Sanders - Twitter Sentiment Corpus', 'Health Care Reform (HCR)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .', 'Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.', 'Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .'], 'highlighted_evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .']}]}, {'question': 'Are results reported only on English datasets?', 'question_id': '1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .', 'Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.', 'Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .', 'Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.', 'For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.'], 'highlighted_evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .', 'Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. ', 'For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). ']}]}, {'question': 'Which three Twitter sentiment classification datasets are used for experiments?', 'question_id': '12d77ac09c659d2e04b5e3955a283101c3ad1058', 'answers': [{'unanswerable': False, 'extractive_spans': ['Stanford - Twitter Sentiment Corpus (STS Corpus)', 'Sanders - Twitter Sentiment Corpus', 'Health Care Reform (HCR)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .', 'Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.', 'Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .'], 'highlighted_evidence': ['Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .']}]}, {'question': 'What semantic rules are proposed?', 'question_id': 'd60a3887a0d434abc0861637bbcd9ad0c596caf4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'rules that compute polarity of words after POS tagging or parsing steps', 'evidence': ['In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:', '@lonedog bwahahah...you are amazing! However, it was quite the letdown.', ""@kirstiealley my dentist is great but she's expensive...=("", 'In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.', 'FLOAT SELECTED: Table I SEMANTIC RULES [12]'], 'highlighted_evidence': ['In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:\n\n@lonedog bwahahah...you are amazing! However, it was quite the letdown.\n\n@kirstiealley my dentist is great but she\'s expensive...=(\n\nIn two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.', 'FLOAT SELECTED: Table I SEMANTIC RULES [12]']}]}]",conf/pacling/NguyenN17,A Deep Neural Architecture for Sentence-Level Sentiment Classification in Twitter Social Networking.,2017,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","(0.8231534361839294, 0.8197391033172607, 0.821442723274231)",0.0,0.1896584928035736
45,Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding,Which knowledge graph completion tasks do they experiment with?,Link prediction in knowledge graphs.,1811.01399,"[{'question': 'Which knowledge graph completion tasks do they experiment with?', 'question_id': '69a7a6675c59a4c5fb70006523b9fe0f01ca415c', 'answers': [{'unanswerable': False, 'extractive_spans': ['link prediction ', 'triplet classification'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250. LSTM is used due to its large expressive capability BIBREF26 .'], 'highlighted_evidence': ['We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification.']}]}, {'question': 'Apart from using desired properties, do they evaluate their LAN approach in some other way?', 'question_id': '60cb756d382b3594d9e1f4a5e2366db407e378ae', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do they evaluate existing methods in terms of desired properties?', 'question_id': '352a1bf734b2d7f0618e9e2b0dbed4a3f1787160', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/aaai/WangHLP19,Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding.,2019,"We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification.","(0.9244136810302734, 0.8582295775413513, 0.8900929689407349)",5.905830512777864e-233,0.6832517981529236
46,Learning with Noisy Labels for Sentence-level Sentiment Classification,How does the model differ from Generative Adversarial Networks?,"This model differs in that it considers multiple text corpora as input data, whereas traditional GANs typically use a single corpus.",1909.00124,"[{'question': 'How does the model differ from Generative Adversarial Networks?', 'question_id': '045dbdbda5d96a672e5c69442e30dbf21917a1ee', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the dataset used to train the model?', 'question_id': 'c20b012ad31da46642c553ce462bc0aad56912db', 'answers': [{'unanswerable': False, 'extractive_spans': [' movie sentence polarity dataset from BIBREF19', 'laptop and restaurant datasets collected from SemEval-201', 'we collected 2,000 reviews for each domain from the same review source'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same.', ""Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9.""], 'highlighted_evidence': ['Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 .', 'Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source.']}]}, {'question': 'What is the performance of the model?', 'question_id': '13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)', 'evidence': ['The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing.', 'The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300.', 'FLOAT SELECTED: Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.', 'FLOAT SELECTED: Figure 2: Accuracy (ACC) on clean test data. For training, the labels of clean data are flipped with the noise rates [0, 0.1, 0.2, 0.3, 0.4, 0.5]. For example, 0.1means that 10% of the labels are flipped. (Color online)'], 'highlighted_evidence': ['The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN', 'The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels.', 'FLOAT SELECTED: Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.', 'FLOAT SELECTED: Figure 2: Accuracy (ACC) on clean test data. For training, the labels of clean data are flipped with the noise rates [0, 0.1, 0.2, 0.3, 0.4, 0.5]. For example, 0.1means that 10% of the labels are flipped. (Color online)']}]}, {'question': 'Is the model evaluated against a CNN baseline?', 'question_id': '89b9a2389166b992c42ca19939d750d88c5fa79b', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.', 'The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300.'], 'highlighted_evidence': ['Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop.']}]}]",conf/emnlp/WangLLYL19,Learning with Noisy Labels for Sentence-level Sentiment Classification.,2019,,"(0.0, 0.0, 0.0)",0.0,0.027148015797138214
47,Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange,Does the model proposed beat the baseline models for all the values of the masking parameter tested?,No.,1909.00088,"[{'question': 'Does the model proposed beat the baseline models for all the values of the masking parameter tested?', 'question_id': 'dccc3b182861fd19ccce5bd00ce9c3f40451ed6e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT', 'FLOAT SELECTED: Table 9: Average results by MRT/RRT'], 'highlighted_evidence': ['FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT', 'FLOAT SELECTED: Table 9: Average results by MRT/RRT']}]}, {'question': 'Has STES been previously used in the literature to evaluate similar tasks?', 'question_id': '98ba7a7aae388b1a77dd6cab890977251d906359', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""We propose a pipeline called SMERTI (pronounced `smarty') for STE. Combining entity replacement (ER), similarity masking (SM), and text infilling (TI), SMERTI can modify the semantic content of text. We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change.""], 'highlighted_evidence': ['We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change.']}]}, {'question': 'What are the baseline models mentioned in the paper?', 'question_id': '3da9a861dfa25ed486cff0ef657d398fdebf8a93', 'answers': [{'unanswerable': False, 'extractive_spans': ['Noun WordNet Semantic Text Exchange Model (NWN-STEM)', 'General WordNet Semantic Text Exchange Model (GWN-STEM)', 'Word2Vec Semantic Text Exchange Model (W2V-STEM)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate on three datasets: Yelp and Amazon reviews BIBREF1, and Kaggle news headlines BIBREF2. We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM).'], 'highlighted_evidence': ['We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM).']}]}]",conf/emnlp/FengLH19,Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.,2019,FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT,"(0.933238685131073, 0.8079006671905518, 0.866058349609375)",0.0,0.021181676536798477
48,CN-CELEB: a challenging Chinese speaker recognition dataset,What was the performance of both approaches on their dataset?,Both SLOR and WP-SLOR correlated significantly better with fluency ratings for the outputs of compression systems than traditional word-overlap metrics.,1911.01799,"[{'question': 'What was the performance of both approaches on their dataset?', 'question_id': '8c0a0747a970f6ea607ff9b18cfeb738502d9a95', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'ERR of 19.05 with i-vectors and 15.52 with x-vectors', 'evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.']}]}, {'question': 'What kind of settings do the utterances come from?', 'question_id': '529dabe7b4a8a01b20ee099701834b60fb0c43b0', 'answers': [{'unanswerable': False, 'extractive_spans': ['entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons.', 'CN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging.'], 'highlighted_evidence': ['CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons.\n\nCN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging.']}]}, {'question': 'What genres are covered?', 'question_id': 'a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement', 'evidence': ['FLOAT SELECTED: Table 1. The distribution over genres.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1. The distribution over genres.']}]}, {'question': 'Do they experiment with cross-genre setups?', 'question_id': '5699996a7a2bb62c68c1e62e730cabf1e3186eef', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Which of the two speech recognition models works better overall on CN-Celeb?', 'question_id': '944d5dbe0cfc64bf41ea36c11b1d378c408d40b8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'x-vector', 'evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.']}]}, {'question': 'By how much is performance on CN-Celeb inferior to performance on VoxCeleb?', 'question_id': '327e6c6609fbd4c6ae76284ca639951f03eb4a4c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb', 'evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.']}]}]",conf/icassp/FanKLLCCZZCW20,CN-Celeb: A Challenging Chinese Speaker Recognition Dataset.,2020,ERR of 19.05 with i-vectors and 15.52 with x-vectors,"(0.8427891731262207, 0.8421025276184082, 0.8424457311630249)",1.1337861261109773e-231,0.12342622876167297
49,Open Named Entity Modeling from Embedding Distribution,What is their model?,NLP system,1909.00170,"[{'question': 'What is their model?', 'question_id': 'a999761aa976458bbc7b4f330764796446d030ff', 'answers': [{'unanswerable': False, 'extractive_spans': ['cross-lingual NE recognition'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Most annotated corpus based NE recognition tasks can benefit a great deal from a known NE dictionary, as NEs are those words which carry common sense knowledge quite differ from the rest ones in any language vocabulary. This work will focus on the NE recognition from plain text instead of corpus based NE recognition. For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition. Our study contains two main steps: First, we explore the NE distribution in monolingual case. Next, we learn a hypersphere mapping between embedding spaces of languages with minimal supervision.'], 'highlighted_evidence': ['For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition.']}]}, {'question': 'Do they evaluate on NER data sets?', 'question_id': 'f229069bcb05c2e811e4786c89b0208af90d9a25', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0. Our results in Table 6 and Table 7 demonstrate the power of hypersphere features, which contribute to nearly all of the three types of entities as shown in Table 6, except for a slight drop in the PER type of BIBREF22 on a strong baseline. HS features stably enhance all strong state-of-the-art baselines, BIBREF22 , BIBREF21 and BIBREF23 by 0.33/0.72/0.23 $F_1$ point and 0.13/0.3/0.1 $F_1$ point on both benchmark datasets, CoNLL-2003 and ONTONOTES 5.0. We show that our HS feature is also comparable with previous much more complicated LS feature, and our model surpasses their baseline (without LS feature) by 0.58/0.78 $F_1$ point with only HS features. We establish a new state-of-the-art $F_1$ score of 89.75 on ONTONOTES 5.0, while matching state-of-the-art performance with a $F_1$ score of 92.95 on CoNLL-2003 dataset.'], 'highlighted_evidence': ['To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0.']}]}]",journals/tkde/LuoZZT22,Open Named Entity Modeling From Embedding Distribution.,2022,"For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition.","(0.7985726594924927, 0.7943471670150757, 0.7964543104171753)",0.0,0.3996535539627075
50,Efficient Twitter Sentiment Classification using Subjective Distant Supervision,What previously proposed methods is this method compared against?,post-hoc text generation,1701.03051,"[{'question': 'What previously proposed methods is this method compared against?', 'question_id': '6b55b558ed581759425ede5d3a6fcdf44b8082ac', 'answers': [{'unanswerable': False, 'extractive_spans': ['Naive Bayes', 'SVM', 'Maximum Entropy classifiers'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment. Their feature vector is either composed of Unigrams, Bigrams, Unigrams + Bigrams, or Unigrams + POS tags.'], 'highlighted_evidence': ['The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment.']}]}, {'question': 'How is effective word score calculated?', 'question_id': '3e3f5254b729beb657310a5561950085fa690e83', 'answers': [{'unanswerable': False, 'extractive_spans': ['We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We define the Effective Word Score of score x as', 'EFWS(x) = N(+x) - N(-x),', 'where N(x) is the number of words in the tweet with polarity score x.'], 'highlighted_evidence': ['We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x.']}]}, {'question': 'How is tweet subjectivity measured?', 'question_id': '5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/comsnets/SahniCCS17,Efficient Twitter sentiment classification using subjective distant supervision.,2017,"The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment.","(0.8168654441833496, 0.8209766149520874, 0.818915843963623)",0.0,0.08828316628932953
51,Low-Level Linguistic Controls for Style Transfer and Content Preservation,Is this style generator compared to some baseline?,Yes.,1911.03385,"[{'question': 'Is this style generator compared to some baseline?', 'question_id': '9213159f874b3bdd9b4de956a88c703aac988411', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. To generate in a specific style, we simply set the appropriate embedding. We use genre embeddings of size 850 which is equivalent to the total size of the $K$ feature embeddings in the StyleEQ model.'], 'highlighted_evidence': ['We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7.']}]}, {'question': 'How they perform manual evaluation, what is criteria?', 'question_id': '5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2', 'answers': [{'unanswerable': False, 'extractive_spans': ['accuracy'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation.'], 'highlighted_evidence': ['Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation.']}]}, {'question': 'What metrics are used for automatic evaluation?', 'question_id': 'a234bcbf2e41429422adda37d9e926b49ef66150', 'answers': [{'unanswerable': False, 'extractive_spans': ['classification accuracy', 'BLEU scores', 'model perplexities of the reconstruction'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight. Beam candidates are ranked according to their length normalized log-likelihood. On these automatic measures we see that StyleEQ is better able to reconstruct the original sentences. In some sense this evaluation is mostly a sanity check, as the feature controls contain more locally specific information than the genre embeddings, which say very little about how many specific function words one should expect to see in the output.', 'In table:fasttext-results we see the results. Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs.'], 'highlighted_evidence': ['In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight.', 'Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs.']}]}, {'question': 'How they know what are content words?', 'question_id': 'c383fa9170ae00a4a24a8e39358c38395c5f034b', 'answers': [{'unanswerable': False, 'extractive_spans': [' words found in the control word lists are then removed', 'The remaining words, which represent the content'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['fig:sentenceinput illustrates the process. Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.', 'In this way we encourage models to construct a sentence using content and style independently. This will allow us to vary the stylistic controls while keeping the content constant, and successfully perform style transfer. When generating a new sentence, the controls correspond to the counts of the corresponding syntactic features that we expect to be realized in the output.'], 'highlighted_evidence': ['Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.\n\nIn this way we encourage models to construct a sentence using content and style independently.']}]}, {'question': 'How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?', 'question_id': '83251fd4a641cea8b180b49027e74920bca2699a', 'answers': [{'unanswerable': False, 'extractive_spans': ['style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples.'], 'highlighted_evidence': ['Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples.']}]}]",conf/inlg/GeroKRC19,Low Level Linguistic Controls for Style Transfer and Content Preservation.,2019,"We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7.","(0.9121072292327881, 0.7969987392425537, 0.8506767153739929)",0.0,0.0659584105014801
52,Incorporating Sememes into Chinese Definition Modeling,Is there an online demo of their system?,No,1905.06512,"[{'question': 'Is there an online demo of their system?', 'question_id': 'e21a8581cc858483a31c6133e53dd0cfda76ae4c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do they perform manual evaluation?', 'question_id': '9f6e877e3bde771595e8aee10c2656a0e7b9aeb2', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [""Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word “旅馆” (hotel), the baseline model fails to generate definition containing the token “旅行者”(tourists). However, by incoporating sememes' information, especially the sememe “旅游” (tour), AAM and SAAM successfully generate “旅行者”(tourists). Manual inspection of others examples also supports our claim.""], 'highlighted_evidence': ['Manual inspection of others examples also supports our claim.']}]}, {'question': 'Do they compare against Noraset et al. 2017?', 'question_id': 'a3783e42c2bf616c8a07bd3b3d503886660e4344', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework. Without utilizing the information of sememes, it learns a probabilistic mapping $P(y | x)$ from the word $x$ to be defined to a definition $y = [y_1, \\dots , y_T ]$ , in which $y_t$ is the $t$ -th word of definition $y$ .'], 'highlighted_evidence': ['The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework.']}]}, {'question': 'What is a sememe?', 'question_id': '0d0959dba3f7c15ee4f5cdee51682656c4abbd8f', 'answers': [{'unanswerable': False, 'extractive_spans': ['Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this work, we introduce a new dataset for the Chinese definition modeling task that we call Chinese Definition Modeling Corpus cdm(CDM). CDM consists of 104,517 entries, where each entry contains a word, the sememes of a specific word sense, and the definition in Chinese of the same word sense. Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 . For a given word sense, CDM annotates the sememes according to HowNet BIBREF5 , and the definition according to Chinese Concept Dictionary (CCD) BIBREF6 . Since sememes have been widely used in improving word representation learning BIBREF7 and word similarity computation BIBREF8 , we argue that sememes can benefit the task of definition modeling.'], 'highlighted_evidence': ['Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 .']}]}]",journals/taslp/YangKCLFY20,Incorporating Sememes into Chinese Definition Modeling.,2020,,"(0.0, 0.0, 0.0)",0.0,0.5032033920288086
53,RobBERT: a Dutch RoBERTa-based Language Model,What data did they use?,They used Twitter Sentiment Classification dataset.,2001.06286,"[{'question': 'What data did they use?', 'question_id': '589be705a5cc73a23f30decba23ce58ec39d313b', 'answers': [{'unanswerable': False, 'extractive_spans': ['the Dutch section of the OSCAR corpus'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16. This Dutch corpus has 6.6 billion words, totalling 39 GB of text. It contains 126,064,722 lines of text, where each line can contain multiple sentences. Subsequent lines are however not related to each other, due to the shuffled nature of the OSCAR data set. For comparison, the French RoBERTa-based language model CamemBERT BIBREF7 has been trained on the French portion of OSCAR, which consists of 138 GB of scraped text.'], 'highlighted_evidence': ['We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16.']}]}, {'question': 'What is the state of the art?', 'question_id': '6e962f1f23061f738f651177346b38fd440ff480', 'answers': [{'unanswerable': False, 'extractive_spans': ['BERTje BIBREF8', 'an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19.', 'mBERT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))', 'We replicated the high-level sentiment analysis task used to evaluate BERTje BIBREF8 to be able to compare our methods. This task uses a dataset called Dutch Book Reviews Dataset (DBRD), in which book reviews scraped from hebban.nl are labeled as positive or negative BIBREF19. Although the dataset contains 118,516 reviews, only 22,252 of these reviews are actually labeled as positive or negative. The DBRD dataset is already split in a balanced 10% test and 90% train split, allowing us to easily compare to other models trained for solving this task. This dataset was released in a paper analysing the performance of an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))', 'This dataset was released in a paper analysing the performance of an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19.']}]}, {'question': 'What language tasks did they experiment on?', 'question_id': '594a6bf37eab64a16c6a05c365acc100e38fcff1', 'answers': [{'unanswerable': False, 'extractive_spans': ['sentiment analysis', 'the disambiguation of demonstrative pronouns,'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluated RobBERT in several different settings on multiple downstream tasks. First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks. Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning.'], 'highlighted_evidence': ['First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks. ', 'Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning.']}]}]",conf/emnlp/DelobelleWB20,RobBERT: a Dutch RoBERTa-based Language Model.,2020,"We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16.","(0.8973979949951172, 0.8349710702896118, 0.8650597333908081)",0.0,0.27137115597724915
54,Query-oriented text summarization based on hypergraph transversals,How does the model compare with the MMR baseline?,No comparison is made in the snippet.,1902.00672,"[{'question': 'How does the model compare with the MMR baseline?', 'question_id': 'babe72f0491e65beff0e5889380e8e32d7a81f78', 'answers': [{'unanswerable': False, 'extractive_spans': [' Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Various classes of NP-hard problems involving a submodular and non-decreasing function can be solved approximately by polynomial time algorithms with provable approximation factors. Algorithms ""Detection of hypergraph transversals for text summarization"" and ""Detection of hypergraph transversals for text summarization"" are our core methods for the detection of approximations of maximal budgeted hypergraph transversals and minimal soft hypergraph transversals, respectively. In each case, a transversal is found and the summary is formed by extracting and aggregating the associated sentences. Algorithm ""Detection of hypergraph transversals for text summarization"" is based on an adaptation of an algorithm presented in BIBREF30 for the maximization of submodular functions under a Knaspack constraint. It is our primary transversal-based summarization model, and we refer to it as the method of Transversal Summarization with Target Length (TL-TranSum algorithm). Algorithm ""Detection of hypergraph transversals for text summarization"" is an application of the algorithm presented in BIBREF20 for solving the submodular set covering problem. We refer to it as Transversal Summarization with Target Coverage (TC-TranSum algorithm). Both algorithms produce transversals by iteratively appending the node inducing the largest increase in the total weight of the covered hyperedges relative to the node weight. While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only. The main difference between algorithms ""Detection of hypergraph transversals for text summarization"" and ""Detection of hypergraph transversals for text summarization"" is the stopping criterion: in algorithm ""Detection of hypergraph transversals for text summarization"" , the approximate minimal soft transversal is obtained whenever the targeted hyperedge coverage is reached while algorithm ""Detection of hypergraph transversals for text summarization"" appends a given sentence to the approximate maximal budgeted transversal only if its addition does not make the summary length exceed the target length $L$ .', 'FLOAT SELECTED: Table 2: Comparison with related graph- and hypergraph-based summarization systems.'], 'highlighted_evidence': ['While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only.', 'FLOAT SELECTED: Table 2: Comparison with related graph- and hypergraph-based summarization systems.']}]}]",journals/ipm/LierdeC19,Query-oriented text summarization based on hypergraph transversals.,2019,"While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only.","(0.8453948497772217, 0.8055276274681091, 0.824979841709137)",2.060849923466276e-159,0.21756279468536377
55,Text-based inference of moral sentiment change,Does the paper discuss previous models which have been applied to the same task?,Yes.,2001.07209,"[{'question': 'Does the paper discuss previous models which have been applied to the same task?', 'question_id': '31ee92e521be110b6a5a8d08cc9e6f90a3a97aae', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16.'], 'highlighted_evidence': ['An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16.']}]}, {'question': 'Which datasets are used in the paper?', 'question_id': '737397f66751624bcf4ef891a10b29cfc46b0520', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n', 'evidence': ['To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text. We use the MFD for moral polarity classification by dividing seed words into positive and negative sets, and for fine-grained categorization by splitting them into the 10 MFT categories.', 'To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.', 'We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:', 'Google N-grams BIBREF31: a corpus of $8.5 \\times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.', 'COHA BIBREF32: a smaller corpus of $4.1 \\times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009.'], 'highlighted_evidence': ['To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text.', 'To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.', 'We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:\n\nGoogle N-grams BIBREF31: a corpus of $8.5 \\times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.\n\nCOHA BIBREF32: a smaller corpus of $4.1 \\times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009.']}]}, {'question': 'How does the parameter-free model work?', 'question_id': '87cb19e453cf7e248f24b5f7d1ff9f02d87fc261', 'answers': [{'unanswerable': False, 'extractive_spans': ['A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;', 'A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF2 specifies the formulation of each model. Note that we adopt a parsimonious design principle in our modelling: both Centroid and Naïve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.', 'A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;', 'A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;'], 'highlighted_evidence': [' Note that we adopt a parsimonious design principle in our modelling: both Centroid and Naïve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.', 'A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;', 'A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;']}]}, {'question': 'How do they quantify moral relevance?', 'question_id': '5fb6a21d10adf4e81482bb5c1ec1787dc9de260d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence', 'evidence': ['To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.'], 'highlighted_evidence': ['To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.']}]}, {'question': 'Which fine-grained moral dimension examples do they showcase?', 'question_id': '542a87f856cb2c934072bacaa495f3c2645f93be', 'answers': [{'unanswerable': False, 'extractive_spans': ['Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We draw from research in social psychology to inform our methodology, most prominently Moral Foundations Theory BIBREF26. MFT seeks to explain the structure and variation of human morality across cultures, and proposes five moral foundations: Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation. Each foundation is summarized by a positive and a negative pole, resulting in ten fine-grained moral categories.'], 'highlighted_evidence': ['We draw from research in social psychology to inform our methodology, most prominently Moral Foundations Theory BIBREF26. MFT seeks to explain the structure and variation of human morality across cultures, and proposes five moral foundations: Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation. Each foundation is summarized by a positive and a negative pole, resulting in ten fine-grained moral categories.']}]}, {'question': 'Which dataset sources to they use to demonstrate moral sentiment through history?', 'question_id': '4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",journals/corr/abs-2001-07209,Text-based inference of moral sentiment change.,2020,"An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16.","(0.9113590717315674, 0.7763962745666504, 0.8384814858436584)",0.0,0.046875789761543274
56,Bringing Stories Alive: Generating Interactive Fiction Worlds,How well did the system do?,It is possible to satisfy one of these properties (plausibility and faithfulness) without the other.,2001.10161,"[{'question': 'How well did the system do?', 'question_id': 'c180f44667505ec03214d44f4970c0db487a8bae', 'answers': [{'unanswerable': False, 'extractive_spans': ['the neural approach is generally preferred by a greater percentage of participants than the rules or random', 'human-made game outperforms them all'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales. This is done in part to test the relative effectiveness of our approach across different genres with varying thematic commonsense knowledge. The dataset used was compiled via story summaries that were scraped from Wikipedia via a recursive crawling bot. The bot searched pages for both for plot sections as well as links to other potential stories. From the process, 695 fairy-tales and 536 mystery stories were compiled from two categories: novels and short stories. We note that the mysteries did not often contain many fantasy elements, i.e. they consisted of mysteries set in our world such as Sherlock Holmes, while the fairy-tales were much more removed from reality. Details regarding how each of the studies were conducted and the corresponding setup are presented below."", 'Each participant was was asked to play the neural game and then another one from one of the three additional models within a genre. The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities. This provided the participant with multiple possible methods of finishing a particular game. On completion, the participant was asked to rank the two games according to overall perceived coherence, interestingness, and adherence to the genre. We additionally provided a required initial tutorial game which demonstrated all of these mechanics. The order in which participants played the games was also randomized as in the graph evaluation to remove potential correlations. We had 75 participants in total, 39 for mystery and 36 for fairy-tales. As each player played the neural model created game and one from each of the other approaches—this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales.', 'FLOAT SELECTED: Table 4: Results of the full game evaluation participant study. *Indicates statistical significance (p < 0.05).', 'In the mystery genre, the neural approach is generally preferred by a greater percentage of participants than the rules or random. The human-made game outperforms them all. A significant exception to is that participants thought that the rules-based game was more interesting than the neural game. The trends in the fairy-tale genre are in general similar with a few notable deviations. The first deviation is that the rules-based and random approaches perform significantly worse than neural in this genre. We see also that the neural game is as coherent as the human-made game.'], 'highlighted_evidence': [""We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales."", 'Each participant was was asked to play the neural game and then another one from one of the three additional models within a genre. The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities. This provided the participant with multiple possible methods of finishing a particular game. On completion, the participant was asked to rank the two games according to overall perceived coherence, interestingness, and adherence to the genre. We additionally provided a required initial tutorial game which demonstrated all of these mechanics. The order in which participants played the games was also randomized as in the graph evaluation to remove potential correlations. We had 75 participants in total, 39 for mystery and 36 for fairy-tales. As each player played the neural model created game and one from each of the other approaches—this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales.', 'FLOAT SELECTED: Table 4: Results of the full game evaluation participant study. *Indicates statistical significance (p < 0.05).', 'In the mystery genre, the neural approach is generally preferred by a greater percentage of participants than the rules or random. The human-made game outperforms them all. A significant exception to is that participants thought that the rules-based game was more interesting than the neural game. The trends in the fairy-tale genre are in general similar with a few notable deviations. The first deviation is that the rules-based and random approaches perform significantly worse than neural in this genre. We see also that the neural game is as coherent as the human-made game.']}]}, {'question': 'How is the information extracted?', 'question_id': '76d62e414a345fe955dc2d99562ef5772130bc7e', 'answers': [{'unanswerable': False, 'extractive_spans': ['neural question-answering technique to extract relations from a story text', 'OpenIE5, a commonly used rule-based information extraction technique'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the “next to” and “has” edges respectively in Figure FIGREF4.', 'While many neural models already exist that perform similar tasks such as named entity extraction and part of speech tagging, they often come at the cost of large amounts of specialized labeled data suited for that task. We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.', 'The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.', 'The next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation—or edge—at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions “What location can I visit from $x$?” and “Who/What is in $x$?”. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. The probability that vertices $x,u$ are related:', 'We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.', 'As in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples—identifying the set of characters and objects in the story.', 'The graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph.'], 'highlighted_evidence': ['The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the “next to” and “has” edges respectively in Figure FIGREF4.', 'We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.', 'The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.', 'The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.\n\nThe next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation—or edge—at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions “What location can I visit from $x$?” and “Who/What is in $x$?”. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model.', 'We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.\n\nAs in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples—identifying the set of characters and objects in the story.\n\nThe graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph.']}]}]",conf/aiide/AmmanabroluCTBR20,Bringing Stories Alive: Generating Interactive Fiction Worlds.,2020,"We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales.","(0.8338502645492554, 0.80965256690979, 0.8215732574462891)",7.222194841772823e-234,0.04315860942006111
57,Generating Classical Chinese Poems from Vernacular Chinese,What are some guidelines in writing input vernacular so model can generate ,guidelines on how to write the input vernacular to generate better poems.,1909.00279,"[{'question': 'What are some guidelines in writing input vernacular so model can generate ', 'question_id': '6b9310b577c6232e3614a1612cbbbb17067b3886', 'answers': [{'unanswerable': False, 'extractive_spans': [' if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score', 'poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['1) In classical Chinese poems, poetic images UTF8gbsn(意象) were widely used to express emotions and to build artistic conception. A certain poetic image usually has some fixed implications. For example, autumn is usually used to imply sadness and loneliness. However, with the change of time, poetic images and their implications have also changed. According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score. As illustrated in Table TABREF12, both paragraph 2 and 3 are generated from pop song lyrics, paragraph 2 uses many poetic images from classical literature (e.g. pear flowers, makeup), while paragraph 3 uses modern poetic images (e.g. sparrows on the utility pole). Obviously, compared with poem 2, sentences in poem 3 seems more confusing, as the poetic images in modern times may not fit well into the language model of classical poems.', '2) We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs. For example, in Table TABREF12, both paragraph 4 (more descriptive) and paragraph 5 (more philosophical) were selected from famous modern prose. However, compared with poem 4, poem 5 seems semantically more confusing. We offer two explanations to the above phenomenon: i. Limited by the 28-character restriction, it is hard for quatrain poems to cover complex logical or philosophical explanation. ii. As vernacular paragraphs are more detailed and lengthy, some information in a vernacular paragraph may be lost when it is summarized into a classical poem. While losing some information may not change the general meaning of a descriptive paragraph, it could make a big difference in a logical or philosophical paragraph.'], 'highlighted_evidence': ['According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score.', 'We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs.']}]}, {'question': 'How much is proposed model better in perplexity and BLEU score than typical UMT models?', 'question_id': 'd484a71e23d128f146182dccc30001df35cdf93f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50.', 'evidence': ['As illustrated in Table TABREF12 (ID 1). Given the vernacular translation of each gold poem in test set, we generate five poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.', 'FLOAT SELECTED: Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set fluctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence.'], 'highlighted_evidence': ['We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.', 'FLOAT SELECTED: Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set fluctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence.']}]}, {'question': 'What dataset is used for training?', 'question_id': '5787ac3e80840fe4cf7bfae7e8983fa6644d6220', 'answers': [{'unanswerable': False, 'extractive_spans': ['We collected a corpus of poems and a corpus of vernacular literature from online resources'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set.'], 'highlighted_evidence': ['We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set.']}]}]",conf/emnlp/YangCFLFCY19,Generating Classical Chinese Poems from Vernacular Chinese.,2019,"According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score.","(0.8800505995750427, 0.863524317741394, 0.8717091679573059)",4.654437852936961e-232,0.6315149068832397
58,Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,What were the evaluation metrics?,"SLOR, WP-SLOR, ROUGE-LM",1909.06762,"[{'question': 'What were the evaluation metrics?', 'question_id': 'ee31c8a94e07b3207ca28caef3fbaf9a38d94964', 'answers': [{'unanswerable': False, 'extractive_spans': ['BLEU', 'Micro Entity F1', 'quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30.', 'We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response.'], 'highlighted_evidence': ['Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. ', 'We provide human evaluation on our framework and the compared models. ', 'We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5.']}]}, {'question': 'What were the baseline systems?', 'question_id': '66d743b735ba75589486e6af073e955b6bb9d2a4', 'answers': [{'unanswerable': False, 'extractive_spans': ['Attn seq2seq', 'Ptr-UNK', 'KV Net', 'Mem2Seq', 'DSR'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare our model with several baselines including:', 'Attn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.', 'Ptr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.', 'KV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.', 'Mem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.', 'DSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding.'], 'highlighted_evidence': ['We compare our model with several baselines including:\n\nAttn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.\n\nPtr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.\n\nKV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.\n\nMem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.\n\nDSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding.']}]}, {'question': 'Which dialog datasets did they experiment with?', 'question_id': 'b9f852256113ef468d60e95912800fab604966f6', 'answers': [{'unanswerable': False, 'extractive_spans': ['Camrest', 'InCar Assistant'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Since dialogue dataset is not typically annotated with the retrieval results, training the KB-retriever is non-trivial. To make the training feasible, we propose two methods: 1) we use a set of heuristics to derive the training data and train the retriever in a distant supervised fashion; 2) we use Gumbel-Softmax BIBREF14 as an approximation of the non-differentiable selecting process and train the retriever along with the Seq2Seq dialogue generation model. Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever. Both the retrievers trained with distant-supervision and Gumbel-Softmax technique outperform the compared systems in the automatic and human evaluations. Analysis empirically verifies our assumption that more than 80% responses in the dataset can be supported by a single KB row and better retrieval results lead to better task-oriented dialogue generation performance.'], 'highlighted_evidence': ['Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever.']}]}, {'question': 'What KB is used?', 'question_id': '88f8ab2a417eae497338514142ac12c3cec20876', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/emnlp/QinLCWLL19,Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.,2019,"Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. ","(0.814288854598999, 0.8204010128974915, 0.8173335194587708)",0.0,0.05484510585665703
59,Can neural networks understand monotonicity reasoning?,Do they release MED?,No,1906.06448,"[{'question': 'Do they release MED?', 'question_id': 'c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section ""Dataset"" ). Compared with manual or automatic construction, we can collect naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning.'], 'highlighted_evidence': ['To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section ""Dataset"" ).']}]}, {'question': 'What NLI models do they analyze?', 'question_id': 'dfc393ba10ec4af5a17e5957fcbafdffdb1a6443', 'answers': [{'unanswerable': False, 'extractive_spans': ['BiMPM', 'ESIM', 'Decomposable Attention Model', 'KIM', 'BERT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment.'], 'highlighted_evidence': ['To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI.']}]}, {'question': 'How do they define upward and downward reasoning?', 'question_id': '311a7fa62721e82265f4e0689b4adc05f6b74215', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.', 'evidence': ['A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where French dinner is replaced by a more general concept dinner. On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where workers is replaced by a more specific concept new workers. Interestingly, the direction of monotonicity can be reversed again by embedding yet another downward entailing context (e.g., not in ( ""Introduction"" )), as witness the fact that ( ""Introduction"" ) entails ( ""Introduction"" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure.', 'All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner'], 'highlighted_evidence': ['A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where French dinner is replaced by a more general concept dinner. ', 'On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where workers is replaced by a more specific concept new workers.', 'All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner']}]}, {'question': 'What is monotonicity reasoning?', 'question_id': '82bcacad668351c0f81bd841becb2dbf115f000e', 'answers': [{'unanswerable': False, 'extractive_spans': ['a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in ( ""Introduction"" ) and ( ""Introduction"" ).'], 'highlighted_evidence': ['Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures.']}]}]",conf/blackboxnlp/YanakaMBISAB19,Can Neural Networks Understand Monotonicity Reasoning?,2019,"To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section ""Dataset"" ).","(0.8171524405479431, 0.7756631374359131, 0.795867383480072)",0.0,0.05066651105880737
60,Synchronising audio and ultrasound by learning cross-modal embeddings,Do they compare their neural network against any other model?,No.,1907.00758,"[{'question': 'Do they compare their neural network against any other model?', 'question_id': '73d657d6faed0c11c65b1ab60e553db57f4971ca', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do they annotate their own dataset or use an existing one?', 'question_id': '9ef182b61461d0d8b6feb1d6174796ccde290a15', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Use an existing one', 'evidence': ['For our experiments, we select a dataset whose utterances have been correctly synchronised at recording time. This allows us to control how the model is trained and verify its performance using ground truth synchronisation offsets. We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). See BIBREF15 for details.'], 'highlighted_evidence': ['We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). ']}]}, {'question': 'Does their neural network predict a single offset in a recording?', 'question_id': 'f6f8054f327a2c084a73faca16cf24a180c094ae', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. The candidate set is independent of the model, and is chosen based on task knowledge (Section SECREF5 ).'], 'highlighted_evidence': ['Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. ']}]}, {'question': 'What kind of neural network architecture do they use?', 'question_id': 'b8f711179a468fec9a0d8a961fb0f51894af4b31', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'CNN', 'evidence': ['We adopt the approach in BIBREF4 , modifying it to synchronise audio with UTI data. Our model, UltraSync, consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise BIBREF21 , BIBREF22 . This model can be viewed as an extension of a siamese neural network BIBREF23 but with two asymmetrical streams and no shared parameters. Figure FIGREF1 illustrates the main architecture. The visual data INLINEFORM0 (ultrasound) and audio data INLINEFORM1 (MFCC), which have different shapes, are mapped to low dimensional embeddings INLINEFORM2 (visual) and INLINEFORM3 (audio) of the same size: DISPLAYFORM0', ""FLOAT SELECTED: Figure 1: UltraSync maps high dimensional inputs to low dimensional vectors using a contrastive loss function, such that the Euclidean distance is small between vectors from positive pairs and large otherwise. Inputs span '200ms: 5 consecutive raw ultrasound frames on one stream and 20 frames of the corresponding MFCC features on the other.""], 'highlighted_evidence': ['Figure FIGREF1 illustrates the main architecture. ', ""FLOAT SELECTED: Figure 1: UltraSync maps high dimensional inputs to low dimensional vectors using a contrastive loss function, such that the Euclidean distance is small between vectors from positive pairs and large otherwise. Inputs span '200ms: 5 consecutive raw ultrasound frames on one stream and 20 frames of the corresponding MFCC features on the other.""]}]}]",conf/interspeech/EshkyRRR19,Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings.,2019,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
61,wav2vec: Unsupervised Pre-training for Speech Recognition,Which unlabeled data do they pretrain with?,They don't mention unlabeled data.,1904.05862,"[{'question': 'Which unlabeled data do they pretrain with?', 'question_id': 'ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '1000 hours of WSJ audio data', 'evidence': ['We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ). For the pre-training experiments we feed the output of the context network to the acoustic model, instead of log-mel filterbank features.', 'Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2. On the TIMIT task, pre-training enables us to match the best reported result in the literature. In a simulated low-resource setup with only eight hours of transcriped audio data, reduces WER by up to 32% compared to a baseline model that relies on labeled data only (§ SECREF3 & § SECREF4 ).'], 'highlighted_evidence': ['We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ). ', 'Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2. ']}]}, {'question': 'How many convolutional layers does their model have?', 'question_id': '12eaaf3b6ebc51846448c6e1ad210dbef7d25a96', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'wav2vec has 12 convolutional layers', 'evidence': ['Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 . Alternatively, one could use other architectures such as the trainable frontend of BIBREF24 amongst others. The encoder layers have kernel sizes INLINEFORM2 and strides INLINEFORM3 . The output of the encoder is a low frequency feature representation INLINEFORM4 which encodes about 30ms of 16KHz of audio and the striding results in representation INLINEFORM5 every 10ms.', 'Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. The total receptive field of the context network is about 180ms.'], 'highlighted_evidence': ['Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 .', 'Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. ']}]}, {'question': 'Do they explore how much traning data is needed for which magnitude of improvement for WER? ', 'question_id': '828615a874512844ede9d7f7d92bdc48bb48b18d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance.'], 'highlighted_evidence': ['What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance.']}]}]",conf/interspeech/SchneiderBCA19,wav2vec: Unsupervised Pre-Training for Speech Recognition.,2019,1000 hours of WSJ audio data,"(0.8545066714286804, 0.843267560005188, 0.8488499522209167)",0.0,0.19743239879608154
62,"Cross-lingual, Character-Level Neural Morphological Tagging",How are character representations from various languages joint?,They are concatenated.,1708.09157,"[{'question': 'How are character representations from various languages joint?', 'question_id': 'a43c400ae37a8705ff2effb4828f4b0b177a74c4', 'answers': [{'unanswerable': False, 'extractive_spans': ['shared character embeddings for taggers in both languages together through optimization of a joint loss function'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our formulation of transfer learning builds on work in multi-task learning BIBREF15 , BIBREF9 . We treat each individual language as a task and train a joint model for all the tasks. We first discuss the current state of the art in morphological tagging: a character-level recurrent neural network. After that, we explore three augmentations to the architecture that allow for the transfer learning scenario. All of our proposals force the embedding of the characters for both the source and the target language to share the same vector space, but involve different mechanisms, by which the model may learn language-specific features.', 'Cross-lingual morphological tagging may be formulated as a multi-task learning problem. We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one. The first loss function we consider is the following:'], 'highlighted_evidence': ['We treat each individual language as a task and train a joint model for all the tasks.', 'We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one.']}]}, {'question': 'On which dataset is the experiment conducted?', 'question_id': '4056ee2fd7a0a0f444275e627bb881134a1c2a10', 'answers': [{'unanswerable': False, 'extractive_spans': ['We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . We list the size of the training, development and test splits of the UD treebanks we used in tab:lang-size. Also, we list the number of unique morphological tags in each language in tab:num-tags, which serves as an approximate measure of the morphological complexity each language exhibits. Crucially, the data are annotated in a cross-linguistically consistent manner, such that words in the different languages that have the same syntacto-semantic function have the same bundle of tags (see sec:morpho-tagging for a discussion). Potentially, further gains would be possible by using a more universal scheme, e.g., the UniMorph scheme.'], 'highlighted_evidence': ['We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 .']}]}]",conf/emnlp/CotterellH17,Cross-lingual Character-Level Neural Morphological Tagging.,2017,We treat each individual language as a task and train a joint model for all the tasks.,"(0.8524788022041321, 0.8453370332717896, 0.848892867565155)",0.0,0.22038625180721283
63,Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,Do they train their own RE model?,No,1911.00069,"[{'question': 'Do they train their own RE model?', 'question_id': 'f6496b8d09911cdf3a9b72aec0b0be6232a6dba1', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.', 'We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.', 'We apply model ensemble to further improve the accuracy of the Bi-LSTM model. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. This Ensemble approach improves the single model by 0.6-1.9 $F_1$ points, except for Arabic.'], 'highlighted_evidence': ['We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.\n\nWe learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.', 'We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models.']}]}, {'question': 'How big are the datasets?', 'question_id': '5c90e1ed208911dbcae7e760a553e912f8c237a5', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents', 'evidence': ['FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.', 'Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.', 'Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).']}]}, {'question': 'What languages do they experiment on?', 'question_id': '3c3b4797e2b21e2c31cf117ad9e52f327721790f', 'answers': [{'unanswerable': False, 'extractive_spans': ['English, German, Spanish, Italian, Japanese and Portuguese', ' English, Arabic and Chinese'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).'], 'highlighted_evidence': ['Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. ', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).']}]}, {'question': 'What datasets are used?', 'question_id': 'a7d72f308444616a0befc8db7ad388b3216e2143', 'answers': [{'unanswerable': False, 'extractive_spans': ['in-house dataset', 'ACE05 dataset '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).', 'In this section, we evaluate the performance of the proposed cross-lingual RE approach on both in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11.'], 'highlighted_evidence': ['Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).', 'The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).', 'the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11.']}]}]",conf/emnlp/NiF19,Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.,2019,"We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.

We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.","(0.8686315417289734, 0.7605305314064026, 0.810994565486908)",0.0,0.07840605825185776
64,Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,Did the collection process use a WoZ method?,No.,1810.00663,"[{'question': 'Did the collection process use a WoZ method?', 'question_id': 'aa800b424db77e634e82680f804894bfa37f2a34', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.', 'While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.'], 'highlighted_evidence': ['This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.', 'While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.']}]}, {'question': 'By how much did their model outperform the baseline?', 'question_id': 'fbd47705262bfa0a2ba1440a2589152def64cbbd', 'answers': [{'unanswerable': False, 'extractive_spans': ['increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively', 'over INLINEFORM0 increase in EM and GM between our model and the next best two models'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.', 'Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models.'], 'highlighted_evidence': ['First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.', 'Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models.']}]}, {'question': 'What baselines did they compare their model with?', 'question_id': '51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search', 'evidence': ['The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path.'], 'highlighted_evidence': ['The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path.']}]}, {'question': 'What was the performance of their model?', 'question_id': '3aee5c856e0ee608a7664289ffdd11455d153234', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81', 'evidence': ['FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better.']}]}, {'question': 'What evaluation metrics are used?', 'question_id': 'f42d470384ca63a8e106c7caf1cb59c7b92dbc27', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'exact match, f1 score, edit distance and goal match', 'evidence': ['We compare the performance of translation approaches based on four metrics:', '[align=left,leftmargin=0em,labelsep=0.4em,font=]', 'As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.', 'The harmonic average of the precision and recall over all the test set BIBREF26 .', 'The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .', 'GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0.'], 'highlighted_evidence': ['We compare the performance of translation approaches based on four metrics:\n\n[align=left,leftmargin=0em,labelsep=0.4em,font=]\n\nAs in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.\n\nThe harmonic average of the precision and recall over all the test set BIBREF26 .\n\nThe minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .\n\nGM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0.']}]}, {'question': 'Did the authors use a crowdsourcing platform?', 'question_id': '29bdd1fb20d013b23b3962a065de3a564b14f0fb', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.'], 'highlighted_evidence': ['This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. ']}]}, {'question': 'How were the navigation instructions collected?', 'question_id': '25b2ae2d86b74ea69b09c140a41593c00c47a82b', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'using Amazon Mechanical Turk using simulated environments with topological maps', 'evidence': ['This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.', 'We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.', 'As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:', 'While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.'], 'highlighted_evidence': ['This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. ', 'We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.\n\nAs shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:\n\nWhile the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.\n\n']}]}, {'question': 'What language is the experiment done in?', 'question_id': 'fd7f13b63f6ba674f5d5447b6114a201fe3137cb', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'english language', 'evidence': ['While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.'], 'highlighted_evidence': ['While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.']}]}]",conf/emnlp/ZangPVCNSS18,Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.,2018,"This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.","(0.9134286046028137, 0.806508481502533, 0.8566451668739319)",0.0,0.08865498751401901
65,Analysis of Risk Factor Domains in Psychosis Patient Health Records,What additional features are proposed for future work?,None.,1809.05752,"[{'question': 'What additional features are proposed for future work?', 'question_id': 'c82e945b43b2e61c8ea567727e239662309e9508', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort', 'evidence': ['Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time.'], 'highlighted_evidence': ['Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time.']}]}, {'question': 'What are their initial results on this task?', 'question_id': 'fbee81a9d90ff23603ee4f5986f9e8c0eb035b52', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.', 'evidence': ['FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs.']}]}, {'question': 'What datasets did the authors use?', 'question_id': '39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8', 'answers': [{'unanswerable': False, 'extractive_spans': [' a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital', 'an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data.', 'These patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort.', ""We also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction.""], 'highlighted_evidence': [""Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data.\n\nThese patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort.\n\nWe also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction.""]}]}]",journals/biomedsem/HoldernessMCBMP19,Analysis of risk factor domains in psychosis patient health records.,2019,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"(0.9303202629089355, 0.8102990388870239, 0.8661717176437378)",0.0,0.06789302825927734
66,Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks,Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?,No.,1910.10324,"[{'question': 'Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?', 'question_id': '55c840a2f1f663ab2bff984ae71501b17429d0c0', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How many layers do they use in their best performing network?', 'question_id': 'fa5357c56ea80a21a7ca88a80f21711c5431042c', 'answers': [{'unanswerable': False, 'extractive_spans': ['36'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF20 shows results for Librispeech with SpecAugment. We test both CTC and CE/hybrid systems. There are consistent gains first from iterated loss, and then from multiple feature presentation. We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results. This shows that our proposed methods can improve even very deep models.'], 'highlighted_evidence': ['We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results. ']}]}, {'question': 'Do they just sum up all the loses the calculate to end up with one single loss?', 'question_id': '35915166ab2fd3d39c0297c427d4ac00e8083066', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We have found it beneficial to apply the loss function at several intermediate layers of the network. Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as', 'where $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. The coefficient $\\lambda $ scales the auxiliary loss and we set $\\lambda = 0.3$ based on our preliminary experiments. We illustrate the auxiliary prediction and loss in Figure FIGREF6.'], 'highlighted_evidence': ['Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as\n\nwhere $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. T']}]}, {'question': 'Does their model take more time to train than regular transformer models?', 'question_id': 'e6c872fea474ea96ca2553f7e9d5875df4ef55cd', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/icassp/TjandraLZZWS0Z20,DEJA-VU: Double Feature Presentation and Iterated Loss in Deep Transformer Networks.,2020,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
67,Dense Information Flow for Neural Machine Translation,what are the baselines?,HELP,1806.00722,"[{'question': 'what are the baselines?', 'question_id': '26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98', 'answers': [{'unanswerable': False, 'extractive_spans': [' 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default. As a comparison, we design a densely connected model with same number of layers, but the hidden size is set as 128 in order to keep the model size consistent. The models adopting DenseAtt-1, DenseAtt-2 are named as DenseNMT-4L-1 and DenseNMT-4L-2 respectively. In order to check the effect of dense connections on deeper models, we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96.'], 'highlighted_evidence': ['As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default.']}]}, {'question': 'did they outperform previous methods?', 'question_id': '8c0621016e96d86a7063cb0c9ec20c76a2dba678', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well.'], 'highlighted_evidence': [' In almost all genres, DenseNMT models are significantly better than the baselines.']}]}, {'question': 'what language pairs are explored?', 'question_id': 'f1214a05cc0e6d870c789aed24a8d4c768e1db2f', 'answers': [{'unanswerable': False, 'extractive_spans': ['German-English', 'Turkish-English', 'English-German'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.'], 'highlighted_evidence': ['We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.']}]}, {'question': 'what datasets were used?', 'question_id': '41d3ab045ef8e52e4bbe5418096551a22c5e9c43', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German', 'evidence': ['We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.'], 'highlighted_evidence': ['We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.']}]}]",conf/naacl/ShenTHQL18,Dense Information Flow for Neural Machine Translation.,2018,"As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default.","(0.8208191990852356, 0.759518027305603, 0.7889796495437622)",0.0,0.06632112711668015
68,Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,Does this approach perform better in the multi-domain or single-domain setting?,No information is provided to determine whether this approach performs better in the multi-domain or single-domain setting.,1909.00754,"[{'question': 'Does this approach perform better in the multi-domain or single-domain setting?', 'question_id': 'ed7a3e7fc1672f85a768613e7d1b419475950ab4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'single-domain setting', 'evidence': ['FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018).'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018).']}]}, {'question': 'What are the performance metrics used?', 'question_id': '72ceeb58e783e3981055c70a3483ea706511fac3', 'answers': [{'unanswerable': False, 'extractive_spans': ['joint goal accuracy'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement.'], 'highlighted_evidence': ['As a convention, the metric of joint goal accuracy is used to compare our model to previous work.']}]}, {'question': 'Which datasets are used to evaluate performance?', 'question_id': '9bfa46ad55136f2a365e090ce585fc012495393c', 'answers': [{'unanswerable': False, 'extractive_spans': ['the single domain dataset, WoZ2.0 ', 'the multi-domain dataset, MultiWoZ'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 .'], 'highlighted_evidence': ['We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. ']}]}]",conf/emnlp/RenNM19,Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.,2019,single-domain setting,"(0.8432014584541321, 0.8947414755821228, 0.8682072758674622)",8.972141065609098e-232,0.6166872978210449
69,Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,Why does not the approach from English work on other languages?,Not mentioned in the snippet.,1906.04571,"[{'question': 'Why does not the approach from English work on other languages?', 'question_id': 'f7817b949605fb04b1e4fec9dd9ca8804fb92ae9', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Because, unlike other languages, English does not mark grammatical genders', 'evidence': ['To date, the NLP community has focused primarily on approaches for detecting and mitigating gender stereotypes in English BIBREF5 , BIBREF6 , BIBREF7 . Yet, gender stereotypes also exist in other languages because they are a function of society, not of grammar. Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . In these languages, the words in a sentence are marked with morphological endings that reflect the grammatical gender of the surrounding nouns. This means that if the gender of one word changes, the others have to be updated to match. As a result, simple heuristics, such as augmenting a corpus with additional sentences in which he and she have been swapped BIBREF9 , will yield ungrammatical sentences. Consider the Spanish phrase el ingeniero experto (the skilled engineer). Replacing ingeniero with ingeniera is insufficient—el must also be replaced with la and experto with experta.'], 'highlighted_evidence': ['Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 .']}]}, {'question': 'How do they measure grammaticality?', 'question_id': '8255f74cae1352e5acb2144fb857758dda69be02', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by calculating log ratio of grammatical phrase over ungrammatical phrase', 'evidence': ['Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0'], 'highlighted_evidence': ['Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase):']}]}, {'question': 'Which model do they use to convert between masculine-inflected and feminine-inflected sentences?', 'question_id': 'db62d5d83ec187063b57425affe73fef8733dd28', 'answers': [{'unanswerable': False, 'extractive_spans': ['Markov random field with an optional neural parameterization'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns. To the best of our knowledge, this task has not been studied previously. As a result, there is no existing annotated corpus of paired sentences that can be used as “ground truth.” Despite this limitation, we evaluated our approach both intrinsically and extrinsically, achieving promising results. For example, we demonstrated that our approach reduces gender stereotyping in neural language models. Finally, we also identified avenues for future work, such as the inclusion of co-reference information.'], 'highlighted_evidence': ['We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns.']}]}]",conf/acl/ZmigrodMWC19,Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology.,2019,"Because, unlike other languages, English does not mark grammatical genders","(0.8665413856506348, 0.8214930295944214, 0.843416154384613)",0.0,0.08915954828262329
70,Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,What is the performance achieved by the model described in the paper?,The model correlates better with human judgments than traditional word-overlap metrics.,1909.04625,"[{'question': 'What is the performance achieved by the model described in the paper?', 'question_id': '946676f1a836ea2d6fe98cb4cfc26b9f4f81984d', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the best performance achieved by supervised models?', 'question_id': '3b090b416c4ad7d9b5b05df10c5e7770a4590f6a', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the size of the datasets employed?', 'question_id': 'a1e07c7563ad038ee2a7de5093ea08efdd6077d4', 'answers': [{'unanswerable': False, 'extractive_spans': ['(about 4 million sentences, 138 million word tokens)', 'one trained on the Billion Word benchmark'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256."", ""We also compare LSTM language models trained on large corpora. We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15. We set the size of the input embeddings and hidden layers to 400 for the LSTM (frWaC) model since it is trained on a large dataset.""], 'highlighted_evidence': [""The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13."", ""We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15.""]}]}, {'question': 'What are the baseline models?', 'question_id': 'a1c4f9e8661d4d488b8684f055e0ee0e2275f767', 'answers': [{'unanswerable': False, 'extractive_spans': ['Recurrent Neural Network (RNN)', 'ActionLSTM', 'Generative Recurrent Neural Network Grammars (RNNG)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models', ""are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256."", 'Methods ::: Models Tested ::: ActionLSTM', 'models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.', 'Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)', 'jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18.'], 'highlighted_evidence': [' Recurrent Neural Network (RNN) Language Models\nare trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10.', 'ActionLSTM\nmodels the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16.', 'Generative Recurrent Neural Network Grammars (RNNG)\njointly model the word sequence as well as the underlying syntactic structure BIBREF18.']}]}]",conf/emnlp/AnQWL19,Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.,2019,,"(0.0, 0.0, 0.0)",0.0,0.05423958599567413
71,Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation,What evaluation metrics are used?,"SLOR, WP-SLOR, ROUGE-LM, and traditional word-overlap metrics.",1809.07629,"[{'question': 'What evaluation metrics are used?', 'question_id': 'c5171daf82107fce0f285fa18f19e91fbd1215c5', 'answers': [{'unanswerable': False, 'extractive_spans': ['the evaluation metrics include BLEU and ROUGE (1, 2, L) scores'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The probability of activating inter-layer and inner-layer teacher forcing is set to 0.5, the probability of teacher forcing is attenuated every epoch, and the decaying ratio is 0.9. The models are trained for 20 training epochs without early stop; when curriculum learning is applied, only the first layer is trained during first five epochs, the second decoder layer starts to be trained at the sixth epoch, and so on. To evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 .'], 'highlighted_evidence': ['o evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 .']}]}, {'question': 'What datasets did they use?', 'question_id': 'baeb6785077931e842079e9d0c9c9040947ffa4e', 'answers': [{'unanswerable': False, 'extractive_spans': ['The E2E NLG challenge dataset BIBREF21'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set. As shown in Figure FIGREF2 , the inputs are semantic frames containing specific slots and corresponding values, and the outputs are the associated natural language utterances with the given semantics. For example, a semantic frame with the slot-value pairs “name[Bibimbap House], food[English], priceRange[moderate], area [riverside], near [Clare Hall]” corresponds to the target sentence “Bibimbap House is a moderately priced restaurant who's main cuisine is English food. You will find this local gem near Clare Hall in the Riverside area.”.""], 'highlighted_evidence': ['The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set. ']}]}]",conf/slt/SuC18,Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation.,2018,"o evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 .","(0.8513996005058289, 0.8330848217010498, 0.8421425819396973)",4.834018398298465e-233,0.40178683400154114
72,Gender Bias in Coreference Resolution,Which coreference resolution systems are tested?,None.,1804.09301,"[{'question': 'Which coreference resolution systems are tested?', 'question_id': 'c2553166463b7b5ae4d9786f0446eb06a90af458', 'answers': [{'unanswerable': False, 'extractive_spans': ['the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.', 'We evaluate examples of each of the three coreference system architectures described in ""Coreference Systems"" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).'], 'highlighted_evidence': ['In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.', 'We evaluate examples of each of the three coreference system architectures described in ""Coreference Systems"" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).']}]}]",conf/cogsci/LiorS23,Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender Bias Evaluation in Coreference Resolution.,2023,"In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.","(0.9191177487373352, 0.7927836775779724, 0.8512891530990601)",0.0,0.09444716572761536
73,A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis,How big is the improvement over the state-of-the-art results?,9.4 BLEU (7.2 →16.6),1909.00324,"[{'question': 'How big is the improvement over the state-of-the-art results?', 'question_id': '1763a029daca7cab10f18634aba02a6bd1b6faa7', 'answers': [{'unanswerable': False, 'extractive_spans': ['AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset', 'Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets', 'In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task', 'We present the overall performance of our model and baseline models in Table TABREF27. Results show that our AGDT outperforms all baseline models on both “restaurant-14” and “restaurant-large” datasets. ATAE-LSTM employs an aspect-weakly associative encoder to generate the aspect-specific sentence representation by simply concatenating the aspect, which is insufficient to exploit the given aspect. Although GCAE incorporates the gating mechanism to control the sentiment information flow according to the given aspect, the information flow is generated by an aspect-independent encoder. Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction. Our model benefits from the following aspects. First, our AGDT utilizes an aspect-guided encoder, which leverages the given aspect to guide the sentence encoding from scratch and generates the aspect-guided representation. Second, the AGDT guarantees that the aspect-specific information has been fully embedded in the sentence representation via reconstructing the given aspect. Third, the given aspect embedding is concatenated on the aspect-guided sentence representation for final predictions.', 'The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets. This indicates that the given aspect information is very pivotal to the accurate sentiment prediction, especially when the sentence has different sentiment labels, which is consistent with existing work BIBREF2, BIBREF3, BIBREF4. Those results demonstrate the effectiveness of our model and suggest that our AGDT has better ability to distinguish the different sentiments of multiple aspects compared to GCAE.', 'Experiments ::: Main Results and Analysis ::: Aspect-Term Sentiment Analysis Task', 'In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets.'], 'highlighted_evidence': ['Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task', 'Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction.', 'The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets.', 'Experiments ::: Main Results and Analysis ::: Aspect-Term Sentiment Analysis Task', 'In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets.']}]}, {'question': 'Is the model evaluated against other Aspect-Based models?', 'question_id': 'f9de9ddea0c70630b360167354004ab8cbfff041', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Experiments ::: Baselines', 'To comprehensively evaluate our AGDT, we compare the AGDT with several competitive models.'], 'highlighted_evidence': ['Experiments ::: Baselines\nTo comprehensively evaluate our AGDT, we compare the AGDT with several competitive models.']}]}]",conf/emnlp/LiangMZXCZ19,A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.,2019,Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task,"(0.8315255641937256, 0.8128830194473267, 0.8220986127853394)",0.0,0.06699055433273315
74,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,Is the baseline a non-heirarchical model like BERT?,No.,1905.06566,"[{'question': 'Is the baseline a non-heirarchical model like BERT?', 'question_id': 'fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'There were hierarchical and non-hierarchical baselines; BERT was one of those baselines', 'evidence': ['FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).', 'Our main results on the CNNDM dataset are shown in Table 1 , with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage BIBREF9 , Abstract-ML+RL BIBREF10 and DCA BIBREF42 are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite BIBREF26 and InconsisLoss BIBREF25 all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summarization (or compression). Bottom-Up BIBREF27 generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; BIBREF3 and NeuSum; BIBREF11 ). They have been extended with reinforcement learning (Refresh; BIBREF4 and BanditSum; BIBREF20 ), Maximal Marginal Relevance (NeuSum-MMR; BIBREF21 ), latent variable modeling (LatentSum; BIBREF5 ) and syntactic compression (JECS; BIBREF38 ). Lead3 is a baseline which simply selects the first three sentences. Our model $\\text{\\sc Hibert}_S$ (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences between them are all significant with a 0.95 confidence interval (estimated with the ROUGE script). Note that pre-training $\\text{\\sc Hibert}_S$ (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages ( $\\text{\\sc Hibert}_S$ ) or larger size ( $\\text{\\sc Hibert}_M$ ) perform even better and $\\text{\\sc Hibert}_M$ outperforms BERT by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in ""Extractive Summarization"" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section ""Pre-training"" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of 1. The positional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model $\\text{\\sc Hibert}_S$1 outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters ( $\\text{\\sc Hibert}_S$2 54.6M v.s. BERT 110M). Results on the NYT50 dataset show the similar trends (see Table 2 ). EXTRACTION is a extractive model based hierarchical LSTM and we use the numbers reported by xu:2019:arxiv. The improvement of $\\text{\\sc Hibert}_S$3 over the baseline without pre-training (HeriTransformer) becomes 2.0 ROUGE. $\\text{\\sc Hibert}_S$4 (in-domain), $\\text{\\sc Hibert}_S$5 (in-domain), $\\text{\\sc Hibert}_S$6 and $\\text{\\sc Hibert}_S$7 all outperform BERT significantly according to the ROUGE script.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).', 'We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in ""Extractive Summarization"" ) without pre-training.']}]}]",conf/acl/ZhangWZ19,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization.,2019,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"(0.9334160089492798, 0.8172451257705688, 0.8714761137962341)",0.0,0.06876198947429657
75,The Role of Pragmatic and Discourse Context in Determining Argument Impact,How better are results compared to baseline models?,"StyleEQ model has a classification accuracy above the base rate from the test data, which is 75%.",2004.03034,"[{'question': 'How better are results compared to baseline models?', 'question_id': 'ca26cfcc755f9d0641db0e4d88b4109b903dbb26', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61."", 'evidence': ['We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\\%$) than distance from the thesis and linguistic features. Claims with higher impact parents are more likely to be have higher impact. Similarity with the parent and thesis is not significantly better than the majority baseline. Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline.', 'We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\\%$).'], 'highlighted_evidence': ['We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\\%$) than distance from the thesis and linguistic features.', 'Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline.', 'We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\\%$).']}]}, {'question': 'What models that rely only on claim-specific linguistic features are used as baselines?', 'question_id': '6cdd61ebf84aa742155f4554456cc3233b6ae2bf', 'answers': [{'unanswerable': False, 'extractive_spans': ['SVM with RBF kernel'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Similar to BIBREF9, we experiment with SVM with RBF kernel, with features that represent (1) the simple characteristics of the argument tree and (2) the linguistic characteristics of the claim.'], 'highlighted_evidence': ['Similar to BIBREF9, we experiment with SVM with RBF kernel, with features that represent (1) the simple characteristics of the argument tree and (2) the linguistic characteristics of the claim.']}]}, {'question': 'How is pargmative and discourse context added to the dataset?', 'question_id': '8e8097cada29d89ca07166641c725e0f8fed6676', 'answers': [{'unanswerable': False, 'extractive_spans': ['While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Claims and impact votes. We collected 47,219 claims from kialo.com for 741 controversial topics and their corresponding impact votes. Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented.'], 'highlighted_evidence': ['Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented.']}]}, {'question': 'What annotations are available in the dataset?', 'question_id': '951098f0b7169447695b47c142384f278f451a1e', 'answers': [{'unanswerable': False, 'extractive_spans': ['5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Claims and impact votes. We collected 47,219 claims from kialo.com for 741 controversial topics and their corresponding impact votes. Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented.'], 'highlighted_evidence': [' Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.']}]}]",journals/corr/abs-2004-03034,The Role of Pragmatic and Discourse Context in Determining Argument Impact.,2020,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"(0.852946937084198, 0.8346396684646606, 0.8436940312385559)",8.432700096708783e-232,0.32320356369018555
76,Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What was their result on Stance Sentiment Emotion Corpus?,Their results are not mentioned in the snippet.,1911.12569,"[{'question': 'What was their result on Stance Sentiment Emotion Corpus?', 'question_id': '3e839783d8a4f2fe50ece4a9b476546f0842b193', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'F1 score of 66.66%', 'evidence': ['FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.', 'We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.', 'We implement our model in Python using Tensorflow on a single GPU. We experiment with six different BiLSTM based architectures. The three architectures correspond to BiLSTM based systems without primary attention i.e. only with secondary attention for sentiment analysis (S1), emotion analysis (E1) and the multi-task system (M1) for joint sentiment and emotion analysis. The remaining three architectures correspond to the systems for sentiment analysis (S2), emotion analysis (E2) and multi-task system (M2), with both primary and secondary attention. The weight matrices were initialized randomly using numbers form a truncated normal distribution. The batch size was 64 and the dropout BIBREF34 was 0.6 with the Adam optimizer BIBREF35. The hidden state vectors of both the forward and backward LSTM were 300-dimensional, whereas the context vector was 150-dimensional. Relu BIBREF36 was used as the activation for the hidden layers, whereas in the output layer we used sigmoid as the activation function. Sigmoid cross-entropy was used as the loss function. F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. Therefore, we report the F1-score for sentiment and precision, recall and F1-score for emotion analysis.'], 'highlighted_evidence': ['FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.', 'We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.', 'F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. ']}]}, {'question': 'What performance did they obtain on the SemEval dataset?', 'question_id': '2869d19e54fb554fcf1d6888e526135803bb7d75', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'F1 score of 82.10%', 'evidence': ['We implement our model in Python using Tensorflow on a single GPU. We experiment with six different BiLSTM based architectures. The three architectures correspond to BiLSTM based systems without primary attention i.e. only with secondary attention for sentiment analysis (S1), emotion analysis (E1) and the multi-task system (M1) for joint sentiment and emotion analysis. The remaining three architectures correspond to the systems for sentiment analysis (S2), emotion analysis (E2) and multi-task system (M2), with both primary and secondary attention. The weight matrices were initialized randomly using numbers form a truncated normal distribution. The batch size was 64 and the dropout BIBREF34 was 0.6 with the Adam optimizer BIBREF35. The hidden state vectors of both the forward and backward LSTM were 300-dimensional, whereas the context vector was 150-dimensional. Relu BIBREF36 was used as the activation for the hidden layers, whereas in the output layer we used sigmoid as the activation function. Sigmoid cross-entropy was used as the loss function. F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. Therefore, we report the F1-score for sentiment and precision, recall and F1-score for emotion analysis.', 'We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.', 'FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.'], 'highlighted_evidence': ['F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. ', 'We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.', 'FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.']}]}, {'question': 'What are the state-of-the-art systems?', 'question_id': '894c086a2cbfe64aa094c1edabbb1932a3d7c38a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN', 'evidence': ['FLOAT SELECTED: TABLE III COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS OF SEMEVAL 2016 TASK 6 ON SENTIMENT DATASET.', 'FLOAT SELECTED: TABLE IV COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS PROPOSED BY [16] ON EMOTION DATASET. THE METRICS P, R AND F STAND FOR PRECISION, RECALL AND F1-SCORE.', 'Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.', 'We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise.'], 'highlighted_evidence': ['FLOAT SELECTED: TABLE III COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS OF SEMEVAL 2016 TASK 6 ON SENTIMENT DATASET.', 'FLOAT SELECTED: TABLE IV COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS PROPOSED BY [16] ON EMOTION DATASET. THE METRICS P, R AND F STAND FOR PRECISION, RECALL AND F1-SCORE.', 'Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset.', 'We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22']}]}, {'question': 'How is multi-tasking performed?', 'question_id': '722e9b6f55971b4c48a60f7a9fe37372f5bf3742', 'answers': [{'unanswerable': False, 'extractive_spans': ['The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks.', 'Each of the shared representations is then fed to the primary attention mechanism'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We propose a novel two-layered multi-task attention based neural network for sentiment analysis where emotion analysis is utilized to improve its efficiency. Figure FIGREF1 illustrates the overall architecture of the proposed multi-task system. The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. The BiLSTM encodes the word representation of each word. This representation is shared between the subsystems of sentiment and emotion analysis. Each of the shared representations is then fed to the primary attention mechanism of both the subsystems. The primary attention mechanism finds the best representation for each word for each task. The secondary attention mechanism acts on top of the primary attention to extract the best sentence representation by focusing on the suitable context for each task. Finally, the representations of both the tasks are fed to two different feed-forward neural networks to produce two outputs - one for sentiment analysis and one for emotion analysis. Each component is explained in the subsequent subsections.'], 'highlighted_evidence': ['The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. The BiLSTM encodes the word representation of each word. This representation is shared between the subsystems of sentiment and emotion analysis. Each of the shared representations is then fed to the primary attention mechanism of both the subsystems. The primary attention mechanism finds the best representation for each word for each task. The secondary attention mechanism acts on top of the primary attention to extract the best sentence representation by focusing on the suitable context for each task. Finally, the representations of both the tasks are fed to two different feed-forward neural networks to produce two outputs - one for sentiment analysis and one for emotion analysis. Each component is explained in the subsequent subsections.']}]}, {'question': 'What are the datasets used for training?', 'question_id': '9c2f306044b3d1b3b7fdd05d1c046e887796dd7a', 'answers': [{'unanswerable': False, 'extractive_spans': ['SemEval 2016 Task 6 BIBREF7', 'Stance Sentiment Emotion Corpus (SSEC) BIBREF15'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016 Task 6 corpus helps to bridge the gap between the unavailability of a corpus with sentiment and emotion labels. The SemEval 2016 corpus contains tweets which are classified into positive, negative or other. It contains 2,914 training and 1,956 test instances. The SSEC corpus is annotated with anger, anticipation, disgust, fear, joy, sadness, surprise and trust labels. Each tweet could belong to one or more emotion classes and one sentiment class. Table TABREF15 shows the data statistics of SemEval 2016 task 6 and SSEC which are used for sentiment and emotion analysis, respectively.'], 'highlighted_evidence': ['We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15.']}]}, {'question': 'How many parameters does the model have?', 'question_id': '3d99bc8ab2f36d4742e408f211bec154bc6696f7', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the previous state-of-the-art model?', 'question_id': '9219eef636ddb020b9d394868959325562410f83', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF7', 'BIBREF39', 'BIBREF37', 'LitisMind', 'Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.', 'We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise.'], 'highlighted_evidence': ['Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features.', 'We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15.']}]}, {'question': 'What is the previous state-of-the-art performance?', 'question_id': 'ff83eea2df9976c1a01482818340871b17ad4f8c', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/ijcnn/KumarEKK19,Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis.,2019,F1 score of 66.66%,"(0.8393367528915405, 0.8134477734565735, 0.8261894583702087)",0.0,0.2167281061410904
77,Mapping (Dis-)Information Flow about the MH17 Plane Crash,How can the classifier facilitate the annotation task for human annotators?,"The classifier can facilitate the annotation task by providing suggestions to human annotators, allowing them to focus on correcting errors rather than starting from scratch.",1910.01363,"[{'question': 'How can the classifier facilitate the annotation task for human annotators?', 'question_id': '0ee20a3a343e1e251b74a804e9aa1393d17b46d6', 'answers': [{'unanswerable': False, 'extractive_spans': ['quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to get high precision predictions for unlabeled tweets, we choose the probability thresholds for predicting a pro-Russian or pro-Ukrainian tweet such that the classifier would achieve 80% precision on the test splits (recall at this precision level is 23%). Table TABREF38 shows the amount of polarized edges we can predict at this precision level. Upon manual inspection, we however find that the quality of predictions is lower than estimated. Hence, we manually re-annotate the pro-Russian and pro-Ukrainian predictions according to the official annotation guidelines used by BIBREF4. This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class).'], 'highlighted_evidence': ['This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class).']}]}, {'question': 'What recommendations are made to improve the performance in future?', 'question_id': 'f0e8f045e2e33a2129e67fb32f356242db1dc280', 'answers': [{'unanswerable': False, 'extractive_spans': ['applying reasoning BIBREF36 or irony detection methods BIBREF37'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['From the error analysis, we conclude that category I errors need further investigation, as here the model makes mistakes on seemingly easy instances. This might be due to the model not being able to correctly represent Twitter specific language or unknown words, such as Eukraine in example e). Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37.'], 'highlighted_evidence': ['Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37.']}]}, {'question': 'What type of errors do the classifiers use?', 'question_id': 'b6c235d5986914b380c084d9535a7b01310c0278', 'answers': [{'unanswerable': False, 'extractive_spans': ['correct class can be directly inferred from the text content easily, even without background knowledge', 'correct class can be inferred from the text content, given that event-specific knowledge is provided', 'orrect class can be inferred from the text content if the text is interpreted correctly'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to integrate automatically labeled examples into a network analysis that studies the flow of polarized information in the network, we need to produce high precision predictions for the pro-Russian and the pro-Ukrainian class. Polarized tweets that are incorrectly classified as neutral will hurt an analysis much less than neutral tweets that are erroneously classified as pro-Russian or pro-Ukrainian. However, the worst type of confusion is between the pro-Russian and pro-Ukrainian class. In order to gain insights into why these confusions happen, we manually inspect incorrectly predicted examples that are confused between the pro-Russian and pro-Ukrainian class. We analyse the misclassifications in the development set of all 10 runs, which results in 73 False Positives of pro-Ukrainian tweets being classified as pro-Russian (referred to as pro-Russian False Positives), and 88 False Positives of pro-Russian tweets being classified as pro-Ukrainian (referred to as pro-Ukrainian False Positives). We can identify three main cases for which the model produces an error:', 'the correct class can be directly inferred from the text content easily, even without background knowledge', 'the correct class can be inferred from the text content, given that event-specific knowledge is provided', 'the correct class can be inferred from the text content if the text is interpreted correctly'], 'highlighted_evidence': ['We can identify three main cases for which the model produces an error:\n\nthe correct class can be directly inferred from the text content easily, even without background knowledge\n\nthe correct class can be inferred from the text content, given that event-specific knowledge is provided\n\nthe correct class can be inferred from the text content if the text is interpreted correctly']}]}, {'question': 'What neural classifiers are used?', 'question_id': 'e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb', 'answers': [{'unanswerable': False, 'extractive_spans': [' convolutional neural network (CNN) BIBREF29'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27. The model performs 1d convolutions over a sequence of word embeddings. We use the same pre-trained fasttext embeddings as for the logistic regression model. We use a model with one convolutional layer and a relu activation function, and one max pooling layer. The number of filters is 100 and the filter size is set to 4.'], 'highlighted_evidence': ['As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27.']}]}, {'question': 'What is the hashtags does the hashtag-based baseline use?', 'question_id': '1e4450e23ec81fdd59821055f998fd9db0398b16', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': ['Hashtags are often used as a means to assess the content of a tweet BIBREF25, BIBREF26, BIBREF27. We identify hashtags indicative of a class in the annotated dataset using the pointwise mutual information (pmi) between a hashtag $hs$ and a class $c$, which is defined as\n\nWe then predict the class for unseen tweets as the class that has the highest pmi score for the hashtags contained in the tweet.']}]}, {'question': 'What languages are included in the dataset?', 'question_id': '02ce4c288df14a90a210cb39973c6ac0fb4cec59', 'answers': [{'unanswerable': False, 'extractive_spans': ['English'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.', ""BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates.""], 'highlighted_evidence': ['For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\nBIBREF4 provide annotations for a subset of the English tweets contained in the dataset.']}]}, {'question': 'What dataset is used for this study?', 'question_id': '60726d9792d301d5ff8e37fbb31d5104a520dea3', 'answers': [{'unanswerable': False, 'extractive_spans': ['MH17 Twitter dataset'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.'], 'highlighted_evidence': ['For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter.']}]}, {'question': 'What proxies for data annotation were used in previous datasets?', 'question_id': 'e39d90b8d959697d9780eddce3a343e60543be65', 'answers': [{'unanswerable': False, 'extractive_spans': ['widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet', 'Natural Language Processing (NLP) models can be used to automatically label text content'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Several studies analyse the framing of the crash and the spread of (dis)information about the event in terms of pro-Russian or pro-Ukrainian framing. These studies analyse information based on manually labeled content, such as television transcripts BIBREF2 or tweets BIBREF4, BIBREF5. Restricting the analysis to manually labeled content ensures a high quality of annotations, but prohibits analysis from being extended to the full amount of available data. Another widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet BIBREF6, BIBREF7, BIBREF8. Often, this approach treats content from uncredible sources as misleading (e.g. misinformation, disinformation or fake news). This methods enables researchers to scale up the number of observations without having to evaluate the fact value of each piece of content from low-quality sources. However, the approach fails to address an important issue: Not all content from uncredible sources is necessarily misleading or false and not all content from credible sources is true. As often emphasized in the propaganda literature, established media outlets too are vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources BIBREF9, BIBREF10, BIBREF11.', 'In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19. Similarly, automatically labeling attitudes expressed in text BIBREF20, BIBREF21, BIBREF22, BIBREF23 can aid the analysis of disinformation and misinformation spread BIBREF24. In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter.'], 'highlighted_evidence': ['Another widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet BIBREF6, BIBREF7, BIBREF8.', 'In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content.']}]}]",journals/corr/abs-1910-01363,Mapping (Dis-)Information Flow about the MH17 Plane Crash.,2019,"This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class).","(0.8786094188690186, 0.8249096870422363, 0.8509131669998169)",2.523617057172283e-156,0.354606032371521
78,Predictive Embeddings for Hate Speech Detection on Twitter,Do they report results only on English data?,No.,1809.10644,"[{'question': 'Do they report results only on English data?', 'question_id': '50690b72dc61748e0159739a9a0243814d37f360', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['In this paper, we use three data sets from the literature to train and evaluate our own classifier. Although all address the category of hateful speech, they used different strategies of labeling the collected data. Table TABREF5 shows the characteristics of the datasets.', 'Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .', 'Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. Such examples may be innocuous in isolation but could potentially be sexist or racist in context. While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.', ""Debra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two."", 'Along these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:', '@LoveAndLonging ...how is that example ""sexism""?', '@amberhasalamb ...in what way?'], 'highlighted_evidence': ['In this paper, we use three data sets from the literature to train and evaluate our own classifier.', 'Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .', 'Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. ', 'While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.\n\nDebra are now my most hated team on #mkr after least night\'s ep. Snakes in the grass those two.\n\nAlong these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:\n\n@LoveAndLonging ...how is that example ""sexism""?\n\n@amberhasalamb ...in what way?']}]}, {'question': 'Which publicly available datasets are used?', 'question_id': '8266642303fbc6a1138b4e23ee1d859a6f584fbb', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF3', 'BIBREF4', 'BIBREF9'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this paper, we use three data sets from the literature to train and evaluate our own classifier. Although all address the category of hateful speech, they used different strategies of labeling the collected data. Table TABREF5 shows the characteristics of the datasets.', 'Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .'], 'highlighted_evidence': ['In this paper, we use three data sets from the literature to train and evaluate our own classifier.', 'Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .']}]}, {'question': 'What embedding algorithm and dimension size are used?', 'question_id': '3685bf2409b23c47bfd681989fb4a763bcab6be2', 'answers': [{'unanswerable': False, 'extractive_spans': ['300 Dimensional Glove'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We tokenize the data using Spacy BIBREF10 . We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task. We experimented extensively with pre-processing variants and our results showed better performance without lemmatization and lower-casing (see supplement for details). We pad each input to 50 words. We train using RMSprop with a learning rate of .001 and a batch size of 512. We add dropout with a drop rate of 0.1 in the final layer to reduce overfitting BIBREF12 , batch size, and input length empirically through random hyperparameter search.'], 'highlighted_evidence': ['We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task']}]}, {'question': 'What data are the embeddings trained on?', 'question_id': '19225e460fff2ac3aebc7fe31fcb4648eda813fb', 'answers': [{'unanswerable': False, 'extractive_spans': ['Common Crawl '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We tokenize the data using Spacy BIBREF10 . We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task. We experimented extensively with pre-processing variants and our results showed better performance without lemmatization and lower-casing (see supplement for details). We pad each input to 50 words. We train using RMSprop with a learning rate of .001 and a batch size of 512. We add dropout with a drop rate of 0.1 in the final layer to reduce overfitting BIBREF12 , batch size, and input length empirically through random hyperparameter search.'], 'highlighted_evidence': ['We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task.']}]}, {'question': 'how much was the parameter difference between their model and previous methods?', 'question_id': 'f37026f518ab56c859f6b80b646d7f19a7b684fa', 'answers': [{'unanswerable': False, 'extractive_spans': ['our model requires 100k parameters , while BIBREF8 requires 250k parameters'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential added value of metadata, we believe a tweet-only classifier has merits because retrieving features from the social graph is not always tractable in production settings. Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters.""], 'highlighted_evidence': ['Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters.']}]}, {'question': 'how many parameters did their model use?', 'question_id': '1231934db6adda87c1b15e571468b8e9d225d6fe', 'answers': [{'unanswerable': False, 'extractive_spans': ['Excluding the embedding weights, our model requires 100k parameters'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential added value of metadata, we believe a tweet-only classifier has merits because retrieving features from the social graph is not always tractable in production settings. Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters.""], 'highlighted_evidence': ['Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters.']}]}, {'question': 'which datasets were used?', 'question_id': '81303f605da57ddd836b7c121490b0ebb47c60e7', 'answers': [{'unanswerable': False, 'extractive_spans': ['Sexist/Racist (SR) data set', 'HATE dataset', 'HAR'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .'], 'highlighted_evidence': ['Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets.']}]}, {'question': ""what was their system's f1 performance?"", 'question_id': 'a3f108f60143d13fe38d911b1cc3b17bdffde3bd', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.', 'evidence': ['FLOAT SELECTED: Table 2: F1 Results3', 'The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points. Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: F1 Results3', 'Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1.']}]}, {'question': 'what was the baseline?', 'question_id': '118ff1d7000ea0d12289d46430154cc15601fd8e', 'answers': [{'unanswerable': False, 'extractive_spans': ['logistic regression'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""All of our results are produced from 10-fold cross validation to allow comparison with previous results. We trained a logistic regression baseline model (line 1 in Table TABREF10 ) using character ngrams and word unigrams using TF*IDF weighting BIBREF13 , to provide a baseline since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model's results on their respective datasets.""], 'highlighted_evidence': [""We trained a logistic regression baseline model (line 1 in Table TABREF10 ) using character ngrams and word unigrams using TF*IDF weighting BIBREF13 , to provide a baseline since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model's results on their respective datasets.""]}]}]",conf/acl-alw/KshirsagarCMM18,Predictive Embeddings for Hate Speech Detection on Twitter.,2018,"In this paper, we use three data sets from the literature to train and evaluate our own classifier.","(0.9176246523857117, 0.8342933654785156, 0.8739771246910095)",0.0,0.032758958637714386
79,An Analysis of Visual Question Answering Algorithms,From when are many VQA datasets collected?,2014,1703.09684,"[{'question': 'From when are many VQA datasets collected?', 'question_id': 'cf93a209c8001ffb4ef505d306b6ced5936c6b63', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'late 2014', 'evidence': [""VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used.""], 'highlighted_evidence': ['VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0']}]}]",conf/iccv/KafleK17,An Analysis of Visual Question Answering Algorithms.,2017,late 2014,"(0.9272429347038269, 0.8866316080093384, 0.9064826369285583)",6.702145341854094e-232,0.8164075016975403
80,Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation,Are synonymous relation taken into account in the Japanese-Vietnamese task?,Yes.,1910.03467,"[{'question': 'Are synonymous relation taken into account in the Japanese-Vietnamese task?', 'question_id': 'b367b823c5db4543ac421d0057b02f62ea16bf9f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Due to the fact that Vietnamese WordNet is not available, we only exploit WordNet to tackle unknown words of Japanese texts in our Japanese$\\rightarrow $Vietnamese translation system. After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. We choose 1-best synonym for each OOV word. Table TABREF18 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3). We observed an improvement of +0.7 BLEU points between (3) and (5) without using data augmentation described in BIBREF21.'], 'highlighted_evidence': ['After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. ']}]}, {'question': 'Is the supervised morphological learner tested on Japanese?', 'question_id': '84737d871bde8058d8033e496179f7daec31c2d3', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.', 'FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems'], 'highlighted_evidence': ['We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.', 'FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems']}]}]",conf/aclwat/NgoHNN19,Overcoming the Rare Word Problem for low-resource language pairs in Neural Machine Translation.,2019,"After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. ","(0.9205706119537354, 0.8255245685577393, 0.8704607486724854)",0.0,0.010911164805293083
81,"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What non-contextual properties do they refer to?,Plausibility and faithfulness.,1911.00523,"[{'question': 'What non-contextual properties do they refer to?', 'question_id': '6adec34d86095643e6b89cda5c7cd94f64381acc', 'answers': [{'unanswerable': False, 'extractive_spans': ['These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.'], 'highlighted_evidence': ['Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.']}]}, {'question': 'What is the baseline?', 'question_id': '62ba1fefc1eb826fe0cbac092d37a3e2098967e9', 'answers': [{'unanswerable': False, 'extractive_spans': ['random method ', 'LSTM '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).', 'To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features. We scale all features to $[0, 1]$ before fitting the models. As introduced in Section SECREF3, we split our tuples of (OP, PC, explanation) into training, validation, and test sets, and use the validation set for hyperparameter tuning. Refer to the supplementary material for additional details in the experiment.'], 'highlighted_evidence': [' To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).', 'To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features.']}]}, {'question': 'What are their proposed features?', 'question_id': '93ac147765ee2573923f68aa47741d4bcbf88fa8', 'answers': [{'unanswerable': False, 'extractive_spans': ['Non-contextual properties of a word', 'Word usage in an OP or PC (two groups)', 'How a word connects an OP and PC.', 'General OP/PC properties'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):', '[itemsep=0pt,leftmargin=*,topsep=0pt]', 'Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.', 'Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.', 'How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.', 'General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.', 'Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:'], 'highlighted_evidence': ['Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]']}]}, {'question': 'What are overall baseline results on new this new task?', 'question_id': '14c0328e8ec6360a913b8ecb3e50cb27650ff768', 'answers': [{'unanswerable': False, 'extractive_spans': ['all of our models outperform the random baseline by a wide margin', 'he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).', 'Overall performance (Figure FIGREF28). Although our word-level task is heavily imbalanced, all of our models outperform the random baseline by a wide margin. As expected, content words are much more difficult to predict than stopwords, but the best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116). Notably, although we strongly improve on our random baseline, even our best F1 scores are relatively low, and this holds true regardless of the model used. Despite involving more tokens than standard tagging tasks (e.g., BIBREF41 and BIBREF42), predicting whether a word is going to be echoed in explanations remains a challenging problem.'], 'highlighted_evidence': ['To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).', 'Overall performance (Figure FIGREF28). Although our word-level task is heavily imbalanced, all of our models outperform the random baseline by a wide margin. As expected, content words are much more difficult to predict than stopwords, but the best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116). Notably, although we strongly improve on our random baseline, even our best F1 scores are relatively low, and this holds true regardless of the model used. Despite involving more tokens than standard tagging tasks (e.g., BIBREF41 and BIBREF42), predicting whether a word is going to be echoed in explanations remains a challenging problem.']}]}, {'question': 'What metrics are used in evaluation of this task?', 'question_id': '6073fa9050da76eeecd8aa3ccc7ecb16a238d83f', 'answers': [{'unanswerable': False, 'extractive_spans': ['F1 score'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).'], 'highlighted_evidence': ['Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).']}]}, {'question': 'Do authors provide any explanation for intriguing patterns of word being echoed?', 'question_id': 'eacd7e540cc34cb45770fcba463f4bf968681d59', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations.'], 'highlighted_evidence': ['Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations.']}]}, {'question': 'What features are proposed?', 'question_id': '1124804c3702499b78cf0678bab5867e81284b6c', 'answers': [{'unanswerable': False, 'extractive_spans': ['Non-contextual properties of a word', 'Word usage in an OP or PC (two groups)', 'How a word connects an OP and PC', 'General OP/PC properties'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):', '[itemsep=0pt,leftmargin=*,topsep=0pt]', 'Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.', 'Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.', 'How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.', 'General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.', 'Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:', ""Although we expect more complicated words (#characters) to be echoed more often, this is not the case on average. We also observe an interesting example of Simpson's paradox in the results for Wordnet depth BIBREF38: shallower words are more likely to be echoed across all words, but deeper words are more likely to be echoed in content words and stopwords.""], 'highlighted_evidence': [""Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nAlthough we expect more complicated words (#characters) to be echoed more often, this is not the case on average. We also observe an interesting example of Simpson's paradox in the results for Wordnet depth BIBREF38: shallower words are more likely to be echoed across all words, but deeper words are more likely to be echoed in content words and stopwords.""]}]}]",conf/emnlp/AtkinsonST19,What Gets Echoed? Understanding the &quot;Pointers&quot; in Explanations of Persuasive Arguments.,2019,Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.,"(0.8292233943939209, 0.8334788680076599, 0.8313456773757935)",9.044859170944218e-235,0.2414058893918991
82,Automatic Reminiscence Therapy for Dementia.,How is performance of this system measured?,Evaluation metrics.,1910.11949,"[{'question': 'How is performance of this system measured?', 'question_id': '11d2f0d913d6e5f5695f8febe2b03c6c125b667c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'using the BLEU score as a quantitative metric and human evaluation for quality', 'evidence': ['We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.', 'Our chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation.'], 'highlighted_evidence': ['We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.\n\nOur chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation.']}]}, {'question': 'How many questions per image on average are available in dataset?', 'question_id': '1c85a25ec9d0c4f6622539f48346e23ff666cd5f', 'answers': [{'unanswerable': False, 'extractive_spans': ['5 questions per image'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual.'], 'highlighted_evidence': ['We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.']}]}, {'question': 'Is machine learning system underneath similar to image caption ML systems?', 'question_id': '37d829cd42db9ae3d56ab30953a7cf9eda050841', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning. In our case, we focus on generating questions from pictures. Our second architecture is inspired by Neural Conversational Model from BIBREF14 where the author presents an end-to-end approach to generate simple conversations. Building an open-domain conversational agent is a challenging problem. As addressed in BIBREF15 and BIBREF16, the lack of a consistent personality and lack of long-term memory which produces some meaningless responses in these models are still unresolved problems.'], 'highlighted_evidence': ['Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning.']}]}, {'question': 'How big dataset is used for training this system?', 'question_id': '4b41f399b193d259fd6e24f3c6e95dc5cae926dd', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.', 'evidence': ['We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual.', ""We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters.""], 'highlighted_evidence': ['We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.', ""We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters.""]}]}]",conf/mir/CarosGRG20,Automatic Reminiscence Therapy for Dementia.,2020,using the BLEU score as a quantitative metric and human evaluation for quality,"(0.9304467439651489, 0.8400590419769287, 0.8829456567764282)",0.0,0.5978173017501831
83,Lattice CNNs for Matching Based Chinese Question Answering,How do they obtain word lattices from words?,"They approach this through an existing lookup vocabulary, which contains frequent words in BaiduBaike.",1902.09087,"[{'question': 'How do they obtain word lattices from words?', 'question_id': '76377e5bb7d0a374b0aefc54697ac9cd89d2eba8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'By considering words as vertices and generating directed edges between neighboring words within a sentence', 'evidence': ['Word Lattice', 'As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 .'], 'highlighted_evidence': ['Word Lattice\nAs shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 .']}]}, {'question': 'Which metrics do they use to evaluate matching?', 'question_id': '85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff', 'answers': [{'unanswerable': False, 'extractive_spans': ['Precision@1', 'Mean Average Precision', 'Mean Reciprocal Rank'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For both datasets, we follow the evaluation metrics used in the original evaluation tasks BIBREF13 . For DBQA, P@1 (Precision@1), MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are adopted. For KBRE, since only one golden candidate is labeled for each question, only P@1 and MRR are used.'], 'highlighted_evidence': ['For both datasets, we follow the evaluation metrics used in the original evaluation tasks BIBREF13 . For DBQA, P@1 (Precision@1), MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are adopted. For KBRE, since only one golden candidate is labeled for each question, only P@1 and MRR are used.']}]}, {'question': 'Which dataset(s) do they evaluate on?', 'question_id': '4b128f9e94d242a8e926bdcb240ece279d725729', 'answers': [{'unanswerable': False, 'extractive_spans': ['DBQA', 'KBRE'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Datasets', 'We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .', 'DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.', 'KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve.'], 'highlighted_evidence': ['Datasets\nWe conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .', 'DBQA is a document based question answering dataset. ', 'KBRE is a knowledge based relation extraction dataset.']}]}]",conf/aaai/LaiFYWXZ19,Lattice CNNs for Matching Based Chinese Question Answering.,2019,By considering words as vertices and generating directed edges between neighboring words within a sentence,"(0.8521422743797302, 0.8459981679916382, 0.8490591049194336)",8.769105200491155e-232,0.34582874178886414
84,Speaker-independent classification of phonetic segments from raw ultrasound in child speech,Do they report results only on English data?,No.,1907.01413,"[{'question': 'Do they report results only on English data?', 'question_id': '1fdcc650c65c11908f6bde67d5052087245f3dde', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Do they propose any further additions that could be made to improve generalisation to unseen speakers?', 'question_id': 'abad9beb7295d809d7e5e1407cbf673c9ffffd19', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. Recurrent architectures are natural candidates for such systems. Additionally, if using these techniques for speech therapy, the audio signal will be available. An extension of these analyses should not be limited to the ultrasound signal, but instead evaluate whether audio and ultrasound can be complementary. Further work should aim to extend the four classes to more a fine-grained place of articulation, possibly based on phonological processes. Similarly, investigating which classes lead to classification errors might help explain some of the observed results. Although we have looked at variables such as age, gender, or amount of data to explain speaker variation, there may be additional factors involved, such as the general quality of the ultrasound image. Image quality could be affected by probe placement, dry mouths, or other factors. Automatically identifying or measuring such cases could be beneficial for speech therapy, for example, by signalling the therapist that the data being collected is sub-optimal.'], 'highlighted_evidence': ['There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame.']}]}, {'question': 'What are the characteristics of the dataset?', 'question_id': '265c9b733e4dfffb76acfbade4c0c9b14d3ccde1', 'answers': [{'unanswerable': False, 'extractive_spans': ['synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male)', 'data was aligned at the phone-level', '121fps with a 135 field of view', 'single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances.'], 'highlighted_evidence': ['We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 .', 'The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames).']}]}, {'question': 'What type of models are used for classification?', 'question_id': '0f928732f226185c76ad5960402e9342c0619310', 'answers': [{'unanswerable': False, 'extractive_spans': ['feedforward neural networks (DNNs)', 'convolutional neural networks (CNNs)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept.'], 'highlighted_evidence': ['The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function.', 'As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes.']}]}, {'question': 'Do they compare to previous work?', 'question_id': '11c5b12e675cfd8d1113724f019d8476275bd700', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How many instances does their dataset have?', 'question_id': 'd24acc567ebaec1efee52826b7eaadddc0a89e8b', 'answers': [{'unanswerable': False, 'extractive_spans': ['10700'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples.""], 'highlighted_evidence': ['This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples.']}]}, {'question': 'What model do they use to classify phonetic segments? ', 'question_id': '2d62a75af409835e4c123a615b06235a352a67fe', 'answers': [{'unanswerable': False, 'extractive_spans': ['feedforward neural networks', 'convolutional neural networks'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept.'], 'highlighted_evidence': ['The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. ', 'As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. ']}]}, {'question': 'How many speakers do they have in the dataset?', 'question_id': 'fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6', 'answers': [{'unanswerable': False, 'extractive_spans': ['58'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances.'], 'highlighted_evidence': ['We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). ']}]}]",conf/icassp/RibeiroERR19,Speaker-independent Classification of Phonetic Segments from Raw Ultrasound in Child Speech.,2019,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
85,Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data,What is the baseline model for the agreement-based mode?,RNNG,1808.09409,"[{'question': 'What is the baseline model for the agreement-based mode?', 'question_id': 'b5d6357d3a9e3d5fdf9b344ae96cddd11a407875', 'answers': [{'unanswerable': False, 'extractive_spans': ['PCFGLA-based parser, viz. Berkeley parser BIBREF5', 'minimal span-based neural parser BIBREF6'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Our second concern is to mimic the human's robust semantic processing ability by computer programs. The feasibility of reusing the annotation specification for L1 implies that we can reuse standard CPB data to train an SRL system to process learner texts. To test the robustness of the state-of-the-art SRL algorithms, we evaluate two types of SRL frameworks. The first one is a traditional SRL system that leverages a syntactic parser and heavy feature engineering to obtain explicit information of semantic roles BIBREF4 . Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 . The other SRL system uses a stacked BiLSTM to implicitly capture local and non-local information BIBREF7 . and we call it the neural syntax-agnostic system. All systems can achieve state-of-the-art performance on L1 texts but show a significant degradation on L2 texts. This highlights the weakness of applying an L1-sentence-trained system to process learner texts.""], 'highlighted_evidence': ['Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 .']}]}, {'question': 'Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?', 'question_id': 'f33a21c6a9c75f0479ffdbb006c40e0739134716', 'answers': [{'unanswerable': False, 'extractive_spans': ['syntax-based system may generate correct syntactic analyses for partial grammatical fragments'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['While the neural syntax-agnostic system obtains superior performance on the L1 data, the two syntax-based systems both produce better analyses on the L2 data. Furthermore, as illustrated in the comparison between different parsers, the better the parsing results we get, the better the performance on L2 we achieve. This shows that syntactic parsing is important in semantic construction for learner Chinese. The main reason, according to our analysis, is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, which provides crucial information for SRL. Therefore, syntactic parsing helps build more generalizable SRL models that transfer better to new languages, and enhancing syntactic parsing can improve SRL to some extent.'], 'highlighted_evidence': ['While the neural syntax-agnostic system obtains superior performance on the L1 data, the two syntax-based systems both produce better analyses on the L2 data. Furthermore, as illustrated in the comparison between different parsers, the better the parsing results we get, the better the performance on L2 we achieve. This shows that syntactic parsing is important in semantic construction for learner Chinese. The main reason, according to our analysis, is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, which provides crucial information for SRL.']}]}, {'question': 'Who manually annotated the semantic roles for the set of learner texts?', 'question_id': '8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Authors', 'evidence': ['In this paper, we manually annotate the predicate–argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese. It is from the above corpus that we carefully select 600 pairs of L2-L1 parallel sentences. We would choose the most appropriate one among multiple versions of corrections and recorrect the L1s if necessary. Because word structure is very fundamental for various NLP tasks, our annotation also contains gold word segmentation for both L2 and L1 sentences. Note that there are no natural word boundaries in Chinese text. We first employ a state-of-the-art word segmentation system to produce initial segmentation results and then manually fix segmentation errors.'], 'highlighted_evidence': ['In this paper, we manually annotate the predicate–argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese.']}]}]",conf/emnlp/LinDZS018,Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.,2018,"Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 .","(0.7960046529769897, 0.7804831266403198, 0.7881674766540527)",0.0,0.17841359972953796
86,Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining,By how much do they outperform existing state-of-the-art VQA models?,"They don't. Simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.",1808.00265,"[{'question': 'By how much do they outperform existing state-of-the-art VQA models?', 'question_id': '17f5f4a5d943c91d46552fb75940b67a72144697', 'answers': [{'unanswerable': False, 'extractive_spans': ['the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding.'], 'highlighted_evidence': ['Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding.']}]}, {'question': 'How do they measure the correlation between manual groundings and model generated ones?', 'question_id': '83f22814aaed9b5f882168e22a3eac8f5fda3882', 'answers': [{'unanswerable': False, 'extractive_spans': ['rank-correlation BIBREF25'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We evaluate the performance of our proposed method using two criteria: i) rank-correlation BIBREF25 to evaluate visual grounding and ii) accuracy to evaluate question answering. Intuitively, rank-correlation measures the similarity between human and model attention maps under a rank-based metric. A high rank-correlation means that the model is `looking at' image areas that agree to the visual information used by a human to answer the same question. In terms of accuracy of a predicted answer INLINEFORM0 is evaluated by: DISPLAYFORM0""], 'highlighted_evidence': [""We evaluate the performance of our proposed method using two criteria: i) rank-correlation BIBREF25 to evaluate visual grounding and ii) accuracy to evaluate question answering. Intuitively, rank-correlation measures the similarity between human and model attention maps under a rank-based metric. A high rank-correlation means that the model is `looking at' image areas that agree to the visual information used by a human to answer the same question. In terms of accuracy of a predicted answer INLINEFORM0 is evaluated by: DISPLAYFORM0\n\n""]}]}, {'question': 'How do they obtain region descriptions and object annotations?', 'question_id': 'ed11b4ff7ca72dd80a792a6028e16ba20fccff66', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'they are available in the Visual Genome dataset', 'evidence': ['In this work, we introduce a methodology that provides VQA algorithms with the ability to generate human interpretable attention maps which effectively ground the answer to the relevant image regions. We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels. Our framework achieves competitive state-of-the-art VQA performance, while generating visual groundings that outperform other algorithms that use human annotated attention during training.'], 'highlighted_evidence': ['In this work, we introduce a methodology that provides VQA algorithms with the ability to generate human interpretable attention maps which effectively ground the answer to the relevant image regions. We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels.']}]}]",conf/wacv/ZhangNS19,Interpretable Visual Question Answering by Visual Grounding From Attention Supervision Mining.,2019,"Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding.","(0.8506473302841187, 0.8015069961547852, 0.8253463506698608)",3.2717523938392194e-234,0.3563299775123596
87,Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,Which training dataset allowed for the best generalization to benchmark sets?,None of the six models trained on three different NLI datasets were able to generalize well across test sets taken from different NLI benchmarks.,1810.09774,"[{'question': 'Which training dataset allowed for the best generalization to benchmark sets?', 'question_id': 'a48c6d968707bd79469527493a72bfb4ef217007', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'MultiNLI', 'evidence': ['FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined.']}]}, {'question': 'Which model generalized the best?', 'question_id': 'b69897deb5fb80bf2adb44f9cbf6280d747271b3', 'answers': [{'unanswerable': False, 'extractive_spans': ['BERT'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Also including a pretrained ELMo language model did not improve the results significantly. The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 .'], 'highlighted_evidence': [' The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points.']}]}, {'question': 'Which models were compared?', 'question_id': 'ad1f230f10235413d1fe501e414358245b415476', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT', 'evidence': ['For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 . KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 . The success of pre-trained language models in multiple NLP tasks make ESIM + ELMo and BERT interesting additions to this experiment. Table 3 lists the different models used in the experiments.'], 'highlighted_evidence': ['For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 .']}]}, {'question': 'Which datasets were used?', 'question_id': '0a521541b9e2b5c6d64fb08eb318778eba8ac9f7', 'answers': [{'unanswerable': False, 'extractive_spans': ['SNLI, MultiNLI and SICK'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We chose three different datasets for the experiments: SNLI, MultiNLI and SICK. All of them have been designed for NLI involving three-way classification with the labels entailment, neutral and contradiction. We did not include any datasets with two-way classification, e.g. SciTail BIBREF14 . As SICK is a relatively small dataset with approximately only 10k sentence pairs, we did not use it as training data in any experiment. We also trained the models with a combined SNLI + MultiNLI training set.', 'The Stanford Natural Language Inference (SNLI) corpus BIBREF4 is a dataset of 570k human-written sentence pairs manually labeled with the labels entailment, contradiction, and neutral. The source for the premise sentences in SNLI were image captions taken from the Flickr30k corpus BIBREF15 .', 'The Multi-Genre Natural Language Inference (MultiNLI) corpus BIBREF5 consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral. MultiNLI contains sentence pairs from ten distinct genres of both written and spoken English. Only five genres are included in the training set. The development and test sets have been divided into matched and mismatched, where the former includes only sentences from the same genres as the training data, and the latter includes sentences from the remaining genres not present in the training data.', 'SICK BIBREF6 is a dataset that was originally constructed to test compositional distributional semantics (DS) models. The dataset contains 9,840 examples pertaining to logical inference (negation, conjunction, disjunction, apposition, relative clauses, etc.). The dataset was automatically constructed taking pairs of sentences from a random subset of the 8K ImageFlickr data set BIBREF15 and the SemEval 2012 STS MSRVideo Description dataset BIBREF16 .'], 'highlighted_evidence': ['We chose three different datasets for the experiments: SNLI, MultiNLI and SICK.', 'The Stanford Natural Language Inference (SNLI) corpus BIBREF4 is a dataset of 570k human-written sentence pairs manually labeled with the labels entailment, contradiction, and neutral. ', 'The Multi-Genre Natural Language Inference (MultiNLI) corpus BIBREF5 consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral.', 'SICK BIBREF6 is a dataset that was originally constructed to test compositional distributional semantics (DS) models. ']}]}]",conf/blackboxnlp/TalmanC19,Testing the Generalization Power of Neural Network Models across NLI Benchmarks.,2019,MultiNLI,"(0.7917072772979736, 0.7905029058456421, 0.7911046743392944)",0.0,0.2453642189502716
88,Joint Learning of Sentence Embeddings for Relevance and Entailment,what were the baselines?,There is no mention of baselines in the provided context.,1605.04655,"[{'question': 'what were the baselines?', 'question_id': '225a567eeb2698a9d3f1024a8b270313a6d15f82', 'answers': [{'unanswerable': False, 'extractive_spans': ['RNN model', 'CNN model ', 'RNN-CNN model', 'attn1511 model', 'Deep Averaging Network model', 'avg mean of word embeddings in the sentence with projection matrix'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We refer the reader to BIBREF6 and its references for detailed model descriptions. We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 .'], 'highlighted_evidence': ['We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 .']}]}, {'question': 'what is the state of the art for ranking mc test answers?', 'question_id': '35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d', 'answers': [{'unanswerable': False, 'extractive_spans': ['ensemble of hand-crafted syntactic and frame-semantic features BIBREF16'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For the MCTest dataset, Fig. FIGREF30 compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features BIBREF16 , as well as past neural models from the literature, all using attention mechanisms — the Attentive Reader of BIBREF26 , Neural Reasoner of BIBREF27 and the HABCNN model family of BIBREF17 . We see that averaging-based models are surprisingly effective on this task, and in particular on the MC-500 dataset it can beat even the best so far reported model of HABCNN-TE. Our proposed transfer model is statistically equivalent to the best model on both datasets (furthermore, previous work did not include confidence intervals, even though their models should also be stochastically initialized).'], 'highlighted_evidence': ['For the MCTest dataset, Fig. FIGREF30 compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features BIBREF16 , as well as past neural models from the literature, all using attention mechanisms — the Attentive Reader of BIBREF26 , Neural Reasoner of BIBREF27 and the HABCNN model family of BIBREF17 . ']}]}, {'question': 'what is the size of the introduced dataset?', 'question_id': 'f5eac66c08ebec507c582a2445e99317a83e9ebe', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'what datasets did they use?', 'question_id': '62613aca3d7c7d534c9f6d8cb91ff55626bb8695', 'answers': [{'unanswerable': False, 'extractive_spans': ['Argus Dataset', 'AI2-8grade/CK12 Dataset', 'MCTest Dataset'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Argus Dataset', 'AI2-8grade/CK12 Dataset', 'We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0). However, we released it to the public and still included it in the comparison as these qualities reflect many realistic datasets of unknown qualities, so we find relative performances of models on such datasets instructive.', 'MCTest Dataset', 'The Machine Comprehension Test BIBREF8 dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehension, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation. The dataset consists of a set of 660 stories spanning multiple sentences, written in simple and clean language (but with less restricted vocabulary than e.g. the bAbI dataset BIBREF9 ). Each story is accompanied by four questions and each of these lists four possible answers; the questions are tagged as based on just one in-story sentence, or requiring multiple sentence inference. We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers.'], 'highlighted_evidence': ['Argus Dataset', 'AI2-8grade/CK12 Dataset', 'We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0). ', 'MCTest Dataset', 'We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers.']}]}]",conf/rep4nlp/BaudisSS16,Joint Learning of Sentence Embeddings for Relevance and Entailment.,2016,We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 .,"(0.8391562700271606, 0.7631000280380249, 0.7993230223655701)",2.143673989489708e-159,0.12212736904621124
89,Aspect Term Extraction with History Attention and Selective Transformation,How do they determine the opinion summary?,Opinion summary is determined as the linear combination of the opinion representations generated from LSTM.,1805.00760,"[{'question': 'How do they determine the opinion summary?', 'question_id': '282aa4e160abfa7569de7d99b8d45cabee486ba4', 'answers': [{'unanswerable': False, 'extractive_spans': ['the weighted sum of the new opinion representations, according to their associations with the current aspect representation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step.'], 'highlighted_evidence': ['As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step.']}]}, {'question': 'Do they explore how useful is the detection history and opinion summary?', 'question_id': 'ecfb2e75eb9a8eba8f640a039484874fa0d2fceb', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Ablation Study', 'To further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.'], 'highlighted_evidence': ['Ablation Study\nTo further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.']}]}, {'question': 'Which dataset(s) do they use to train the model?', 'question_id': 'a6950c22c7919f86b16384facc97f2cf66e5941d', 'answers': [{'unanswerable': False, 'extractive_spans': ['INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To evaluate the effectiveness of the proposed framework for the ATE task, we conduct experiments over four benchmark datasets from the SemEval ABSA challenge BIBREF1 , BIBREF18 , BIBREF12 . Table TABREF24 shows their statistics. INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain. In these datasets, aspect terms have been labeled by the task organizer.'], 'highlighted_evidence': ['To evaluate the effectiveness of the proposed framework for the ATE task, we conduct experiments over four benchmark datasets from the SemEval ABSA challenge BIBREF1 , BIBREF18 , BIBREF12 . Table TABREF24 shows their statistics. INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain. In these datasets, aspect terms have been labeled by the task organizer.']}]}, {'question': 'By how much do they outperform state-of-the-art methods?', 'question_id': '54be3541cfff6574dba067f1e581444537a417db', 'answers': [{'unanswerable': False, 'extractive_spans': ['Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.', ""Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours). We find that INLINEFORM2 and INLINEFORM3 contain many informal reviews, thus RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts.""], 'highlighted_evidence': [""As shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.\n\nOur framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours). We find that INLINEFORM2 and INLINEFORM3 contain many informal reviews, thus RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts.""]}]}]",conf/ijcai/LiBLLY18,Aspect Term Extraction with History Attention and Selective Transformation.,2018,"As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step.","(0.879098653793335, 0.8049391508102417, 0.8403860330581665)",1.7068313988713885e-158,0.6445529460906982
90,Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset,What is the average number of turns per dialog?,23,1909.05358,"[{'question': 'What is the average number of turns per dialog?', 'question_id': '221e9189a9d2431902d8ea833f486a38a76cbd8e', 'answers': [{'unanswerable': False, 'extractive_spans': ['The average number of utterances per dialog is about 23 '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors.'], 'highlighted_evidence': ['Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors.']}]}, {'question': 'What baseline models are offered?', 'question_id': 'a276d5931b989e0a33f2a0bc581456cca25658d9', 'answers': [{'unanswerable': False, 'extractive_spans': ['3-gram and 4-gram conditional language model', 'Convolution', 'LSTM models BIBREF27 with and without attention BIBREF28', 'Transformer', 'GPT-2'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model.', 'Convolution: We use the fconv architecture BIBREF24 and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2.', 'LSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors.', 'Transformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2.', 'GPT-2: Apart from supervised seq2seq models, we also include results from pre-trained GPT-2 BIBREF30 containing 117M parameters.'], 'highlighted_evidence': ['n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model.\n\nConvolution: We use the fconv architecture BIBREF24 and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2.\n\nLSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors.\n\nTransformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2.\n\nGPT-2: Apart from supervised seq2seq models, we also include results from pre-trained GPT-2 BIBREF30 containing 117M parameters.']}]}, {'question': 'Which six domains are covered in the dataset?', 'question_id': 'c21d26130b521c9596a1edd7b9ef3fe80a499f1e', 'answers': [{'unanswerable': False, 'extractive_spans': ['ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. For the spoken dialogs, we created a “Wizard of Oz” (WOz) system BIBREF12 to collect two-person, spoken conversations. Crowdsourced workers playing the “user"" interacted with human operators playing the “digital assistant” using a web-based interface. In this way, users were led to believe they were interacting with an automated system while it was in fact a human, allowing them to express their turns in natural ways but in the context of an automated interface. We refer to this spoken dialog type as “two-person dialogs"". For the written dialogs, we engaged crowdsourced workers to write the full conversation themselves based on scenarios outlined for each task, thereby playing roles of both the user and assistant. We refer to this written dialog type as “self-dialogs"". In a departure from traditional annotation techniques BIBREF10, BIBREF8, BIBREF13, dialogs are labeled with simple API calls and arguments. This technique is much easier for annotators to learn and simpler to apply. As such it is more cost effective and, in addition, the same model can be used for multiple service providers.'], 'highlighted_evidence': ['To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. ']}]}]",conf/emnlp/ByrneKSNGDYDKC19,Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.,2019,Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors.,"(0.874023973941803, 0.8084345459938049, 0.8399507999420166)",2.05020181241367e-238,0.17509634792804718
91,Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,How are the auxiliary signals from the morphology table incorporated in the decoder?,They are used as an extra output channel.,1804.06506,"[{'question': 'How are the auxiliary signals from the morphology table incorporated in the decoder?', 'question_id': '7aab78e90ba1336950a2b0534cc0cb214b96b4fd', 'answers': [{'unanswerable': False, 'extractive_spans': ['an additional morphology table including target-side affixes.', 'We inject the decoder with morphological properties of the target language.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section ""Proposed Architecture"" provides more details on our models.'], 'highlighted_evidence': ['In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. ']}]}, {'question': 'What type of morphological information is contained in the ""morphology table""?', 'question_id': 'b7fe91e71da8f4dc11e799b3bd408d253230e8c6', 'answers': [{'unanswerable': False, 'extractive_spans': ['target-side affixes'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section ""Proposed Architecture"" provides more details on our models.'], 'highlighted_evidence': ['In the first scenario we equip the decoder with an additional morphology table including target-side affixes.']}]}]",conf/naacl/PassbanLW18,Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation.,2018,"In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. ","(0.8769469261169434, 0.8071320652961731, 0.8405924439430237)",6.345404944257706e-238,0.18875253200531006
92,Learning Twitter User Sentiments on Climate Change with Limited Labeled Data,Do they report results only on English data?,No.,1904.07342,"[{'question': 'Do they report results only on English data?', 'question_id': '16fa6896cf4597154363a6c9a98deb49fffef15f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We henceforth refer to a tweet affirming climate change as a “positive"" sample (labeled as 1 in the data), and a tweet denying climate change as a “negative"" sample (labeled as -1 in the data). All data were downloaded from Twitter in two separate batches using the “twint"" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change"" or “global warming"", and further included disaster-specific search terms (e.g., “bomb cyclone,"" “blizzard,"" “snowstorm,"" etc.). We refer to the first data batch as “influential"" tweets, and the second data batch as “event-related"" tweets.'], 'highlighted_evidence': ['All data were downloaded from Twitter in two separate batches using the “twint"" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change"" or “global warming"", and further included disaster-specific search terms (e.g., “bomb cyclone,"" “blizzard,"" “snowstorm,"" etc.). ']}]}, {'question': 'Do the authors mention any confounds to their study?', 'question_id': '0f60864503ecfd5b048258e21d548ab5e5e81772', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting"" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters.'], 'highlighted_evidence': ['There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting"" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters.']}]}, {'question': 'Which machine learning models are used?', 'question_id': 'fe578842021ccfc295209a28cf2275ca18f8d155', 'answers': [{'unanswerable': False, 'extractive_spans': ['RNNs', 'CNNs', 'Naive Bayes with Laplace Smoothing', 'k-clustering', 'SVM with linear kernel'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our first goal is to train a sentiment analysis model (on training and validation datasets) in order to perform classification inference on event-based tweets. We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 .'], 'highlighted_evidence': [' Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). ']}]}, {'question': 'What methodology is used to compensate for limited labelled data?', 'question_id': '00ef9cc1d1d60f875969094bb246be529373cb1d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.', 'evidence': ['The first data batch consists of tweets relevant to blizzards, hurricanes, and wildfires, under the constraint that they are tweeted by “influential"" tweeters, who we define as individuals certain to have a classifiable sentiment regarding the topic at hand. For example, we assume that any tweet composed by Al Gore regarding climate change is a positive sample, whereas any tweet from conspiracy account @ClimateHiJinx is a negative sample. The assumption we make in ensuing methods (confirmed as reasonable in Section SECREF2 ) is that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets. Here, we enforce binary labels for all tweets composed by each of the 133 influential tweeters that we identified on Twitter (87 of whom accept climate change), yielding a total of 16,360 influential tweets.'], 'highlighted_evidence': ['The first data batch consists of tweets relevant to blizzards, hurricanes, and wildfires, under the constraint that they are tweeted by “influential"" tweeters, who we define as individuals certain to have a classifiable sentiment regarding the topic at hand. For example, we assume that any tweet composed by Al Gore regarding climate change is a positive sample, whereas any tweet from conspiracy account @ClimateHiJinx is a negative sample. The assumption we make in ensuing methods (confirmed as reasonable in Section SECREF2 ) is that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets. ']}]}, {'question': 'Which five natural disasters were examined?', 'question_id': '279b633b90fa2fd69e84726090fadb42ebdf4c02', 'answers': [{'unanswerable': False, 'extractive_spans': ['the East Coast Bomb Cyclone', ' the Mendocino, California wildfires', 'Hurricane Florence', 'Hurricane Michael', 'the California Camp Fires'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). For each disaster, we scraped tweets starting from two weeks prior to the beginning of the event, and continuing through two weeks after the end of the event. Summary statistics on the downloaded event-specific tweets are provided in Table TABREF1 . Note that the number of tweets occurring prior to the two 2018 sets of California fires are relatively small. This is because the magnitudes of these wildfires were relatively unpredictable, whereas blizzards and hurricanes are often forecast weeks in advance alongside public warnings. The first (influential tweet data) and second (event-related tweet data) batches are de-duplicated to be mutually exclusive. In Section SECREF2 , we perform geographic analysis on the event-related tweets from which we can scrape self-reported user city from Twitter user profile header cards; overall this includes 840 pre-event and 5,984 post-event tweets.'], 'highlighted_evidence': ['The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). ']}]}]",conf/icwsm/KoeneckeF20,Learning Twitter User Sentiments on Climate Change with Limited Labeled Data.,2020,"All data were downloaded from Twitter in two separate batches using the “twint"" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change"" or “global warming"", and further included disaster-specific search terms (e.g., “bomb cyclone,"" “blizzard,"" “snowstorm,"" etc.). ","(0.9148522615432739, 0.773964524269104, 0.8385316729545593)",0.0,0.06367188692092896
93,Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization,What is the size of the dataset?,Not specified.,1911.00547,"[{'question': 'What is the size of the dataset?', 'question_id': 'acd05f31e25856b9986daa1651843b8dc92c2d99', 'answers': [{'unanswerable': False, 'extractive_spans': [' 9,892 stories of sexual harassment incidents'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser"", “time"", “location"", “trigger""), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below.'], 'highlighted_evidence': ['We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser"", “time"", “location"", “trigger""), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below.']}]}, {'question': 'What model did they use?', 'question_id': '8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb', 'answers': [{'unanswerable': False, 'extractive_spans': ['joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['2. We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6.'], 'highlighted_evidence': ['We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6.']}]}, {'question': 'What patterns were discovered from the stories?', 'question_id': 'af60462881b2d723adeb4acb5fbc07ea27b6bde2', 'answers': [{'unanswerable': False, 'extractive_spans': ['we demonstrate that harassment occurred more frequently during the night time than the day time', 'it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives', 'we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) ', 'We also found that the majority of young perpetrators engaged in harassment behaviors on the streets', 'we found that adult perpetrators of sexual harassment are more likely to act alone', 'we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location ', 'commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.', 'Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.', 'In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street.'], 'highlighted_evidence': ['We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.', 'Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). ', 'We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. ', 'In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.', 'In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators.', 'In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street.']}]}, {'question': 'Did they use a crowdsourcing platform?', 'question_id': '879bec20c0fdfda952444018e9435f91e34d8788', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': [' foogreen!0.11444976553320885 foothe foogreen!0.013002995729038958 foomicro foogreen!0.016201976904994808 foobus foogreen!0.14046543219592422 fooand foogreen!0.12413455988280475 foothere foogreen!0.18423641449771821 foowas foogreen!0.3394613158889115 fooa foogreen!1.0372470133006573 foogirl foogreen!0.20553644571918994 fooopposite foogreen!0.2821453963406384 footo foogreen!0.5574009846895933 foome foogreen!0.2709480468183756 foojust foogreen!0.2582515007816255 foothen foogreen!0.9223996312357485 fooa', ' foogreen!0.11444976553320885 foothe foogreen!0.013002995729038958 foomicro foogreen!0.016201976904994808 foobus foogreen!0.14046543219592422 fooand foogreen!0.12413455988280475 foothere foogreen!0.18423641449771821 foowas foogreen!0.3394613158889115 fooa foogreen!1.0372470133006573 foogirl foogreen!0.20553644571918994 fooopposite foogreen!0.2821453963406384 footo foogreen!0.5574009846895933 foome foogreen!0.2709480468183756 foojust foogreen!0.2582515007816255 foothen foogreen!0.9223996312357485 fooa']}]}]",conf/emnlp/LiuLCLZS19,Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.,2019,"We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser"", “time"", “location"", “trigger""), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below.","(0.8486518859863281, 0.7836877703666687, 0.8148770928382874)",0.0,0.11752471327781677
94,Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding,Does the performance increase using their method?,No.,1604.00117,"[{'question': 'Does the performance increase using their method?', 'question_id': '3c378074111a6cc7319c0db0aced5752c30bfffb', 'answers': [{'unanswerable': False, 'extractive_spans': ['The multi-task model outperforms the single-task model at all data sizes', 'but none have an overall benefit from the open vocabulary system'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In Figure 1 we show the single-task vs. multi-task model performance for each of three different applications. The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%. Because the performance of the multi-task model decays much more slowly as the amount of training data is reduced, the multi-task model can deliver the same performance with a considerable reduction in the amount of labeled data.', 'Table 4 reports F1 scores on the test set for both the closed and open vocabulary systems. The results differ between the tasks, but none have an overall benefit from the open vocabulary system. Looking at the subset of sentences that contain an OOV token, the open vocabulary system delivers increased performance on the Airbnb and Greyhound tasks. These two are the most difficult apps out of the four and therefore had the most room for improvement. The United app is also all lower case and casing is an important clue for detecting proper nouns that the open vocabulary model takes advantage of.'], 'highlighted_evidence': ['The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%.', 'The results differ between the tasks, but none have an overall benefit from the open vocabulary system.']}]}, {'question': 'What tasks are they experimenting with in this paper?', 'question_id': 'b464bc48f176a5945e54051e3ffaea9a6ad886d7', 'answers': [{'unanswerable': False, 'extractive_spans': ['Slot filling', 'we consider the actions that a user might perform via apps on their phone', 'The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action. For example, dates, departure cities and destinations represent slots to fill in a flight booking task. This information is extracted from natural language queries leveraging typical context associated with each slot type. Researchers have been exploring data-driven approaches to learning models for automatic identification of slot information since the 90's, and significant advances have been made BIBREF0 . Our paper builds on recent work on slot-filling using recurrent neural networks (RNNs) with a focus on the problem of training from minimal annotated data, taking an approach of sharing data from multiple tasks to reduce the amount of data for developing a new task."", 'As candidate tasks, we consider the actions that a user might perform via apps on their phone. Typically, a separate slot-filling model would be trained for each app. For example, one model understands queries about classified ads for cars BIBREF1 and another model handles queries about the weather BIBREF2 . As the number of apps increases, this approach becomes impractical due to the burden of collecting and labeling the training data for each model. In addition, using independent models for each task has high storage costs for mobile devices.', 'Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant. In order to elicit natural language, crowd workers were instructed to simulate a conversation with a friend planning an activity as opposed to giving a command to the computer. Workers were prompted with a slot type/value pair and asked to form a reply to their friend using that information. The instructions were to not include any other potential slots in the sentence but this instruction was not always followed by the workers.'], 'highlighted_evidence': ['Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action', 'As candidate tasks, we consider the actions that a user might perform via apps on their phone.', 'Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.']}]}, {'question': 'What is the size of the open vocabulary?', 'question_id': '3b40799f25dbd98bba5b526e0a1d0d0bb51173e0', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/interspeech/JaechHO16,Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding.,2016,"The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%.","(0.9141964316368103, 0.8008383512496948, 0.8537710905075073)",0.0,0.11720602214336395
95,What we write about when we write about causality: Features of causal statements across large-scale social discourse,How do they extract causality from text?,"They use various methods such as text mining, iterative topic modeling with time series feedback, and visual adaptation of perception.",1604.05781,"[{'question': 'How do they extract causality from text?', 'question_id': '4c822bbb06141433d04bbc472f08c48bc8378865', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""They identify documents that contain the unigrams 'caused', 'causing', or 'causes'"", 'evidence': [""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.""], 'highlighted_evidence': [""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.""]}]}, {'question': 'What is the source of the ""control"" corpus?', 'question_id': '1baf87437b70cc0375b8b7dc2cfc2830279bc8b5', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Randomly selected from a Twitter dump, temporally matched to causal documents', 'evidence': [""Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work."", ""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.""], 'highlighted_evidence': ['Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.', ""Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.""]}]}, {'question': 'What are the selection criteria for ""causal statements""?', 'question_id': '0b31eb5bb111770a3aaf8a3931d8613e578e07a8', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""Presence of only the exact unigrams 'caused', 'causing', or 'causes'"", 'evidence': [""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.""], 'highlighted_evidence': [""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.""]}]}, {'question': 'Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?', 'question_id': '7348e781b2c3755b33df33f4f0cab4b94fcbeb9b', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Only automatic methods', 'evidence': ['The rest of this paper is organized as follows: In Sec. ""Materials and Methods"" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal\' corpus and a corresponding `control\' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. ""Results"" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. ""Discussion"" .'], 'highlighted_evidence': ['In Sec. ""Materials and Methods"" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal\' corpus and a corresponding `control\' corpus, and the details of the statistical and language analysis tools we studied these corpora with.']}]}, {'question': 'how do they collect the comparable corpus?', 'question_id': 'f68bd65b5251f86e1ed89f0c858a8bb2a02b233a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Randomly from a Twitter dump', 'evidence': [""Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work."", ""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.""], 'highlighted_evidence': ['Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.', ""Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.""]}]}, {'question': 'How do they collect the control corpus?', 'question_id': 'e111925a82bad50f8e83da274988b9bea8b90005', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Randomly from Twitter', 'evidence': [""Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work."", ""Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.""], 'highlighted_evidence': ['Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.', ""Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. ""]}]}]",conf/asunam/McAndrewBDDHB16,What we write about when we write about causality: Features of causal statements across large-scale social discourse.,2016,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","(0.8481754660606384, 0.8126600384712219, 0.8300380110740662)",8.614911585158347e-232,0.3172348737716675
96,Multi-style Generative Reading Comprehension,How do they measure the quality of summaries?,"They use a BERT-based model called SUM-QE to predict linguistic qualities of summaries, which are evaluated based on five criteria: Grammaticality, Non-redundancy, Referential Clarity, Focus, and Structure & Coherence.",1901.02262,"[{'question': 'How do they measure the quality of summaries?', 'question_id': '6ead576ee5813164684a8cdda36e6a8c180455d9', 'answers': [{'unanswerable': False, 'extractive_spans': ['Rouge-L', 'Bleu-1'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state-of-the-art extractive approaches BIBREF6 , BIBREF7 .'], 'highlighted_evidence': ['In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1.']}]}, {'question': 'Does their model also take the expected answer style as input?', 'question_id': '0117aa1266a37b0d2ef429f1b0653b9dde3677fe', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on the decoder side prevents the passage ranker and answer possibility classifier from depending on the answer style.'], 'highlighted_evidence': ['Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles.']}]}, {'question': 'What do they mean by answer styles?', 'question_id': '5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'well-formed sentences vs concise answers', 'evidence': ['We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL.'], 'highlighted_evidence': ['The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question.']}]}, {'question': 'Is there exactly one ""answer style"" per dataset?', 'question_id': '6c80bc3ed6df228c8ca6e02c0a8a1c2889498688', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL.'], 'highlighted_evidence': ['The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question.']}]}, {'question': 'What are the baselines that Masque is compared against?', 'question_id': '2d274c93901c193cf7ad227ab28b1436c5f410af', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D', 'evidence': ['FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.', 'FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.', 'FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.']}]}, {'question': 'What is the performance achieved on NarrativeQA?', 'question_id': 'e63bde5c7b154fbe990c3185e2626d13a1bad171', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87', 'evidence': ['FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.']}]}, {'question': 'What is an ""answer style""?', 'question_id': 'cb8a6f5c29715619a137e21b54b29e9dd48dad7d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'well-formed sentences vs concise answers', 'evidence': ['We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL.'], 'highlighted_evidence': ['The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question.']}]}]",conf/aaai/Yang0Z21,Multi-span Style Extraction for Generative Reading Comprehension.,2021,"In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1.","(0.825707197189331, 0.8584499359130859, 0.8417602777481079)",1.0332112772921225e-231,0.3797890543937683
97,A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading,What was the previous state of the art model for this task?,"There is no mention of a specific ""previous state of the art model"" in the provided text snippet. However, it mentions that available methods have successfully converted natural language to programming language in fixed or tightly bounded linguistic paradigms, but does not specify what those methods are.",1908.04917,"[{'question': 'What was the previous state of the art model for this task?', 'question_id': '8a7bd9579d2783bfa81e055a7a6ebc3935da9d20', 'answers': [{'unanswerable': False, 'extractive_spans': ['WAS', 'LipCH-Net-seq', 'CSSMCM-w/o video'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.', 'LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.', 'CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character.'], 'highlighted_evidence': ['WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.', 'LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.', 'CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character.']}]}, {'question': 'What syntactic structure is used to model tones?', 'question_id': '27b01883ed947b457d3bab0c66de26c0736e4f90', 'answers': [{'unanswerable': False, 'extractive_spans': ['syllables'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Based on the above considerations, in this paper, we present CSSMCM, a sentence-level Chinese Mandarin lip reading network, which contains three sub-networks. Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation. Compared with pinyin characters, syllables are a longer linguistic unit, and can reduce the difficulty of syllable choices in the decoder by sequence-to-sequence attention-based models BIBREF6 . Chen et al. BIBREF7 find that there might be a relationship between the production of lexical tones and the visible movements of the neck, head, and mouth. Motivated by this observation, in the second sub-network, both video and pinyin sequence is used as input to predict tone. Then in the third sub-network, video, pinyin, and tone sequence work together to predict the Chinese character sequence. At last, three sub-networks are jointly finetuned to improve overall performance.'], 'highlighted_evidence': ['Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation.']}]}, {'question': 'What visual information characterizes tones?', 'question_id': '9714cb7203c18a0c53805f6c889f2e20b4cab5dd', 'answers': [{'unanswerable': False, 'extractive_spans': ['video sequence is first fed into the VGG model BIBREF9 to extract visual feature'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence. This problem is modeled as a sequence-to-sequence learning problem too. The corresponding model architecture is shown in Figure FIGREF8 .', 'In order to take both video and pinyin information into consideration when producing tone, a dual attention mechanism BIBREF3 is employed. Two independent attention mechanisms are used for video and pinyin sequence. Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.', 'The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0', 'The pinyin prediction sub-network transforms video sequence into pinyin sequence, which corresponds to INLINEFORM0 in Equation ( EQREF6 ). This sub-network is based on the sequence-to-sequence architecture with attention mechanism BIBREF8 . We name the encoder and decoder the video encoder and pinyin decoder, for the encoder process video sequence, and the decoder predicts pinyin sequence. The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder. The video encoder can be denoted as: DISPLAYFORM0'], 'highlighted_evidence': ['As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence.', 'Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.', 'The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0', 'The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder.']}]}]",conf/mmasia/ZhaoXS19,A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading.,2019,WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.,"(0.8210338950157166, 0.8327151536941528, 0.8268333077430725)",9.840085157783916e-232,0.2123030573129654
98,Dissecting Content and Context in Argumentative Relation Analysis,Do they report results only on English data?,No.,1906.03338,"[{'question': 'Do they report results only on English data?', 'question_id': 'a22b900fcd76c3d36b5679691982dc6e9a3d34bf', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How do they demonstrate the robustness of their results?', 'question_id': 'fb2593de1f5cc632724e39d92e4dd82477f06ea1', 'answers': [{'unanswerable': False, 'extractive_spans': ['performances of a purely content-based model naturally stays stable'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The results are displayed in Figure FIGREF43 . In the standard setting (Figure FIGREF43 ), the models that have access to the context besides the content ( INLINEFORM0 ) and the models that are only allowed to access the context ( INLINEFORM1 ), always perform better than the content-based models ( INLINEFORM2 ) (bars above zero). However, when we randomly flip contexts of the test instances (Figure FIGREF43 ), or suppress them entirely (Figure FIGREF43 ), the opposite picture emerges: the content-based models always outperform the other models. For some classes (support, INLINEFORM3 ) the difference can exceed 50 F1 percentage points. These two studies, where testing examples are varied regarding their context (randomized-context or no-context) simulates what can be expected if we apply our systems for relation class assignment to EAUs stemming from heterogeneous sources. While the performances of a purely content-based model naturally stays stable, the performance of the other systems decrease notably – they perform worse than the content-based model.'], 'highlighted_evidence': ['While the performances of a purely content-based model naturally stays stable, the performance of the other systems decrease notably – they perform worse than the content-based model.']}]}, {'question': 'What baseline and classification systems are used in experiments?', 'question_id': '476d0b5579deb9199423bb843e584e606d606bc7', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF13', 'majority baseline'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features.', ""The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1. Intriguingly, the content-ignorant models ( INLINEFORM6 ) always perform significantly better than the models which only have access to the EAUs' content ( INLINEFORM7 , INLINEFORM8 ). In the most general task formulation ( INLINEFORM9 ), we observe that INLINEFORM10 even significantly outperforms the model which has maximum access (seeing both EAU spans and surrounding contexts: INLINEFORM11 ).""], 'highlighted_evidence': ['The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features.', 'The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1.']}]}, {'question': 'How are the EAU text spans annotated?', 'question_id': 'eddabb24bc6de6451bcdaa7940f708e925010912', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.', 'evidence': [""Tree-based sentiment annotations are sentiment scores assigned to nodes in constituency parse trees BIBREF26 . We represent these scores by a one-hot vector of dimension 5 (5 is very positive, 1 is very negative). We determine the contextual ( INLINEFORM0 ) sentiment by looking at the highest possible node of the context which does not contain the EAU (ADVP in Figure FIGREF26 ). The sentiment for an EAU span ( INLINEFORM1 ) is assigned to the highest possible node covering the EAU span which does not contain the context sub-tree (S in Figure FIGREF26 ). The full-access ( INLINEFORM2 ) score is assigned to the lowest possible node which covers both the EAU span and its surrounding context (S' in Figure FIGREF26 ). Next to the sentiment scores for the selected tree nodes and analogously to the word embeddings, we also calculate the element-wise subtraction of the one-hot sentiment source vectors from the one-hot sentiment target vectors. This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors."", 'Results'], 'highlighted_evidence': ['This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors.\n\nResults']}]}, {'question': 'How are elementary argumentative units defined?', 'question_id': 'f0946fb9df9839977f4d16c43476e4c2724ff772', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/argmining/OpitzF19,Dissecting Content and Context in Argumentative Relation Analysis.,2019,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
99,Efficient Attention using a Fixed-Size Memory Representation,Which baseline methods are used?,BiDAF,1707.00110,"[{'question': 'Which baseline methods are used?', 'question_id': '2d3bf170c1647c5a95abae50ee3ef3b404230ce4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'standard parametrized attention and a non-attention baseline', 'evidence': ['Table 1 shows the BLEU scores of our model on different sequence lengths while varying $K$ . This is a study of the trade-off between computational time and representational power. A large $K$ allows us to compute complex source representations, while a $K$ of 1 limits the source representation to a single vector. We can see that performance consistently increases with $K$ up to a point that depends on the data length, with longer sequences requiring more complex representations. The results with and without position encodings are almost identical on the toy data. Our technique learns to fit the data as well as the standard attention mechanism despite having less representational power. Both beat the non-attention baseline by a significant margin.', 'All models are implemented using TensorFlow based on the seq2seq implementation of BIBREF15 and trained on a single machine with a Nvidia K40m GPU. We use a 2-layer 256-unit, a bidirectional LSTM BIBREF16 encoder, a 2-layer 256-unit LSTM decoder, and 256-dimensional embeddings. For the attention baseline, we use the standard parametrized attention BIBREF2 . Dropout of 0.2 (0.8 keep probability) is applied to the input of each cell and we optimize using Adam BIBREF17 at a learning rate of 0.0001 and batch size 128. We train for at most 200,000 steps (see Figure 3 for sample learning curves). BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses. We decode using beam search with a beam'], 'highlighted_evidence': ['Both beat the non-attention baseline by a significant margin.', 'For the attention baseline, we use the standard parametrized attention BIBREF2 .']}]}, {'question': 'How much is the BLEU score?', 'question_id': '6e8c587b6562fafb43a7823637b84cd01487059a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Ranges from 44.22 to 100.00 depending on K and the sequence length.', 'evidence': ['FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention.']}]}, {'question': 'Which datasets are used in experiments?', 'question_id': 'ab9453fa2b927c97b60b06aeda944ac5c1bfef1e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ""Sequence Copy Task and WMT'17"", 'evidence': ['Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.', ""Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016.""], 'highlighted_evidence': ['To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 .', ""For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples).""]}]}]",conf/emnlp/BritzGL17,Efficient Attention using a Fixed-Size Memory Representation.,2017,standard parametrized attention and a non-attention baseline,"(0.7763439416885376, 0.8028960824012756, 0.7893967628479004)",0.0,0.04598686099052429
100,Team Papelo: Transformer Networks at FEVER,How big is their training set?,"8.8k questions with 182k question-sentence pairs for DBQA, and 14.3k questions with 273k question-predicate pairs for KBRE.",1901.02534,"[{'question': 'How big is their training set?', 'question_id': '9efd025cfa69c6ff2777528bd158f79ead9353d1', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What baseline do they compare to?', 'question_id': '559c1307610a15427caeb8aff4d2c01ae5c9de20', 'answers': [{'unanswerable': False, 'extractive_spans': ['For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.'], 'highlighted_evidence': ['Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.']}]}, {'question': 'Which pre-trained transformer do they use?', 'question_id': '4ecb6674bcb4162bf71aea8d8b82759255875df3', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF5'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.'], 'highlighted_evidence': ['For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . ']}]}, {'question': 'What is the FEVER task?', 'question_id': 'eacc1eb65daad055df934e0e878f417b73b2ecc1', 'answers': [{'unanswerable': False, 'extractive_spans': ['tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted.', 'As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher.', 'The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence.'], 'highlighted_evidence': ['The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted.', 'As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher.', 'The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence.']}]}]",conf/emnlp/Malon18,Team Papelo: Transformer Networks at FEVER.,2018,,"(0.0, 0.0, 0.0)",0.0,0.11931867897510529
101,Automatic Differentiation in ROOT,How is correctness of automatic derivation proved?,It is not mentioned in the given text.,2004.04435,"[{'question': 'How is correctness of automatic derivation proved?', 'question_id': 'd353a6bbdc66be9298494d0c853e0d8d752dec4b', 'answers': [{'unanswerable': False, 'extractive_spans': ['empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND.'], 'highlighted_evidence': ['In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND.']}]}, {'question': 'Is this AD implementation used in any deep learning framework?', 'question_id': 'e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",journals/corr/abs-2004-04435,Automatic Differentiation in ROOT.,2020,"In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND.","(0.8548982739448547, 0.8214505910873413, 0.8378406763076782)",1.7509392933412236e-233,-0.07986053824424744
102,Spectral decomposition method of dialog state tracking via collective matrix factorization,What state-of-the-art models are compared against?,BERT model,1606.05286,"[{'question': 'What state-of-the-art models are compared against?', 'question_id': '73abb173a3cc973ab229511cf53b426865a2738b', 'answers': [{'unanswerable': False, 'extractive_spans': ['a deep neural network (DNN) architecture proposed in BIBREF24 ', 'maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a comparison to the state of the art methods, Table 1 presents accuracy results of the best Collective Matrix Factorization model, with a latent space dimension of 350, which has been determined by cross-validation on a development set, where the value of each slot is instantiated as the most probable w.r.t the inference procedure presented in Section ""Spectral decomposition model for state tracking in slot-filling dialogs"" . In our experiments, the variance is estimated using standard dataset reshuffling. The same results are obtained for several state of the art methods of generative and discriminative state tracking on this dataset using the publicly available results as reported in BIBREF22 . More precisely, as provided by the state-of-the-art approaches, the accuracy scores computes $p(s^*_{t+1}|s_t,z_t)$ commonly name the joint goal. Our proposition is compared to the 4 baseline trackers provided by the DSTC organisers. They are the baseline tracker (Baseline), the focus tracker (Focus), the HWU tracker (HWU) and the HWU tracker with “original” flag set to (HWU+) respectively. Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented.'], 'highlighted_evidence': ['Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented.\n\n']}]}]",journals/dad/Perez16,Spectral decomposition method of dialog state tracking via collective matrix factorization.,2016,"Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented.

","(0.8373618125915527, 0.7879836559295654, 0.8119226694107056)",2.842407596381456e-238,0.2604934573173523
103,Torch-Struct: Deep Structured Prediction Library,Does API provide ability to connect to models written in some other deep learning framework?,No,2002.00876,"[{'question': 'Does API provide ability to connect to models written in some other deep learning framework?', 'question_id': '1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [""The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can request samples $z \\sim \\textsc {CRF}(\\ell )$, probabilities $\\textsc {CRF}(z;\\ell )$, modes $\\arg \\max _z \\textsc {CRF}(\\ell )$, or other distributional properties such as $\\mathbb {H}(\\textsc {CRF}(\\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning.""], 'highlighted_evidence': ['The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29.']}]}, {'question': 'Is this library implemented into Torch or is framework agnostic?', 'question_id': '093039f974805952636c19c12af3549aa422ec43', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'It uses deep learning framework (pytorch)', 'evidence': ['With this challenge in mind, we introduce Torch-Struct with three specific contributions:', 'Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.', 'Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python.', 'Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization.'], 'highlighted_evidence': ['With this challenge in mind, we introduce Torch-Struct with three specific contributions:\n\nModularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.\n\nCompleteness: a broad array of classical algorithms are implemented and new models can easily be added in Python.\n\nEfficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization.']}]}, {'question': 'What baselines are used in experiments?', 'question_id': '8df89988adff57279db10992846728ec4f500eaa', 'answers': [{'unanswerable': False, 'extractive_spans': ['Typical implementations of dynamic programming algorithms are serial in the length of the sequence', 'Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized', 'Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Optimizations ::: a) Parallel Scan Inference', 'The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \\bigoplus _c \\ell _{t, \\cdot , c} \\otimes \\ell _{t^{\\prime }, c, \\cdot }$. Under this approach, we only need $O(\\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models.', 'Optimizations ::: b) Vectorized Parsing', 'Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$,', 'Optimizations ::: c) Semiring Matrix Operations', 'The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing,'], 'highlighted_evidence': ['Parallel Scan Inference\nThe commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence.', 'Vectorized Parsing\nComputational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized.', 'Semiring Matrix Operations\nThe two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost.']}]}, {'question': 'What general-purpose optimizations are included?', 'question_id': '94edac71eea1e78add678fb5ed2d08526b51016b', 'answers': [{'unanswerable': False, 'extractive_spans': ['Parallel Scan Inference', 'Vectorized Parsing', 'Semiring Matrix Operations'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Optimizations ::: a) Parallel Scan Inference', 'Optimizations ::: b) Vectorized Parsing', 'Optimizations ::: c) Semiring Matrix Operations', 'Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms.'], 'highlighted_evidence': ['a) Parallel Scan Inference', 'b) Vectorized Parsing', 'c) Semiring Matrix Operations', 'Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming.']}]}]",conf/acl/Rush20,Torch-Struct: Deep Structured Prediction Library.,2020,The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29.,"(0.8226938247680664, 0.7838650941848755, 0.8028101921081543)",0.0,-0.03880178555846214
104,Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study,what baseline do they compare to?,They don't mention what baseline they compare to.,1906.10519,"[{'question': 'what baseline do they compare to?', 'question_id': '9c4ed8ca59ba6d240f031393b01f634a9dc3615d', 'answers': [{'unanswerable': False, 'extractive_spans': ['VecMap', 'Muse', 'Barista'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section ""Previous Work"" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources. For all models (Mono, MT, VecMap, Muse, Barista), we take the average of the word embeddings in the source-language training examples and train a linear SVM. We report this instead of using the same feed-forward network as in Blse as it is the stronger upper bound. We choose the parameter $c$ on the target language development set and evaluate on the target language test set.'], 'highlighted_evidence': ['We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section ""Previous Work"" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources.']}]}]",journals/corr/abs-1906-10519,Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study.,2019,"We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section ""Previous Work"" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources.","(0.873666524887085, 0.7842600345611572, 0.8265525698661804)",2.5476016393291407e-233,0.3647845983505249
105,Improving Open Information Extraction via Iterative Rank-Aware Learning,How does this compare to traditional calibration methods like Platt Scaling?,Not mentioned in the snippet.,1905.13413,"[{'question': 'How does this compare to traditional calibration methods like Platt Scaling?', 'question_id': 'ca7e71131219252d1fab69865804b8f89a2c0a8f', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.', 'evidence': ['Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. Ideally, the resulting model after one-round of training becomes better not only at confidence modeling, but also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\\mathcal {D}$ , initial model $\\theta ^{(0)}$ model after convergence $\\theta $ $t \\leftarrow 0$ # iteration', 'A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 . However, we observe that this often yields sub-optimal ranking results, with incorrect extractions of one sentence having higher likelihood than correct extractions of another sentence. We hypothesize this is due to the issue of a disconnect between training and test-time objectives. Specifically, the system is trained solely to raise likelihood of gold-standard extractions, and during training the model is not aware of its test-time behavior of ranking a set of system-generated assertions across sentences that potentially include incorrect extractions.', 'We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score. An extraction is judged as correct if the predicate and arguments include the syntactic head of the gold standard counterparts.'], 'highlighted_evidence': ['Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision.', 'For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 ', 'We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score.']}]}, {'question': ""What's the input representation of OpenIE tuples into the model?"", 'question_id': 'd77c9ede2727c28e0b5a240b2521fd49a19442e0', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'word embeddings', 'evidence': ['Our training method in sec:ours could potentially be used with any probabilistic open IE model, since we make no assumptions about the model and only the likelihood of the extraction is required for iterative rank-aware learning. As a concrete instantiation in our experiments, we use RnnOIE BIBREF3 , BIBREF9 , a stacked BiLSTM with highway connections BIBREF10 , BIBREF11 and recurrent dropout BIBREF12 . Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $'], 'highlighted_evidence': ['Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $']}]}]",conf/acl/JiangYN19,Improving Open Information Extraction via Iterative Rank-Aware Learning.,2019,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"(0.8771916031837463, 0.8236551880836487, 0.8495808243751526)",0.0,0.07337456941604614
106,Detecting Online Hate Speech Using Context Aware Models,How do they combine the models?,"They use multi-task learning to consider RC, passage ranking, and answer possibility classification together.",1710.07395,"[{'question': 'How do they combine the models?', 'question_id': 'dd2046f5481f11b7639a230e8ca92904da75feed', 'answers': [{'unanswerable': False, 'extractive_spans': ['maximum of two scores assigned by the two separate models', 'average score'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Table TABREF24 shows performance of ensemble models by combining prediction results of the best context-aware logistic regression model and the best context-aware neural network model. We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions.'], 'highlighted_evidence': ['We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions.']}]}, {'question': 'What is their baseline?', 'question_id': '47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579', 'answers': [{'unanswerable': False, 'extractive_spans': ['Logistic regression model with character-level n-gram features'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For logistic regression model implementation, we use l2 loss. We adopt the balanced class weight as described in Scikit learn. Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective. BIBREF0 , BIBREF9'], 'highlighted_evidence': [' Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective.']}]}, {'question': 'What context do they use?', 'question_id': '569ad21441e99ae782d325d5f5e1ac19e08d5e76', 'answers': [{'unanswerable': False, 'extractive_spans': ['title of the news article', 'screen name of the user'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In logistic regression models, we extract four types of features, word-level and character-level n-gram features as well as two types of lexicon derived features. We extract these four types of features from the target comment first. Then we extract these features from two sources of context texts, specifically the title of the news article that the comment was posted for and the screen name of the user who posted the comment.'], 'highlighted_evidence': ['Then we extract these features from two sources of context texts, specifically the title of the news article that the comment was posted for and the screen name of the user who posted the comment.']}]}, {'question': 'What is their definition of hate speech?', 'question_id': '90741b227b25c42e0b81a08c279b94598a25119d', 'answers': [{'unanswerable': False, 'extractive_spans': ['language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful.'], 'highlighted_evidence': ['We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.']}]}, {'question': 'What architecture has the neural network?', 'question_id': '1d739bb8e5d887fdfd1f4b6e39c57695c042fa25', 'answers': [{'unanswerable': False, 'extractive_spans': ['three parallel LSTM BIBREF21 layers'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our neural network model mainly consists of three parallel LSTM BIBREF21 layers. It has three different inputs, including the target comment, its news title and its username. Comment and news title are encoded into a sequence of word embeddings. We use pre-trained word embeddings in word2vec. Username is encoded into a sequence of characters. We use one-hot encoding of characters.'], 'highlighted_evidence': ['Our neural network model mainly consists of three parallel LSTM BIBREF21 layers.']}]}]",conf/ranlp/GaoH17,Detecting Online Hate Speech Using Context Aware Models.,2017,"We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions.","(0.8609146475791931, 0.8403196930885315, 0.8504924774169922)",7.1981198831513995e-233,0.3591141104698181
107,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",How is human interaction consumed by the model?,"Human interaction is consumed by the model through user input, which is lowercased and tokenized to match the model training data. The system also handles out-of-vocabulary (OOV) words using WordNet by recursively searching for hypernyms and hyponyms until an in-vocabulary word is found or a maximum distance from the initial word is reached.",1904.02357,"[{'question': 'How is human interaction consumed by the model?', 'question_id': '5c70fdd3d6b67031768d3e28336942e49bf9a500', 'answers': [{'unanswerable': False, 'extractive_spans': ['displays three different versions of a story written by three distinct models for a human to compare', 'human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""gordon2009sayanything use an information retrieval based system to write by alternating turns between a human and their system. clark2018mil use a similar turn-taking approach to interactivity, but employ a neural model for generation and allow the user to edit the generated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. roemmele2017eval experiment with a user-driven setup, where the machine doesn't generate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful generation. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user's ability to knit a coherent overall story together out of locally related sentences. They also do not allow a user to control the novelty or “unexpectedness” of the generations, which clark2018mil find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sentences and have the system update later generations. We develop a system that allows a user to interact in all of these ways that were limitations in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged. We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories. The full range of interactions available to a user is: select a model, provide a topic, change diversity of content, collaborate on the planning for the story, and collaborate on the story sentences. It is entirely user-driven, as the users control how much is their own work and how much is the machine's at every stage. It supports revision; a user can modify an earlier part of a written story or of the story plan at any point, and observe how this affects later generations.""], 'highlighted_evidence': ['We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories.']}]}, {'question': 'How do they evaluate generated stories?', 'question_id': 'f27502c3ece9ade265389d5ace90ca9ca42b46f3', 'answers': [{'unanswerable': False, 'extractive_spans': ['separate set of Turkers to rate the stories for overall quality and the three improvement areas'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis.'], 'highlighted_evidence': ['We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis.']}]}, {'question': 'Do they evaluate in other language appart from English?', 'question_id': 'ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What are the baselines?', 'question_id': 'aa4b38f601cc87bf93849245d5f65124da3dc112', 'answers': [{'unanswerable': False, 'extractive_spans': ['Title-to-Story system'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The Title-to-Story system is a baseline, which generates directly from topic.'], 'highlighted_evidence': ['The Title-to-Story system is a baseline, which generates directly from topic.']}]}]",conf/naacl/Goldfarb-Tarrant19,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation.",2019,"We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories.","(0.8219403624534607, 0.8367408514022827, 0.829274594783783)",1.8572164252332132e-155,0.37027713656425476
108,Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,What is used a baseline?,Nothing.,1907.02636,"[{'question': 'What is used a baseline?', 'question_id': '08b87a90139968095433f27fc88f571d939cd433', 'answers': [{'unanswerable': False, 'extractive_spans': ['As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As shown in TABLE TABREF24 , we report the micro average of precision, recall and F1-score for all 11 types of labels for a baseline as well as the proposed model. As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 . As presented in TABLE TABREF24 , the score obtained by the proposed model is clearly higher than the baseline. Here, as described in Section SECREF14 , the sizes of window and lower bounds of frequency for selecting contextual keywords are tuned as 4 and 7 throughout the evaluation of English dataset, and tuned as 3 and 4 throughout the evaluation of Chinese dataset. The number of extracted contextual keywords from the English dataset is 1,328, and from the Chinese dataset is 331.'], 'highlighted_evidence': ['As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 .']}]}, {'question': 'What contextual features are used?', 'question_id': 'ef872807cb0c9974d18bbb886a7836e793727c3d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.', 'evidence': ['IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word “ntdll.exe” is the name of a malicious file on the basis of the words “download” and “compromised” from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as ""download"", ""malware"", ""malicious"", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.'], 'highlighted_evidence': [' In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.']}]}, {'question': 'Where are the cybersecurity articles used in the model sourced from?', 'question_id': '4db3c2ca6ddc87209c31b20763b7a3c1c33387bc', 'answers': [{'unanswerable': False, 'extractive_spans': [' from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018. All of these cybersecurity articles are used to train the English word embedding. Afterwards, we randomly select 370 articles, and manually annotate the IOCs contained in the articles. Among the selected articles, we randomly select 70 articles as the validation set and 70 articles as the test set; the remaining articles are used for training.'], 'highlighted_evidence': ['For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018. ']}]}, {'question': 'What type of hand-crafted features are used in state of the art IOC detection systems?', 'question_id': '63337fd803f6fdd060ebd0f53f9de79d451810cd', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/ijcnn/LongTZHL19,Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling.,2019,"As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 .","(0.9178511500358582, 0.8140941262245178, 0.8628647327423096)",0.0,0.03275664150714874
109,Polysemy Detection in Distributed Representation of Word Sense,How is the fluctuation in the sense of the word and its neighbors measured?,Volatility.,1709.08858,"[{'question': 'How is the fluctuation in the sense of the word and its neighbors measured?', 'question_id': '238ec3c1e1093ce2f5122ee60209b969f7669fae', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.\n4) Computing the mean m and the sample variance σ for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.', 'evidence': [""Distributed representation of word sense provides us with the ability to perform several operations on the word. One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. When a word has several senses, it is called a polysemic word. When a word has only one sense, it is called a monosemic word. We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word. We can explain this fact as follows. Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation."", 'To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor. We have found that there is a difference in the surrounding uniformity between a monosemic word and a polysemic word. This paper describes how to compute surrounding uniformity for a given word, and discuss the relationship between surrounding uniformity and polysemy.', 'We choose the uniformity of vectors, which can be regarded as general case of triangle inequality. The uniformity of a set of vectors is a ratio, i.e., the size of the vector of the vector addition of the vectors divided by the scalar sum of the sizes of the vectors. If and only if all directions of the vectors are the same, the uniformity becomes 1.0. We compute this uniformity for the neighbors, including the word itself. Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$', 'where $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$'], 'highlighted_evidence': ['One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word.', 'We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word.', "" Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation."", 'To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity.', 'The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor', 'Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$\n\nwhere $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$']}]}]",conf/kst/OomotoOY0OU17,Polysemy detection in distributed representation of word sense.,2017,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.","(0.8418673276901245, 0.7734856605529785, 0.8062290549278259)",0.0,0.10818590223789215
110,Neural Domain Adaptation for Biomedical Question Answering,"Among various transfer learning techniques, which technique yields to the best performance?",XR training using our method.,1706.03610,"[{'question': 'Among various transfer learning techniques, which technique yields to the best performance?', 'question_id': 'f704d182c9e01a2002381b76bf21e4bb3c0d3efc', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/conll/WieseWN17,Neural Domain Adaptation for Biomedical Question Answering.,2017,,"(0.0, 0.0, 0.0)",0.0,0.18206694722175598
111,MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,How big are improvements of MMM over state of the art?,MMM gives us another ∼4% improvement.,1910.00458,"[{'question': 'How big are improvements of MMM over state of the art?', 'question_id': '53d6cbee3606dd106494e2e98aa93fdd95920375', 'answers': [{'unanswerable': False, 'extractive_spans': ['test accuracy of 88.9%, which exceeds the previous best by 16.9%'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We first evaluate our method on the DREAM dataset. The results are summarized in Table TABREF16. In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large. Although the RoBERTa-Large baseline has already outperformed the BERT-Large baseline by around 18%, MMM gives us another $\\sim $4% improvement, pushing the accuracy closer to the human performance. Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%.'], 'highlighted_evidence': ['Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%.']}]}, {'question': 'What out of domain datasets authors used for coarse-tuning stage?', 'question_id': '9dc844f82f520daf986e83466de0c84d93953754', 'answers': [{'unanswerable': False, 'extractive_spans': ['MultiNLI BIBREF15 and SNLI BIBREF16 '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use four MCQA datasets as the target datasets: DREAM BIBREF6, MCTest BIBREF9, TOEFL BIBREF5, and SemEval-2018 Task 11 BIBREF14, which are summarized in Table TABREF11. For the first coarse-tuning stage with NLI tasks, we use MultiNLI BIBREF15 and SNLI BIBREF16 as the out-of-domain source datasets. For the second stage, we use the current largest MCQA dataset, i.e., RACE BIBREF7 as in-domain source dataset. For all datasets, we use the official train/dev/test splits.'], 'highlighted_evidence': ['For the first coarse-tuning stage with NLI tasks, we use MultiNLI BIBREF15 and SNLI BIBREF16 as the out-of-domain source datasets. ']}]}, {'question': 'What are state of the art methods MMM is compared to?', 'question_id': '9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'FTLM++, BERT-large, XLNet', 'evidence': ['FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines.']}]}, {'question': 'What four representative datasets are used for bechmark?', 'question_id': '36d892460eb863220cd0881d5823d73bbfda172c', 'answers': [{'unanswerable': False, 'extractive_spans': ['DREAM, MCTest, TOEFL, and SemEval-2018 Task 11'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Recently large and powerful pre-trained language models such as BERT BIBREF8 have been achieving the state-of-the-art (SOTA) results on various tasks, however, its potency on MCQA datasets has been severely limited by the data insufficiency. For example, the MCTest dataset has two variants: MC160 and MC500, which are curated in a similar way, and MC160 is considered easier than MC500 BIBREF9. However, BERT-based models perform much worse on MC160 compared with MC500 (8–10% gap) since the data size of the former is about three times smaller. To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.'], 'highlighted_evidence': ['To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.']}]}]",conf/aaai/JinGKCH20,MMM: Multi-Stage Multi-Task Learning for Multi-Choice Reading Comprehension.,2020,"Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%.","(0.8799349069595337, 0.8458067774772644, 0.8625333905220032)",1.3335187715540836e-232,0.4634722173213959
112,Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,How do they measure the diversity of inferences?,They don't explicitly state how they measure the diversity of inferences.,1909.08824,"[{'question': 'How do they measure the diversity of inferences?', 'question_id': '7d59374d9301a0c09ea5d023a22ceb6ce07fb490', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by number of distinct n-grams', 'evidence': ['We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens.'], 'highlighted_evidence': ['Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. ']}]}, {'question': 'By how much do they improve the accuracy of inferences over state-of-the-art methods?', 'question_id': '8e2b125426d1220691cceaeaf1875f76a6049cbd', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.', 'evidence': ['We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens.', 'FLOAT SELECTED: Table 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind. The the best result for each dimension is emboldened.', 'FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened.'], 'highlighted_evidence': ['Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. ', 'FLOAT SELECTED: Table 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind. The the best result for each dimension is emboldened.', 'FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened.']}]}, {'question': 'Which models do they use as baselines on the Atomic dataset?', 'question_id': '42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1', 'answers': [{'unanswerable': False, 'extractive_spans': ['RNN-based Seq2Seq', 'Variational Seq2Seq', 'VRNMT ', 'CWVAE-Unpretrained'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compared our proposed model with the following four baseline methods:', 'RNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.', 'Variational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.', 'VRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.', 'CWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.', 'Note that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.', 'FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened.'], 'highlighted_evidence': ['We compared our proposed model with the following four baseline methods:\n\nRNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.\n\nVariational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.\n\nVRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.\n\nCWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.\n\nNote that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.', 'FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened.']}]}, {'question': 'How does the context-aware variational autoencoder learn event background information?', 'question_id': 'fb76e994e2e3fa129f1e94f1b043b274af8fb84c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': ' CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.', 'evidence': ['In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.).'], 'highlighted_evidence': ['In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.).']}]}, {'question': 'What is the size of the Atomic dataset?', 'question_id': '99ef97336c0112d9f60df108f58c8b04b519a854', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/emnlp/DuDLL19,Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.,2019,by number of distinct n-grams,"(0.8512325286865234, 0.8675711750984192, 0.8593242764472961)",1.0003688322288243e-231,0.1756710708141327
113,Comparing Human and Machine Errors in Conversational Speech Transcription,what standard speech transcription pipeline was used?,None.,1708.08615,"[{'question': 'what standard speech transcription pipeline was used?', 'question_id': '95d8368b1055d97250df38d1e8c4a2b283d2b57e', 'answers': [{'unanswerable': False, 'extractive_spans': ['pipeline that is used at Microsoft for production data'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The goal of reaching “human parity” in automatic CTS transcription raises the question of what should be considered human accuracy on this task. We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol. Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling BIBREF9 , BIBREF10 , BIBREF7 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 and language modeling BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.8% versus 5.9% on Switchboard data, and 11.0% versus 11.3% on CallHome English data) BIBREF19 . Since then, Saon et al. have reported even better results, along with a separate transcription experiment that puts the human error rate, on the same test data, at a lower point than measured by us (5.1% for Switchboard, 6.8% for CallHome) BIBREF20 .'], 'highlighted_evidence': ['We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol. ']}]}]",conf/interspeech/StolckeD17,Comparing Human and Machine Errors in Conversational Speech Transcription.,2017,"We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol. ","(0.9300968050956726, 0.8127210736274719, 0.8674563765525818)",0.0,0.13870340585708618
114,Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources,How is speed measured?,Speed is not mentioned in the given context.,1805.12032,"[{'question': 'How is speed measured?', 'question_id': 'b54fc86dc2cc6994e10c1819b6405de08c496c7b', 'answers': [{'unanswerable': False, 'extractive_spans': ['time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The first metric we report is the reaction type. Recent studies have found that 59% of bitly-URLs on Twitter are shared without ever being read BIBREF11 , and 73% of Reddit posts were voted on without reading the linked article BIBREF12 . Instead, users tend to rely on the commentary added to retweets or the comments section of Reddit-posts for information on the content and its credibility. Faced with this reality, we ask: what kind of reactions do users find when they browse sources of varying credibility? Discourse acts, or speech acts, can be used to identify the use of language within a conversation, e.g., agreement, question, or answer. Recent work by Zhang et al. zhang2017characterizing classified Reddit comments by their primary discourse act (e.g., question, agreement, humor), and further analyzed patterns from these discussions.', 'The second metric we report is reaction speed. A study by Jin et al. jin2013epidemiological found that trusted news stories spread faster than misinformation or rumor; Zeng et al. zeng2016rumors found that tweets which deny rumors had shorter delays than tweets of support. Our second goal is to determine if these trends are maintained for various types of news sources on Twitter and Reddit.', 'To examine whether users react to content from trusted sources differently than from deceptive sources, we measure the reaction delay, which we define as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred. We report the cumulative distribution functions (CDFs) for each source type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility.'], 'highlighted_evidence': ['The first metric we report is the reaction type.', 'The second metric we report is reaction speed. A study by Jin et al. jin2013epidemiological found that trusted news stories spread faster than misinformation or rumor; Zeng et al. zeng2016rumors found that tweets which deny rumors had shorter delays than tweets of support. Our second goal is to determine if these trends are maintained for various types of news sources on Twitter and Reddit.', 'To examine whether users react to content from trusted sources differently than from deceptive sources, we measure the reaction delay, which we define as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred. We report the cumulative distribution functions (CDFs) for each source type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility.']}]}, {'question': 'What is the architecture of their model?', 'question_id': 'b43a8a0f4b8496b23c89730f0070172cd5dca06a', 'answers': [{'unanswerable': False, 'extractive_spans': ['we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.', 'We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.'], 'highlighted_evidence': ['Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.', 'We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.']}]}, {'question': 'What are the nine types?', 'question_id': 'b161febf86cdd58bd247a934120410068b24b7d1', 'answers': [{'unanswerable': False, 'extractive_spans': ['agreement', 'answer', 'appreciation', 'disagreement', 'elaboration', 'humor', 'negative reaction', 'question', 'other'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models.'], 'highlighted_evidence': ['\n', 'In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models.']}]}]",conf/acl/GlenskiWV18,Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources.,2018,The first metric we report is the reaction type.,"(0.8715895414352417, 0.859590470790863, 0.8655484318733215)",1.1368587676511996e-231,0.11895203590393066
115,Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing,Do they evaluate the quality of the paraphrasing model?,No.,1601.06068,"[{'question': 'Do they evaluate the quality of the paraphrasing model?', 'question_id': '117aa7811ed60e84d40cd8f9cb3ca78781935a98', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How many paraphrases are generated per question?', 'question_id': 'c359ab8ebef6f60c5a38f5244e8c18d85e92761d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans', 'evidence': ['For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.? $\\mid $ NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging BIBREF38 . For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases. We generate ungrounded graphs for these paraphrases and treat the final entity disambiguation and paraphrase selection as part of the semantic parsing problem.'], 'highlighted_evidence': ['For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases. ']}]}, {'question': 'What latent variables are modeled in the PCFG?', 'question_id': 'ad362365656b0b218ba324ae60701eb25fe664c1', 'answers': [{'unanswerable': False, 'extractive_spans': ['syntactic information', 'semantic and topical information'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In their traditional use, the latent states in L-PCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions.'], 'highlighted_evidence': ['We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions.']}]}, {'question': 'What are the baselines?', 'question_id': '423bb905e404e88a168e7e807950e24ca166306c', 'answers': [{'unanswerable': False, 'extractive_spans': ['GraphParser without paraphrases', 'monolingual machine translation based model for paraphrase generation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases.', 'We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions.'], 'highlighted_evidence': ['We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases', 'We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions.']}]}]",conf/inlg/NarayanRC16,Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing.,2016,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
116,"Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter",Do they evaluate only on English data?,No,1709.07916,"[{'question': 'Do they evaluate only on English data?', 'question_id': 'e5ae8ac51946db7475bb20b96e0a22083b366a6d', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': [""This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 . Twitter's APIs provides both historic and real-time data collections. The latter method randomly collects 1% of publicly available tweets. This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. We used the queries to collect approximately 4.5 million related tweets between 06/01/2016 and 06/30/2016. The data will be available in the first author's website. Figure FIGREF3 shows a sample of collected tweets in this research.""], 'highlighted_evidence': ['This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. ']}]}, {'question': 'How strong was the correlation between exercise and diabetes?', 'question_id': '18288c7b0f8bd7839ae92f9c293e7fb85c7e146a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'weak correlation with p-value of 0.08', 'evidence': [""The main DDEO topics showed some level of interrelationship by appearing as subtopics of other DDEO topics. The words with italic and underline styles in Table 2 demonstrate the relation among the four DDEO areas. Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ). The strongest correlation among the topics was determined to be between exercise and obesity ( INLINEFORM0 ). Other notable correlations were: diabetes and obesity ( INLINEFORM1 ), and diet and obesity ( INLINEFORM2 )."", 'FLOAT SELECTED: Figure 2: DDEO Correlation P-Value'], 'highlighted_evidence': ['The main DDEO topics showed some level of interrelationship by appearing as subtopics of other DDEO topics.', ""Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 )."", 'FLOAT SELECTED: Figure 2: DDEO Correlation P-Value']}]}, {'question': 'How were topics of interest about DDEO identified?', 'question_id': 'b5e883b15e63029eb07d6ff42df703a64613a18a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'using topic modeling model Latent Dirichlet Allocation (LDA)', 'evidence': ['To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning “diabetes"", “cancer"", and “influenza"" into a topic that has an overall “disease"" theme BIBREF44 , BIBREF45 . Topic modeling has a wide range of applications in health and medical domains such as predicting protein-protein relationships based on the literature knowledge BIBREF46 , discovering relevant clinical concepts and structures in patients\' health records BIBREF47 , and identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 .', 'Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 .', 'We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets. Before identifying the opinions, two pre-processing steps were implemented: (1) using a standard list for removing stop words, that do not have semantic value for analysis (such as “the""); and, (2) finding the optimum number of topics. To determine a proper number of topics, log-likelihood estimation with 80% of tweets for training and 20% of tweets for testing was used to find the highest log-likelihood, as it is the optimum number of topics BIBREF57 . The highest log-likelihood was determined 425 topics.'], 'highlighted_evidence': ['To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning “diabetes"", “cancer"", and “influenza"" into a topic that has an overall “disease"" theme BIBREF44 , BIBREF45 .', 'Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 .', 'We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets.']}]}]",journals/corr/abs-1709-07916,"Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter.",2017,This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. ,"(0.8209550976753235, 0.7777740955352783, 0.7987815141677856)",0.0,0.09220519661903381
117,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,Does this model train faster than state of the art models?,Yes.,1909.02480,"[{'question': 'Does this model train faster than state of the art models?', 'question_id': 'b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What is the performance difference between proposed method and state-of-the-arts on these datasets?', 'question_id': 'ba6422e22297c7eb0baa381225a2f146b9621791', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Difference is around 1 BLEU score lower on average than state of the art methods.', 'evidence': ['Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring. The first block in Table TABREF40 includes the baseline results from autoregressive Transformer. For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model BIBREF11 to obtain high-quality samples. We vary the temperature within $\\lbrace 0.1, 0.2, 0.3, 0.4, 0.5, 1.0\\rbrace $ and select the best temperature based on the performance on development sets. The analysis of the impact of sampling temperature and other hyper-parameters on samples is in § SECREF50. For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind auto-regressive Transformer on model data distributions. Comparing with CMLM BIBREF8 with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality. Leveraging iterative refinement to further improve the performance of FlowSeq has been left to future work.', 'FLOAT SELECTED: Table 2: BLEU scores on two WMT datasets of models using advanced decoding methods. The first block are Transformer-base (Vaswani et al., 2017). The second and the third block are results of models trained w/w.o. knowledge distillation, respectively. n = l × r is the total number of candidates for rescoring.'], 'highlighted_evidence': ['Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring.', 'FLOAT SELECTED: Table 2: BLEU scores on two WMT datasets of models using advanced decoding methods. The first block are Transformer-base (Vaswani et al., 2017). The second and the third block are results of models trained w/w.o. knowledge distillation, respectively. n = l × r is the total number of candidates for rescoring.']}]}, {'question': 'What non autoregressive NMT models are used for comparison?', 'question_id': '65e72ad72a9cbfc379f126b10b0ce80cfe44579b', 'answers': [{'unanswerable': False, 'extractive_spans': ['NAT w/ Fertility', 'NAT-IR', 'NAT-REG', 'LV NAR', 'CTC Loss', 'CMLM'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility BIBREF6, NAT-IR BIBREF7, NAT-REG BIBREF25, LV NAR BIBREF26, CTC Loss BIBREF27, and CMLM BIBREF8.'], 'highlighted_evidence': ['We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility BIBREF6, NAT-IR BIBREF7, NAT-REG BIBREF25, LV NAR BIBREF26, CTC Loss BIBREF27, and CMLM BIBREF8.']}]}, {'question': 'What are three neural machine translation (NMT) benchmark datasets used for evaluation?', 'question_id': 'cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9', 'answers': [{'unanswerable': False, 'extractive_spans': ['WMT2014, WMT2016 and IWSLT-2014'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FlowSeq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation – WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.'], 'highlighted_evidence': [' Experimentally, on three benchmark datasets for machine translation – WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.']}]}]",conf/emnlp/MaZLNH19,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.,2019,,"(0.0, 0.0, 0.0)",0.0,0.46818166971206665
118,On Leveraging the Visual Modality for Neural Machine Translation,What is result of their attention distribution analysis?,They found that the three attention maps are fairly similar despite the completely different decisions.,1910.02754,"[{'question': 'What is result of their attention distribution analysis?', 'question_id': '04aff4add28e6343634d342db92b3ac36aa8c255', 'answers': [{'unanswerable': False, 'extractive_spans': ['visual attention is very sparse', "" visual component of the attention hasn't learnt any variation over the source encodings""], 'yes_no': None, 'free_form_answer': '', 'evidence': [""In this section, we analyze the visual and text based attention mechanisms. We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation. Thus, in practice, we find that a small weight ($\\gamma =0.1$) is necessary to prevent degradation due to this sparse visual attention component. Figure FIGREF18 & FIGREF19 shows the comparison of visual and text based attention for two sentences, one long source sentence of length 21 and one short source sentence of length 7. In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths.""], 'highlighted_evidence': ['We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation.', ""In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths.""]}]}, {'question': 'What is result of their Principal Component Analysis?', 'question_id': 'a8e4522ce2ce7336e731286654d6ad0931927a4e', 'answers': [{'unanswerable': False, 'extractive_spans': [""existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT""], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Figure FIGREF14 (Top) shows the variance explained by the Top 100 principal components, obtained by applying PCA on the How2 and Multi30k training set visual features. The original feature dimensions are 2048 in both the cases. It is clear from the Figure FIGREF14 that most of the energy of the visual feature space resides in a low-dimensional subspace BIBREF14. In other words, there exist a few directions in the embedding space which disproportionately explain the variance. These ""common"" directions affect all of the embeddings in the same way, rendering them less discriminative. Figure FIGREF14 also shows the cumulative variance explained by Top 10, 20, 50 and 100 principal components respectively. It is clear that the visual features in the case of How2 dataset are much more dominated by the ""common"" dimensions, when compared to the Multi30k dataset. Further, this analysis is still at the sentence level, i.e. the visual features are much less discriminative among individual sentences, further aggravating the problem at the token level. This suggests that the existing visual features aren\'t sufficient enough to expect benefits from the visual modality in NMT, since they won\'t provide discriminativeness among the vocabulary elements at the token level during prediction. Further, this also indicates that under subword vocabulary such as BPE BIBREF15 or Sentence-Piece BIBREF16, the utility of such visual embeddings will only aggravate.'], 'highlighted_evidence': ['In other words, there exist a few directions in the embedding space which disproportionately explain the variance.', 'It is clear that the visual features in the case of How2 dataset are much more dominated by the ""common"" dimensions, when compared to the Multi30k dataset. Further, this analysis is still at the sentence level, i.e. the visual features are much less discriminative among individual sentences, further aggravating the problem at the token level. This suggests that the existing visual features aren\'t sufficient enough to expect benefits from the visual modality in NMT, since they won\'t provide discriminativeness among the vocabulary elements at the token level during prediction.']}]}, {'question': 'What are 3 novel fusion techniques that are proposed?', 'question_id': 'f6202100cfb83286dc51f57c68cffdbf5cf50a3f', 'answers': [{'unanswerable': False, 'extractive_spans': ['Step-Wise Decoder Fusion', 'Multimodal Attention Modulation', 'Visual-Semantic (VS) Regularizer'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Proposed Fusion Techniques ::: Step-Wise Decoder Fusion', 'Our first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. This differs from the usual practice of passing the visual feature only at the beginning of the decoding process BIBREF5.', 'Proposed Fusion Techniques ::: Multimodal Attention Modulation', 'Similar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Then, the true attention distribution $a_{t}(s)$ is computed as an interpolation between the visual and text based attention scores. The score function is a content based scoring mechanism as usual.', 'Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer', ""In terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction. However, to our knowledge, visual-semantic supervision hasn't been much explored for multimodal translation in terms of loss functions.""], 'highlighted_evidence': ['Proposed Fusion Techniques ::: Step-Wise Decoder Fusion\nOur first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process.', 'Proposed Fusion Techniques ::: Multimodal Attention Modulation\nSimilar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well.', 'Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer\nIn terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction.']}]}]",conf/inlg/RaunakCLXM19,On Leveraging the Visual Modality for Neural Machine Translation.,2019,"We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation.","(0.8874933123588562, 0.8476650714874268, 0.8671221137046814)",9.16014445299868e-156,0.37457743287086487
119,Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,What are two models' architectures in proposed solution?,"There is no mention of specific model architectures in the provided context. The discussion revolves around the concepts of faithfulness, plausibility, and interpretation methods for NLP systems, without delving into architectural details.",2004.02393,"[{'question': ""What are two models' architectures in proposed solution?"", 'question_id': 'bd7039f81a5417474efa36f703ebddcf51835254', 'answers': [{'unanswerable': False, 'extractive_spans': ['Reasoner model, also implemented with the MatchLSTM architecture', 'Ranker model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Method ::: Passage Ranking Model', 'The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages.', 'Method ::: Cooperative Reasoner', 'To alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:'], 'highlighted_evidence': ['Method ::: Passage Ranking Model\nThe key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages.', 'Method ::: Cooperative Reasoner\nTo alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages.']}]}, {'question': 'How do two models cooperate to select the most confident chains?', 'question_id': '022e5c996a72aeab890401a7fdb925ecd0570529', 'answers': [{'unanswerable': False, 'extractive_spans': ['Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:'], 'highlighted_evidence': ['Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards.']}]}, {'question': 'How many hand-labeled reasoning chains have been created?', 'question_id': '2a950ede24b26a45613169348d5db9176fda4f82', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What benchmarks are created?', 'question_id': '34af2c512ec38483754e94e1ea814aa76552d60a', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples', 'evidence': ['In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).', 'In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.', 'The accuracy is defined as the ratio:'], 'highlighted_evidence': ['In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).\n\nIn MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.\n\nThe accuracy is defined as the ratio:', 'The accuracy is defined as the ratio:']}]}]",conf/ai/FengYXGHCCGZ21,"extending PySyft with N.-IID Federated Learning Benchmarkby Houda Bouraqqadi, Ayoub Berrag, Mohamed Mhaouach, Afaf Bouhoute, Khalid Fardousse, and Ismail Berrada: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games.",2021,"Method ::: Passage Ranking Model
The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\mathcal {P} = \lbrace p_1, p_2 ... p_K\rbrace $ from a pool of candidates, and outputs a chain of selected passages.","(0.842652440071106, 0.7786742448806763, 0.8094010353088379)",8.044360098315643e-232,0.14730244874954224
120,A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What empricial investigations do they reference?,Miller (2018),2004.01694,"[{'question': 'What empricial investigations do they reference?', 'question_id': 'c1429f7fed5a4dda11ac7d9643f97af87a83508b', 'answers': [{'unanswerable': False, 'extractive_spans': ['empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human–machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human–machine parity in particular. All of our data are made publicly available for external validation and further analysis.', 'In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human–machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.', 'We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3–SECREF5. Based on our findings, we then distil a set of recommendations for human evaluation of strong MT systems, with a focus on assessing human–machine parity (Section SECREF6).'], 'highlighted_evidence': ['We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation.', 'In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human–machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.', 'We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3–SECREF5. ']}]}, {'question': 'What languages do they investigate for machine translation?', 'question_id': 'a93d4aa89ac3abbd08d725f3765c4f1bed35c889', 'answers': [{'unanswerable': False, 'extractive_spans': ['English ', 'Chinese '], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article:'], 'highlighted_evidence': ['We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article:']}]}, {'question': 'What recommendations do they offer?', 'question_id': 'bc473c5bd0e1a8be9b2037aa7006fd68217c3f47', 'answers': [{'unanswerable': False, 'extractive_spans': [' Choose professional translators as raters', ' Evaluate documents, not sentences', 'Evaluate fluency in addition to adequacy', 'Do not heavily edit reference translations for fluency', 'Use original source texts'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Our experiments in Sections SECREF3–SECREF5 show that machine translation quality has not yet reached the level of professional human translation, and that human evaluation methods which are currently considered best practice fail to reveal errors in the output of strong NMT systems. In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human–machine parity, and will strengthen the human evaluation of MT in general.', 'Recommendations ::: (R1) Choose professional translators as raters.', 'In our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.', 'Recommendations ::: (R2) Evaluate documents, not sentences.', 'When evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).', 'Recommendations ::: (R3) Evaluate fluency in addition to adequacy.', ""Raters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality."", 'Recommendations ::: (R4) Do not heavily edit reference translations for fluency.', 'In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).', 'Recommendations ::: (R5) Use original source texts.', 'Raters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT.'], 'highlighted_evidence': ["" In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human–machine parity, and will strengthen the human evaluation of MT in general.\n\nRecommendations ::: (R1) Choose professional translators as raters.\nIn our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.\n\nRecommendations ::: (R2) Evaluate documents, not sentences.\nWhen evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).\n\nRecommendations ::: (R3) Evaluate fluency in addition to adequacy.\nRaters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.\n\nRecommendations ::: (R4) Do not heavily edit reference translations for fluency.\nIn professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).\n\nRecommendations ::: (R5) Use original source texts.\nRaters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT.""]}]}, {'question': 'What percentage fewer errors did professional translations make?', 'question_id': 'cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '36%', 'evidence': ['FLOAT SELECTED: Table 5: Classification of errors in machine translation MT1 and two professional human translation outputs HA and HB. Errors represent the number of sentences (out of N = 150) that contain at least one error of the respective type. We also report the number of sentences that contain at least one error of any category (Any), and the total number of error categories present in all sentences (Total). Statistical significance is assessed with Fisher’s exact test (two-tailed) for each pair of translation outputs.', 'To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 5: Classification of errors in machine translation MT1 and two professional human translation outputs HA and HB. Errors represent the number of sentences (out of N = 150) that contain at least one error of the respective type. We also report the number of sentences that contain at least one error of any category (Any), and the total number of error categories present in all sentences (Total). Statistical significance is assessed with Fisher’s exact test (two-tailed) for each pair of translation outputs.', 'To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3', ' The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32.']}]}, {'question': ""What was the weakness in Hassan et al's evaluation design?"", 'question_id': '9299fe72f19c1974564ea60278e03a423eb335dc', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set\n', 'evidence': ['The human evaluation of MT output in research scenarios is typically conducted by crowd workers in order to minimise costs. BIBREF13 shows that aggregated assessments of bilingual crowd workers are very similar to those of MT developers, and BIBREF14, based on experiments with data from WMT 2012, similarly conclude that with proper quality control, MT systems can be evaluated by crowd workers. BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. We hypothesise that expert translators will provide more nuanced ratings than non-experts, and that their ratings will show a higher difference between MT outputs and human translations.', 'MT has been evaluated almost exclusively at the sentence level, owing to the fact that most MT systems do not yet take context across sentence boundaries into account. However, when machine translations are compared to those of professional translators, the omission of linguistic context—e. g., by random ordering of the sentences to be evaluated—does not do justice to humans who, in contrast to most MT systems, can and do take inter-sentential context into account BIBREF15, BIBREF16. We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.', 'The human reference translations with which machine translations are compared within the scope of a human–machine parity assessment play an important role. BIBREF3 used all source texts of the WMT 2017 Chinese–English test set for their experiments, of which only half were originally written in Chinese; the other half were translated from English into Chinese. Since translated texts are usually simpler than their original counterparts BIBREF17, they should be easier to translate for MT systems. Moreover, different human translations of the same source text sometimes show considerable differences in quality, and a comparison with an MT system only makes sense if the human reference translations are of high quality. BIBREF3, for example, had the WMT source texts re-translated as they were not convinced of the quality of the human translations in the test set. At WMT 2018, the organisers themselves noted that the manual evaluation included several reports of ill-formed reference translations BIBREF5. We hypothesise that the quality of the human translations has a significant effect on findings of human–machine parity, which would indicate that it is necessary to ensure that human translations used to assess parity claims need to be carefully vetted for their quality.'], 'highlighted_evidence': [' BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. ', 'We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.', 'BIBREF3 used all source texts of the WMT 2017 Chinese–English test set for their experiments, of which only half were originally written in Chinese; ']}]}]",journals/jair/LaubliCNSST20,A Set of Recommendations for Assessing Human-Machine Parity in Language Translation.,2020,"We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation.","(0.8697521090507507, 0.801537275314331, 0.8342525362968445)",0.0,0.13893288373947144
121,Effective Use of Transformer Networks for Entity Tracking,Do they report results only on English?,No.,1909.02635,"[{'question': 'Do they report results only on English?', 'question_id': '0e45aae0e97a6895543e88705e153f084ce9c136', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What evidence do they present that the model attends to shallow context clues?', 'question_id': 'c515269b37cc186f6f82ab9ada5d9ca176335ded', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues', 'evidence': [""One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.""], 'highlighted_evidence': [""One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics.""]}]}, {'question': 'In what way is the input restructured?', 'question_id': '43f86cd8aafe930ebb35ca919ada33b74b36c7dd', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'In four entity-centric ways - entity-first, entity-last, document-level and sentence-level', 'evidence': [""Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. The input sequence begins with a [START] token, then the entity under consideration, then a [SEP] token. After each sentence, a [CLS] token is used to anchor the prediction for that sentence. In this model, the transformer can always observe the entity it should be primarily “attending to” from the standpoint of building representations. We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. These variants are naturally more computationally-intensive than post-conditioned models, as we need to rerun the transformer for each distinct entity we want to make a prediction for."", 'As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). In a sentence level model, we formulate each pair of entity $e$ and process step $t$ as a separate instance for our classification task. Thus, for a process with $T$ steps and $m$ entities we get $T \\times m$ input sequences for fine tuning our classification task.'], 'highlighted_evidence': ['Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. ', ""We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. "", 'As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model).']}]}]",conf/emnlp/GuptaD19,Effective Use of Transformer Networks for Entity Tracking.,2019,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
122,Recognizing Musical Entities in User-generated Content,What are their results on the entity recognition task?,"Their model performs surprisingly well, nearly as well as the first occurrence baseline.",1904.00648,"[{'question': 'What are their results on the entity recognition task?', 'question_id': 'aa60b0a6c1601e09209626fd8c8bdc463624b0b3', 'answers': [{'unanswerable': False, 'extractive_spans': ['With both test sets performances decrease, varying between 94-97%'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The performances of the NER experiments are reported separately for three different parts of the system proposed.', ""Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the complex nature of the users' informal language in comparison to the structured message created by the bot.""], 'highlighted_evidence': ['The performances of the NER experiments are reported separately for three different parts of the system proposed.', 'Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease.']}]}, {'question': 'What task-specific features are used?', 'question_id': '3837ae1e91a4feb27f11ac3b14963e9a12f0c05e', 'answers': [{'unanswerable': False, 'extractive_spans': ['6)Contributor first names', '7)Contributor last names', '8)Contributor types (""soprano"", ""violinist"", etc.)', '9)Classical work types (""symphony"", ""overture"", etc.)', '10)Musical instruments', '11)Opus forms (""op"", ""opus"")', '12)Work number forms (""no"", ""number"")', '13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp"")', '14)Work Modes (""major"", ""minor"", ""m"")'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In total, we define 26 features for describing each token: 1)POS tag; 2)Chunk tag; 3)Position of the token within the text, normalized between 0 and 1; 4)If the token starts with a capital letter; 5)If the token is a digit. Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (""soprano"", ""violinist"", etc.); 9)Classical work types (""symphony"", ""overture"", etc.); 10)Musical instruments; 11)Opus forms (""op"", ""opus""); 12)Work number forms (""no"", ""number""); 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp""); 14)Work Modes (""major"", ""minor"", ""m""). Finally, we complete the tokens\' description including as token\'s features the surface form, the POS and the chunk tag of the previous and the following two tokens (12 features).'], 'highlighted_evidence': ['Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (""soprano"", ""violinist"", etc.); 9)Classical work types (""symphony"", ""overture"", etc.); 10)Musical instruments; 11)Opus forms (""op"", ""opus""); 12)Work number forms (""no"", ""number""); 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp""); 14)Work Modes (""major"", ""minor"", ""m"").']}]}, {'question': 'What kind of corpus-based features are taken into account?', 'question_id': 'ef4d6c9416e45301ea1a4d550b7c381f377cacd9', 'answers': [{'unanswerable': False, 'extractive_spans': ['standard linguistic features, such as Part-Of-Speech (POS) and chunk tag', ""series of features representing tokens' left and right context""], 'yes_no': None, 'free_form_answer': '', 'evidence': [""We define a set of features for characterizing the text at the token level. We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context. For extracting the POS and the chunk tag we use the Python library twitter_nlp, presented in BIBREF1 .""], 'highlighted_evidence': [""We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context.""]}]}, {'question': 'Which machine learning algorithms did the explore?', 'question_id': '689d1d0c4653a8fa87fd0e01fa7e12f75405cd38', 'answers': [{'unanswerable': False, 'extractive_spans': ['biLSTM-networks'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments. In the first, we train the model using only the word embeddings as feature. In the second, together with the word embeddings we use the POS and chunk tag. In the third, all the features previously defined are included, in addition to the word embeddings. For every experiment, we use both the pre-trained embeddings and the ones that we created with our Twitter corpora. In section 4, results obtained from the several experiments are reported.'], 'highlighted_evidence': ['However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments.']}]}, {'question': 'What language is the Twitter content in?', 'question_id': '7920f228de6ef4c685f478bac4c7776443f19f39', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'English', 'evidence': ['In this work, we analyze a set of tweets related to a specific classical music radio channel, BBC Radio 3, interested in detecting two types of musical named entities, Contributor and Musical Work.'], 'highlighted_evidence': ['In this work, we analyze a set of tweets related to a specific classical music radio channel, BBC Radio 3, interested in detecting two types of musical named entities, Contributor and Musical Work.']}]}]",journals/cys/PorcaroS19,Recognizing Musical Entities in User-generated Content.,2019,The performances of the NER experiments are reported separately for three different parts of the system proposed.,"(0.8688791990280151, 0.8500087261199951, 0.8593403697013855)",7.053317181506347e-232,0.12838506698608398
123,MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge,What is the architecture of the siamese neural network?,"Symmetrical component to extract high level features from different input channels, which share parameters and map inputs to the same vector space.",1709.00387,"[{'question': 'What is the architecture of the siamese neural network?', 'question_id': '41844d1d1ee6d6d38f31b3a17a2398f87566ed92', 'answers': [{'unanswerable': False, 'extractive_spans': ['two parallel convolutional networks, INLINEFORM0 , that share the same set of weights'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['To further distinguish speech from different Arabic dialects, while making speech from the same dialect more similar, we adopted a Siamese neural network architecture BIBREF24 based on an i-vector feature space. The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a). Let INLINEFORM2 and INLINEFORM3 be a pair of i-vectors for which we wish to compute a distance. Let INLINEFORM4 be the label for the pair, where INLINEFORM5 = 1 if the i-vectors INLINEFORM6 and INLINEFORM7 belong to same dialect, and INLINEFORM8 otherwise. To optimize the network, we use a Euclidean distance loss function between the label and the cosine distance, INLINEFORM9 , where INLINEFORM10'], 'highlighted_evidence': ['The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a).']}]}, {'question': 'How do they explore domain mismatch?', 'question_id': 'ae17066634bd2731a07cd60e9ca79fc171692585', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How do they explore dialect variability?', 'question_id': '4fa2faa08eeabc09d78d89aaf0ea86bb36328172', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Which are the four Arabic dialects?', 'question_id': 'e87f47a293e0b49ab8b15fc6633d9ca6dc9de071', 'answers': [{'unanswerable': False, 'extractive_spans': ['Egyptian (EGY)', 'Levantine (LEV)', 'Gulf (GLF)', 'North African (NOR)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['For the MGB-3 ADI task, the challenge organizers provided 13,825 utterances (53.6 hours) for the training (TRN) set, 1,524 utterances (10 hours) for a development (DEV) set, and 1,492 utterances (10.1 hours) for a test (TST) set. Each dataset consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA). Detailed statistics of the ADI dataset can be found in BIBREF23 . Table TABREF3 shows some facts about the evaluation conditions and data properties. Note that the development set is relatively small compared to the training set. However, it is matched with the test set channel domain. Thus, the development set provides valuable information to adapt or compensate the channel (recording) domain mismatch between the train and test sets.'], 'highlighted_evidence': ['Each dataset consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA).']}]}]",conf/asru/ShonAG17,MIT-QCRI Arabic dialect identification system for the 2017 multi-genre broadcast challenge.,2017,"The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a).","(0.8493146896362305, 0.8138810396194458, 0.8312204480171204)",3.5304446123589778e-155,0.24846838414669037
124,Bias in Semantic and Discourse Interpretation,What factors contribute to interpretive biases according to this research?,"The framework developed in this research provides tools for understanding and analyzing the range of interpretive biases, but it does not specify what factors contribute to them.",1806.11322,"[{'question': 'What factors contribute to interpretive biases according to this research?', 'question_id': '7426a6e800d6c11795941616fc4a243e75716a10', 'answers': [{'unanswerable': False, 'extractive_spans': ['Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""While the choice of wording helps to convey bias, just as crucial is the way that the reporters portray the march as being related to other events. Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias. Townhall's bias against the March of Science expressed in the argument that it politicizes science cannot be traced back to negative opinion words; it relies on a comparison between the March for Science and the Women's March, which is portrayed as a political, anti-Trump event. Newsbusters takes a different track: the opening paragraph conveys an overall negative perspective on the March for Science, despite its neutral language, but it achieves this by contrasting general interest in the march with a claimed negative view of the march by many “actual scientists.” On the other hand, the New York Times points to an important and presumably positive outcome of the march, despite its controversiality: a renewed look into the role of science in public life and politics. Like Newsbusters, it lacks any explicit evaluative language and relies on the structural relations between events to convey an overall positive perspective; it contrasts the controversy surrounding the march with a claim that the march has triggered an important discussion, which is in turn buttressed by the reporter's mentioning of the responses of the Times' readership.""], 'highlighted_evidence': ['Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias.']}]}, {'question': 'Which interpretative biases are analyzed in this paper?', 'question_id': 'da4535b75e360604e3ce4bb3631b0ba96f4dadd3', 'answers': [{'unanswerable': False, 'extractive_spans': ['in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""An epistemic ME game is an ME game with a Harsanyi type space and a type/history correspondence as we've defined it. By adding types to an ME game, we provide the beginnings of a game theoretic model of interpretive bias that we believe is completely new. Our definition of bias is now: [Interpretive Bias] An interpretive bias in an epistemic ME game is the probability distribution over types given by the belief function of the conversationalists or players, or the Jury. Note that in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury.""], 'highlighted_evidence': ['Our definition of bias is now: [Interpretive Bias] An interpretive bias in an epistemic ME game is the probability distribution over types given by the belief function of the conversationalists or players, or the Jury.', 'Note that in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury.']}]}]",journals/corr/abs-1806-11322,Bias in Semantic and Discourse Interpretation.,2018,"Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias.","(0.8631500005722046, 0.8514422178268433, 0.8572561144828796)",1.1097877379432508e-231,0.38648363947868347
125,QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships,How does the QuaSP+Zero model work?,It uses an entity-linking approach applied to properties.,1811.08048,"[{'question': 'How does the QuaSP+Zero model work?', 'question_id': 'ce14b87dacfd5206d2a5af7c0ed1cfeb7b181922', 'answers': [{'unanswerable': False, 'extractive_spans': ['does not just consider the question tokens, but also the relationship between those tokens and the properties'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model. For example, a question token such as “longer” can act as a cue for (the property) length, even if unseen in the training data, because “longer” and a lexical form of length (e.g.,“length”) are similar. This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing. Here, we modify their entity linking score INLINEFORM1 , linking question tokens INLINEFORM2 and property “entities” INLINEFORM3 , to be: INLINEFORM4'], 'highlighted_evidence': ['We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model.', 'This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing.']}]}, {'question': 'Which off-the-shelf tools do they use on QuaRel?', 'question_id': '709a4993927187514701fe3cc491ac3030da1215', 'answers': [{'unanswerable': False, 'extractive_spans': ['information retrieval system', 'word-association method', ' CCG-style rule-based semantic parser written specifically for friction questions', 'state-of-the-art neural semantic parser'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We use four systems to evaluate the difficulty of this dataset. (We subsequently present two new models, extending the baseline neural semantic parser, in Sections SECREF36 and SECREF44 ). The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn.'], 'highlighted_evidence': ['We use four systems to evaluate the difficulty of this dataset.', ' The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn.']}]}, {'question': 'How do they obtain the logical forms of their questions in their dataset?', 'question_id': 'a3c6acf900126bc9bd9c50ce99041ea00761da6a', 'answers': [{'unanswerable': False, 'extractive_spans': [' workers were given a seed qualitative relation', 'asked to enter two objects, people, or situations to compare', 'created a question, guided by a large number of examples', 'LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 ).', 'Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:', 'From this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred). Three independent workers answer these follow-up questions to ensure reliable results.'], 'highlighted_evidence': ['First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words.', 'Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:\n\nFrom this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred).']}]}, {'question': 'Do all questions in the dataset allow the answers to pick from 2 options?', 'question_id': '31b631a8634f6180b20a72477040046d1e085494', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 ).'], 'highlighted_evidence': ['We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare.']}]}]",conf/aaai/TafjordC0YS19,QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships.,2019,"We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model.","(0.8801933526992798, 0.8182915449142456, 0.8481144309043884)",8.717413430555888e-235,0.28289371728897095
126,Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts,What accuracy does CNN model achieve?,0.373,1912.07025,"[{'question': 'What accuracy does CNN model achieve?', 'question_id': '79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Combined per-pixel accuracy for character line segments is 74.79', 'evidence': ['FLOAT SELECTED: TABLE IV: Class-wise average IoUs and per-pixel accuracies on the test set. Refer to Table I for full names of abbreviated region types listed at top of the table.', 'FLOAT SELECTED: TABLE I: Counts for various annotated region types in INDISCAPES dataset. The abbreviations used for region types are given below each region type.'], 'highlighted_evidence': ['FLOAT SELECTED: TABLE IV: Class-wise average IoUs and per-pixel accuracies on the test set. Refer to Table I for full names of abbreviated region types listed at top of the table.', 'FLOAT SELECTED: TABLE I: Counts for various annotated region types in INDISCAPES dataset. The abbreviations used for region types are given below each region type.']}]}, {'question': 'How many documents are in the Indiscapes dataset?', 'question_id': '26faad6f42b6d628f341c8d4ce5a08a591eea8c2', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '508', 'evidence': ['FLOAT SELECTED: TABLE III: Scripts in the INDISCAPES dataset.'], 'highlighted_evidence': ['FLOAT SELECTED: TABLE III: Scripts in the INDISCAPES dataset.']}]}, {'question': 'What language(s) are the manuscripts written in?', 'question_id': '20be7a776dfda0d3c9dc10270699061cb9bc8297', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/icdar/PrustyATS19,Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts.,2019,Combined per-pixel accuracy for character line segments is 74.79,"(0.9006454348564148, 0.8377649188041687, 0.8680679798126221)",0.0,0.1665174812078476
127,Evaluating Rewards for Question Generation Models,What human evaluation metrics were used in the paper?,Human-targeted metrics for machine translation.,1902.11049,"[{'question': 'What human evaluation metrics were used in the paper?', 'question_id': 'bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context', 'evidence': ['For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer.'], 'highlighted_evidence': ['For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer.']}]}]",conf/naacl/Hosking019,Evaluating Rewards for Question Generation Models.,2019,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"(0.8524984121322632, 0.8074283599853516, 0.8293514847755432)",0.0,0.3122156262397766
128,Deep contextualized word representations for detecting sarcasm and irony,Do they evaluate only on English?,No,1809.09795,"[{'question': 'Do they evaluate only on English?', 'question_id': 'e196e2ce72eb8b2d50732c26e9bf346df6643f69', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .', 'In Table TABREF1 , we see a notable difference in terms of size among the Twitter datasets. Given this circumstance, and in light of the findings by BIBREF18 , we are interested in studying how the addition of external soft-annotated data impacts on the performance. Thus, in addition to the datasets introduced before, we use two corpora for augmentation purposes. The first dataset was collected using the Twitter API, targeting tweets with the hashtags #sarcasm or #irony, resulting on a total of 180,000 and 45,000 tweets respectively. On the other hand, to obtain non-sarcastic and non-ironic tweets, we relied on the SemEval 2018 Task 1 dataset BIBREF25 . To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems. We later extract all the hashtags in each target dataset and proceed to augment only using those external tweets that contain any of these hashtags. This allows us to, for each class, add a total of 36,835 tweets for the Ptáček corpus, 8,095 for the Riloff corpus and 26,168 for the SemEval-2018 corpus.'], 'highlighted_evidence': ['We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 ', 'Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .', ' To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems.']}]}, {'question': 'What are the 7 different datasets?', 'question_id': '46570c8faaeefecc8232cfc2faab0005faaba35f', 'answers': [{'unanswerable': False, 'extractive_spans': ['SemEval 2018 Task 3', 'BIBREF20', 'BIBREF4', 'SARC 2.0', 'SARC 2.0 pol', 'Sarcasm Corpus V1 (SC-V1)', 'Sarcasm Corpus V2 (SC-V2)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .'], 'highlighted_evidence': ['FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC).']}]}, {'question': 'What are the three different sources of data?', 'question_id': '982d375378238d0adbc9a4c987d633ed16b7f98f', 'answers': [{'unanswerable': False, 'extractive_spans': ['Twitter', 'Reddit', 'Online Dialogues'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .'], 'highlighted_evidence': ['We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.', 'Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.', 'Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.', 'Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .']}]}, {'question': 'What type of model are the ELMo representations used in?', 'question_id': 'bbdb2942dc6de3d384e3a1b705af996a5341031b', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'A bi-LSTM with max-pooling on top of it', 'evidence': ['The usage of a purely character-based input would allow us to directly recover and model these features. Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .', 'Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification.'], 'highlighted_evidence': ['Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10', 'Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .', 'Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification.']}]}, {'question': 'Which morphosyntactic features are thought to indicate irony or sarcasm?', 'question_id': '4ec538e114356f72ef82f001549accefaf85e99c', 'answers': [{'unanswerable': False, 'extractive_spans': ['all caps', 'quotation marks', 'emoticons', 'emojis', 'hashtags'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['On the other hand, deep models for irony and sarcasm detection, which are currently offer state-of-the-art performance, have exploited sequential neural networks such as LSTMs and GRUs BIBREF15 , BIBREF23 on top of distributed word representations. Recently, in addition to using a sequential model, BIBREF14 proposed to use intra-attention to compare elements in a sequence against themselves. This allowed the model to better capture word-to-word level interactions that could also be useful for detecting sarcasm, such as the incongruity phenomenon BIBREF3 . Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags.'], 'highlighted_evidence': ['Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags.']}]}]",conf/wassa/IlicMBM18,Deep contextualized word representations for detecting sarcasm and irony.,2018,"We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.","(0.8150520324707031, 0.7790663242340088, 0.7966530323028564)",0.0,0.09037347137928009
129,Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data,Which frozen acoustic model do they use?,None.,2003.01769,"[{'question': 'Which frozen acoustic model do they use?', 'question_id': '7dce1b64c0040500951c864fce93d1ad7a1809bc', 'answers': [{'unanswerable': False, 'extractive_spans': ['a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics.'], 'highlighted_evidence': ['Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics.']}]}, {'question': 'By how much does using phonetic feedback improve state-of-the-art systems?', 'question_id': 'e1b36927114969f3b759cba056cfb3756de474e4', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9', 'evidence': ['In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.', 'FLOAT SELECTED: Table 2. Speech enhancement scores for the state-of-the-art system trained with the parallel data available in the CHiME4 corpus. Evaluation is done on channel 5 of the simulation et05 data. Mimic loss is applied to the AECNN model trained with time-domain mapping loss only, as well as time-domain and spectral magnitude mapping losses. The joint training system is done with an identical setup to the mimic system with all three losses.'], 'highlighted_evidence': ['In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.', 'FLOAT SELECTED: Table 2. Speech enhancement scores for the state-of-the-art system trained with the parallel data available in the CHiME4 corpus. Evaluation is done on channel 5 of the simulation et05 data. Mimic loss is applied to the AECNN model trained with time-domain mapping loss only, as well as time-domain and spectral magnitude mapping losses. The joint training system is done with an identical setup to the mimic system with all three losses.']}]}]",conf/icassp/PlantingaBF20,Phonetic Feedback for Speech Enhancement with and Without Parallel Speech Data.,2020,"Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics.","(0.9149612188339233, 0.8033794164657593, 0.8555474877357483)",0.0,0.10882855951786041
130,Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction,What features are used?,linguistic characteristics of a claim and its context length (Cl),1910.04006,"[{'question': 'What features are used?', 'question_id': '186ccc18c6361904bee0d58196e341a719fb31c2', 'answers': [{'unanswerable': False, 'extractive_spans': ['Sociodemographics: gender, age, marital status, etc.', 'Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.', 'Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):', 'Sociodemographics: gender, age, marital status, etc.', 'Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.', 'Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.', ""The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features."", 'Feature Extraction ::: Structured Features', 'Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:', 'Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.', 'Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).', 'Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).', ""These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation."", 'Feature Extraction ::: Unstructured Features', 'Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.', 'These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.'], 'highlighted_evidence': [""45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):\n\nSociodemographics: gender, age, marital status, etc.\n\nPast medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.\n\nInformation from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\nThe Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.\n\nFeature Extraction ::: Structured Features\nStructure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:\n\nGlobal Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.\n\nInsight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).\n\nCompliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).\n\nThese features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.\n\nFeature Extraction ::: Unstructured Features\nUnstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.\n\nThese unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.""]}]}, {'question': 'Do they compare to previous models?', 'question_id': 'fd5412e2784acefb50afc3bfae1e087580b90ab9', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6. We then compare two models incorporating the unstructured features. In the ""Baseline+Domain Sentences"" model, we consider whether adding the counts of sentences per EHR that involve each of the seven risk factor domains as identified by our topic extraction model improved the model performance. In the ""Baseline+Clinical Sentiment"" model, we evaluate whether adding clinical sentiment scores for each risk factor domain improved the model performance. We also experimented with combining both sets of features and found no additional improvement.'], 'highlighted_evidence': ['To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6.']}]}, {'question': 'How do they incorporate sentiment analysis?', 'question_id': 'c7f087c78768d5c6f3ff26921858186d627fd4fd', 'answers': [{'unanswerable': False, 'extractive_spans': ['features per admission were extracted as inputs to the readmission risk classifier'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.', 'These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component.', '45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):'], 'highlighted_evidence': ['These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.\n\nThese sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text.', '45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier.']}]}, {'question': 'What is the dataset used?', 'question_id': '82596190560dc2e2ced2131779730f40a3f3eb8c', 'answers': [{'unanswerable': False, 'extractive_spans': ['EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission.'], 'highlighted_evidence': ['The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission.']}]}, {'question': 'How do they extract topics?', 'question_id': '345f65eaff1610deecb02ff785198aa531648e75', 'answers': [{'unanswerable': False, 'extractive_spans': [' automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component.'], 'highlighted_evidence': ['These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text.']}]}]",conf/acl-louhi/Alvarez-Mellado19,Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction.,2019,"45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):

Sociodemographics: gender, age, marital status, etc.

Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.

Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.

The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.

Feature Extraction ::: Structured Features
Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:

Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.

Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).

Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).

These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.

Feature Extraction ::: Unstructured Features
Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.

These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.","(0.8325968384742737, 0.7409234046936035, 0.7840896844863892)",1.0383425877119129e-246,0.2595030665397644
131,Analysing Coreference in Transformer Outputs,What translationese effects are seen in the analysis?,"Translationese source texts exhibit less lexical variety than original Chinese text, making them simpler and easier for machine translation (MT) systems to score well on. The difference between human and machine translation quality is smaller when source texts are translated from English (translationese) rather than original Chinese.",1911.01188,"[{'question': 'What translationese effects are seen in the analysis?', 'question_id': '96c20af8bbef435d0d534d10c42ae15ff2f926f8', 'answers': [{'unanswerable': False, 'extractive_spans': ['potentially indicating a shining through effect', 'explicitation effect'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set.', 'We also characterised the originals and translations according to coreference features such as total number of chains and mentions, average chain length and size of the longest chain. We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do. As future work, we consider a more detailed comparison of the human and machine translations, and analyse the purpose of the additional mentions added by the NMT systems. It would be also interesting to evaluate of the quality of the automatically computed coreferences chains used for S3.'], 'highlighted_evidence': ['Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set.', 'We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do.']}]}, {'question': 'What languages are seen in the news and TED datasets?', 'question_id': '9544cc0244db480217ce9174aa13f1bf09ba0d94', 'answers': [{'unanswerable': False, 'extractive_spans': ['English', 'German'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an English-German corpus annotated with full coreference chains BIBREF46. The corpus contains ca. 160.7 thousand tokens manually annotated with about 14.9 thousand mentions and 4.7 thousand coreference chains. For our analysis, we select a portion of English news texts and TED talks from ParCorFull and translate them with the three NMT systems described in SECREF4 above. As texts considerably differ in their length, we select 17 news texts (494 sentences) and four TED talks (518 sentences). The size (in tokens) of the total data set under analysis – source (src) and human translations (ref) from ParCorFull and the automatic translations produced within this study (S1, S2 and S3) are presented in Table TABREF20.'], 'highlighted_evidence': ['As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an English-German corpus annotated with full coreference chains BIBREF46.']}]}, {'question': 'How are the coreference chain translations evaluated?', 'question_id': 'c97a4a1c0e3d00137a9ae8d6fbb809ba6492991d', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How are the (possibly incorrect) coreference chains in the MT outputs annotated?', 'question_id': '3758669426e8fb55a4102564cf05f2864275041b', 'answers': [{'unanswerable': False, 'extractive_spans': ['allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause)', 'The mentions referring to the same discourse item are linked between each other.', 'chain members are annotated for their correctness'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The English sources and their corresponding human translations into German were already manually annotated for coreference chains. We follow the same scheme as BIBREF47 to annotate the MT outputs with coreference chains. This scheme allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause). The mentions can be defined further in terms of their cohesive function (antecedent, anaphoric, cataphoric, comparative, substitution, ellipsis, apposition). Antecedents can either be marked as simple or split or as entity or event. The annotation scheme also includes pronoun type (personal, possessive, demonstrative, reflexive, relative) and modifier types of NPs (possessive, demonstrative, definite article, or none for proper names), see BIBREF46 for details. The mentions referring to the same discourse item are linked between each other. We use the annotation tool MMAX2 BIBREF48 which was also used for the annotation of ParCorFull.', 'In the next step, chain members are annotated for their correctness. For the incorrect translations of mentions, we include the following error categories: gender, number, case, ambiguous and other. The latter category is open, which means that the annotators can add their own error types during the annotation process. With this, the final typology of errors also considered wrong named entity, wrong word, missing word, wrong syntactic structure, spelling error and addressee reference.'], 'highlighted_evidence': ['We follow the same scheme as BIBREF47 to annotate the MT outputs with coreference chains. This scheme allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause). The mentions can be defined further in terms of their cohesive function (antecedent, anaphoric, cataphoric, comparative, substitution, ellipsis, apposition). Antecedents can either be marked as simple or split or as entity or event. The annotation scheme also includes pronoun type (personal, possessive, demonstrative, reflexive, relative) and modifier types of NPs (possessive, demonstrative, definite article, or none for proper names), see BIBREF46 for details. The mentions referring to the same discourse item are linked between each other. We use the annotation tool MMAX2 BIBREF48 which was also used for the annotation of ParCorFull.', 'In the next step, chain members are annotated for their correctness. For the incorrect translations of mentions, we include the following error categories: gender, number, case, ambiguous and other. The latter category is open, which means that the annotators can add their own error types during the annotation process. With this, the final typology of errors also considered wrong named entity, wrong word, missing word, wrong syntactic structure, spelling error and addressee reference.']}]}, {'question': 'Which three neural machine translation systems are analyzed?', 'question_id': '1ebd6f703458eb6690421398c79abf3fc114148f', 'answers': [{'unanswerable': False, 'extractive_spans': ['first two systems are transformer models trained on different amounts of data', 'The third system includes a modification to consider the information of full coreference chains'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We train three systems (S1, S2 and S3) with the corpora summarised in Table TABREF5. The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sentences as seen in the Table). The third system includes a modification to consider the information of full coreference chains throughout a document augmenting the sentence to be translated with this information and it is trained with the same amount of sentence pairs as S1. A variant of the S3 system participated in the news machine translation of the shared task held at WMT 2019 BIBREF43.'], 'highlighted_evidence': ['We train three systems (S1, S2 and S3) with the corpora summarised in Table TABREF5. The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sentences as seen in the Table). The third system includes a modification to consider the information of full coreference chains throughout a document augmenting the sentence to be translated with this information and it is trained with the same amount of sentence pairs as S1.']}]}, {'question': 'Which coreference phenomena are analyzed?', 'question_id': '15a1df59ed20aa415a4daf0acb256747f6766f77', 'answers': [{'unanswerable': False, 'extractive_spans': ['shining through', 'explicitation'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Therefore, in our analysis, we focus on the chain features related to the phenomena of shining through and explicitation. These features include number of mentions, number of chains, average chain length and the longest chain size. Machine-translated texts are compared to their sources and the corresponding human translations in terms of these features. We expect to find shining through and explicitation effects in automatic translations.'], 'highlighted_evidence': ['Therefore, in our analysis, we focus on the chain features related to the phenomena of shining through and explicitation.']}]}]",conf/discomt/Lapshinova-Koltunski19,Analysing Coreference in Transformer Outputs.,2019,"Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set.","(0.8494495153427124, 0.8568823337554932, 0.8531497120857239)",9.761249998758007e-232,0.5323491096496582
132,NumNet: Machine Reading Comprehension with Numerical Reasoning,what are the existing models they compared with?,BERT model without context and BERT model with flat context representation.,1910.06701,"[{'question': 'what are the existing models they compared with?', 'question_id': '81669c550d32d756f516dab5d2b76ff5f21c0f36', 'answers': [{'unanswerable': False, 'extractive_spans': ['Syn Dep', 'OpenIE', 'SRL', 'BiDAF', 'QANet', 'BERT', 'NAQANet', 'NAQANet+'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Experiments ::: Baselines', 'For comparison, we select several public models as baselines including semantic parsing models:', 'BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;', 'QANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;', 'BERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;', 'and numerical MRC models:', 'NAQANet BIBREF6, a numerical version of QANet model.', 'NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.', 'Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;', 'OpenIE BIBREF6, KDG with open information extraction based sentence representations;', 'SRL BIBREF6, KDG with semantic role labeling based sentence representations;', 'and traditional MRC models:'], 'highlighted_evidence': ['Experiments ::: Baselines\nFor comparison, we select several public models as baselines including semantic parsing models:', 'BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;\n\nQANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;\n\nBERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;\n\nand numerical MRC models:', 'NAQANet BIBREF6, a numerical version of QANet model.\n\nNAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc.', 'Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;\n\nOpenIE BIBREF6, KDG with open information extraction based sentence representations;\n\nSRL BIBREF6, KDG with semantic role labeling based sentence representations;\n\nand traditional MRC models:']}]}]",conf/emnlp/RanLLZL19,NumNet: Machine Reading Comprehension with Numerical Reasoning.,2019,"Experiments ::: Baselines
For comparison, we select several public models as baselines including semantic parsing models:","(0.8545474410057068, 0.833777129650116, 0.8440345525741577)",0.0,0.3471223711967468
133,Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks,What model achieves state of the art performance on this task?,BERT-Base,1705.02394,"[{'question': 'What model achieves state of the art performance on this task?', 'question_id': 'a0fbf90ceb520626b80ff0f9160b3cd5029585a5', 'answers': [{'unanswerable': False, 'extractive_spans': ['BIBREF16'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Multitask learning, on the other hand, does not appear to have any positive impact on performance. Comparing the two CNN models, the addition of multitask learning actually appears to impair performance, with MultitaskCNN doing worse than BasicCNN in all three metrics. The difference is smaller when comparing BasicDCGAN and MultitaskDCGAN, and may not be enough to decidedly conclude that the use of multitask learning has a net negative impact there, but certainly there is no indication of a net positive impact. The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16 . It needs to be noted that in BIBREF16 data from the test speaker's session partner was utilized in the training of the model. Our models in contrast are trained on only four of the five sessions as discussed in SECREF5 . Further, the here presented models are trained on the raw spectrograms of the audio and no feature extraction was employed whatsoever. This representation learning approach is employed in order to allow the DCGAN component of the model to train on vast amounts of unsupervised speech data.""], 'highlighted_evidence': ['The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16 ']}]}, {'question': 'Which multitask annotated corpus is used?', 'question_id': 'e8ca81d5b36952259ef3e0dbeac7b3a622eabe8e', 'answers': [{'unanswerable': False, 'extractive_spans': ['IEMOCAP'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Due to the semi-supervised nature of the proposed Multitask DCGAN model, we utilize both labeled and unlabeled data. For the unlabeled data, we use audio from the AMI BIBREF8 and IEMOCAP BIBREF7 datasets. For the labeled data, we use audio from the IEMOCAP dataset, which comes with labels for activation and valence, both measured on a 5-point Likert scale from three distinct annotators. Although IEMOCAP provides per-word activation and valence labels, in practice these labels do not generally change over time in a given audio file, and so for simplicity we label each audio clip with the average valence and activation. Since valence and activation are both measured on a 5-point scale, the labels are encoded in 5-element one-hot vectors. For instance, a valence of 5 is represented with the vector INLINEFORM0 . The one-hot encoding can be thought of as a probability distribution representing the likelihood of the correct label being some particular value. Thus, in cases where the annotators disagree on the valence or activation label, this can be represented by assigning probabilities to multiple positions in the label vector. For instance, a label of 4.5 conceptually means that the “correct” valence is either 4 or 5 with equal probability, so the corresponding vector would be INLINEFORM1 . These “fuzzy labels” have been shown to improve classification performance in a number of applications BIBREF14 , BIBREF15 . It should be noted here that we had generally greater success with this fuzzy label method than training the neural network model on the valence label directly, i.e. classification task vs. regression.'], 'highlighted_evidence': ['For the labeled data, we use audio from the IEMOCAP dataset, which comes with labels for activation and valence, both measured on a 5-point Likert scale from three distinct annotators.']}]}, {'question': 'What are the tasks in the multitask learning setup?', 'question_id': 'e75685ef5f58027be44f42f30cb3988b509b2768', 'answers': [{'unanswerable': False, 'extractive_spans': ['set of related tasks are learned (e.g., emotional activation)', 'primary task (e.g., emotional valence)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Within this work, we particularly target emotional valence as the primary task, as it has been shown to be the most challenging emotional dimension for acoustic analyses in a number of studies BIBREF10 , BIBREF11 . Apart from solely targeting valence classification, we further investigate the principle of multitask learning. In multitask learning, a set of related tasks are learned (e.g., emotional activation), along with a primary task (e.g., emotional valence); both tasks share parts of the network topology and are hence jointly trained, as depicted in Figure FIGREF4 . It is expected that data for the secondary task models information, which would also be discriminative in learning the primary task. In fact, this approach has been shown to improve generalizability across corpora BIBREF12 .'], 'highlighted_evidence': ['In multitask learning, a set of related tasks are learned (e.g., emotional activation), along with a primary task (e.g., emotional valence); both tasks share parts of the network topology and are hence jointly trained, as depicted in Figure FIGREF4 .']}]}, {'question': 'What are the subtle changes in voice which have been previously overshadowed?', 'question_id': '1df24849e50fcf22f0855e0c0937c1288450ed5c', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/icassp/ChangS17,Learning representations of emotional speech with deep convolutional generative adversarial networks.,2017,"The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16 ","(0.8090707063674927, 0.8017677068710327, 0.8054026365280151)",0.0,0.09597977995872498
134,Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities,Do the authors mention any possible confounds in their study?,No,1608.04917,"[{'question': 'Do the authors mention any possible confounds in their study?', 'question_id': '938688871913862c9f8a28b42165237b7324e0de', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': True, 'free_form_answer': '', 'evidence': ['On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration. We observed the same phenomenon recently during the Brexit campaign BIBREF38 . Along our interpretation the Brexit was “won” to some extent due to these social media activities, which are practically non-existent among the pro-EU political groups. The fact that ENL and EFDD are the least cohesive groups in the European Parliament can be attributed to their political focus. It seems more important for the group to agree on its anti-EU stance and to call for independence and sovereignty, and much less important to agree on other issues put forward in the parliament.'], 'highlighted_evidence': ['On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration.']}]}, {'question': 'What is the relationship between the co-voting and retweeting patterns?', 'question_id': '4170ed011b02663f5b1b1a3c1f0415b7abfaa85d', 'answers': [{'unanswerable': False, 'extractive_spans': ['we observe a positive correlation between retweeting and co-voting', 'strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets', 'Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union', 'significantly negative coefficient, is the area Economic and monetary system'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns. Results from section “sec:coalitionpolicy”, confirm that this is indeed the case. Especially noteworthy are the coalitions between GUE-NGL and Greens-EFA on the left wing, and EFDD and ENL on the right wing. In the section “sec:coalitionpolicy” we interpret these results as a combination of Euroscepticism on both sides, motivated on the left by a skeptical attitude towards the market orientation of the EU, and on the right by a reluctance to give up national sovereignty.'], 'highlighted_evidence': ['Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns.']}]}, {'question': 'Does the analysis find that coalitions are formed in the same way for different policy areas?', 'question_id': 'fd08dc218effecbe5137a7e3b73d9e5e37ace9c1', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['In the area of Economic and monetary system we see a strong cooperation between EPP and S&D (Fig FIGREF42 A), which is on a par with the cohesion of the most cohesive groups (GUE-NGL, S&D, Greens-EFA, ALDE, and EPP), and is above the cohesion of the other groups. As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B. On one hand, we observe cooperation between S&D, ALDE, EPP, and ECR, and on the other, cooperation between GUE-NGL, Greens-EFA, EFDD, ENL, and NI. This division in blocks is seen again in Fig FIGREF42 C, which shows the strongest disagreements. Here, we observe two blocks composed of S&D, EPP, and ALDE on one hand, and GUE-NGL, EFDD, ENL, and NI on the other, which are in strong opposition to each other.', 'In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.', 'The patterns of coalitions forming on Twitter closely resemble those in the European Parliament. In Fig FIGREF42 J we see that the strongest degrees of cooperation on Twitter are within the groups. The only group with low cohesion is the NI, whose members have only seven retweets between them. The coalitions on Twitter follow the seating order in the European Parliament remarkably well (see Fig FIGREF42 K). What is striking is that the same blocks form on the left, center, and on the center-right, both in the European Parliament and on Twitter. The largest difference between the coalitions in the European Parliament and on Twitter is on the far-right, where we observe ENL and NI as isolated blocks.'], 'highlighted_evidence': ['As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B.', 'In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.\n\nThe patterns of coalitions forming on Twitter closely resemble those in the European Parliament.']}]}, {'question': 'What insights does the analysis give about the cohesion of political groups in the European parliament?', 'question_id': 'a85c2510f25c7152940b5ac4333a80e0f91ade6e', 'answers': [{'unanswerable': False, 'extractive_spans': ['Greens-EFA, S&D, and EPP exhibit the highest cohesion', 'non-aligned members NI have the lowest cohesion, followed by EFDD and ENL', 'two methods disagree is the level of cohesion of GUE-NGL'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As with INLINEFORM0 , Greens-EFA, S&D, and EPP exhibit the highest cohesion, even though their ranking is permuted when compared to the ranking obtained with INLINEFORM1 . At the other end of the scale, we observe the same situation as with INLINEFORM2 . The non-aligned members NI have the lowest cohesion, followed by EFDD and ENL.', 'The only place where the two methods disagree is the level of cohesion of GUE-NGL. The Alpha attributes GUE-NGL a rather high level of cohesion, on a par with ALDE, whereas the ERGM attributes them a much lower cohesion. The reason for this difference is the relatively high abstention rate of GUE-NGL. Whereas the overall fraction of non-attending and abstaining MEPs across all RCVs and all political groups is 25%, the GUE-NGL abstention rate is 34%. This is reflected in an above average cohesion by INLINEFORM0 where only yes/no votes are considered, and in a relatively lower, below average cohesion by ERGM. In the later case, the non-attendance is interpreted as a non-cohesive voting of a political groups as a whole.'], 'highlighted_evidence': ['As with INLINEFORM0 , Greens-EFA, S&D, and EPP exhibit the highest cohesion, even though their ranking is permuted when compared to the ranking obtained with INLINEFORM1 . At the other end of the scale, we observe the same situation as with INLINEFORM2 . The non-aligned members NI have the lowest cohesion, followed by EFDD and ENL.', 'The only place where the two methods disagree is the level of cohesion of GUE-NGL. The Alpha attributes GUE-NGL a rather high level of cohesion, on a par with ALDE, whereas the ERGM attributes them a much lower cohesion.']}]}, {'question': 'Do they authors account for differences in usage of Twitter amongst MPs into their model?', 'question_id': 'fa572f1f3f3ce6e1f9f4c9530456329ffc2677ca', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs. The resulting retweet network is an undirected, weighted network.', 'We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 . The higher the ratio, the more each MEP (on average) retweets the MEPs from the same political group, hence, the higher the cohesion of the political group. The definition of the average retweeets ( INLINEFORM3 ) of a group INLINEFORM4 is: INLINEFORM5'], 'highlighted_evidence': ['The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs', 'We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 ']}]}, {'question': 'Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?', 'question_id': '5e057e115f8976bf9fe70ab5321af72eb4b4c0fc', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",journals/corr/CherepnalkoskiK16,Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities.,2016,"On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration.","(0.818499743938446, 0.7801229953765869, 0.7988507151603699)",0.0,0.07197204232215881
135,Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter,How many languages are included in the tweets?,Multiple languages.,1805.09055,"[{'question': 'How many languages are included in the tweets?', 'question_id': 'e78a47aec37d9a3bec5a18706b0a462c148c118b', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What languages are explored?', 'question_id': '351510da69ab6879df5ff5c7c5f49a8a7aea4632', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'Which countries did they look at?', 'question_id': 'd43e868cae91b3dc393c05c55da0754b0fb3a46a', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}]",conf/acl-peoples/VilaresG18,Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter.,2018,,"(0.0, 0.0, 0.0)",0.0,0.18922065198421478
136,QA4IE: A Question Answering based Framework for Information Extraction,What QA models were used?,Scoremul and Scoreavg,1804.03396,"[{'question': 'What QA models were used?', 'question_id': 'fd8b6723ad5f52770bec9009e45f860f4a8c4321', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.', 'evidence': ['The input of our model are the words in the input text $x[1], ... , x[n]$ and query $q[1], ... , q[n]$ . We concatenate pre-trained word embeddings from GloVe BIBREF40 and character embeddings trained by CharCNN BIBREF41 to represent input words. The $2d$ -dimension embedding vectors of input text $x_1, ... , x_n$ and query $q_1, ... , q_n$ are then fed into a Highway Layer BIBREF42 to improve the capability of word embeddings and character embeddings as', '$$\\begin{split} g_t &= {\\rm sigmoid}(W_gx_t+b_g) \\\\ s_t &= {\\rm relu } (W_xx_t+b_x) \\\\ u_t &= g_t \\odot s_t + (1 - g_t) \\odot x_t~. \\end{split}$$ (Eq. 18)', 'Here $W_g, W_x \\in \\mathbb {R}^{d \\times 2d}$ and $b_g, b_x \\in \\mathbb {R}^d$ are trainable weights, $u_t$ is a $d$ -dimension vector. The function relu is the rectified linear units BIBREF43 and $\\odot $ is element-wise multiply over two vectors. The same Highway Layer is applied to $q_t$ and produces $v_t$ .', 'Next, $u_t$ and $v_t$ are fed into a Bi-Directional Long Short-Term Memory Network (BiLSTM) BIBREF44 respectively in order to model the temporal interactions between sequence words:', 'Here we obtain $\\mathbf {U} = [u_1^{^{\\prime }}, ... , u_n^{^{\\prime }}] \\in \\mathbb {R}^{2d \\times n}$ and $\\mathbf {V} = [v_1^{^{\\prime }}, ... , v_m^{^{\\prime }}] \\in \\mathbb {R}^{2d \\times m}$ . Then we feed $\\mathbf {U}$ and $\\mathbf {V}$ into the attention flow layer BIBREF27 to model the interactions between the input text and query. We obtain the $8d$ -dimension query-aware context embedding vectors $h_1, ... , h_n$ as the result.', 'After modeling interactions between the input text and queries, we need to enhance the interactions within the input text words themselves especially for the longer text in IE settings. Therefore, we introduce Self-Matching Layer BIBREF29 in our model as', '$$\\begin{split} o_t &= {\\rm BiLSTM}(o_{t-1}, [h_t, c_t]) \\\\ s_j^t &= w^T {\\rm tanh}(W_hh_j+\\tilde{W_h}h_t)\\\\ \\alpha _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\alpha _i^th_i ~. \\end{split}$$ (Eq. 20)', 'Here $W_h, \\tilde{W_h} \\in \\mathbb {R}^{d \\times 8d}$ and $w \\in \\mathbb {R}^d$ are trainable weights, $[h, c]$ is vector concatenation across row. Besides, $\\alpha _i^t$ is the attention weight from the $t^{th}$ word to the $i^{th}$ word and $c_t$ is the enhanced contextual embeddings over the $t^{th}$ word in the input text. We obtain the $2d$ -dimension query-aware and self-enhanced embeddings of input text after this step. Finally we feed the embeddings $\\mathbf {O} = [o_1, ... , o_n]$ into a Pointer Network BIBREF39 to decode the answer sequence as', '$$\\begin{split} p_t &= {\\rm LSTM}(p_{t-1}, c_t) \\\\ s_j^t &= w^T {\\rm tanh}(W_oo_j+W_pp_{t-1})\\\\ \\beta _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\beta _i^to_i~. \\end{split}$$ (Eq. 21)', 'Here $\\beta _{n+1}^t$ denotes the probability of generating the “ ${\\rm eos}$ ” symbol since the decoder also needs to determine when to stop. Therefore, the probability of generating the answer sequence $\\textbf {a}$ is as follows', '$${\\rm P}(\\textbf {a}|\\mathbf {O}) = \\prod _t {\\rm P}(a^t | a^1, ... , a^{t-1}, \\mathbf {O})~.$$ (Eq. 23)'], 'highlighted_evidence': ['The input of our model are the words in the input text $x[1], ... , x[n]$ and query $q[1], ... , q[n]$ . We concatenate pre-trained word embeddings from GloVe BIBREF40 and character embeddings trained by CharCNN BIBREF41 to represent input words. The $2d$ -dimension embedding vectors of input text $x_1, ... , x_n$ and query $q_1, ... , q_n$ are then fed into a Highway Layer BIBREF42 to improve the capability of word embeddings and character embeddings as\n\n$$\\begin{split} g_t &= {\\rm sigmoid}(W_gx_t+b_g) \\\\ s_t &= {\\rm relu } (W_xx_t+b_x) \\\\ u_t &= g_t \\odot s_t + (1 - g_t) \\odot x_t~. \\end{split}$$ (Eq. 18)', 'The same Highway Layer is applied to $q_t$ and produces $v_t$ .', 'Next, $u_t$ and $v_t$ are fed into a Bi-Directional Long Short-Term Memory Network (BiLSTM) BIBREF44 respectively in order to model the temporal interactions between sequence words:', 'Then we feed $\\mathbf {U}$ and $\\mathbf {V}$ into the attention flow layer BIBREF27 to model the interactions between the input text and query.', 'Therefore, we introduce Self-Matching Layer BIBREF29 in our model as\n\n$$\\begin{split} o_t &= {\\rm BiLSTM}(o_{t-1}, [h_t, c_t]) \\\\ s_j^t &= w^T {\\rm tanh}(W_hh_j+\\tilde{W_h}h_t)\\\\ \\alpha _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\alpha _i^th_i ~. \\end{split}$$ (Eq. 20)', 'Finally we feed the embeddings $\\mathbf {O} = [o_1, ... , o_n]$ into a Pointer Network BIBREF39 to decode the answer sequence as\n\n$$\\begin{split} p_t &= {\\rm LSTM}(p_{t-1}, c_t) \\\\ s_j^t &= w^T {\\rm tanh}(W_oo_j+W_pp_{t-1})\\\\ \\beta _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\beta _i^to_i~. \\end{split}$$ (Eq. 21)', 'Therefore, the probability of generating the answer sequence $\\textbf {a}$ is as follows\n\n$${\\rm P}(\\textbf {a}|\\mathbf {O}) = \\prod _t {\\rm P}(a^t | a^1, ... , a^{t-1}, \\mathbf {O})~.$$ (Eq. 23)']}]}, {'question': 'Can this approach model n-ary relations?', 'question_id': '4ce3a6632e7d86d29a42bd1fcf325114b3c11d46', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. Besides, processing longer documents and improving the quality of our benchmark are all challenging problems as we mentioned previously. We hope this work can provide new thoughts for the area of information extraction.', 'The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. We ignore the adverbials and only consider the entity pairs and their relations as in standard RE settings. Note that we process the entire document as a whole instead of processing individual sentences separately as in previous systems. As shown in Figure 1 , our QA4IE framework consists of four key steps:'], 'highlighted_evidence': ['For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper.', 'The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation.']}]}, {'question': 'Was this benchmark automatically created from an existing dataset?', 'question_id': 'e7c0cdc05b48889905cc03215d1993ab94fb6eaa', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': ['As mentioned above, step 1, 2 and 4 in the QA4IE framework can be solved by existing work. Therefore, in this paper, we mainly focus on step 3. According to the recent progress in QA and MRC, deep neural networks are very good at solving this kind of problem with a large-scale dataset to train the network. However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark. Inspired by WikiReading BIBREF33 , a recent large-scale QA benchmark over Wikipedia, we find that the articles in Wikipedia together with the high quality triples in knowledge bases such as Wikidata BIBREF34 and DBpedia can form the supervision we need. Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.', 'Incorporating DBpedia. Unlike WikiData, DBpedia is constructed automatically without human verification. Relations and properties in DBpedia are coarse and noisy. Thus we fix the existing 636 relation types in WikiData and build a projection from DBpedia relations to these 636 relation types. We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations. Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles.'], 'highlighted_evidence': ['However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark.', 'Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.', 'We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations.']}]}]",conf/semweb/QiuZQZLRRQTY18,QA4IE: A Question Answering Based Framework for Information Extraction.,2018,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","(0.7703118920326233, 0.7891542911529541, 0.7796193361282349)",2.3120046804627377e-236,0.10674585402011871
137,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,What baseline is used to compare the experimental results against?,3-class representation for the impact labels.,1911.03842,"[{'question': 'What baseline is used to compare the experimental results against?', 'question_id': 'd0b005cb7ed6d4c307745096b2ed8762612480d2', 'answers': [{'unanswerable': False, 'extractive_spans': ['Transformer generation model'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous. While ALL has more data than CDA and CT, more data alone is not enough — the Positive-Bias Data Collection model does not achieve as good results. Both the CT and ALL models benefit from knowing the data split ($\\text{F}^{0}\\text{M}^{0}$, for example), and both models yield a genderedness ratio closest to ground truth.'], 'highlighted_evidence': ['Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous.']}]}, {'question': 'How does counterfactual data augmentation aim to tackle bias?', 'question_id': '9d9b11f86a96c6d3dd862453bf240d6e018e75af', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The training dataset is augmented by swapping all gendered words by their other gender counterparts', 'evidence': ['One of the solutions that has been proposed for mitigating gender bias on the word embedding level is Counterfactual Data Augmentation (CDA) BIBREF25. We apply this method by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by BIBREF21. For example, all instances of grandmother are swapped with grandfather.'], 'highlighted_evidence': ['One of the solutions that has been proposed for mitigating gender bias on the word embedding level is Counterfactual Data Augmentation (CDA) BIBREF25. We apply this method by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by BIBREF21.']}]}, {'question': 'In the targeted data collection approach, what type of data is targetted?', 'question_id': '415f35adb0ef746883fb9c33aa53b79cc4e723c3', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Gendered characters in the dataset', 'evidence': ['There are a larger number of male-gendered character personas than female-gendered character personas (see Section SECREF2), so we balance existing personas using gender-swapping. For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns. Additionally, we ask annotators to swap the gender of any characters that are referred to in the persona text for a given character.'], 'highlighted_evidence': ['For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns.']}]}]",conf/emnlp/DinanFWUKW20,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation.,2020,"Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous.","(0.8527161478996277, 0.8183016777038574, 0.8351545333862305)",5.619960642505782e-234,0.20584778487682343
138,Microsoft Research Asia's Systems for WMT19,What is their best performance on the largest language direction dataset?,Their best performance on the largest language direction dataset is 98.5%.,1911.06191,"[{'question': 'What is their best performance on the largest language direction dataset?', 'question_id': '9846f84747b89f5c692665c4ea7111671ad9839a', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'How does soft contextual data augmentation work?', 'question_id': 'eecf62e18a790bcfdd8a56f0c4f498927ff2fb47', 'answers': [{'unanswerable': False, 'extractive_spans': ['softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words', 'replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is relatively limited. SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary. It was applied in Russian$\\rightarrow $English translation in our submitted systems.'], 'highlighted_evidence': ['SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary.']}]}, {'question': 'How does muli-agent dual learning work?', 'question_id': 'acda028a21a465c984036dcbb124b7f03c490b41', 'answers': [{'unanswerable': False, 'extractive_spans': ['MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models.'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The core idea of dual learning is to leverage the duality between the primal task (mapping from domain $\\mathcal {X}$ to domain $\\mathcal {Y}$) and dual task (mapping from domain $\\mathcal {Y}$ to $\\mathcal {X}$ ) to boost the performances of both tasks. MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models. It was integrated into our submitted systems for German$\\leftrightarrow $English and German$\\leftrightarrow $French translations.'], 'highlighted_evidence': ['The core idea of dual learning is to leverage the duality between the primal task (mapping from domain $\\mathcal {X}$ to domain $\\mathcal {Y}$) and dual task (mapping from domain $\\mathcal {Y}$ to $\\mathcal {X}$ ) to boost the performances of both tasks. MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models.']}]}, {'question': 'Which language directions are machine translation systems of WMT evaluated on?', 'question_id': '42af0472e6895eaf7b9392674b0d956e64e86b03', 'answers': [{'unanswerable': False, 'extractive_spans': ['German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English', 'Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We participated in the WMT19 shared news translation task in 11 translation directions. We achieved first place for 8 directions: German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, and three other directions were placed second (ranked by teams), which included Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh.'], 'highlighted_evidence': ['We achieved first place for 8 directions: German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, and three other directions were placed second (ranked by teams), which included Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh.']}]}]",conf/wmt/XiaTTGHCFGLLWWZ19,Microsoft Research Asia&apos;s Systems for WMT19.,2019,,"(0.0, 0.0, 0.0)",0.0,0.043020039796829224
139,Few-shot Natural Language Generation for Task-Oriented Dialog,What was the criteria for human evaluation?,"RE Match, Fluency, Sentiment",2002.12328,"[{'question': 'What was the criteria for human evaluation?', 'question_id': 'd803b782023553bbf9b36551fbc888ad189b1f29', 'answers': [{'unanswerable': False, 'extractive_spans': ['to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges.'], 'highlighted_evidence': ['We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges.']}]}, {'question': 'What automatic metrics are used to measure performance of the system?', 'question_id': 'fc5f9604c74c9bb804064f315676520937131e17', 'answers': [{'unanswerable': False, 'extractive_spans': ['BLEU scores and the slot error rate (ERR)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Following BIBREF3, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. $\\text{ERR}=(p+q)/M$, where $M$ is the total number of slots in the dialog act, and $p$, $q$ is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output.'], 'highlighted_evidence': ['Following BIBREF3, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. $\\text{ERR}=(p+q)/M$, where $M$ is the total number of slots in the dialog act, and $p$, $q$ is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output.']}]}, {'question': 'What existing methods is SC-GPT compared to?', 'question_id': 'b37fd665dfa5fad43977069d5623f4490a979305', 'answers': [{'unanswerable': False, 'extractive_spans': ['$({1})$ SC-LSTM BIBREF3', '$({2})$ GPT-2 BIBREF6 ', '$({3})$ HDSA BIBREF7'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['We compare with three baseline methods. $({1})$ SC-LSTM BIBREF3 is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. $({2})$ GPT-2 BIBREF6 is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. $({3})$ HDSA BIBREF7 is a state-of-the-art model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting, showing superior performance than SC-LSTM.'], 'highlighted_evidence': ['We compare with three baseline methods. $({1})$ SC-LSTM BIBREF3 is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. $({2})$ GPT-2 BIBREF6 is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. $({3})$ HDSA BIBREF7 is a state-of-the-art model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting, showing superior performance than SC-LSTM.']}]}]",conf/acl/WangZFAH23,DSPM-NLG: A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System.,2023,"We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges.","(0.8247534036636353, 0.7760477662086487, 0.7996596097946167)",0.0,0.427201509475708
140,Finding Street Gang Members on Twitter,Do they evaluate only on English datasets?,No.,1610.09516,"[{'question': 'Do they evaluate only on English datasets?', 'question_id': '3460393d6888dd34113fa0813a1b3a1514c66aa6', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What are the differences in the use of emojis between gang member and the rest of the Twitter population?', 'question_id': 'd491ee69db39ec65f0f6da9ec03450520389699a', 'answers': [{'unanswerable': False, 'extractive_spans': ['32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members', 'only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them', 'gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Motivated by recent work involving the use of emojis by gang members BIBREF22 , we also studied if and how gang and non-gang members use emoji symbols in their tweets. Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets. The frequency of each emoji symbol used across the set of user's tweets are thus considered as features for our classifier.""], 'highlighted_evidence': [""Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets.""]}]}, {'question': 'What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?', 'question_id': 'd3839c7acee4f9c8db0a4a475214a8dcbd0bc26f', 'answers': [{'unanswerable': False, 'extractive_spans': ['76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""It has been recognized that music is a key cultural component in an urban lifestyle and that gang members often want to emulate the scenarios and activities the music conveys BIBREF7 . Our analysis confirms that the influence of gangster rap is expressed in gang members' Twitter posts. We found that 51.25% of the gang members collected have a tweet that links to a YouTube video. Following these links, a simple keyword search for the terms gangsta and hip-hop in the YouTube video description found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre. Moreover, this high proportion is not driven by a small number of profiles that prolifically share YouTube links; eight YouTube links are shared on average by a gang member.""], 'highlighted_evidence': ['We found that 51.25% of the gang members collected have a tweet that links to a YouTube video. Following these links, a simple keyword search for the terms gangsta and hip-hop in the YouTube video description found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre.']}]}, {'question': 'What are the differences in the use of images between gang member and the rest of the Twitter population?', 'question_id': 'a6d00f44ff8f83b6c1787e39333e759b0c3daf15', 'answers': [{'unanswerable': False, 'extractive_spans': ['user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash. Descriptions of these images may thus empower our classifier. Thus, we translated profile images into features with the Clarifai web service. Clarifai offers a free API to query a deep learning system that tags images with a set of scored keywords that reflect what is seen in the image. We tagged the profile image and cover image for each profile using 20 tags identified by Clarifai. Figure FIGREF36 offers the 20 most often used tags applied to gang and non-gang member profiles. Since we take all the tags returned for an image, we see common words such as people and adult coming up in the top 20 tag set. However, gang member profile images were assigned unique tags such as trigger, bullet, worship while non-gang images were uniquely tagged with beach, seashore, dawn, wildlife, sand, pet. The set of tags returned by Clarifai were thus considered as features for the classifier.'], 'highlighted_evidence': ['In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash.']}]}, {'question': 'What are the differences in language use between gang member and the rest of the Twitter population?', 'question_id': '0d4aa05eb00d9dee74000ea5b21b08f693ba1e62', 'answers': [{'unanswerable': False, 'extractive_spans': ['Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word', 'gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us'], 'yes_no': None, 'free_form_answer': '', 'evidence': [""Figure FIGREF14 summarizes the words seen most often in the gang and non-gang members' tweets as clouds. They show a clear difference in language. For example, we note that gang members more frequently use curse words in comparison to ordinary users. Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, which is nearly five times more than the average curse word usage on Twitter. The clouds also reflect the fact that gang members often talk about drugs and money with terms such as smoke, high, hit, and money, while ordinary users hardly speak about finances and drugs. We also noticed that gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us. These differences make it clear that the individual words used by gang and non-gang members will be relevant features for gang profile classification.""], 'highlighted_evidence': ['Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, which is nearly five times more than the average curse word usage on Twitter.', 'The clouds also reflect the fact that gang members often talk about drugs and money with terms such as smoke, high, hit, and money, while ordinary users hardly speak about finances and drugs. We also noticed that gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us.']}]}, {'question': 'How is gang membership verified?', 'question_id': '382bef47d316d7c12ea190ae160bf0912a0f4ffe', 'answers': [{'unanswerable': False, 'extractive_spans': ['Manual verification'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['3. Manual verification of Twitter profiles: We verified each profile discovered manually by examining the profile picture, profile background image, recent tweets, and recent pictures posted by a user. During these checks, we searched for terms, activities, and symbols that we believed could be associated with a gang. For example, profiles whose image or background included guns in a threatening way, stacks of money, showing gang hand signs and gestures, and humans holding or posing with a gun, appeared likely to be from a gang member. Such images were often identified in profiles of users who submitted tweets that contain messages of support or sadness for prisoners or recently fallen gang members, or used a high volume of threatening and intimidating slang language. Only profiles where the images, words, and tweets all suggested gang affiliation were labeled as gang affiliates and added to our dataset. Although this manual verification does have a degree of subjectivity, in practice, the images and words used by gang members on social media are so pronounced that we believe any reasonable analyst would agree that they are gang members. We found that not all the profiles collected belonged to gang members; we observed relatives and followers of gang members posting the same hashtags as in Step 1 to convey similar feelings in their profile descriptions.'], 'highlighted_evidence': ['Manual verification of Twitter profiles: We verified each profile discovered manually by examining the profile picture, profile background image, recent tweets, and recent pictures posted by a user.']}]}, {'question': ""Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?"", 'question_id': '32a232310babb92991c4b1b75f7aa6b4670ec447', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': False, 'free_form_answer': '', 'evidence': [""Street gang members have established online presences coinciding with their physical occupation of neighborhoods. The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . Gang members typically use social networking sites and social media to develop online respect for their street gang BIBREF5 and to post intimidating, threatening images or videos BIBREF6 . This “Cyber-” or “Internet banging” BIBREF7 behavior is precipitated by the fact that an increasing number of young members of the society are joining gangs BIBREF8 , and these young members have become enamored with technology and with the notion of sharing information quickly and publicly through social media. Stronger police surveillance in the physical spaces where gangs congregate further encourages gang members to seek out virtual spaces such as social media to express their affiliation, to sell drugs, and to celebrate their illegal activities BIBREF9 .""], 'highlighted_evidence': [""The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 .""]}]}]",conf/asunam/BalasuriyaWDS16,Finding street gang members on Twitter.,2016,,"(0.0, 0.0, 0.0)",0.0,0.48065316677093506
141,A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,What is English mixed with in the TRAC dataset?,German station and street names.,2001.05493,"[{'question': 'What is English mixed with in the TRAC dataset?', 'question_id': '5845d1db7f819dbadb72e7df69d49c3f424b5730', 'answers': [{'unanswerable': False, 'extractive_spans': ['Hindi'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['In future, we will explore other methods to increase the understanding of deep learning models on group targeted text, although the categories are well defined we will look after if we further fine-tune the categories with more data. In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).', 'The block diagram of the proposed system is shown in Figure FIGREF22. The proposed system does not use any data augmentation techniques like BIBREF14, which is the top performer in TRAC (in English code-mixed Facebook data). This means the performance achieved by our system totally depends on the training dataset provided by TRAC. This also proves the effectiveness of our approach. Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set. The remaining part of this paper is organized as follows: Section SECREF2 is an overview of related work. Section SECREF3 presents the methodology and algorithmic details. Section SECREF4 discusses the experimental evaluation of the system, and Section SECREF5 concludes this paper.', 'The fine-grained definition of the aggressiveness/aggression identification is provided by the organizers of TRAC-2018 BIBREF0, BIBREF2. They have classified the aggressiveness into three labels (Overtly aggressive(OAG), Covertly aggressive(CAG), Non-aggressive(NAG)). The detailed description for each of the three labels is described as follows:', 'Overtly Aggressive(OAG) - This type of aggression shows direct verbal attack pointing to the particular individual or group. For example, ""Well said sonu..you have courage to stand against dadagiri of Muslims"".', 'Covertly Aggressive(CAG) - This type of aggression the attack is not direct but hidden, subtle and more indirect while being stated politely most of the times. For example, ""Dear India, stop playing with the emotions of your people for votes.""', 'Non-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive.'], 'highlighted_evidence': [' In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).', 'Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set.', 'The fine-grained definition of the aggressiveness/aggression identification is provided by the organizers of TRAC-2018 BIBREF0, BIBREF2. They have classified the aggressiveness into three labels (Overtly aggressive(OAG), Covertly aggressive(CAG), Non-aggressive(NAG)). The detailed description for each of the three labels is described as follows:\n\nOvertly Aggressive(OAG) - This type of aggression shows direct verbal attack pointing to the particular individual or group. For example, ""Well said sonu..you have courage to stand against dadagiri of Muslims"".\n\nCovertly Aggressive(CAG) - This type of aggression the attack is not direct but hidden, subtle and more indirect while being stated politely most of the times. For example, ""Dear India, stop playing with the emotions of your people for votes.""\n\nNon-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive.']}]}, {'question': 'Which psycholinguistic and basic linguistic features are used?', 'question_id': 'e829f008d62312357e0354a9ed3b0827c91c9401', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features', 'evidence': ['Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term ""NLP Features"" to represent it in the entire paper.', 'We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.).', 'FLOAT SELECTED: Table 1: Details of NLP features'], 'highlighted_evidence': ['Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term ""NLP Features"" to represent it in the entire paper.', 'We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.).', 'FLOAT SELECTED: Table 1: Details of NLP features']}]}, {'question': 'How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?', 'question_id': '54fe8f05595f2d1d4a4fd77f4562eac519711fa6', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Systems do not perform well both in Facebook and Twitter texts', 'evidence': ['Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. So, we concentrated to develop a robust system for English code-mixed texts, and uni-lingual texts, which can also handle different writing styles. Our approach is based on three main ideas:'], 'highlighted_evidence': ['Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets.']}]}, {'question': 'What are the key differences in communication styles between Twitter and Facebook?', 'question_id': '61404466cf86a21f0c1783ce535eb39a01528ce8', 'answers': [{'unanswerable': True, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '', 'evidence': [], 'highlighted_evidence': []}]}, {'question': 'What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?', 'question_id': 'fbe5e513745d723aad711ceb91ce0c3c2ceb669e', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'None', 'evidence': ['The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset.'], 'highlighted_evidence': ['The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset.']}]}]",conf/comad/KhandelwalK20,A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts.,2020," In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).","(0.8573687076568604, 0.8072856664657593, 0.8315737843513489)",2.024329193682772e-234,0.04591238871216774
142,An Emotional Analysis of False Information in Social Media and News Articles,What is the baseline?,"There is no mention of a ""baseline"" in the provided context.",1908.09951,"[{'question': 'What is the baseline?', 'question_id': '1571e16063b53409f2d1bd6ec143fccc5b29ebb9', 'answers': [{'unanswerable': False, 'extractive_spans': ['Majority Class baseline (MC) ', 'Random selection baseline (RAN)'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Emotions have been used in many natural language processing tasks and they showed their efficiency BIBREF35. We aim at investigating their efficiency to detect false information. In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN).'], 'highlighted_evidence': [' In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN).']}]}, {'question': 'What datasets did they use?', 'question_id': 'd71937fa5da853f7529f767730547ccfb70e5908', 'answers': [{'unanswerable': False, 'extractive_spans': ['News Articles', 'Twitter'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Evaluation Framework ::: Datasets ::: News Articles', ""Our dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites. These news articles include satires, hoaxes, and propagandas but not clickbaits. Since we are interested also in analyzing clickbaits, we slice a sample from an available clickbait dataset BIBREF33 that was originally collected from two sources: Wikinews articles' headlines and other online sites that are known to publish clickbaits. The satire, hoax, and propaganda news articles are considerably long (some of them reach the length of 5,000 words). This length could affect the quality of the analysis as we mentioned before. We focus on analyzing the initial part of the article. Our intuition is that it is where emotion-bearing words will be more frequent. Therefore, we shorten long news articles into a maximum length of N words (N=300). We choose the value of N based on the length of the shortest articles. Moreover, we process the dataset by removing very short articles, redundant articles or articles that do not have a textual content."", 'With the complicated political and economic situations in many countries, some agendas are publishing suspicious news to affect public opinions regarding specific issues BIBREF0. The spreading of this phenomenon is increasing recently with the large usage of social media and online news sources. Many anonymous accounts in social media platforms start to appear, as well as new online news agencies without presenting a clear identity of the owner. Twitter has recently detected a campaign organized by agencies from two different countries to affect the results of the last U.S. presidential elections of 2016. The initial disclosures by Twitter have included 3,841 accounts. A similar attempt was done by Facebook, as they detected coordinated efforts to influence U.S. politics ahead of the 2018 midterm elections.', 'For this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. For the real news, we merge this list with another 32 Twitter accounts from BIBREF34. In this work we could not use the previous dataset and we decide to collect tweets again. For each of these accounts, we collected the last M tweets posted (M=1000). By investigating these accounts manually, we found that many tweets just contain links without textual news. Therefore, to ensure of the quality of the crawled data, we chose a high value for M (also to have enough data). After the collecting process, we processed these tweets by removing duplicated, very short tweets, and tweets without textual content. Table TABREF35 shows a summary for both datasets.'], 'highlighted_evidence': [' News Articles\nOur dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites.', 'Twitter\nFor this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. ']}]}]",journals/toit/GhanemRP20,An Emotional Analysis of False Information in Social Media and News Articles.,2020," In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN).","(0.8396372199058533, 0.8164552450180054, 0.8278839588165283)",3.4036208240678663e-233,0.249537855386734
143,STransE: a novel embedding model of entities and relationships in knowledge bases,What scoring function does the model use to score triples?,"fr(h,t) = ∥Wr,1h + r −Wr,2t∥ℓ1/2",1606.08140,"[{'question': 'What scoring function does the model use to score triples?', 'question_id': '8d258899e36326183899ebc67aeb4188a86f682c', 'answers': [{'unanswerable': False, 'extractive_spans': ['$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['Let $\\mathcal {E}$ denote the set of entities and $\\mathcal {R}$ the set of relation types. For each triple $(h, r, t)$ , where $h, t \\in \\mathcal {E}$ and $r \\in \\mathcal {R}$ , the STransE model defines a score function $f_r(h, t)$ of its implausibility. Our goal is to choose $f$ such that the score $f_r(h,t)$ of a plausible triple $(h,r,t)$ is smaller than the score $f_{r^{\\prime }}(h^{\\prime },t^{\\prime })$ of an implausible triple $\\mathcal {R}$0 . We define the STransE score function $\\mathcal {R}$1 as follows:', '$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $', 'using either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results). To learn the vectors and matrices we minimize the following margin-based objective function: $ \\mathcal {L} & = & \\sum _{\\begin{array}{c}(h,r,t) \\in \\mathcal {G} \\\\ (h^{\\prime },r,t^{\\prime }) \\in \\mathcal {G}^{\\prime }_{(h, r, t)}\\end{array}} [\\gamma + f_r(h, t) - f_r(h^{\\prime }, t^{\\prime })]_+ $'], 'highlighted_evidence': ['We define the STransE score function $\\mathcal {R}$1 as follows:\n\n$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $\n\nusing either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results).']}]}, {'question': 'What datasets are used to evaluate the model?', 'question_id': '955ca31999309685c1daa5cb03867971ca99ec52', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'WN18, FB15k', 'evidence': ['As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. We expect that the STransE will also be able to serve as the basis for extended models that exploit a wider variety of information sources, just as TransE does.'], 'highlighted_evidence': ['As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion.']}]}]",conf/naacl/NguyenSQJ16,STransE: a novel embedding model of entities and relationships in knowledge bases.,2016,"We define the STransE score function $\mathcal {R}$1 as follows:

$ f_r(h, t) & = & \Vert \textbf {W}_{r,1}\textbf {h} + \textbf {r} - \textbf {W}_{r,2}\textbf {t}\Vert _{\ell _{1/2}} $

using either the $\ell _1$ or the $\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\ell _1$ norm gave slightly better results).","(0.8530778288841248, 0.7701229453086853, 0.809480607509613)",1.446089787577154e-235,0.3029654622077942
144,Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks,What are the differences with previous applications of neural networks for this task?,Simple models can outperform or closely match performance of complex architectures; all the models considered are task-independent and were successfully used in different contexts than Hypothesis Evaluation.,1710.01507,"[{'question': 'What are the differences with previous applications of neural networks for this task?', 'question_id': 'acc8d9918d19c212ec256181e51292f2957b37d7', 'answers': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'This approach considers related images', 'evidence': ['One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task.'], 'highlighted_evidence': ['One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task.']}]}]",conf/sigir/KumarKGLV18,Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks.,2018,This approach considers related images,"(0.8258789777755737, 0.8674986362457275, 0.846177339553833)",0.0,0.17885228991508484
