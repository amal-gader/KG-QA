title,question,response,ground_truth
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,What is the primary purpose of BERT?,BERT's primary purpose is pre-training deep bidirectional transformers for language understanding.,"BERT's primary purpose is to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, enabling fine-tuning without substantial task-specific architecture modifications."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,What are the two unsupervised pre-training tasks used in BERT?,Masked language model (MLM) and next sentence prediction.,"The two unsupervised pre-training tasks used in BERT are:

Masked Language Model (MLM): Randomly masks some tokens from the input and predicts the original vocabulary id of the masked word based only on its context.

Next Sentence Prediction (NSP): Task that jointly pretrains text-pair representations."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,What is the purpose of the next sentence prediction (NSP) task in BERT?,"To pre-train the model towards the next sentence prediction task, which is beneficial to both QA and NLI.","In order to train a model that understands sentence relationships, a binarized next sentence prediction task that can be trivially generated from any monolingual corpus is used."
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,How does BERT's architecture differ from OpenAI GPT and ELMo?,"BERT's architecture differs from OpenAI GPT in its bi-directionality and two pre-training tasks, as well as differences in training data and the use of special tokens. Unlike ELMo, BERT is a fine-tuning approach rather than a feature-based approach.",BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMs to generate features for downstream tasks.
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,How is BERT input structured for sentence pairs?,"BERT input for sentence pairs consists of a special classification token ([CLS]), the two sentences packed together, separated by a special token ([SEP]), and learned embeddings indicating whether each token belongs to sentence A or sentence B.","Sentence pairs are packed together into a single sequence, separated with a special token ([SEP]), and a learned embedding is added to every token indicating whether it belongs to sentence A or sentence B."
SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers,What is SemEval 2018 Task 7 about?,Semantic Relation Extraction and Classification in Scientific Papers.,This paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at SemEval 2018.
SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers,What are the three subtasks included in SemEval-2018 Task 7?,Semantic Relation Extraction and Classification in Scientific Papers,"The three subtasks are:

- Relation classification on clean data (Subtask 1.1).
- Relation classification on noisy data (Subtask 1.2).
- Relation extraction and classification scenario (Subtask 2)."
SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers,What does relation classification on clean data subtask involve?,"Relation classification on clean data subtask involves manually annotated entities that are typically long noun phrases, which may be more complex and specific compared to automatically annotated entities.",The subtask involves in entity occurrences which are manually annotated in both the training and the test data.
SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers,What does relation extraction and classification scenario involve?,Relation extraction involves identifying instances of semantic relations between concepts in scientific papers. Relation classification involves classifying these instances into 6 discrete categories specific to the science domain.,"The subtask consists in identifying instances of semantic relations between entities in the same sentence, and assigning class labels."
SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers,How was the data prepared for the task?,"For the SQuAD v2.0 task, the data preparation involved treating questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions was extended to include the position of the [CLS] token.","The data consisted of abstracts from published research papers in computational linguistics. Two corpora, ACL RD-TEC 2.0 and ACL-RelAcS, were used for entity annotation. Manual annotations were used for clean data while automatic annotations were used for noisy data."
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",Wwhat does SCIERC include?,"Annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts.","SCIERC is a dataset created for scientific information extraction, including annotations annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts."
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",What is the SCIIE ?,"SCIIE is a unified multi-task model for identifying entities, relations, and coreference in scientific texts.",SCIIE (Scientific Information Extractor)is a unified framework with shared span representations.
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",What are the types of scientific entities annotated in SCIERC?,"Task, Method, Metric, Material, Other-ScientificTerm, Generic","There are six types for annotating scientific entities (Task, Method, Metric, Material, Other-ScientificTerm and Generic)."
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",What tasks does SCIIE model include?,"Entity recognition, relation extraction, and coreference resolution.","SCIIE includes three tasks of entity recognition, relation extraction, and coreference resolution. These tasks are treated as multinomial classification problems with shared span representations."
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction", What does the SCIIE-based knowledge graph represent?,A scientific knowledge graph.,Nodes in the knowledge graph correspond to scientific entities. Edges correspond to scientific relations between pairs of entities.
End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,What is the proposed model in the paper?,BERT (Bidirectional Encoder Representations from Transformers),The study presents a novel end-to-end neural model for extracting entities and relations between them. 
End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,What are the key components of the proposed architecture?,"L (number of layers), H (hidden size), A (number of self-attention heads)","The proposed architecture consists of three main layers: Embedding Layer,Sequence Layer and Dependency Layer."
End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,What is the key architecture design of the proposed model?,A multi-layer bidirectional Transformer encoder.,The model allows joint modeling of entities and relations in a single model by using both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTM-RNNs. 
End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,What enhancements are introduced during training to improve performance?,Data augmentation by fine-tuning on TriviaQA before fine-tuning on SQuAD.,Two key enhancements are introduced: (1) Entity Pretraining: involves pretraining the entity model and (2) Scheduled Sampling: replaces (unreliable) predicted labels with gold labels in a certain probability.
End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,What datasets were used for evaluation?,ACL RD-TEC 2.0 and ACL-RelAcS,"The model was evaluated on ACE05, ACE04 for end-to-end relation extraction and SemEval-2010 Task 8 for relation classification. "
SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,What is the SemEval 2017 Task 10?,ScienceIE - Extracting Keyphrases and Relations from Scientific Publications.,The SemEval 2017 Task 10 involves extracting keyphrases and relations between them from scientific documents. 
SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,What are the three subtasks defined in SemEval 2017 Task 10?,"Subtask A (keyphrase identification), Subtask B, and Subtask C.","The three subtasks are: (1) Subtask A: Mention-level keyphrase identification, (2) Subtask B: Mention-level keyphrase classification, and (3) Subtask C: Mention-level semantic relation extraction between keyphrases with the same keyphrase types."
SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,What is the SemEval 2010 pilot task?,ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,. The pilot task involves extracting a list of keyphrases representing key topics from scientific documents.
SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,What is the evaluation metric used for keyphrase identification?,Exact matches with the gold standard., Keyphrase identification Subtask has traditionally been evaluated by calculating the exact matches with the gold standard.
SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,What kind of corpus was built for the task?,TriviaQA-Wiki,"A corpus for the task was built from ScienceDirect open access publications and was available freely for participants, without the need to sign a copyright agreement."
Overview of BioCreAtIvE task 1B: normalized gene lists,What is the goal of the BioCreAtIvE task 1B?,The goal of the BioCreAtIvE task 1B is to evaluate the performance of systems in identifying genes mentioned in a given article.,The goal of BioCreAtIvE task 1B is the 'Normalized Gene List' task which aim to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms.
Overview of BioCreAtIvE task 1B: normalized gene lists,What are the key steps involved in task 1B for generating normalized gene lists?,Not explicitly stated.,"The key steps include identifying gene mentions in the abstract, linking them to unique gene identifiers, resolving ambiguity among potential matches and compiling a final list of gene identifiers for each document."
Overview of BioCreAtIvE task 1B: normalized gene lists,What defines the Gold Standard used in evaluation?,Human-annotated data.,"The Gold Standard consisted of verifing gene lists associated with each abstract, based on gene mentions actually found in the abstract."
Overview of BioCreAtIvE task 1B: normalized gene lists,What does task 1B reflect in the biological curation process?,"A step in the curation process for the model organism databases, where an important step is to list those genes discussed in the article that have sufficient experimental evidence to merit curation.",Task 1B reflects a step in the curation process where curators list genes discussed in an article that have sufficient experimental evidence to merit curation.
Overview of BioCreAtIvE task 1B: normalized gene lists,What are the requirements for task 1B?,Normalized gene lists.,"The four requirements in Task 1B are identifying gene mentions, associating them to one or more unique gene identifiers, selecting the correct gene identifier in cases of ambiguity, and assembling the final gene list for each abstract."
Overview of BioCreAtIvE task 1B: normalized gene lists,How did annotators create the test set gene lists?,Annotators compared annotated text segments.,"Annotators added all genes mentioned in the abstract by hand, regardless of whether they met the curation criteria."
Coreference annotation and resolution in the Colorado Richly Annotated Full Text (CRAFT) corpus of biomedical journal articles,What does the term 'coreference' include in this paper?,"A broad range of phenomena, including identity, pronominal anaphora, and apposition.","the term coreference to refer to a broad range of phenomena, including identity, pronominal anaphora, and apposition."
Coreference annotation and resolution in the Colorado Richly Annotated Full Text (CRAFT) corpus of biomedical journal articles,What are the coreference relations annotated in the CRAFT corpus?,coreferential and appositive relations,The two relations that are annotated in the corpus are the IDENTITY relation and the APPOSITIVE relation.
Coreference annotation and resolution in the Colorado Richly Annotated Full Text (CRAFT) corpus of biomedical journal articles,What is the IDENTITY relation?,The IDENTITY relation holds when two units of annotation refer to the same thing in the world.,The identity relation holds when two units of annotation refer to the same thing in the world.
Coreference annotation and resolution in the Colorado Richly Annotated Full Text (CRAFT) corpus of biomedical journal articles,What is the APPOSITIVE relation?,"The APPOSITIVE relation holds when two noun phrases are adjacent and not linked by a copula (typically the verb ""be"") or some other linking word.",The appositive annotation holds when two noun phrases are adjacent and not linked by a copula or some other linking word.
Coreference annotation and resolution in the Colorado Richly Annotated Full Text (CRAFT) corpus of biomedical journal articles,What are the key challenges identified in applying coreference resolution systems to the CRAFT biomedical corpus?,Coreference resolution approaches from other domains do not necessarily transfer well to the biomedical domain.,"Challenges include the difficulty of automated coreference resolution in biomedical journal articles than in newswire text, systems tuned for newswire text need significant alteration to perform well, and the much greater length of the documents leads to much longer coreference chains."
Named Entity Recognition for Bacterial Type IV Secretion Systems,What does the NER system focus on in relation to Type IV secretion systems?,Identifying concepts related to T4SS at the top levels of the GO ontology.," NER system focuses on four entities related to Type IV secretion systems: 1) bacteria names, 2) biological processes, 3) molecular functions, and 4) cellular components."
Named Entity Recognition for Bacterial Type IV Secretion Systems,What is the main function of T4SSs?,"T4SSs function predominantly in conjugation, naked DNA uptake and release, and the propagation of genomic islands.","T4SSs function predominantly in conjugation, naked DNA uptake and release, and the propagation of genomic islands"
Named Entity Recognition for Bacterial Type IV Secretion Systems,What are T4SSs?,Type IV Secretion Systems,"T4SSs are the only group of translocation machines that span the broad distribution of Prokaryota, being encoded within many genomes of both Gram negative and Gram positive species, as well as within some wall-less bacteria and Archaea."
Named Entity Recognition for Bacterial Type IV Secretion Systems,What is the output of the NER task? ,"tA = O, B, I for tokens being outside, at the beginning, or inside a keyphrase","The output of the NER task is a tagged span of text identifying bacteria name or a concept that is a member of the set of concepts constituting the intersection of T4SS concepts and one of biological process, cellular component, or molecular function."
Named Entity Recognition for Bacterial Type IV Secretion Systems,Which recognition techniques were evaluated in the study?,"Entity recognition, relation extraction, and coreference resolution.","Three recognition techniques were evaluated:a pure dictionary approach, a dictionary plus corpus enrichment, and a machine learning approach."
Bacteria Biotope at BioNLP Open Shared Tasks 2019,What is the Bacteria Biotope task?,"The Bacteria Biotope task is an annotation task that involves identifying and extracting information about microbial biodiversity in food, specifically focusing on bacteria biotopes.","The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources."
Bacteria Biotope at BioNLP Open Shared Tasks 2019,What are the four entity types defined in the Bacteria Biotope task?,Microorganism,"The four entity types are:

- Microorganism
- Habitat
- Geographical
- Phenotype"
Bacteria Biotope at BioNLP Open Shared Tasks 2019,What are the two relation types defined in the task?,"synonym, hypernym","The two relation types are:

- Lives in: Links a microorganism entity to its location (either a habitat or a geographical entity, or in few rare cases a microorganism entity).
- Exhibits: Links a microorganism entity to a phenotype entity."
Bacteria Biotope at BioNLP Open Shared Tasks 2019,What are phenotypes in the context of the BB task?,"Observable characteristics such as morphology, environment requirement (e.g. acidity, oxygen), host interaction characteristics (e.g. symbiont), and community behavior and growth habit (e.g. epilithic).","Phenotypes are observable characteristics such as morphology, or environment requirement."
Bacteria Biotope at BioNLP Open Shared Tasks 2019,What is an Exhibits relation?,"An Exhibits relation refers to a situation where one entity exhibits some property or behavior, such as ""The patient exhibited symptoms of the disease.""",Exhibits relations which link a microorganism entity to a phenotype entity.
RDoC Task at BioNLP-OST 2019,What is the purpose of the RDoC initiative?,Not explicitly stated in this snippet.,The RDoC initiative intends ‘to foster integration not only of psychological and biological measures but also of the psychological and biological constructs those measures measure.
RDoC Task at BioNLP-OST 2019,What are the two subtasks of the RDoC Task introduced at BioNLP-OST 2019?,Information retrieval and sentence extraction through National Institutes of Mental Health's Research Domain Criteria framework.,"The RDoC Task consists of two subtasks: (1) RDoC-IR, which involves retrieving PubMed Abstracts related to RDoC constructs, and (2) RDoC-SE, which focuses on extracting the most relevant sentence for a given RDoC construct from a known relevant abstract."
RDoC Task at BioNLP-OST 2019,What includes the RDoC construct-level annotations?,"environmental, social, and biological factors influencing relational and physical aggression.",the RDoC construct-level annotations includes a) whether a given abstract is relevant to a given construct and b) which sentence in the abstract is the most relevant for the construct.
RDoC Task at BioNLP-OST 2019,What is the structure of the RDoC corpus?,The text does not provide information about the structure of the RDoC corpus.,"Each instance in the corpus is made up of an abstract and a construct, and it is labeled with: 1) binary label for relevance, 2) index of most relevant sentence , and 3) metadata."
RDoC Task at BioNLP-OST 2019,What is the RDoC framework?,"RDoC stands for Research Domain Criteria. It is a framework used to analyze and annotate scientific papers, particularly in the field of psychology and neuroscience. The framework consists of various constructs that are used to categorize and understand the content of research papers. However, the provided context does not give a detailed explanation of what RDoC is or how it works, but rather discusses the challenges faced by annotators while using this framework.","RDoC framework is made up six major domains of human functioning, which is further broken down to multiple constructs that comprise different aspects of the overall range of functions."
Overview of the Regulatory Network of Plant Seed Development (SeeDev) Task at the BioNLP Shared Task 2016.,What is the purpose of the SeeDev Task introduced in the BioNLP Shared Task 2016?,"To evaluate the reusability of methods, characterize the peculiarities of information extraction for the plant biology domain, and develop dedicated approaches.","The purpose of the SeeDev Task is the extraction from scientific articles of the descriptions of genetic and molecular mechanisms involved in seed development of the model plant, Arabidopsis thaliana."
Overview of the Regulatory Network of Plant Seed Development (SeeDev) Task at the BioNLP Shared Task 2016.,What is the Gene Regulatory Network for Arabidopsis (GRNA) model?,"A knowledge model that meets the constraints of manual annotation of texts and automatic annotation by IE methods, designed for the representation of events in the SeeDev task.",The GRNA model is a knowledge representation framework used in the SeeDev Task. It defines 16 entity types and 21 event types.
Overview of the Regulatory Network of Plant Seed Development (SeeDev) Task at the BioNLP Shared Task 2016.,What are the two subtasks of the SeeDev Task?,SeeDev-binary and SeeDev-full.,The two subtasks are SeeDev-binary and SeeDev-full.
Overview of the Regulatory Network of Plant Seed Development (SeeDev) Task at the BioNLP Shared Task 2016.,What are SeeDev's characteristics?,"SeeDev has 22 different types of binary relations without modality (no negation), n-ary events with 2-8 arguments and a negation modality, and an average of three arguments per event.","The SeeDev corpus consists of 86 paragraphs from 20 full-text articles selected by plant biology experts. It includes 44,857 words, 7,082 entities, 2,583 events, and 3,575 relations."
Overview of the Regulatory Network of Plant Seed Development (SeeDev) Task at the BioNLP Shared Task 2016.,What is the goal of SeeDev-binary?,The goal of SeeDev-binary is the extraction of binary relations of 22 different types without modality (no negation).,The goal of SeeDev-binary is the extraction of binary relations of 22 different types without modality .
Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016,What is the objective of the Bacteria Biotope (BB) task?,"The objective of the Bacteria Biotope (BB) task is to extract bacteria and their locations from the text, categorize them according to dedicated knowledge sources, and link bacteria to their locations through ""Lives in"" events.",The Bacteria Biotope (BB) task aim to contribute to progress by drawing general lessons from the individual contributions and assessment of the participants..
Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016,What are the three types of entities involved in the BB task?,"Bacteria, Habitats, and Geographical places.","The BB task involves three types of entities, Bacteria, Habitats and Geographical places."
Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016,What is the goal of the knowledge base extraction subtask?,Extracting semantic relations between entities in scientific papers.,The goal is to build a knowledge base composed of all distinct pairs of Bacteria and Habitat categories linked through the Lives in relation that can be extracted from the corpus.
Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016,What does the BB corpus consist of?,"Training, development, and test sets.",The BB corpus consists of titles and abstracts of PubMed entries. 
Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016,How was the representativeness of corpus samples evaluated?,F1 score,The representativeness was evaluated by the mean squared error (MSE) between the sample and the original collection. 
"CRAFT Shared Tasks 2019 Overview – Integrated Structure, Semantics, and Coreference",What are the three key biomedical NLP tasks addressed in the 2019 BioNLP Open Shared Tasks focusing on the CRAFT corpus?,"Named Entity Recognition, Event Extraction and Coreference Resolution.","The three key biomedical NLP tasks are dependency parse construction, coreference resolution, and ontology concept identification."
"CRAFT Shared Tasks 2019 Overview – Integrated Structure, Semantics, and Coreference",What is the CRAFT Shared Tasks 2019?,CRAFT-ST 2019 marks the inaugural use and subsequent release of thirty articles annotated in CRAFT that had previously been held in reserve.,the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks.
"CRAFT Shared Tasks 2019 Overview – Integrated Structure, Semantics, and Coreference",What is the coreference resolution task?,"Coreference resolution is an active area in the NLP research community that involves identifying identity chains curated in a corpus, such as the OntoNotes project.",The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. 
"CRAFT Shared Tasks 2019 Overview – Integrated Structure, Semantics, and Coreference",What is the ontology concept annotation task?,Concept Annotation Task (CRAFT-CA),"The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes."
"CRAFT Shared Tasks 2019 Overview – Integrated Structure, Semantics, and Coreference",What is the Labeled Attachment Score (LAS) metric?,The de facto standard metric for evaluating dependency parse performance.,"The Labeled Attachment Score (LAS) metric is the de facto standard metric for evaluating dependency parsing performance, and is commonly defined simply as the fraction of tokens for which the predicted head and dependency relation type (label) match the gold standard."
"PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",What is the primary focus of the PharmaCoNER task?,"Named Entity Recognition (NER) for pharmacological substances, compounds, and proteins.",The PharmaCoNER shared task focuses on recognizing pharmaceutical drugs and chemical entities in medical texts in Spanish.
"PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",What was the first sub-track of PharmaCoNER?,Recognition and classification of entities.,NER offset and entity classification. The first sub-track focused on the recognition and classification of entities.
"PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",What was the second sub-track of PharmaCoNER?,concept-indexing,"The second sub-track consisted of concept indexing, where, for each document, the participating teams had to generate the list of the unique SNOMED-CT concept identifiers, which were compared to the manually annotated concept IDs corresponding to the pharmaceutical drugs and chemical entities."
"PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",What is the Spanish Clinical Case Corpus (SPACCC)?,A manually classiﬁed collection of clinical case report sections derived from open access Spanish medical publications.,SPACCC is a manually classified collection of clinical case report sections derived from open access Spanish medical publications.
"PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",What are the two sub-tracks involved in the PharmaCoNER challenge?,"Named entity recognition track of chemicals/drugs, Concept-indexing sub-track.",(1) NER offset and entity classification and (2) Concept indexing. 
An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks,What is the Active Gene Annotation Corpus (AGAC) ?,The AGAC is a corpus used for the BioNLP OST 2019 AGAC Track Tasks.,The Active Gene Annotation Corpus (AGAC) is a human-annotated biomedical corpus developed to support knowledge discovery for drug repurposing 
An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks,What are the three main tasks involved in the AGAC BioNLP shared task?,"1) named entity recognition, 2) thematic relation extraction, and 3) loss of function (LOF) / gain of function (GOF) topic classification.",The three main tasks are: 1) Named Entity Recognition (NER); 2) Thematic relation extraction; and 3) loss of function (LOF) / gain of function (GOF) topic classification.
An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks,What is drug repurposing?,Drug repurposing (also known as drug repositioning) is to find new indications of approved drugs.,"Drug repurposing (AKA drug repositioning) is to find new indications of approved drugs, which is an important mean for investigating novel drug efficiency in the pharmaceutical industry."
An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks,What type of models were used in the AGAC shared task?,BERT,Participants used BERT-based joint learning models. These models were effective because they can understand full semantic context and integrate sequence labeling with relation extraction.
An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks,What characterize the AGAC corpus?,"It is annotated for eleven types of named entities (bio-concepts, regulation types, and other entities) and two types of thematic relations between them.","The AGAC corpus is characterized in three terms: imbalanced data, selective annotation, and latent topic annotation."
WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,What is WikiNEuRal?,"WikiNEuRal is a dataset that can be used to improve domain generalization in named entity recognition (NER) tasks, particularly when gold-standard training data are available but do not match the target test set in terms of textual genre or domains covered.","WikiNEuRal is an automatic, language-independent approach for generating labeled datasets for Named Entity Recognition (NER). It combines knowledge-based techniques using Wikipedia and BabelNet with neural models like multilingual BERT to improve the quality and coverage of NER annotations across languages."
WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,What is the proposed architecture in the NER data creation process?,WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER.,"The architecture uses a BERT-based classifier to determine whether Wikipedia articles represent named entities or concepts, helping enrich and verify NER labels."
WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,What is Named Entity Recognition (NER)?,"Named Entity Recognition (NER) is the task of identifying specific words as belonging to pre-defined semantic types, such as Person, Location, Organization, etc.","Named Entity Recognition (NER) is the task of identifying specific words as belonging to predefined semantic types, such as Person, Location, Organization, etc."
WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,What role does domain adaptation play in the proposed NER data creation method?,"Domain adaptation plays a crucial role in improving the performance of the proposed NER data creation method, WikiNEuRal. It helps to cope with the problem of mismatched topics and genres between different datasets, such as CoNLL and OntoNotes. By adapting the domain, WikiNEuRal can boost its domain coverage, leading to improved results on domain adaptation tasks.",Domain adaptation enables the creation of domain-adapted training data for NER systems when given domain-specific texts.
WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,How does the paper evaluate the effectiveness of the WikiNEuRal dataset in improving NER performance?,"It is evaluated by comparing its performance with that of WikiNER, the current best-performing approach for NER silver data creation.","The authors evaluated WikiNEuRal using benchmarks like CoNLL and WikiGold across multiple languages. The models trained on this data showed significant performance gains, particularly in domain generalization and low-resource scenarios, with improvements of up to 6 span-based F1-score points on common benchmarks for NER against state-of-the-art alternative data production methods."
Corpus annotation for mining biomedical events from literature,What is the primary focus of the annotation principles discussed in the paper ?,Coreference annotation and resolution.,"The focus is on a new type of semantic annotation, event annotation, achieving biology-oriented annotation that reflects biologists’ interpretation of text, and ensuring homogeneity of annotation quality"
Corpus annotation for mining biomedical events from literature,What is a pathway?,"A pathway is a representation of biological events organized around state-changes of continuants, with entities as major players, and showing general causality between intertwined events.","A pathway is a detailed graphical representation of a biological system, which comprises a set of mutually related events."
Corpus annotation for mining biomedical events from literature,What is the GENIA event ontology?,"The GENIA event ontology provides vocabulary for predicates (e.g. ""binding"", ""phosphorylation"", etc.) and defines a hierarchy of event classes, including some inherited from GO with modified definitions, as well as three additional classes not found in GO: Gene Expression, Artificial Process, and Correlation.",The GENIA event ontology defines and classifies events (or occurrents in the terminology of philosophical ontology ) which are of interest in the GENIA domain.
Corpus annotation for mining biomedical events from literature,How does the paper differentiate between pathway representations and natural language event annotations?,"Pathway representations are entity-centered, organizing information around state-changes of continuants, whereas natural language event annotations are predicate-centered, focusing on events organized around predicates.","Pathway representations, such as SBML, are entity-centered and explicitly encode causality through sequences of state changes, whereas natural language event annotations are predicate-centered, often lack explicit causality, and require nuanced annotation strategies to capture complex biological phenomena."
Corpus annotation for mining biomedical events from literature,What is Artificial_process?,Artificial_process describes experimental processes which are performed by human researchers.,Artificial_process describes experimental processes which are performed by human researchers.
Cross-lingual Name Tagging and Linking for 282 Languages,What is the main goal of the framework proposed in this paper?,Not explicitly stated.,"The main goal of the framework is to is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia, which can identify name mentions, assign entity types, and link them to an English Knowledge Base if possible."
Cross-lingual Name Tagging and Linking for 282 Languages,How does the framework generate initial entity annotations for name tagging across languages?,"It uses Wikipedia data as ""silver-standard"" to train and test the model, with 70% of selected sentences used for training and 30% for testing.","It classifies English Wikipedia entries into certain entity types and then propagate these labels to other languages using cross-lingual Wikipedia links, creating 'silver-standard' training annotations."
Cross-lingual Name Tagging and Linking for 282 Languages,What machine learning architecture does the system use for the name tagging task?,BERT-based neural model,"The system uses a Bi-directional Long Short-Term Memory (Bi-LSTM) combined with a Conditional Random Fields (CRF) network, trained with features derived from Wikipedia markups such as stems, affixes, and gazetteers."
Cross-lingual Name Tagging and Linking for 282 Languages,How do the authors refine annotations?,"They don't. They leave some events with missing arguments to be filled in from context, which is left as future work.",The authors apply self-training to label other mentions without links in Wikipedia articles even if they have different surface forms from the linked mentions.
Cross-lingual Name Tagging and Linking for 282 Languages,What is the solution proposed by the authors,"To ensure that each annotation is grounded in textual evidence and to prevent unbounded interpretation, the authors propose two main solutions: (1) requiring annotators to provide ""clue words"" or ""clue expressions"" for each annotation, which must be present in the text; and (2) aligning annotations to single sentences, with some exceptions allowed for anaphoric expressions, which are explicitly indicated by a special link (Co-Ref link).","The authors propose to break language barriers by extracting information (e.g., entities) from a massive variety of languages and ground the information into an existing knowledge base which is accessible to a user in his/her own language."
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,What is the main objective of the WNUT2017 Shared Task discussed in the paper?,"To provide a definition of emerging and rare entities, and to create datasets for detecting these entities.",The main objective of the WNUT2017 Shared Task is to provide a definition of emerging and of rare entities and provide datasets for detecting these entities.
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,What is the dataset used in this paper?,Twitter streaming API and StackExchange.,"To focus the task on emerging and rare entities,they used a dataset where very few surface forms occur in regular training data and very few surface forms occur more than once"
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,Why is recognizing emerging entities challenging??,"Recognizing emerging entities is challenging because entities mentioned vary over time, as does the linguistic context in which entities are situated."," Recognizing emerging entities in noisy text is more difficult than on high frequent entities, and systems often fail to generalize successfully."
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,What types of entities are mostly contained in the dataset?,Novel toponyms.,"The dataset contained mostly rare and novel entities of the types person, location, corporation, product, creative-work, and group"
Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,What methodS do the authors propose to tackle the issue of entity recognition in their research?,"Tokenization using twokenizer and processing through GATE (Cunningham et al., 2012) for crowdsourcing.","The authors propose a combination of supervised and unsupervised learning techniques, leveraging existing corpuses and incorporating techniques for zero-shot learning to enhance the detection of entities that have not been previously encountered."
BioCreAtIvE Task 1A: gene mention finding evaluation,What is the primary focus of the BioCreAtIvE Task 1A as discussed in the paper?,Gene mention finding evaluation.,The primary focus of the BioCreAtIvE Task 1A  deals with finding mentions of genes and related entities in text. .
BioCreAtIvE Task 1A: gene mention finding evaluation,How do the authors define 'gene mention' in the context of their study?,They don't.,"In the context of their study, a 'gene mention' refers to any text segment that specifically denotes or references a gene or genetic element within biological literature."
BioCreAtIvE Task 1A: gene mention finding evaluation,What methodological approach does the paper utilize for the evaluation of gene mention identification systems?,Open common evaluation,"The paper utilizes a comparative evaluation approach, where various systems are tested against a benchmark dataset to assess their performance in accurately identifying gene mentions."
BioCreAtIvE Task 1A: gene mention finding evaluation,What is a key difficulty in boundary detection for gene mentions?,Tokenization affects what counts as a mention.,Determining the starting and ending boundaries of gene or protein mentions is difficult because tokens cannot be split between marked and unmarked parts.
BioCreAtIvE Task 1A: gene mention finding evaluation,What are the evaluation metrics used in the study to assess the performance of the gene mention identification systems?,F-measure (balanced precision and recall),"The study employs evaluation metrics such as precision, recall, and F-score."
Overview of BioCreative II gene mention recognition,What is the Gene Mention Task ?,Gene mention finding evaluation.,"The Gene Mention Task is a challenge where teams develop systems to identify substrings in sentences that mention genes within scientific texts, aiming to improve gene recognition in written material."
Overview of BioCreative II gene mention recognition,What was the highest F1-Score achieved by the participating teams in the Gene Mention Task?,0.872,The highest F1-Score achieved by the participating teams in the Gene Mention Task was 0.8721.
Overview of BioCreative II gene mention recognition,What is the gene normalization task?,The goal of the gene normalization task is to normalize genes.,The gene normalization task is an alternative approach to finding gene names in text is to decide upon the actual gene database identifiers that are referenced in a sentence.
Overview of BioCreative II gene mention recognition,What includes the training corpus for the BioCreative II GM task?,mainly of the training  and testing corpora from the previous task,"The training corpus consists of the training and testing corpora (text collections) from the previous task, and the testing corpus consists of an additional 5,000 sentences that were held 'in reserve' from the previous task."
Overview of BioCreative II gene mention recognition,What criteria were established for evaluating the performances of the systems in the Gene Mention Task?,"Precision (p), recall (r), and their harmonic average F.","The evaluation criteria included metrics such as precision, recall, and F1-Score."
"The Protein-Protein Interaction tasks of BioCreative III: classification, ranking of articles and linking bio-ontology concepts to full text",What is the goal of the BioCreative III PPI tasks?,"To address aspects including how the end user would oversee the generated output, for instance by providing ranked results, textual evidence for human interpretation or measuring time savings by using automated systems.",The PPI tasks of BioCreative III tried to address relevant aspects for both database curators as well as general biologists interested in improving the retrieval of interaction relevant articles and association of ontology terms and experimental techniques to full text papers.
"The Protein-Protein Interaction tasks of BioCreative III: classification, ranking of articles and linking bio-ontology concepts to full text",What are the two main tasks introduced in the BioCreative III PPI challenge?,"The two main tasks introduced in the BioCreative III PPI challenge are classification and ranking of articles, as well as linking bio-ontology concepts to full text.",The two main tasks are the Article Classification Task (ACT) and the Interaction Method Task (IMT).
"The Protein-Protein Interaction tasks of BioCreative III: classification, ranking of articles and linking bio-ontology concepts to full text",What type of dataset was provided for the Article Classification Task (ACT)?,Mixed domains.,"For the ACT, the BCIII-ACT corpus was provided."
"The Protein-Protein Interaction tasks of BioCreative III: classification, ranking of articles and linking bio-ontology concepts to full text",What are the high-frequency terms in IMT data?,No information is provided about the high-frequency terms in IMT data.,"These 4 high-frequency terms are (from most to least frequent): ‘anti bait coimmunoprecipitation’, ‘anti tag coimmunoprecipitation’ (these two represent 1/3 of all annotations), ‘pull down’, and ‘two hybrid’."
"The Protein-Protein Interaction tasks of BioCreative III: classification, ranking of articles and linking bio-ontology concepts to full text",What were the main challenges identified in the IMT?,"The main challenges identified are to improve NER in novel and emerging situations, where there is a high degree of drift, and to generalise instead of relying on stable context- or sub-word-level cues.","The main difficulties for the Interaction Method Task arise from the many different ways of describing a given experimental method, handling PDF articles, and the heterogeneous journal composition."
The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,What is Biological Expression Language (BEL),"Biological Expression Language (BEL) is an alternative representation format that allows for the encoding of findings or observations from literature, particularly suited for biocuration and representing causal relationships expressed in scientific literature.",Biological expression language (BEL) is a syntax representation allowing for the structured representation of a broad range of biological relationships.
The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,What is BELIEF?,BELIEF is not defined in this snippet of text.,"BEL information extraction workflow (BELIEF) supports such semi-automatic curation. BELIEF is a public web service with underlying text mining workflows and a curation interface that enables the semi-automatic extraction of complex relationships, and their coding, in BEL."
The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,What is the main challenge in extracting BEL statements?,Predicting relations.,"The extraction of relationships and their coding in BEL is a complex task as a multitude of entity, relationship and function types can be involved in a single relationship."
The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,What is the proposed architecture in the study ?,WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER,The study combines rule-based systems with new methods involving hierarchical sequence labeling and neural networks were applied for BEL statement extraction.
The extraction of complex relationships and their conversion to biological expression language (BEL) overview of the BioCreative VI (2017) BEL track,What was the effect of the second round of the BEL track?,"There is no mention of a ""second round"" in the provided context.","Through the second round of the BEL track and the provision of training data, the performance of participating systems increased drastically."
Overview of the BioCreative VI text-mining services for Kinome Curation Track,What is the BioCreative VI Kinome Track?,"A document classification task where rich co-occurrence and linguistic features are used to prioritize biomedical articles involving relations among kinases, diseases, and biological processes.",The BioCreative VI Kinome Track proposed a competition in literature triage based on the neXtProt unpublished protein kinase dataset.
Overview of the BioCreative VI text-mining services for Kinome Curation Track,What is the purpose of triage systems?,"Triage systems are used to quickly sort patients according to the severity of their injuries, so that medical professionals can prioritize treatment and allocate resources effectively.","Triage systems that retrieve, filter and/or prioritize the literature can hence help curators focus on articles appropriate for curation."
Overview of the BioCreative VI text-mining services for Kinome Curation Track,What metrics were used for evaluation?,"F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error, robustness, time complexity.","Metrics used are P10 or Precision at rank 10, R30 or Recall at rank 30, P at R0, Mean average precision (MAP), R-prec."
Overview of the BioCreative VI text-mining services for Kinome Curation Track,How were the Kinome Track benchmarks composed in the study?,"The collection was reduced from the MEDLINE corpus by filtering out papers published in 2014 and after, resulting in a total of 5.3 M papers.","The Kinome Track benchmarks are composed of three parts:: a collection of documents, a set of queries (the query was a human kinase and a curation axis), and relevance judgments used to evaluate system outputs."
Overview of the BioCreative VI text-mining services for Kinome Curation Track,What were the key components of the Kinome Track dataset mentioned in the document?,MEDLINE corpus papers published before 2014.,"The dataset contains >30 000 annotations. Each annotation is supported by a reference to a publication, a PubMed identifier (PMID)."
Overview of the protein-protein interaction annotation extraction task of BioCreative II,What was the main objective of the BioCreative II Protein-Protein Interaction (PPI) task?,"The main objective of the BioCreative II PPI task is not explicitly stated in the provided snippet. However, based on the context, it appears to be related to the extraction of mentions of protein-protein interactions from scientific literature and possibly ranking or classifying articles based on this information.",The main objective of the BioCreative II PPI task was to compare the performance of text-mining tools specific for each of the basic steps of the PPI extraction pipeline.
Overview of the protein-protein interaction annotation extraction task of BioCreative II,What are the four subtasks included in the BioCreative II PPI task?,The four subtasks included in the BioCreative II PPI task are not explicitly mentioned in the provided context information.,The four subtasks are: (a) detection of protein interaction-relevant articles; (b) extraction and normalization of protein interaction pairs; (c) retrieval of the interaction detection methods used; and (d) retrieval of actual text passages that provide evidence for protein interactions.
Overview of the protein-protein interaction annotation extraction task of BioCreative II,What is the purpose of text-mining systems in PPI extraction?,Capturing dependency relationships between words to detect protein-protein interaction (PPI) information at both the article and sentence level.,Text-mining systems have been implemented to extract binary protein interactions from articles.
Overview of the protein-protein interaction annotation extraction task of BioCreative II,What does the Interaction Pair Subtask (IPS) involve?,Not mentioned in this snippet.,The Interaction Pair Subtask (IPS) involves the extraction of binary protein-protein interaction pairs from full-text articles.
Overview of the protein-protein interaction annotation extraction task of BioCreative II,What does the Interaction Method Subtask (IMS) involve?,"Extraction of the interaction detection method used to characterize protein interactions described in full-text articles, characterized by MI ontology identifiers.",The Interaction Method Subtask (IMS) involves the extraction of the interaction detection method used to characterize the protein interactions described in full-text articles.
Ontology design patterns to disambiguate relations between genes and gene products in GENIA,What was the main goal of this work on the GENIA corpus?,To develop an annotated corpus for mining biomedical events from literature.,"The main goal was to provide a formal characterization of relations used in the GENIA corpus, including defining axioms and ontology design patterns, enabling automated verification, deductive inference, and knowledge-based applications."
Ontology design patterns to disambiguate relations between genes and gene products in GENIA,How were GENIA corpus abstracts represented after this work?,They were used for training NLP tools such as Named Entity Recognizers (NERs).,"GENIA abstracts were converted into OWL ontologies by combining the ontology of relations with ontology design patterns, making them amenable for automated verification, reasoning, and other knowledge-based applications."
Ontology design patterns to disambiguate relations between genes and gene products in GENIA,What kinds of entities are connected through relations in the GENIA corpus?,Names and other biomedical domain terms.,"Relations in GENIA corpus annotations typically connect names and biomedical domain terms, especially genes, gene products (GGPs), DNA, RNA, and proteins, which are treated as classes for named entity recognition purposes."
Ontology design patterns to disambiguate relations between genes and gene products in GENIA,What is the equivalence assumption in GENIA relation annotations?,"Names or terms referring to either a class of genes, DNA, proteins, RNAs and their splice variants, gene products, arbitrary transcripts or similar are considered to be equivalent within the context of the GENIA relation annotations.","The equivalence assumption states that names or terms referring to the same class of genes, proteins, RNAs, or gene products are considered equivalent within the context of the GENIA relation annotations, simplifying automatic extraction and enabling generic relations."
Ontology design patterns to disambiguate relations between genes and gene products in GENIA,How does the formalization connect different types of gene/gene product entities?,It considers them to be equivalent within the context of the GENIA relation annotations.,"The formalization defines a class for a GGP and connects DNA, RNA, and protein entities through transcription and translation relations. For example, a protein instance is linked to its RNA through a translated-from relation, and RNA is linked to DNA through a transcribed-from relation, forming a structured hierarchy."
Evaluation of BioCreAtIvE assessment of task 2,What was the main goal of BioCreAtIvE Task 2?,"BioCreative II had three tasks: gene mention (GM), gene normalization (GN), and protein-protein interaction (PPI).","The main goal was to evaluate text mining tools for automatically extracting and assigning Gene Ontology (GO) annotations of human proteins from full-text articles, including providing relevant textual evidence passages."
Evaluation of BioCreAtIvE assessment of task 2,How were the predictions structured in Task 2?,There were groups that prioritized recall (predictions for every query case) while others focused on precision (only predicting a relatively small number of high confidence cases).,"Predictions were structured as triplets consisting of protein – GO term – article passage, with participants returning the annotation-relevant text passages for evaluation."
Evaluation of BioCreAtIvE assessment of task 2,Who evaluated the submitted predictions and what criteria were used?,"GOA database curators; they assessed the returned evidence text, as well as the prediction of the GO code itself in case of sub-part 2.2, together with the annotation text passage.",Expert curators from the GO Annotation (GOA) team at the European Bioinformatics Institute evaluated the submissions. They assessed whether the protein and GO term were correctly predicted and whether the annotations were traceable through the submitted text fragments.
Evaluation of BioCreAtIvE assessment of task 2,What were the main challenges encountered in Task 2?,"The main challenges encountered in Task 2 were to improve NER in a novel and emerging situation, where there is a high degree of drift, and to generalise as best they can, instead of e.g. memorising or relying on stable context- or sub-word-level cues.","The main challenges included the complex nature of GO terms and protein names, the large variability in how proteins and GO terms are expressed in free text, and the lack of a standardized training dataset."
Evaluation of BioCreAtIvE assessment of task 2,What was concluded regarding the performance of text mining tools in this task?,"The 80% plus F-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire.","Although the results were promising, text mining tools were still far from achieving the level of performance required for real-world applications. The variety of strategies used showed potential, and the publicly available dataset provides a foundation for further improvement."
BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language,What was the main objective of BioCreative V Track 4 (BEL track)?,The main objective of BioCreative V Track 4 (BEL track) is to extract causal network information using the Biological Expression Language.,"The main objective was to evaluate text mining systems capable of automatically constructing Biological Expression Language (BEL) statements from evidence text and retrieving evidence text for given BEL statements, facilitating the construction of causal biological networks from text."
BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language,What datasets were used for training and testing in the BEL track?,The description of the training set selection and curation are described in detail by Fluck et al. (9). The test set was generated using a real-world case and extracted new data in the disease context of ulcerative colitis.,"The BEL track used three main datasets: the BEL_Extraction training corpus with 6,353 evidence texts and 11,066 BEL statements, the smaller BEL_Extraction sample corpus with 191 sentences and 296 BEL statements for development, and the BEL_Extraction test corpus with 105 sentences and 202 BEL statements for final evaluation."
BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language,How was system performance evaluated for BEL statement extraction?,System performance was evaluated in multiple subsiding steps.,"Performance was evaluated at multiple levels of BEL statements: entities, functions, relations, and full statements. Precision, recall, and F-measure were computed, and partial credit was given for partially correct statements. Two stages were conducted: one with system-detected entities and one using gold-standard entities to highlight the importance of term recognition."
BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language,What were the key results for Task 1 (extracting BEL statements from text)?,Not mentioned in this snippet.,"For Task 1, the best system achieved 20% F-measure on full statements in stage 1 and 35.2% in stage 2 using gold-standard entities. High scores on relation or function levels did not always correlate with full statement performance. Ensemble approaches improved recall but often at the cost of precision."
BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language,What was the significance of Task 2 (retrieving evidence sentences for BEL statements)?,"Task 2 involved retrieving evidence texts from Medline abstracts and PMC full-text corpus to support given BEL statements, with a maximum of two sentences accepted as a single piece of evidence text.","Task 2 addressed the challenge of finding supporting evidence in biomedical literature. Although only one team participated, they provided 806 evidence sentences for 96 BEL statements. The task highlighted the importance of contextual information and provides a valuable large-scale training resource for future text mining development."
The gene normalization task in BioCreative III,What was the main goal of the Gene Normalization (GN) challenge in BioCreative III?,To link genes or gene products mentioned in the literature to standard database identifiers.,"The main goal of the GN challenge was to have participating teams return a ranked list of gene identifiers detected in full-text biomedical articles, simulating a real literature curation task."
The gene normalization task in BioCreative III,How were the training and test datasets prepared for the GN challenge?,"Candidate development and test data was filtered for common entities. To ensure that all entities in the development and test data were novel, surface forms marked as entities in the training data were gathered into a blacklist.","For training, 32 fully annotated and 500 partially annotated articles were prepared. A total of 507 articles were selected as the test set. Due to high annotation costs, only a small number of test articles were manually annotated, chosen using an Expectation Maximization (EM) algorithm."
The gene normalization task in BioCreative III,What role did the Expectation Maximization (EM) algorithm play in the challenge?,"The EM algorithm was used to produce the silver standard, which depends on agreements between different teams and is influenced by the data where agreement between teams is high. It works similarly to majority voting, but also weights votes based on how much a particular team submission agrees with others in the process.","The EM algorithm was used to select the most informative test articles for manual annotation and later to infer ground truth for the full test set based solely on team submissions, helping differentiate team performance without requiring full manual annotation."
The gene normalization task in BioCreative III,"What metric was used to evaluate team performance, and how does it work?",Micro F1 score. It measures a model's accuracy by calculating the harmonic mean of precision and recall.,"Team performance was evaluated using Threshold Average Precision (TAP-k), which measures the precision of the top k ranked gene predictions for each article, allowing assessment of both gold-standard and inferred ground truth results."
The gene normalization task in BioCreative III,What were the key findings and implications of the GN challenge?,Not mentioned in the given text snippet.,"The challenge showed that using full text and being species non-specific made the task more realistic and challenging. The EM-based inferred ground truth was as effective as gold-standard annotations in differentiating team performance. Combining team results with machine learning further improved TAP-k scores, demonstrating the potential for ensemble approaches in gene normalization."
The Web as a Knowledge-Base for Answering Complex Questions,What is the main approach proposed for answering complex questions in this study?,Decomposing complex questions into simple ones.,"The study proposes decomposing complex questions into a sequence of simple questions and then computing the final answer from the sequence of answers, leveraging a search engine and a reading comprehension model for the simple questions."
The Web as a Knowledge-Base for Answering Complex Questions,What is a computation tree and how is it used in the proposed framework?,"A computation tree is a tree where leaves are labeled with strings, and inner nodes are labeled with functions. The arguments of a function are its children sub-trees. It is used to decompose a question and compute an answer by recursively applying the function at the root to its children.",A computation tree is a tree where leaves are labeled with strings and inner nodes with functions. The tree recursively applies functions to its children to compute the answer. It represents the decomposition of a complex question into simpler steps and guides the QA model in combining the answers.
The Web as a Knowledge-Base for Answering Complex Questions,What functions are included in the formal language of the proposed framework?,"COMP (·, ·), CONJ (·, ·), ADD(·, ·)","The formal language includes SIMPQA(·) for answering simple questions, COMP(·,·) for function composition over previously-computed answers, CONJ(·,·) for intersections of answer sets, and ADD(·,·) for numeric addition. Other logical and set operations can also be supported but were not required in the dataset."
The Web as a Knowledge-Base for Answering Complex Questions,What is the COMPLEXWEBQUESTIONS dataset and its purpose?,"COMPLEX WEBQUESTIONS is a dataset that builds on the WEBQUESTIONS dataset, containing questions about properties of entities, events with multiple entities, and four compositionality types (composition questions, conjunctions, superlatives, and comparatives). Its purpose is to develop and evaluate models capable of reasoning for natural language understanding.","COMPLEXWEBQUESTIONS is a dataset of complex questions paired with their decompositions, answers, SPARQL programs, and web snippets. It is designed to support research on question decomposition, compositionality, and interaction with the web for complex question answering."
The Web as a Knowledge-Base for Answering Complex Questions,What are the limitations and future directions mentioned for this framework?,"The limitations of this framework include handling superlative and comparative questions, which can be cumbersome to answer from the web. Future directions may involve adding functions to handle such constructions or improving the extraction of lists of entities and numerical values.","Limitations include handling superlative, comparative, and negation questions directly from the web, since these require structured knowledge. Future work involves incorporating tables and knowledge bases for set operations, and training the model with weak supervision to extract information from both web and structured sources."
The Value of Semantic Parse Labeling for Knowledge Base Question Answering,How does training with semantic parses compare to training with question-answer pairs in knowledge base QA?,"Training with semantic parses results in higher performance, with an average F1 score 4.9 points higher (71.7% vs 66.8%) and complete answer set accuracy 5.1% higher (63.9% vs 58.8%), compared to training with question-answer pairs only.","Training with labeled semantic parses significantly improves knowledge base question answering performance compared to training with question-answer pairs, yielding an absolute 5-point gain in accuracy and reducing the number of incorrect candidate query graphs."
The Value of Semantic Parse Labeling for Knowledge Base Question Answering,Which algorithm is used to generate and evaluate semantic parses in this study?,S-MART,"The study uses the Staged Query Graph Generation (STAGG) algorithm, which represents semantic parses as query graphs and scores candidate graphs based on how well they match the question and semantic parse labels."
The Value of Semantic Parse Labeling for Knowledge Base Question Answering,What is the WEBQUESTIONSSP dataset and its significance?,"WEBQUESTIONS is a dataset that contains questions about properties of entities, often with filters for the semantic type of the answer. It also includes questions referring to events with multiple entities. The dataset is significant as it provides an opportunity to develop and evaluate models capable of reasoning for natural language understanding.","WEBQUESTIONSSP is the largest semantic-parse labeled dataset to date. It provides high-quality annotations for entity linking, relation extraction, temporal constraints, and other semantic tasks, serving as a benchmark for knowledge base question answering research."
The Value of Semantic Parse Labeling for Knowledge Base Question Answering,How can semantic parse labels improve answer quality and robustness?,"Semantic parse labels can provide additional value over answer labels by yielding answers that are more accurate and consistent, as well as being updatable if the knowledge base changes.","Semantic parse labels ensure answers are faithful to the knowledge base, improve completeness for questions with large answer sets, and remain robust when knowledge base facts are updated, because answers are computed via execution of the semantic representation."
The Value of Semantic Parse Labeling for Knowledge Base Question Answering,How can user interface design impact the collection of semantic parses?,"A well-designed user interface can improve the efficiency of collecting semantic parses by breaking down the complex task into separate, manageable sub-tasks, allowing the annotator to focus on one task at a time, and automatically constructing a coherent semantic parse from the user's choices.","An appropriate user interface allows annotators to provide semantic parses accurately and efficiently, at a cost comparable to collecting answers. Future improvements may include dialog-driven interfaces and knowledge graph visualizations to handle more complex questions."
CitationIE: Leveraging the Citation Graph for Scientific Information Extraction,What is the main objective of the CitationIE model?,To leverage the citation graph for scientific information extraction (SciIE).,"The main objective of CitationIE is to improve scientific information extraction (SciIE) by leveraging citation graph information, in addition to the content of individual papers, to enhance mention identification, salient entity classification, and relation extraction."
CitationIE: Leveraging the Citation Graph for Scientific Information Extraction,How does CitationIE incorporate citation graph information into SciIE?,CitationIE leverages the citation graph for Scientific Information Extraction (SciIE) by incorporating citation information to improve the extraction of structured information from scientific articles.,"CitationIE uses graph embeddings and citances (text from citing or cited documents) to augment text representations, allowing the model to capture context from related documents in the citation graph, which improves entity saliency and relation extraction."
CitationIE: Leveraging the Citation Graph for Scientific Information Extraction,Which SciIE tasks showed the most improvement when using citation graph information?,Different scientiﬁc information extraction tasks showed signiﬁcant gains.,"Salient entity classification and relation extraction benefited the most from citation graph information, with significant gains in document-level and corpus-level F1 scores. Mention identification showed little change."
CitationIE: Leveraging the Citation Graph for Scientific Information Extraction,How was the CitationIE model evaluated and compared to baselines?,"The CitationIE model was evaluated using the document-level F1 metric, and its performance was compared to baselines. The results showed that the proposed method outperformed the baseline at every seed, but the gains were not statistically significant due to inter-model variability and small test set size.","CitationIE was evaluated on English-language scientific documents using end-to-end and per-task metrics. It was compared against the baseline SciREX model and the DocTAET model, using document-level and corpus-level F1 scores, and significance was tested with paired bootstrap sampling."
CitationIE: Leveraging the Citation Graph for Scientific Information Extraction,What are the future directions suggested for citation-aware SciIE?,Extracting and organizing scientific information from a collection of research articles to help researchers identify new methods or materials for a given task.,"Future directions include developing more sophisticated methods to extract richer information from citation graphs, better capturing the interplay between citation structure and content, and validating the approach on other scientific domains beyond machine learning."
An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,What was the main goal of the first BIOASQ challenge?,to establish BIOASQ as a reference point for the biomedical community.,"The main goal of the first BIOASQ challenge was to assess systems’ ability to semantically index large numbers of biomedical articles and provide concise, user-understandable answers to natural language questions by combining information from biomedical articles and ontologies."
An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,What were the two main tasks in the BIOASQ 2013 competition?,Task 1a and Task 1b,"Task 1a involved automatically annotating new PubMed documents with MESH headings, while Task 1b required participants to automatically produce answers to benchmark English questions using biomedical articles and ontologies."
An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,"How many teams participated in Tasks 1a and 1b, and how did they perform?","Four teams participated in Task 1b. Their F1 scores ranged from 0.43 to 0.64. In Task 1a, five teams participated with the following F1 scores: HCC-NLP (0.16), NITK IT PG (0.14), Surukam (0.10), and GMBUAP (0.04).","In Task 1a, 12 teams submitted 46 system runs, with one team performing consistently better than NLM’s MTI indexer. In Task 1b, 3 teams submitted 11 system runs, achieving high scores in manual evaluation for the 'ideal' answers, producing high-quality summaries."
An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,What resources and infrastructure did BIOASQ provide for evaluation?,"BIOASQ provided publicly available datasets, results of participating teams, an in-house stop word list specific to the domain, and the Indri search engine for indexing documents. Additionally, they offered prizes for participants and made project deliverables publicly available on their website.","BIOASQ provided benchmark datasets, evaluation mechanisms, and publicly available infrastructure to evaluate systems for MESH annotation, retrieval of relevant articles or RDF triples, and generation of exact and paragraph-sized 'ideal' answers."
An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,What was the overall significance and impact of the BIOASQ 2013 challenge?,"The BIOASQ 2013 challenge had a positive impact, with 11 participating teams and 117 registered users, indicating it is on the right track to becoming a reference point for the biomedical community.","BIOASQ helped unify techniques from text classification, semantic indexing, document and passage retrieval, question answering, and multi-document summarization, enabling biomedical experts to obtain concise, understandable answers to real-world information needs."
A Hierarchical Attention Retrieval Model for Healthcare Question Answering,What is the HAR model and what problem does it address?,"HAR (Hierarchical Attention Retrieval) model is designed for Healthcare Question Answering. It addresses the problem of effectively retrieving relevant information from documents to answer healthcare-related questions by utilizing a hierarchical attention mechanism that incorporates both cross-attention between query and document words, and inner attention within the document structure.",The HAR (Hierarchical Attention Retrieval) model is a neural network designed to rank documents for healthcare-related queries. It addresses the challenge of efficiently retrieving relevant information from long or short documents for both factoid and non-factoid questions.
A Hierarchical Attention Retrieval Model for Healthcare Question Answering,What are the main components of the HAR model?,"Cross-attention mechanism between query and document words, hierarchical inner attention over document words and sentences.","HAR consists of several components: word embeddings, query and document Bi-GRU encoders, cross attention between query and document words, query inner attention, hierarchical inner attention over words and sentences in documents, and a score computation layer to output relevance scores."
A Hierarchical Attention Retrieval Model for Healthcare Question Answering,How does the cross-attention mechanism in HAR work?,It allows the model to use interaction features between query and document word vectors in the scoring process.,The cross-attention layer computes the relevance between each query word and each word in the document. It uses document-to-query and query-to-document attention to generate attended embeddings that highlight important words in both the query and document.
A Hierarchical Attention Retrieval Model for Healthcare Question Answering,How does HAR handle variable-length queries and long documents?,"HAR handles variable-length queries and long documents by using a formal language that supports set operations, such as union, intersection, and addition. The language also allows for the definition of functions like COMP, CONJ, and ADD, which can be used to manipulate sets and perform mathematical operations. Additionally, HAR uses S IMP QA directly to handle certain constructions, such as superlative and comparative questions, by assuming they are mentioned verbatim on some web document.","HAR uses query inner attention to create a fixed-size embedding that emphasizes important query words. For documents, it applies a two-level hierarchical attention: word-level attention within sentences and sentence-level attention across the document, allowing it to focus on relevant sections."
A Hierarchical Attention Retrieval Model for Healthcare Question Answering,What are the main contributions and results of the paper?,Not explicitly stated in the snippet.,"The paper proposes the HAR model, introduces a large-scale healthcare QA dataset (HealthQA), and demonstrates that HAR outperforms several state-of-the-art retrieval methods. The model can also extract probable answer snippets, making healthcare information more accessible."
"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction",What is the main objective of the study presented in this paper?,"To evaluate the performance of various methods for information extraction tasks, specifically for the RDoC task at BioNLP-OST 2019.","The study aims to develop a system that can automatically extract structured information—namely tasks, datasets, evaluation metrics, and numeric scores (TDMS)—from scientific papers in NLP. This enables the construction of scientific leaderboards and helps track progress in the field."
"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction","What is TDMS-IE, and what components does it extract from scientific literature?","TDMS-IE is a system that associates TDM triples with experimental scientific papers. It extracts two main components from scientific literature: (1) the best numeric score for each predicted TDM triple, and (2) the context information (SC) containing column headers and table captions corresponding to boldfaced numeric scores in tables.","TDMS-IE is the proposed information extraction framework that identifies four key components from NLP papers: Tasks, Datasets, Metrics, and Scores. These elements are extracted to support the automatic generation of leaderboards."
"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction",What data resources did the authors build to train and evaluate their system?,WikiNEuRal,"The authors constructed two datasets: one for identifying mentions of tasks, datasets, and metrics (TDMS-Span), and another for linking these mentions with numeric scores (TDMS-Relation). These resources were manually annotated from ACL anthology papers."
"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction","What model architecture was used in TDMS-IE, and how was it trained?",DocTAET-TDM,"The architecture is a neural model based on BiLSTM-CRF for span extraction, and a feed-forward network for relation classification. It was trained using the manually annotated TDMS datasets to recognize relevant spans and link them to their corresponding numeric scores."
"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction",What were the key findings from the evaluation of TDMS-IE?,"TDMS-IE outperformed baselines in extracting TDM triples, achieving macro F1 scores of 56.6 and micro F1 scores of 66.0. However, when score extraction (TDMS) was added, the overall performance was still unsatisfactory due to errors triggered by noise from table parsers.","The system significantly outperformed strong baselines in extracting tasks, datasets, metrics, and scores. It demonstrated the feasibility of building automatic leaderboard generation systems, though challenges like ambiguous phrasing and domain variation remain."
SciREX: A Challenge Dataset for Document-Level Information Extraction,What is the main motivation behind the SciREX dataset?,Not mentioned in this snippet.,"The main motivation is to address the limitations of existing information extraction datasets, which typically focus on sentence- or paragraph-level relations. SciREX supports document-level IE, where entities and relations can span multiple sentences or sections, making the task more realistic and challenging for scientific literature."
SciREX: A Challenge Dataset for Document-Level Information Extraction,What key tasks are encompassed within the SciREX dataset?,Document-level information extraction.,"SciREX includes multiple document-level IE tasks: salient entity identification, N-ary relation extraction (linking task, method, and dataset), evidence span detection (supporting text), and coreference resolution to link entity mentions across the document."
SciREX: A Challenge Dataset for Document-Level Information Extraction,How was the SciREX dataset constructed?,It is formulated as an entailment task with the information encoded as [CLS] document [SEP] relation in a BERT style model.,The dataset was constructed using a semi-automated process that combined automatic heuristics—leveraging document structure and metadata—with human annotation. Tools like SciIE and scientific knowledge bases helped identify candidate entities and relations for manual validation.
SciREX: A Challenge Dataset for Document-Level Information Extraction,What architecture did the authors propose as a baseline model for the SciREX dataset?,BERT style model,"The authors extended the SciIE model for document-level use, applying a multi-task learning framework with a shared SciBERT-based document encoder. Task-specific layers then predicted entities, coreference clusters, and relations."
SciREX: A Challenge Dataset for Document-Level Information Extraction,What challenges and gaps were revealed through evaluation of the baseline model?,"Two challenges/gaps were revealed. Firstly, the baseline models outperformed at least one participating system (often more), despite being unsophisticated. Secondly, the baseline methods did not utilize training data, while the participating methods may have been overfitted to the training data.","Evaluation revealed a substantial performance gap between the baseline model and human annotators, especially in identifying document-level relations and evidence. This underscores the complexity of the task and the need for more advanced models capable of long-range context understanding."
End-to-end Neural Coreference Resolution,What is the primary characteristic of the neural coreference resolution model introduced in the paper?,End-to-end,The model is the first end-to-end neural coreference resolution model that does not rely on syntactic parsers or hand-engineered mention detectors.
End-to-end Neural Coreference Resolution,How does the model handle span selection to manage computational complexity?,"The model handles span selection through aggressive pruning of spans that are unlikely to belong to a coreference cluster according to the mention score sm(i), enabled by the factoring of the pairwise coreference score.","The model considers spans up to length L, keeps the top λT spans based on mention scores, and limits antecedents to K per span, enforcing non-crossing structures."
End-to-end Neural Coreference Resolution,What methods are used to compute span embeddings in the model?,"Bi-directional LSTMs from word, character and ELMo embeddings.",Span embeddings are computed using boundary representations and attention-based head word features.
End-to-end Neural Coreference Resolution,How does the model learn mention detection and clustering?,"The model uses a two-level BERT+BiLSTM method to get token representations, which are then passed to a CRF layer to identify mentions. Mention clustering is performed using agglomerative hierarchical clustering on pairwise coreference scores.",The model jointly learns mention detection and clustering by maximizing the likelihood of gold coreference links.
End-to-end Neural Coreference Resolution,What are some of the strengths and weaknesses of the model as discussed in the paper?,One strength is that having partially relevant negative samples in the training dataset gives the model higher discriminative power.,Strengths include attention mechanisms aligning with syntactic heads and handling complex noun phrases; weaknesses involve false positives due to word similarity and lack of world knowledge.
CLPsych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,What is the primary focus of the shared task discussed in the paper?,Coreference resolution.,The shared task focuses on using language-based assessments to predict psychological and mental health outcomes over time.
CLPsych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,Which models were primarily used by participants in the study to predict mental health scores?,Generative models.,"Participants used various models, primarily regularized linear regression, neural networks, and ensemble methods, with top systems often combining multiple techniques."
CLPsych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,What evaluation metrics were employed to assess the performance of the models?,"Precision (p), recall (r), and their harmonic average F.","Evaluation metrics include disattenuated Pearson correlations and mean absolute error, accounting for measurement reliability."
CLPsych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,How did the baseline models differ from the participant systems in terms of features?,They utilized training data.,"The baseline models relied on simple unigrams and demographic controls, but most participant systems outperformed these baselines."
CLPsych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,What additional methods were used to evaluate the quality of generated essays in the study?,Embedding Similarity score.,"Additional evaluation involved comparing generated essays to actual ones using BLEU scores and semantic similarity measures, though these metrics have limitations."
DSTC7 Task 1: Noetic End-to-End Response Selection,What are the primary neural models used by teams in the shared task for dialogue systems as discussed in the paper?,"Enhanced LSTM model (ESIM), Convolutional Neural Networks, Memory Networks, the Transformer, Attention, and Recurrent Neural Network variants.","The primary neural models used include Enhanced LSTM (ESIM), Transformers, Memory Networks, CNNs, and models incorporating contextual embeddings like ELMo."
DSTC7 Task 1: Noetic End-to-End Response Selection,How do the models evaluated in the paper measure their performance in the dialogue system shared task?,They use evaluation metrics.,"Performance was evaluated using metrics such as Recall@N, MRR, and MAP across multiple subtasks and datasets, including Ubuntu and Advising dialogues."
DSTC7 Task 1: Noetic End-to-End Response Selection,What are some of the external data sources incorporated into the neural dialogue models described in the paper?,Pre-trained embeddings that were publicly available prior to the release of the data.,"External data sources include Ubuntu man pages, advising information, and other domain-specific datasets."
DSTC7 Task 1: Noetic End-to-End Response Selection,What are the main challenges identified in the shared task for neural dialogue models?,"The main challenges identified in the shared task for neural dialogue models include (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set.",The main challenges include handling large candidate sets and domain generalization.
DSTC7 Task 1: Noetic End-to-End Response Selection,What is the overarching goal of the shared task as outlined in the paper?,"To improve the performance of downstream tasks such as information extraction by successfully resolving coreference, which would add over 106,000 additional concept annotations to the CRAFT corpus.","The task aims to foster development of more realistic, challenging dialogue systems, with datasets and baselines publicly available for future research."
Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task,What are the primary challenges faced in Named Entity Recognition (NER) for code-switched social media data as discussed in the paper?,"Code-switching phenomenon, diversity of entities, and social media challenges.","The primary challenges include the informal and heterogeneous nature of social media text, the presence of code-switching, social media noise, data imbalance, and the difficulty of accurately identifying entities across closely related language variants like Modern Standard Arabic and Egyptian dialect."
Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task,"What datasets were created and annotated for the purpose of NER in the study, and what languages do they focus on?","CoNLL-2002 and 2003 datasets. They were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entity types: PER (Person), ORG (Organization), LOC (Location), and MISC (Miscellaneous).","The datasets were created and annotated for NER in code-switched social media data focusing on Arabic-Egyptian and English-Spanish language pairs, with careful annotation using crowdsourcing and validation."
Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task,"Which types of models were employed in the systems summarized in the paper, and what was the general outcome of their performance?","TDMS-IE; The general outcome of their performance was that adding more information (e.g., Experimental Setup, Table Information) generally improved the model's performance.","The systems employed neural models such as bidirectional LSTM combined with Conditional Random Fields (CRF), as well as traditional models. The results showed that NER performance remains challenging in social media contexts, especially for closely related language variants."
Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task,What was the main objective of the CALCS 2018 shared task as described in the paper?,Named Entity Recognition on Code-Switched Data.,"The main objective was to develop models to identify 9 entity types (e.g., person, organization, location, etc.) in Twitter data containing code-switched language pairs (English-Spanish and Modern Standard Arabic-Egyptian), providing a benchmark for future research in multilingual, code-switched NER."
Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task,"According to the paper, what are the key factors that contribute to the difficulty of NER in noisy, social media, and code-switched data?",Code-switching behavior.,"The key factors include data noise, domain variation, code-switching, and the complexity of multilingual data, which cause most approaches to achieve lower scores than monolingual text."
UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,What is the primary focus of the shared task discussed in the paper?,Coreference resolution.,"The primary focus of the shared task is predicting when clarifications are needed in text, specifically classifying sentences as requiring or not requiring clarification based on revision histories."
UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,Which types of revision differences are highlighted as potentially influencing the results between in-domain and out-of-domain data?,Revision type distributions.,Differences in revision types such as insertions of modifiers or complements are highlighted as potentially influencing the results.
UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,What approaches did participants use to identify clarification needs in the study?,"Participants used various machine learning models, including logistic regression with uni-grams, bi-grams, and sentence length as features.",Participants used approaches like identifying pronouns requiring revision and leveraging sentence label distributions.
UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,What was the top accuracy achieved by transformer-based models in the shared task?,68.8%,Transformer-based models achieved a top accuracy of 68.4%.
UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,What are some of the challenges identified in predicting clarification needs according to the paper?,"Low performance on out-of-domain data, with results not exceeding the accuracy of a logistic regression classifier.","Challenges include predicting subtle, context-specific edits and understanding what constitutes a necessary revision."
SemEval-2021 Task 5: Toxic Spans Detection,What is the definition of a toxic span as introduced in the paper?,"A toxic span is defined as a maximal sequence of contiguous toxic character-offsets, where a character offset is considered toxic if the majority of raters included it in their toxic spans.","A toxic span is defined as a sequence of words that attribute to the post’s toxicity, with gold labels provided at the character level counting from zero."
SemEval-2021 Task 5: Toxic Spans Detection,What is the architecture of BENCHMARK I described in the paper?,"Random baseline and logistic regression classifier that makes use of uni-grams, bi-grams and sentence length as features.","BENCHMARK I is based on a RoBERTa model fine-tuned to predict if a post is toxic or not, and further fine-tuned to predict toxic spans by using a CRF layer on top."
SemEval-2021 Task 5: Toxic Spans Detection,What is the architecture of BENCHMARK II described in the paper?,BENCHMARK II is a high-precision classifier.,"BENCHMARK II is a lexicon-based system that extracts likely toxic words from the training data and tags them during inference, using a lexicon of words that frequently appear inside toxic spans and not outside."
SemEval-2021 Task 5: Toxic Spans Detection,What is the official evaluation measure used in the task?,Accuracy,"The official evaluation measure is the macro-averaged F1 score computed on character offsets between predicted and ground truth toxic spans, assigning a perfect score when both are empty and zero otherwise."
SemEval-2021 Task 5: Toxic Spans Detection,What are the main challenges in detecting long toxic spans as discussed in the paper?,"Long toxic spans are rather context-dependent and more challenging to detect, as they do not comprise common cuss or clearly abusive words that can be directly classified as toxic.","Long toxic spans are more likely context-dependent and less frequent in the dataset compared to single-word spans, making their detection a challenge."
The CoNLL-2014 Shared Task on Grammatical Error Correction,What is the primary goal of the CoNLL-2014 shared task?,To evaluate algorithms and systems for automatically detecting and correcting grammatical errors.,The goal of the CoNLL-2014 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors present in English essays written by second language learners of English.
The CoNLL-2014 Shared Task on Grammatical Error Correction,What key change was made to the evaluation metric in CoNLL-2014 compared to CoNLL-2013?,Alternative answers were added.,"The evaluation metric was changed from F1 to F0.5, to emphasize precision over recall."
The CoNLL-2014 Shared Task on Grammatical Error Correction,What is the MaxMatch (M2) scorer used for in the shared task?,It efficiently searches for a set of system edits that maximally matches the set of gold-standard edits specified by an annotator.,The MaxMatch (M2) scorer efficiently searches for a set of system edits that maximally matches the set of gold-standard edits specified by an annotator.
The CoNLL-2014 Shared Task on Grammatical Error Correction,What is one of the most popular approaches used by participating teams to correct all error types?,Language Model (LM) based approach.,"One of the most popular approaches to nonspecific error type correction was the Language Model (LM) based approach, where the probability of a learner n-gram is compared with the probability of a candidate corrected n-gram, and if the difference is greater than some threshold, an error was perceived to have been detected and a higher scoring replacement n-gram could be suggested."
The CoNLL-2014 Shared Task on Grammatical Error Correction,Why does the CoNLL-2014 shared task include all 28 error types instead of focusing on just a few?,No information is provided in the given snippet to answer this question.,"It was felt that the community is now ready to deal with all error types, and including all 28 error types increases the complexity of the task by introducing a greater chance of encountering multiple, interacting errors in a sentence."
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,What is the main contribution of the paper in terms of sequence labeling systems?,Proposing a novel neural network architecture for linguistic sequence labeling.,"The paper introduces a novel neural network architecture that benefits from both word- and character-level representations automatically, by using a combination of bidirectional LSTM, CNN and CRF. The system is truly end-to-end, requiring no feature engineering or data preprocessing, making it applicable to a wide range of sequence labeling tasks."
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,How does the proposed CNN extract character-level representations?,"It uses only character embeddings as inputs to CNN, without character type features.","The CNN extracts morphological information (like the prefix or suffix of a word) from characters of words and encodes it into neural representations. The CNN uses only character embeddings as inputs, without character type features, and a dropout layer is applied before character embeddings are input to the CNN."
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,What is the purpose of using a Bi-directional LSTM (BLSTM) in the architecture?,To capture both past and future information in sequence labeling tasks.,"The purpose of using a BLSTM is to have access to both past (left) and future (right) contexts in sequence labeling tasks. BLSTM presents each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively, and then concatenates the two hidden states to form the final output."
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,Why is a Conditional Random Field (CRF) layer added on top of the BLSTM output?,To jointly decode the best label sequence.,"A CRF layer is added to model label sequences jointly and consider correlations between labels in neighborhoods, rather than decoding each label independently. This allows the system to decode the best chain of labels for a given input sentence, which significantly benefits the final performance of the neural network model."
End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,How is the final BLSTM-CNNs-CRF model structured?,It uses CNN to model character-level information.,"For each word, the character-level representation is computed by the CNN with character embeddings as inputs. This character-level representation vector is concatenated with the word embedding vector and fed into the BLSTM network. Finally, the output vectors of BLSTM are fed to the CRF layer to jointly decode the best label sequence, with dropout layers applied on both the input and output vectors of BLSTM."
Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,"What is the unified mechanism relation schema proposed in the paper, and what are its key properties?","The unified mechanism relation schema proposed in the paper consists of a relation (E1, E2, class) between entities E1 and E2, where each entity E is a text span and the class indicates the type of the mechanism relation. The three key properties of this schema are: (1) it uses a generalized concept of mechanism relations; (2) it includes flexible, generic entities not limited to pre-defined types; and (3) it is simple enough for human annotators and models to identify in the natural language of scientific texts.","The schema is defined as a relation (E1, E2, class) between entities E1 and E2, where each entity is a text span and the class indicates the type of the mechanism relation. Its three key properties are: (1) it uses a generalized concept of mechanism relations, capturing and extending existing types of mechanisms; (2) it includes flexible, generic entities not limited to predefined types; and (3) it is simple enough for human annotators and models to identify in scientific texts."
Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,How are mechanisms categorized in the proposed schema?,"Mechanisms are categorized as relations (E1, E2, class) between entities E1 and E2, where each entity E is a text span and the class indicates the type of the mechanism relation.","Mechanisms are categorized into two coarse-grained classes: Direct mechanisms, which include activities of a mechanistic nature or explicit functions (e.g., a virus binding to a cell or a drug being used for treatment), and Indirect mechanisms, which include influences or associations without explicit mechanistic information (e.g., COVID-19 may lead to economic impacts)."
Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,What process was followed to construct the MECHANIC dataset of mechanism annotations?,"A three-stage process of (1) annotating entities and relations using biomedical experts, (2) unifying span boundaries with an NLP expert, and (3) verifying annotations with a bio-NLP expert.","The dataset was constructed through a three-stage process: (1) biomedical experts annotated entities and relations as direct or indirect mechanisms; (2) an NLP expert unified annotation span boundaries for consistency; and (3) a bio-NLP expert verified and corrected annotations. This resulted in 2,370 relation instances (1,645 direct, 725 indirect) from 1,000 sentences in 250 abstracts."
Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,"Which model was used for extracting mechanism relations, and how was it applied to build the KB?",A DyGIE model trained on MECHANIC.,"The DyGIE++ model with SciBERT embeddings was trained on MECHANIC to jointly extract entities and classify relations as {DIRECT, INDIRECT}. It was then applied to 160K abstracts in the CORD-19 corpus, extracting and filtering mechanism relations with high confidence to construct the COMB knowledge base, which contains 1.5M relations."
Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,How does the semantic relation search over COMB work?,It replaces the variable with each answer string representation and returns their union.,"For a given query (Eanswer, Equestion, class), entity spans are encoded into a vector space using a fine-tuned language model. Relations in COMB are ranked by the minimum similarity between query and candidate entity encodings, with the class used to filter relation types. An index of 900K unique entity surface forms is stored using FAISS for fast similarity-based search."
SPECTER: Document-level Representation Learning using Citation-informed Transformers,What is the main contribution of SPECTER as proposed in the paper?,"SPECTER is a model for learning representations of scientific papers, based on a Transformer language model that is pretrained on citations.","SPECTER is a new method to generate document-level embeddings of scientific documents by pretraining a Transformer language model on the citation graph as a signal of document-level relatedness. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning."
SPECTER: Document-level Representation Learning using Citation-informed Transformers,Why are existing Transformer language models like BERT limited for document-level representation learning?,They only consider a fixed-size window of context.,"Existing models such as BERT are primarily based on masked language modeling objectives, which only consider intra-document context and do not leverage inter-document information, thereby limiting their ability to learn optimal document-level representations."
SPECTER: Document-level Representation Learning using Citation-informed Transformers,How does SPECTER use citations to improve representation learning?,"SPECTER uses citations as an inter-document incidental supervision signal, formulating it into a triplet-loss pretraining objective.","SPECTER formulates citation information as a triplet loss learning objective, where the model is pretrained on a large corpus of citations, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not."
SPECTER: Document-level Representation Learning using Citation-informed Transformers,What is SCIDOCS and why was it introduced?,SCIDOCS is an evaluation benchmark that includes an anonymized clickthrough dataset from an online recommendations system. It was introduced to address the challenge of a lack of publicly available datasets in academic paper recommendation research.,"SCIDOCS is a new evaluation benchmark consisting of seven document-level tasks, ranging from citation prediction to document classification and recommendation. It was introduced to encourage further research on document-level models and to evaluate the effectiveness of approaches like SPECTER."
SPECTER: Document-level Representation Learning using Citation-informed Transformers,What directions for future work are suggested in the paper?,None.,"The paper suggests future work in initializing model weights from newer Transformer models, developing multitask approaches to leverage multiple signals of relatedness beyond citations, exploring bibliometrics-based relatedness metrics, and incorporating additional inputs such as outgoing citations."
TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,What is the main contribution of the paper?,Extracting Keyphrases and Relations from Scientific Publications.,"The paper presents a new corpus (TDMSci) containing expert annotations for Task (T), Dataset (D), and Metric (M) entities on 2,000 sentences extracted from NLP papers. The authors also develop a TDM tagger using a data augmentation strategy and apply it to around 30,000 NLP papers from the ACL Anthology."
TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,How is the TDM entity extraction task modeled in this study?,"As extracting {task, dataset, metric} triples from NLP papers based on a small, manually created task/dataset/metric (TDM) taxonomy.","The TDM entity extraction task is modeled as a sequence tagging problem. The authors apply a CRF model with lexical features, a BiLSTM-CRF model using the Flair framework, and adapt the SciIE model (Luan et al., 2018) for TDM entity extraction."
TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,What features are used in the CRF model for TDM extraction?,Lexical features.,"The CRF model uses features such as unigrams of the previous, current, and next words, current word character n-grams, current POS tag, surrounding POS tag sequence, current word shape, and surrounding word shape sequence."
TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,What is the purpose of the proposed data augmentation strategy?,There is no mention of a proposed data augmentation strategy in the given text.,"The data augmentation strategy is designed to improve TDM entity extraction by generating additional masked training data. It replaces every token within an annotated TDM entity with UNK, leveraging the importance of surrounding context in identifying entities."
TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics,What is the significance of the TDMSci corpus and TDM tagger?,"The TDMSci corpus provides domain expert annotations for Task, Dataset, and Metric entities in scientific literature, while the TDM tagger can be used to extract these entities from large volumes of text, such as 30,000 NLP papers from the ACL Anthology.","The TDMSci corpus and TDM tagger enable the extraction of essential scientific information (Task, Dataset, Metric) from NLP papers. Experiments on 30,000 NLP papers demonstrate that they can help build TDM knowledge resources for the NLP domain."
Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,What is Falcon 2.0 and what does it achieve?,"Falcon 2.0 is an entity and relation linking tool over Wikidata, achieving new baselines in relation linking performance on two datasets: SimpleQuestion and LC-QuAD 2.0.","Falcon 2.0 is the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in English and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata, represented by their Internationalized Resource Identifier (IRI)."
Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,What are the main components of the Falcon 2.0 architecture?,Entity linker and relation linker,"Falcon 2.0 comprises several modules including POS Tagging, Tokenization & Compounding, N-Gram Tiling, Candidate List Generation, Matching & Ranking, Query Classifier, and N-Gram Splitting. It also relies on a background knowledge base combining Wikidata labels and aliases, alignments stored in a text search engine, and rules maintained in a catalog for English morphology."
Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,What role does the catalog of rules play in Falcon 2.0?,"It is used to extract entities and relations from the text, based on English morphological principles.","The catalog of rules, based on English morphological principles, guides entity and relation extraction. For example, verbs are excluded from entity candidates, stopwords between entities are ignored to treat them as one entity, and question headwords like 'When' and 'Where' are used to resolve relation ambiguities."
Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,How does Falcon 2.0 perform the recognition phase?,"Falcon 2.0 performs the recognition phase through three modules: POS tagging, tokenization & compounding, and N-Gram tiling.","The recognition phase includes POS Tagging, Tokenization & Compounding, and N-Gram Tiling. POS Tagging identifies word types such as nouns and verbs. Tokenization & Compounding removes stopwords and separates verbs from nouns, while N-Gram Tiling combines tokens separated only by stopwords, producing candidate surface forms for entities and relations."
Falcon 2.0: An Entity and Relation Linking Tool over Wikidata,How does Falcon 2.0 handle candidate linking and ranking?,"Falcon 2.0 handles candidate linking and ranking through four modules: candidate list generation, matching & ranking, relevant rule selection, and n-gram splitting.","In the linking phase, Candidate List Generation queries the text search engine for each token, producing candidate lists. Matching & Ranking checks candidate triples against Wikidata with ASK queries, ranking entities and relations accordingly. Relevant Rule Selection adjusts rankings based on catalog rules, and N-Gram Splitting addresses cases where compounding incorrectly merged separate entities."
Deep Reinforcement Learning for Mention-Ranking Coreference Models,What problem does this paper address?,Detecting Clarification Requirements in Instructional Text.,The paper addresses the reliance on heuristic loss functions for training coreference resolution models and proposes reinforcement learning to directly optimize for evaluation metrics.
Deep Reinforcement Learning for Mention-Ranking Coreference Models,What baseline model do the authors build on?,"A simple logistic regression classifier that makes use of uni-grams, bi-grams and sentence length as features.","They build on the neural mention-ranking model by Clark and Manning (2016), which scores mention-antecedent pairs using a feedforward neural network."
Deep Reinforcement Learning for Mention-Ranking Coreference Models,Why are heuristic loss functions limited?,"Heuristic loss functions are limited because they do not capture the full complexity of the problem, relying instead on approximations or simplifications that may not always hold true.",They require extensive hyperparameter tuning to balance error types and only indirectly optimize for coreference evaluation metrics.
Deep Reinforcement Learning for Mention-Ranking Coreference Models,How does reinforcement learning improve coreference resolution?,"Reinforcement learning improves coreference resolution by directly optimizing a neural mention-ranking model for coreference evaluation metrics, rather than relying on heuristic loss functions that require careful tuning of hyperparameters.","It treats coreference decisions as actions, uses evaluation metrics as rewards, and directly optimizes the model to improve accuracy without tuned hyperparameters."
Deep Reinforcement Learning for Mention-Ranking Coreference Models,What are the main results of this approach?,"The main results of this approach include the ability to answer complex questions by composing functions such as COMP, CONJ, and ADD, which allow for operations like union, intersection, and addition. Additionally, the approach can handle superlative and comparative questions using S IMP QA directly.","The reinforcement learning approach, especially reward rescaling, achieves significant improvements over the state of the art on English and Chinese CoNLL 2012 tasks."
AxCell: Automatic Extraction of Results from Machine Learning Papers,What is AXCELL and what problem does it solve?,"AXCELL is a pipeline for automatic extraction of results from machine learning papers. It solves the problem of laborious and error-prone human annotation of results, which can lead to omission or misreporting of paper results.","AXCELL is an automatic machine learning pipeline for extracting structured results from research papers. It addresses the challenge of tracking ML progress by reliably extracting tuples of the form (task, dataset, metric, value) from tables in papers."
AxCell: Automatic Extraction of Results from Machine Learning Papers,What are the key subtasks in the AXCELL pipeline?,"Table type classification, table segmentation, and linking results to leaderboards.","The pipeline includes: (i) table type classification (leaderboard, ablation, irrelevant), (ii) table segmentation (classifying table cells), (iii) cell context generation, (iv) linking results to leaderboards, and (v) filtering results to keep the best entries."
AxCell: Automatic Extraction of Results from Machine Learning Papers,How does AXCELL perform table type classification?,"AXCELL performs table type classification using a model trained with features such as ""is emphasised"", ""cell style"", ""text mentions"", ""cell content"", ""row context"", ""column context"", and ""cell reference"". The model is trained for 10 epochs and the best micro F1 score on the validation set is used.","It uses a ULMFiT-based classifier with LSTM layers and SentencePiece tokenization to categorize tables as leaderboard, ablation, or irrelevant, trained on a large dataset of segmented tables."
AxCell: Automatic Extraction of Results from Machine Learning Papers,How does AXCELL link numeric values to leaderboards?,"AXCELL links numeric values to leaderboards through results tuples, which capture the performance of models in papers. Each results tuple (task, dataset, metric name, metric value) is linked to a particular table, row, and cell it originates from.","AXCELL collects contextual evidence from table structure and paper text, then applies a generative probabilistic model to associate metric values with leaderboards defined by task, dataset, and metric."
AxCell: Automatic Extraction of Results from Machine Learning Papers,What are the main contributions and results of the paper?,"The main contributions and results of the paper include:

* The participating systems performed substantially better than both baselines (random baseline and logistic regression classifier) on the test set, with accuracy ranging from 66.4% to 68.8%.
* The best participating system was able to identify over 90% of cases that require replacements of pronouns and replacements of occurrences of 'do' as a main verb.
* The results suggest that it is generally possible to predict whether a sentence requires clarification, and models can pick up reliable patterns for most types of revision.","The paper introduces AXCELL, releases new datasets for training and evaluation, and demonstrates significant performance improvements over the state of the art in results extraction. The pipeline also makes semi-automated extraction practically viable."
