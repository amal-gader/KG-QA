title,question,response,ground_truth
Minimally Supervised Learning of Affective Events Using Discourse Relations,What is the seed lexicon?,A set of words with known polarity scores (+1 for positive events and -1 for negative events).,a vocabulary of positive and negative predicates that helps determine the polarity score of an event | seed lexicon consists of positive and negative predicates
Minimally Supervised Learning of Affective Events Using Discourse Relations,What are the results?,AL: Propagate the same polarity; CA: Encourage them to have the same polarity; CO: Encourage them to have the reverse polarity.,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."
Minimally Supervised Learning of Affective Events Using Discourse Relations,How are relations used to propagate polarity?,"Relations are used to propagate polarity by identifying the discourse relation between two events (CAUSE or CONCESSION) and then applying rules to determine the polarity of the former event based on the latter event's polarity. For CAUSE, the same polarity is used, while for CONCESSION, the reversed polarity is used.","based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event  | cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"
Minimally Supervised Learning of Affective Events Using Discourse Relations,How big is the Japanese data?,"32,000","7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus | The ACP corpus has around 700k events split into positive and negative polarity "
Minimally Supervised Learning of Affective Events Using Discourse Relations,What are labels available in dataset for supervision?,polarity-tagged,negative | positive
Minimally Supervised Learning of Affective Events Using Discourse Relations,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,"According to Table 4, the proposed approach achieves better results than the basic approach when trained on small labeled data.",3%
Minimally Supervised Learning of Affective Events Using Discourse Relations,How does their model learn using mostly raw data?,"Their model learns non-compositional expressions and negation without explicit supervision, suggesting it can pick up on patterns in the data through minimally supervised learning.",by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity
Minimally Supervised Learning of Affective Events Using Discourse Relations,How big is seed lexicon used for training?,5760,30 words
Minimally Supervised Learning of Affective Events Using Discourse Relations,How large is raw corpus used for training?,Japanese Wikipedia,100 million sentences
Community Identity and User Engagement in a Multi-Community Landscape,How do the various social phenomena examined manifest in different types of communities?,"They reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity.","Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
"
Community Identity and User Engagement in a Multi-Community Landscape,What patterns do they observe about how user engagement varies with the characteristics of a community?,They observe that variations in user behavior are related to structural features such as group size and volume of content.,"communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members | within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers "
Community Identity and User Engagement in a Multi-Community Landscape,How did the select the 300 Reddit communities for comparison?,"They selected all subreddits with at least 500 words in the vocabulary used to estimate their measures, in at least 4 months of the subreddit's history.","They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language. | They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language."
Community Identity and User Engagement in a Multi-Community Landscape,How do the authors measure how temporally dynamic a community is?,They use word-level measures of volatility.,the average volatility of all utterances
Community Identity and User Engagement in a Multi-Community Landscape,How do the authors measure how distinctive a community is?,They measure it as the average specificity of all utterances in the community., the average specificity of all utterances
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,What data is the language model pretrained on?,Chinese general corpus.,Chinese general corpus
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,What baselines is the proposed model compared against?,Strong baseline models,BERT-Base | QANet | QANet BIBREF39 | BERT-Base BIBREF26
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,How is the clinical text structuring task defined?,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is are obtained.","Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. | Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text.  | CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text."
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,What are the specific tasks being unified?,Extracting commonality of the problem and unifying the output format for various clinical tasks.," three types of questions, namely tumor size, proximal resection margin and distal resection margin"
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,"Is all text in this dataset a question, or are there unrelated sentences in between questions?","No, not all text is a question. The text includes paragraph text X and query text Q, as well as other types of text such as named entity information and contextualized representation vectors.",the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences 
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,How many questions are in the dataset?,3 types of questions,"2,714 "
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,How they introduce domain-specific features into pre-trained language model?,"They adapt pre-trained language models such as BERT into specific domains, for example, BioBERT.",integrate clinical named entity information into pre-trained language model
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,How big is QA-CTS task dataset?,No specific size of the QA-CTS task dataset is mentioned in the given context.,"17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,How big is dataset of pathology reports collected from Ruijing Hospital?,"17,833 sentences, 826,987 characters and 2,714 question-answer pairs.","17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,What are strong baseline models in specific tasks?,"Task-speciﬁc end-to-end methods [4], [5] use large amount of data to automatically model the speciﬁc task.",state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26
LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,How is the intensity of the PTSD established?,"The intensity of PTSD is established through surveys used by veterans peer support organizations for local weekly assessments, as well as through analysis of Twitter posts.","Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error. | defined into four categories from high risk, moderate risk, to low risk"
LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,How is LIWC incorporated into this system?,"LIWC is used to analyze language use in social media, specifically to estimate survey scores similar to sentiment category scores. The system also uses a modified LIWC algorithm that incorporates clinically validated survey tools and dictionary words based on survey questions."," For each user, we calculate the proportion of tweets scored positively by each LIWC category. | to calculate the possible scores of each survey question using PTSD Linguistic Dictionary "
LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,How many twitter users are surveyed using the clinically validated survey?,210,210
LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,Which clinically validated survey tools are used?,None,"DOSPERT, BSSS and VIAS"
Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data,Which vision-based approaches does this approach outperform?,"Bergsma and Van Durme 2011; Kiela, Vulic, and Clark 2015; Hewitt et al. 2018",CNN-mean | CNN-avgmax
Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data,What baseline is used for the experimental setup?,None,CNN-mean | CNN-avgmax
Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data,Which languages are used in the multi-lingual caption model?,No specific language is mentioned.,"German-English, French-English, and Japanese-English | multiple language pairs including German-English, French-English, and Japanese-English."
Generative Adversarial Nets for Multiple Text Corpora,Which GAN do they use?,"Energy-based GAN (Zhao et al., 2017)","We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 .  | weGAN | deGAN"
Generative Adversarial Nets for Multiple Text Corpora,Which corpora do they use?,A Japanese web corpus compiled through the procedures proposed by Kawahara and Kurohashi (2006).,"CNN, TIME, 20 Newsgroups, and Reuters-21578"
Stacked DeBERT: All Attention in Incomplete Data for Text Classification,How do the authors define or exemplify 'incorrect words'?,"They are exemplified as spelling mistakes (e.g. ""teh"" instead of ""the""), casual pronunciation (e.g. ""wanna"" instead of ""want to""), abbreviations (e.g. ""Lit"" instead of ""Literature""), repeated letters (e.g. ""thursdayyyyyy""), onomatopoeia (e.g. ""Woohoo""), and others (e.g. ""im"" instead of ""I'm"").",typos in spellings or ungrammatical words
Stacked DeBERT: All Attention in Incomplete Data for Text Classification,By how much do they outperform other models in the sentiment in intent classification tasks?,1,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average
Gunrock: A Social Bot for Complex and Engaging Long Conversations,What is the sample size of people used to measure user satisfaction?,Not explicitly stated.,"34,432 user conversations | 34,432 "
Gunrock: A Social Bot for Complex and Engaging Long Conversations,What are all the metrics to measure user engagement?,"group size, volume of content",overall rating | mean number of turns | overall rating | mean number of turns
Gunrock: A Social Bot for Complex and Engaging Long Conversations,What the system designs introduced?,Stacked DeBERT.,Amazon Conversational Bot Toolkit | natural language understanding (NLU) (nlu) module | dialog manager | knowledge bases | natural language generation (NLG) (nlg) module | text to speech (TTS) (tts)
Gunrock: A Social Bot for Complex and Engaging Long Conversations,How do they correlate user backstory queries to user satisfaction?,They modeled overall (log) Rating with a linear regression by the (log) ‘Number of Backstory Questions Asked’ and found that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses.,"modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions"
Towards Detection of Subjective Bias using Contextualized Word Embeddings,What is the baseline for the experiments?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,FastText | BiLSTM | BERT | FastText | BERT  | two-layer BiLSTM architecture with GloVe word embeddings
Towards Detection of Subjective Bias using Contextualized Word Embeddings,Which experiments are perfomed?,Experiments using discourse relations to propagate polarities of affective events from seeds.,They used BERT-based models to detect subjective language in the WNC corpus
"Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",Is ROUGE their only baseline?,No,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU."
"Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",what language models do they use?,BERT-base,LSTM LMs
An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,What misbehavior is identified?,None.,"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations | if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations"
An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,Which attention mechanisms do they compare?,"Soft attention, hard stochastic attention, and grounded mechanism.",Soft attention | Hard Stochastic attention | Local Attention
What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016,What are the country-specific drivers of international development rhetoric?,The paper examines the country-speciﬁc drivers of international development rhetoric.,wealth  | democracy  | population | levels of ODA | conflict 
What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016,How are the main international development topics that states raise identified?,"Through natural language processing (NLP) of countries' speeches in the UN, specifically during the General Debate at the United Nations General Assembly."," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence."
QnAMaker: Data to Bot in 2 Minutes,What experiments do the authors present to validate their system?,None., we measure our system's performance for datasets across various domains | evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs
QnAMaker: Data to Bot in 2 Minutes,What components is the QnAMaker composed of?,"QnAMaker is composed of two main components: 1) a Knowledge Base (KB), which refers to one instance of Azure Search index where extracted Question-Answer pairs are stored, and 2) a conversational layer over the KB.",QnAMaker Portal | QnaMaker Management APIs | Azure Search Index | QnaMaker WebApp | Bot | QnAMaker Portal | QnaMaker Management APIs | Azure Search Index | QnaMaker WebApp | Bot
Procedural Reasoning Networks for Understanding Multimodal Procedures,What multimodality is available in the dataset?,Multimodal neural machine translation with images.,"context is a procedural text, the question and the multiple choice answers are composed of images | images and text"
Procedural Reasoning Networks for Understanding Multimodal Procedures,What are previously reported models?,word2vec,Hasty Student | Impatient Reader | BiDAF | BiDAF w/ static memory
Procedural Reasoning Networks for Understanding Multimodal Procedures,How better is accuracy of new model compared to previously reported models?,Accuracy improvement was achieved even without trade-off.,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59"
Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications,What datasets are used to evaluate this approach?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus.," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs  | WN18 and YAGO3-10"
Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications,How is this approach used to detect incorrect facts?,"This approach is not explicitly stated to be used for detecting incorrect facts, but rather for detecting subjective bias in natural language. However, it can be inferred that by identifying biased language, the approach may indirectly help identify potentially incorrect or misleading information presented as factual.","if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. "
Learning Supervised Topic Models for Classification and Regression from Crowds,what are the advantages of the proposed model?,(i) it does not rely on references; (ii) can naturally be applied at the sentence level; and (iii) does not need human fluency annotations of any kind.,"he proposed model outperforms all the baselines, being the svi version the one that performs best. | the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
Learning Supervised Topic Models for Classification and Regression from Crowds,what are the state of the art approaches?,Traditional methods regard both steps as a whole.,Bosch 2006 (mv) | LDA + LogReg (mv) | LDA + Raykar | LDA + Rodrigues | Blei 2003 (mv) | sLDA (mv)
Learning Supervised Topic Models for Classification and Regression from Crowds,what datasets were used?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,Reuters-21578 BIBREF30 |  LabelMe BIBREF31 | 20-Newsgroups benchmark corpus BIBREF29  |  20-Newsgroups benchmark corpus  | Reuters-21578 | LabelMe
BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,What models other than standalone BERT is new model compared to?,RoBERTa,Only Bert base and Bert large are compared to proposed approach.
BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?,"For rare frequency words, BERTRAM-ADD improves representation by 0.157 (from 0.112 to 0.269) compared to standalone BERT (base), and by 0.018 (from 0.251 to 0.269) compared to previous work (+ AM). For medium frequency words, BERTRAM-ADD improves representation by 0.133 (from 0.234 to 0.367) compared to standalone BERT (base), and by 0.100 (from 0.267 to 0.367) compared to previous work (+ AM).",improving the score for WNLaMPro-medium by 50% compared to BERT$_\text{base}$ and 31% compared to Attentive Mimicking
BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,What are three downstream task datasets?,"20 Newsgroups, IMDB, and Chatbot NLU Evaluation Corpus.",MNLI BIBREF21 | AG's News BIBREF22 | DBPedia BIBREF23 | MNLI | AG's News | DBPedia
BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,What is dataset for word probing task?,Not mentioned in the given context.,WNLaMPro dataset
Joint Entity Linking with Deep Reinforcement Learning,How big is the performance difference between this method and the baseline?,The text doesn't provide explicit numbers for the performance difference.,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores."
Joint Entity Linking with Deep Reinforcement Learning,What datasets used for evaluation?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,AIDA-B | ACE2004 | MSNBC | AQUAINT | WNED-CWEB | WNED-WIKI | AIDA-CoNLL | ACE2004 | MSNBC | AQUAINT | WNED-CWEB | WNED-WIKI | OURSELF-WIKI
Joint Entity Linking with Deep Reinforcement Learning,what are the mentioned cues?,"Seed lexicon, negation, intensification","output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7"
Marrying Universal Dependencies and Universal Morphology,What are the main sources of recall errors in the mapping?,"Spelling mistakes, casual pronunciation, abbreviations, repeated letters, onomatopoeia, and others (e.g. ""im"" instead of ""I'm"", ""your/ur"" instead of ""you're"", etc.)","irremediable annotation discrepancies | differences in choice of attributes to annotate | The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them | the two annotations encode distinct information | incorrectly applied UniMorph annotation | cross-lingual inconsistency in both resources"
Marrying Universal Dependencies and Universal Morphology,Which languages do they validate on?,English,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur | We apply this conversion to the 31 languages | Arabic, Hindi, Lithuanian, Persian, and Russian.  | Dutch | Spanish"
Revisiting Low-Resource Neural Machine Translation: A Case Study,what amounts of size were used on german-english?,20,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development | ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)"
Revisiting Low-Resource Neural Machine Translation: A Case Study,what were their experimental results in the low-resource dataset?,A character-level model performed poorly.,10.37 BLEU
Revisiting Low-Resource Neural Machine Translation: A Case Study,what are the methods they compare with in the korean-english dataset?,PBSMT and NMT,gu-EtAl:2018:EMNLP1
Revisiting Low-Resource Neural Machine Translation: A Case Study,what pitfalls are mentioned in the paper?,None.,highly data-inefficient | underperform phrase-based statistical machine translation
CAiRE: An End-to-End Empathetic Chatbot,What pretrained LM is used?,LSTM (Long Short Term Memory),Generative Pre-trained Transformer (GPT) | Generative Pre-trained Transformer (GPT)
Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?,What approaches they propose?,"They argue for a more practical view of faithfulness, calling for a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.","Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. | Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?,What faithfulness criteria does they propose?,"A graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.","Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. | Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?,Which are three assumptions in current approaches for defining faithfulness?,That faithfulness is a binary property.,"Two models will make the same predictions if and only if they use the same reasoning process. | On similar inputs, the model makes similar decisions if and only if its reasoning is similar. | Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other. | Two models will make the same predictions if and only if they use the same reasoning process. | On similar inputs, the model makes similar decisions if and only if its reasoning is similar. | Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?,Which are key points in guidelines for faithfulness evaluation?,"A graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.",Be explicit in what you evaluate. | Faithfulness evaluation should not involve human-judgement on the quality of interpretation. | Faithfulness evaluation should not involve human-provided gold labels. | Do not trust “inherent interpretability” claims. | Faithfulness evaluation of IUI systems should not rely on user performance.
Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference,Did they use the state-of-the-art model to analyze the attention?,"No, they used ESIM-50.",we provide an extensive analysis of the state-of-the-art model
Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference,How many layers are there in their model?,6,two LSTM layers
SUM-QE: a BERT-based Summary Quality Estimation Model,What are their correlation results?,"Pearson correlation (ρX,Y = cov(X,Y ) / σXσY) and mean squared error (MSEX,Y = 1/|X|∑i=1(f(xi) −yi)2).",High correlation results range from 0.472 to 0.936
SUM-QE: a BERT-based Summary Quality Estimation Model,What dataset do they use?,They use a given training dataset.,"datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks"
SUM-QE: a BERT-based Summary Quality Estimation Model,What simpler models do they look at?,They look at post-hoc text generation.,"BiGRU s with attention | ROUGE | Language model (LM) | Next sentence prediction | BiGRUs with attention, ROUGE, Language model, and next sentence prediction "
SUM-QE: a BERT-based Summary Quality Estimation Model,What linguistic quality aspects are addressed?,"Annotation difficulty categories of missing values, language-specific attributes, and multiword expressions.","Grammaticality, non-redundancy, referential clarity, focus, structure & coherence"
Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction,What benchmark datasets are used for the link prediction task?,WN18 and YAGO3-10.,WN18RR | FB15k-237 | YAGO3-10 | WN18RR BIBREF26 | FB15k-237 BIBREF18 | YAGO3-10 BIBREF27
Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction,What are state-of-the art models for this task?,Post-hoc text generation.,TransE | DistMult | ComplEx | ConvE | RotatE
Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction,How better does HAKE model peform than state-of-the-art methods?,HAKE significantly outperforms state-of-the-art methods.,"0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively | doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 | HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively"
Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction,How are entities mapped onto polar coordinate system?,"Entities are mapped onto a polar coordinate system where the radial coordinate represents entities at different levels of the hierarchy, and the angular coordinate distinguishes entities at the same level of the hierarchy.","radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively"
Machine Translation from Natural Language to Code using Long-Short Term Memory,What additional techniques are incorporated?,Phrase-based word embedding and Abstract Syntax Tree (AST),incorporating coding syntax tree model
Machine Translation from Natural Language to Code using Long-Short Term Memory,What dataset do they use?,They use a given training dataset.,A parallel corpus where the source is an English expression of code and the target is Python code. |  text-code parallel corpus
Machine Translation from Natural Language to Code using Long-Short Term Memory,What is the architecture of the system?,"The text does not explicitly describe the architecture of the system. However, it mentions that an additional ""generator"" component can be used to output a textual explanation of the model's decision, and this generator is learned with supervision of textual explanations.",seq2seq translation
Machine Translation from Natural Language to Code using Long-Short Term Memory,What additional techniques could be incorporated to further improve accuracy?,More appropriate hyperparameters and more complex neural classifiers.,phrase-based word embedding | Abstract Syntax Tree(AST)
Machine Translation from Natural Language to Code using Long-Short Term Memory,What programming language is target language?,Python,Python
Machine Translation from Natural Language to Code using Long-Short Term Memory,What dataset is used to measure accuracy?,Twitter dataset,validation data
A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,Is text-to-image synthesis trained is suppervized or unsuppervized manner?,Supervised.,"unsupervised  | Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis"
A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,What challenges remain unresolved?,Nuclear weapons.,give more independence to the several learning methods (e.g. less human intervention) involved in the studies | increasing the size of the output images
A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,What is the conclusion of comparison of proposed solution?,"The authors conclude that their proposed practical view of faithfulness, which involves a graded criteria to measure the extent and likelihood of an interpretation to be faithful, is more productive than treating faithfulness as a binary property. They argue that this approach can lead to more meaningful evaluations and discussions about the desiderata of a learned system's interpretation.","HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset | In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor | text to image synthesis is continuously improving the results for better visual perception and interception"
A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,What is typical GAN architecture for each text-to-image synhesis group?,StackGAN,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN"
Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,Where do they employ feature-wise sigmoid gating?,Vector gate (vg),gating mechanism acts upon each dimension of the word and character-level vectors
Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,Which model architecture do they use to obtain representations?,They don't mention it.,BiLSTM with max pooling
Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,Which downstream sentence-level tasks do they evaluate on?,They don't mention any specific downstream sentence-level tasks.,"BIBREF13 , BIBREF18"
Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,Which similarity datasets do they use?,"WS353R, WS353S, SimLex999, SimVerb3500, MEN, MTurk287, MTurk771, RG, and WS353.",MEN | MTurk287 | MTurk771 | RG | RW | SimLex999 | SimVerb3500 | WS353 | WS353R | WS353S | WS353S | SimLex999 | SimVerb3500
Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,Which one of two proposed approaches performed better in experiments?,Neither. The text does not mention any experiment results comparing the performance of the two approaches.,WordDecoding (WDec) model
Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,What is previous work authors reffer to?,Lipton (2018); Guidotti et al. (2018),SPTree | Tagging | CopyR | HRL | GraphR | N-gram Attention
Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,How higher are F1 scores compared to previous work?,Not mentioned in this snippet.,"WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively | PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively | Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively | In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores"
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,How much better is performance of proposed method than state-of-the-art methods in experiments?,9.4 BLEU (7.2 →16.6),"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively."
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,What further analysis is done?,"Further analysis is done to provide a more practical view of faithfulness, calling for a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.",we use t-SNE tool BIBREF27 to visualize the learned embedding
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,What seven state-of-the-art methods are used for comparison?,None.,"TransE, TransR and TransH | PTransE, and ALL-PATHS | R-GCN BIBREF24 and KR-EAR BIBREF26"
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,What three datasets are used to measure performance?,"ROUGE, WPSLOR, ROUGE-LM","FB24K | DBP24K | Game30K | Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph"
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,"How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?","KANE captures both high-order structural and attribute information of KGs through its overall architecture, which includes an attention-based embedding propagation method that recursively propagates the embeddings of entities from their neighbors and aggregates them with different weights. Additionally, it uses two encoders (Bag-of-Words Encoder and another unspecified encoder) to model the attribute value in a fixed-length vector.","To capture both high-order structural information of KGs, we used an attention-based embedding propagation method."
Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding,What are recent works on knowedge graph embeddings authors mention?,"Lin et al. 2015; Guo, Sun, and Hu 2019; Nguyen et al. 2019","entity types or concepts BIBREF13 | relations paths BIBREF17 |  textual descriptions BIBREF11, BIBREF12 | logical rules BIBREF23 | deep neural network models BIBREF24"
A Computational Approach to Automatic Prediction of Drunk Texting,What baseline model is used?,LSTM (Long Short-Term Memory),Human evaluators
A Computational Approach to Automatic Prediction of Drunk Texting,What stylistic features are used to detect drunk texts?,Topic categorization and emotion classification.,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio | LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio."
Answering Complex Questions Using Open Information Extraction,What corpus was the source of the OpenIE extractions?,English Gigaword fifth edition,domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining
Answering Complex Questions Using Open Information Extraction,What is the accuracy of the proposed technique?,There is no mention of the accuracy of any proposed technique in the given text.,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge
Answering Complex Questions Using Open Information Extraction,What method was used to generate the OpenIE extractions?,LSTM (Long Short-Term Memory),"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S | take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$"
Answering Complex Questions Using Open Information Extraction,What was the textual source to which OpenIE was applied?,English Gigaword fifth edition,domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining
Answering Complex Questions Using Open Information Extraction,What OpenIE method was used to generate the extractions?,None.,"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S | take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$"
ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples,What datasets were used?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,"datasets given on the shared task, without using any additional external data"
ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples,How did they do compared to other teams?,France beat Croatia 4-2 in the final.,second on Subtask A with an F1 score of 77.78% among 33 other team submissions | performs well on Subtask B with an F1 score of 79.59%
DENS: A Dataset for Multi-class Emotion Analysis,Which tested technique was the worst performer?,Skip-bigram statistics.,Depeche + SVM
DENS: A Dataset for Multi-class Emotion Analysis,How many emotions do they look at?,8 primary emotions,9
DENS: A Dataset for Multi-class Emotion Analysis,What are the baseline benchmarks?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,TF-IDF + SVM | Depeche + SVM | NRC + SVM | TF-NRC + SVM | Doc2Vec + SVM |  Hierarchical RNN | BiRNN + Self-Attention | ELMo + BiRNN |  Fine-tuned BERT
DENS: A Dataset for Multi-class Emotion Analysis,What is the size of this dataset?,206 samples (100 train + 106 test),"9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words"
DENS: A Dataset for Multi-class Emotion Analysis,How many annotators were there?,"There is no specific number of annotators mentioned in the given context. However, it mentions ""several annotators"" and also refers to ""in-house data annotators"" (plural) and ""MTurk annotators"", indicating that there were multiple annotators involved in the process.",3 
Transfer Learning Between Related Tasks Using Expected Label Proportions,How accurate is the aspect based sentiment classifier trained only using the XR loss?,Not explicitly stated.,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
"
Transfer Learning Between Related Tasks Using Expected Label Proportions,How is the expectation regularization loss defined?,It is not explicitly defined in the given snippet.,DISPLAYFORM0
The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection,What were the non-neural baselines used for the task?,None.,The Lemming model in BIBREF17
Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU,Which publicly available NLU dataset is used?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,ROMULUS dataset | NLU-Benchmark dataset
Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU,What metrics other than entity tagging are compared?,String similarity (Jaro-Winkler distance) and semantic similarity (cosine distance).,We also report the metrics in BIBREF7 for consistency | we report the span F1 |  Exact Match (EM) accuracy of the entire sequence of labels | metric that combines intent and entities
Exploring Hate Speech Detection in Multimodal Publications,What models do they propose?,"They propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.",Feature Concatenation Model (FCM) | Spatial Concatenation Model (SCM) | Textual Kernels Model (TKM)
Exploring Hate Speech Detection in Multimodal Publications,How large is the dataset?,206 samples.," $150,000$ tweets"
Exploring Hate Speech Detection in Multimodal Publications,What is the results of multimodal compared to unimodal models?,Multimodality helps human and machine for translation and image captioning.,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 "
Exploring Hate Speech Detection in Multimodal Publications,What is author's opinion on why current multimodal models cannot outperform models analyzing only text?,"The author does not explicitly state their opinion on this matter. However, they mention that ""the efficient integration of multimodal information still remains a challenging task"" in the field of Machine Translation (MT), implying that current multimodal models may struggle to effectively combine diverse modality vector representations.",Noisy data | Complexity and diversity of multimodal relations | Small set of multimodal examples
Exploring Hate Speech Detection in Multimodal Publications,What metrics are used to benchmark the results?,ROUGE-LM,F-score | Area Under the ROC Curve (AUC) | mean accuracy (ACC) | Precision vs Recall plot | ROC curve (which plots the True Positive Rate vs the False Positive Rate)
Exploring Hate Speech Detection in Multimodal Publications,"How is data collected, manual collection or Twitter api?",Manual collection.,Twitter API
Exploring Hate Speech Detection in Multimodal Publications,"How many tweats does MMHS150k contains, 150000?",Yes.,"$150,000$ tweets"
Exploring Hate Speech Detection in Multimodal Publications,What unimodal detection models were used?,Textual models., single layer LSTM with a 150-dimensional hidden state for hate / not hate classification
Exploring Hate Speech Detection in Multimodal Publications,What different models for multimodal detection were proposed?,"None. The text only mentions that state of the art multimodal models were evaluated, but it doesn't specify which ones.",Feature Concatenation Model (FCM) | Spatial Concatenation Model (SCM) | Textual Kernels Model (TKM)
Exploring Hate Speech Detection in Multimodal Publications,What annotations are available in the dataset - tweat used hate speach or not?,Binary labels (hate / not hate),No attacks to any community |  racist | sexist | homophobic | religion based attacks | attacks to other communities
Self-Taught Convolutional Neural Networks for Short Text Clustering,What were the evaluation metrics used?,"SLOR, WP-SLOR, ROUGE-LM, and traditional word-overlap metrics.",accuracy | normalized mutual information
Self-Taught Convolutional Neural Networks for Short Text Clustering,What were their performance results?,Their proposed referenceless metrics correlated significantly better with fluency ratings for the outputs of compression systems than traditional word-overlap metrics.,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%"
Self-Taught Convolutional Neural Networks for Short Text Clustering,By how much did they outperform the other methods?,They correlated significantly better.,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI"
Self-Taught Convolutional Neural Networks for Short Text Clustering,Which popular clustering methods did they experiment with?,Several popular clustering methods.,"K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods"
Self-Taught Convolutional Neural Networks for Short Text Clustering,What datasets did they use?,They used Project Gutenberg titles.,SearchSnippets | StackOverflow | Biomedical
Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations,What neural configurations are explored?,LSTMs (Long Short-Term Memory),"tried many configurations of our network models, but report results with only three configurations | Transformer Type 1 | Transformer Type 2 | Transformer Type 3"
Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations,How is this problem evaluated?,This problem is evaluated by observing whether the generated expressions have seemingly random numbers that are not present in the problem sentences.,BLEU-2 | average accuracies over 3 test trials on different randomly sampled test sets
Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations,What datasets do they use?,"intent classiﬁcation data (complete data and two TTS-STT variants) and Twitter sentiment classiﬁcation data (original text, corrected text and incorrect with correct texts).",AI2 BIBREF2 | CC BIBREF19 | IL BIBREF4 | MAWPS BIBREF20
CamemBERT: a Tasty French Language Model,What is CamemBERT trained on?,CamemBERT is trained on the Masked Language Modeling (MLM) task.,unshuffled version of the French OSCAR corpus
CamemBERT: a Tasty French Language Model,Which tasks does CamemBERT not improve on?,None are mentioned in the provided snippet.,"its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa"
CamemBERT: a Tasty French Language Model,What is the state of the art?,"There is considerable research effort in attempting to deﬁne and categorize the desiderata of a learned system’s interpretation, most of which revolves around speciﬁc use-cases.","POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)"
CamemBERT: a Tasty French Language Model,How much better was results of CamemBERT than previous results on these tasks?,No comparison is made in the snippet.,"2.36 point increase in the F1 score with respect to the best SEM architecture | on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) | lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa | For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT | For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT"
CamemBERT: a Tasty French Language Model,What data is used for training CamemBERT?,138GB of raw French text data,unshuffled version of the French OSCAR corpus
Semantic Sentiment Analysis of Twitter Data,What is the current SOTA for sentiment analysis on Twitter at the time of writing?,Kouloumpis et al. (2011),"deep convolutional networks BIBREF53 , BIBREF54"
Semantic Sentiment Analysis of Twitter Data,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?","Sentiment analysis on Twitter has difficulties due to the shortness of social media messages, which makes word and phrase polarity lexicons less useful.","Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text"
Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization,How better are state-of-the-art results than this model? ,5.4 BLEU points,we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features |  RegSum achieves a similar ROUGE-2 score
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,What were their results on the three datasets?,Their results are presented in Table 3.,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR"
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,What was the baseline?,BLEU,"We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. | we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. "
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,Which datasets did they use?,They used Project Gutenberg titles.,Stanford - Twitter Sentiment Corpus (STS Corpus) | Sanders - Twitter Sentiment Corpus | Health Care Reform (HCR)
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,Which three Twitter sentiment classification datasets are used for experiments?,"Inc, BERT, Sentiment140 Corpus",Stanford - Twitter Sentiment Corpus (STS Corpus) | Sanders - Twitter Sentiment Corpus | Health Care Reform (HCR)
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,What semantic rules are proposed?,R11 to R15,rules that compute polarity of words after POS tagging or parsing steps
Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding,Which knowledge graph completion tasks do they experiment with?,Link prediction in knowledge graphs.,link prediction  | triplet classification
Learning with Noisy Labels for Sentence-level Sentiment Classification,What is the dataset used to train the model?,A given training dataset," movie sentence polarity dataset from BIBREF19 | laptop and restaurant datasets collected from SemEval-201 | we collected 2,000 reviews for each domain from the same review source"
Learning with Noisy Labels for Sentence-level Sentiment Classification,What is the performance of the model?,The model correlates better with human judgments than traditional word-overlap metrics.,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)"
Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange,What are the baseline models mentioned in the paper?,"FreqSum (Nenkova et al., 2006); TsSum (Conroy et al., 2006); traditional graph-based approaches such as Cont. Lex-Rank (Erkan and Radev, 2004); Centroid (Radev et al., 2004); CLASSY04 (Conroy et al., 2004); its improved version CLASSY11 (Conroy et al., 2011) and the greedy model GreedyKL (Haghighi and Vanderwende, 2009).",Noun WordNet Semantic Text Exchange Model (NWN-STEM) | General WordNet Semantic Text Exchange Model (GWN-STEM) | Word2Vec Semantic Text Exchange Model (W2V-STEM)
CN-CELEB: a challenging Chinese speaker recognition dataset,What was the performance of both approaches on their dataset?,Both SLOR and WP-SLOR correlated significantly better with fluency ratings for the outputs of compression systems than traditional word-overlap metrics.,ERR of 19.05 with i-vectors and 15.52 with x-vectors
CN-CELEB: a challenging Chinese speaker recognition dataset,What kind of settings do the utterances come from?,Japanese,"entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"
CN-CELEB: a challenging Chinese speaker recognition dataset,What genres are covered?,Movie reviews.,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"
CN-CELEB: a challenging Chinese speaker recognition dataset,Which of the two speech recognition models works better overall on CN-Celeb?,x-vector,x-vector
CN-CELEB: a challenging Chinese speaker recognition dataset,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,"4.9% (14.24 - 11.34 = 2.9, and 19.05 - 15.52 = 3.53; taking the average of these two differences gives approximately 4.9%)","For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb"
Open Named Entity Modeling from Embedding Distribution,What is their model?,NLP system,cross-lingual NE recognition
Efficient Twitter Sentiment Classification using Subjective Distant Supervision,What previously proposed methods is this method compared against?,post-hoc text generation,Naive Bayes | SVM | Maximum Entropy classifiers
Efficient Twitter Sentiment Classification using Subjective Distant Supervision,How is effective word score calculated?,"Effective Word Score (EFWS) of score x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x.","We define the Effective Word Score of score x as

EFWS(x) = N(+x) - N(-x),

where N(x) is the number of words in the tweet with polarity score x."
Low-Level Linguistic Controls for Style Transfer and Content Preservation,"How they perform manual evaluation, what is criteria?","Manual evaluation is performed by human evaluators who assess the quality of machine translation output based on various criteria such as fluency, accuracy, and adequacy. The specific criteria used for evaluation are not explicitly stated in the provided text snippet.",accuracy
Low-Level Linguistic Controls for Style Transfer and Content Preservation,What metrics are used for automatic evaluation?,"BLEU, ROUGE, METEOR",classification accuracy | BLEU scores | model perplexities of the reconstruction
Low-Level Linguistic Controls for Style Transfer and Content Preservation,How they know what are content words?,"They don't. The annotators mentioned that content is a heavy predictor of genre, implying that certain content words can give away the style or genre of a sentence. However, it's not explicitly stated how they identify these content words."," words found in the control word lists are then removed | The remaining words, which represent the content"
Low-Level Linguistic Controls for Style Transfer and Content Preservation,"How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?",They train a neural encoder-decoder model to reconstruct reference sentences given only content words and the setting of the controls.,"style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style"
Incorporating Sememes into Chinese Definition Modeling,What is a sememe?,A sememe is a unit of meaning in a language.,"Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes"
RobBERT: a Dutch RoBERTa-based Language Model,What data did they use?,They used Twitter Sentiment Classification dataset.,the Dutch section of the OSCAR corpus
RobBERT: a Dutch RoBERTa-based Language Model,What is the state of the art?,"There is considerable research effort in attempting to deﬁne and categorize the desiderata of a learned system’s interpretation, most of which revolves around speciﬁc use-cases.",BERTje BIBREF8 | an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19. | mBERT
RobBERT: a Dutch RoBERTa-based Language Model,What language tasks did they experiment on?,Morphological inﬂection.,"sentiment analysis | the disambiguation of demonstrative pronouns,"
Query-oriented text summarization based on hypergraph transversals,How does the model compare with the MMR baseline?,The model outperforms the MMR baseline.," Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\%$ ) and MRMR ( $7\%$ )"
Text-based inference of moral sentiment change,Which datasets are used in the paper?,Project Gutenberg titles.,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
"
Text-based inference of moral sentiment change,How does the parameter-free model work?,It is not mentioned in the given context.,"A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule; | A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"
Text-based inference of moral sentiment change,How do they quantify moral relevance?,They use the slope of moral relevance change (ρ(w)) as a quantification of moral relevance.,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence
Text-based inference of moral sentiment change,Which fine-grained moral dimension examples do they showcase?,None.,"Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation"
Bringing Stories Alive: Generating Interactive Fiction Worlds,How well did the system do?,It is possible to satisfy one of these properties (plausibility and faithfulness) without the other.,the neural approach is generally preferred by a greater percentage of participants than the rules or random | human-made game outperforms them all
Bringing Stories Alive: Generating Interactive Fiction Worlds,How is the information extracted?,Open IE v4 is used to extract information from sentences.,"neural question-answering technique to extract relations from a story text | OpenIE5, a commonly used rule-based information extraction technique"
Generating Classical Chinese Poems from Vernacular Chinese,What are some guidelines in writing input vernacular so model can generate ,guidelines on how to write the input vernacular to generate better poems.," if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score | poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs"
Generating Classical Chinese Poems from Vernacular Chinese,How much is proposed model better in perplexity and BLEU score than typical UMT models?,"According to Table 3, the proposed model (+Anti OT&UT) has a lower perplexity (65.58) compared to the Transformer model (105.79), indicating an improvement of around 38%. Additionally, the BLEU score for the proposed model is higher than the Transformer model, with improvements ranging from 1-2% across different BLEU metrics.","Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50."
Generating Classical Chinese Poems from Vernacular Chinese,What dataset is used for training?,A given training dataset,We collected a corpus of poems and a corpus of vernacular literature from online resources
Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,What were the evaluation metrics?,"SLOR, WP-SLOR, ROUGE-LM","BLEU | Micro Entity F1 | quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5"
Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,What were the baseline systems?,There is no mention of baseline systems in the provided context.,Attn seq2seq | Ptr-UNK | KV Net | Mem2Seq | DSR
Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,Which dialog datasets did they experiment with?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus.,Camrest | InCar Assistant
Can neural networks understand monotonicity reasoning?,What NLI models do they analyze?,Recurrent and attention-based neural models.,BiMPM | ESIM | Decomposable Attention Model | KIM | BERT
Can neural networks understand monotonicity reasoning?,How do they define upward and downward reasoning?,"Upward inference is defined as inferences from a sentence to a longer sentence, while downward inference is defined as inferences from a sentence to a shorter sentence.","Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific."
Can neural networks understand monotonicity reasoning?,What is monotonicity reasoning?,"Monotonicity reasoning refers to the ability of neural networks to understand and infer the polarity (i.e., upward or downward) of phrases in a sentence, based on the interaction of monotonicity properties and syntactic structures.","a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures"
Synchronising audio and ultrasound by learning cross-modal embeddings,Do they annotate their own dataset or use an existing one?,They use an existing one.,Use an existing one
Synchronising audio and ultrasound by learning cross-modal embeddings,What kind of neural network architecture do they use?,Dynamic Convolutional Neural Network (DCNN),CNN
wav2vec: Unsupervised Pre-training for Speech Recognition,Which unlabeled data do they pretrain with?,They don't mention unlabeled data.,1000 hours of WSJ audio data
wav2vec: Unsupervised Pre-training for Speech Recognition,How many convolutional layers does their model have?,2,wav2vec has 12 convolutional layers
"Cross-lingual, Character-Level Neural Morphological Tagging",How are character representations from various languages joint?,They are concatenated.,shared character embeddings for taggers in both languages together through optimization of a joint loss function
"Cross-lingual, Character-Level Neural Morphological Tagging",On which dataset is the experiment conducted?,SearchSnippets,We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\text{th}$ and $6^\text{th}$ columns of the file format) BIBREF13 . 
Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,How big are the datasets?,"SearchSnippets (12,340), StackOverﬂow (20,000), Biomedical (20,000)","In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents"
Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,What languages do they experiment on?,Python,"English, German, Spanish, Italian, Japanese and Portuguese |  English, Arabic and Chinese"
Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,What datasets are used?,Project Gutenberg titles.,in-house dataset | ACE05 dataset 
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,By how much did their model outperform the baseline?,"Above the base rate from the test data, which is 75%.","increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively | over INLINEFORM0 increase in EM and GM between our model and the next best two models"
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,What baselines did they compare their model with?,They compared their StyleEQ model with a Baseline model.,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,What was the performance of their model?,Their models correlated better with human judgments than traditional word-overlap metrics.,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81"
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,What evaluation metrics are used?,"SLOR, WP-SLOR, ROUGE-LM.","exact match, f1 score, edit distance and goal match"
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,How were the navigation instructions collected?,Not specified.,using Amazon Mechanical Turk using simulated environments with topological maps
Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,What language is the experiment done in?,Python,english language
Analysis of Risk Factor Domains in Psychosis Patient Health Records,What additional features are proposed for future work?,None.,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort
Analysis of Risk Factor Domains in Psychosis Patient Health Records,What are their initial results on this task?,"Their initial results show that although the generated code is incoherent and often predicts wrong code tokens, the translator is successfully generating whole codelines automatically but missing the noun part (parameter and function name) of the syntax.","Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models."
Analysis of Risk Factor Domains in Psychosis Patient Health Records,What datasets did the authors use?,Project Gutenberg titles.," a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital | an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)"
Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks,How many layers do they use in their best performing network?,3,36
Dense Information Flow for Neural Machine Translation,what are the baselines?,HELP," 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256"
Dense Information Flow for Neural Machine Translation,what language pairs are explored?,Japanese,German-English | Turkish-English | English-German
Dense Information Flow for Neural Machine Translation,what datasets were used?,Project Gutenberg titles.,"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German"
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,How is order of binomials tracked across time?,"Academic treatment of binomial orderings dates back more than a century to Jespersen [11], who proposed in 1905 that the ordering of many common English binomials could be predicted by the rhythm of the words.","draw our data from news publications, wine reviews, and Reddit | develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time |  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time"
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,What types of various community texts have been investigated for exploring global structure of binomials?,"Books, news articles, web search results, Google books.","news publications, wine reviews, and Reddit"
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,Are there any new finding in analasys of trinomials that was not present binomials?,"Yes, one new finding is that for most trinomials, the last word tends to keep the same position.",Trinomials are likely to appear in exactly one order
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,What new model is proposed for binomial lists?,"None. The paper discusses existing theories of binomials, but it does not propose a new model.",null model 
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,How was performance of previously proposed rules at very large scale?,They have lacked a large-scale text corpus.," close to random,"
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,What previously proposed rules for predicting binoial ordering are used?,Most previously proposed rules.,"word length, number of phonemes, number of syllables, alphabetical order, and frequency"
Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text,What online text resources are used to test binomial lists?,None mentioned.,"news publications, wine reviews, and Reddit"
Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,Does this approach perform better in the multi-domain or single-domain setting?,No information is provided to determine whether this approach performs better in the multi-domain or single-domain setting.,single-domain setting
Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,What are the performance metrics used?,"SLOR, WP-SLOR, ROUGE-LM",joint goal accuracy
Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,Which datasets are used to evaluate performance?,benchmark dataset,"the single domain dataset, WoZ2.0  | the multi-domain dataset, MultiWoZ"
Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,Why does not the approach from English work on other languages?,Not mentioned in this snippet.,"Because, unlike other languages, English does not mark grammatical genders"
Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,How do they measure grammaticality?,They use longest common subsequence and skip-bigram statistics.,by calculating log ratio of grammatical phrase over ungrammatical phrase
Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,Which model do they use to convert between masculine-inflected and feminine-inflected sentences?,"An unsupervised model using dependency trees, lemmata, part-of-speech (POS) tags, and morpho-syntactic tags from Universal Dependencies corpora.",Markov random field with an optional neural parameterization
Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,What is the size of the datasets employed?,8; 20; 20,"(about 4 million sentences, 138 million word tokens) | one trained on the Billion Word benchmark"
Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,What are the baseline models?,"The Baseline model doesn't create outputs that move far from the reference sentence, making only minor modifications such as changing the type of a single pronoun.",Recurrent Neural Network (RNN) | ActionLSTM | Generative Recurrent Neural Network Grammars (RNNG)
Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation,What evaluation metrics are used?,"SLOR, WP-SLOR, ROUGE-LM, and traditional word-overlap metrics.","the evaluation metrics include BLEU and ROUGE (1, 2, L) scores"
Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation,What datasets did they use?,They used Project Gutenberg titles.,The E2E NLG challenge dataset BIBREF21
Gender Bias in Coreference Resolution,Which coreference resolution systems are tested?,None.,"the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis,How big is the improvement over the state-of-the-art results?,9.4 BLEU (7.2 →16.6),"AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset | Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets | In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain"
HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,Is the baseline a non-heirarchical model like BERT?,No.,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines
The Role of Pragmatic and Discourse Context in Determining Argument Impact,How better are results compared to baseline models?,"StyleEQ model has a classification accuracy above the base rate from the test data, which is 75%.",F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.
The Role of Pragmatic and Discourse Context in Determining Argument Impact,What models that rely only on claim-specific linguistic features are used as baselines?,None.,SVM with RBF kernel
The Role of Pragmatic and Discourse Context in Determining Argument Impact,How is pargmative and discourse context added to the dataset?,It is not explicitly stated in the provided context how pragmatic and discourse context are added to the dataset.,"While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument."
The Role of Pragmatic and Discourse Context in Determining Argument Impact,What annotations are available in the dataset?,UD-schema annotations and UniMorph annotations.,"5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact"
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What was their result on Stance Sentiment Emotion Corpus?,They achieved an F1 score of 0.83 on the Stance Sentiment Emotion Corpus.,F1 score of 66.66%
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What performance did they obtain on the SemEval dataset?,They obtained 83.5% accuracy on the SemEval dataset.,F1 score of 82.10%
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What are the state-of-the-art systems?,"post-hoc text generation—where an additional “generator” component outputs a textual explanation of the model’s decision, and the generator is learned with supervision of textual explanations (Zaidan and Eisner, 2008; Rajani et al., 2019; Strout et al., 2019).","For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN"
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,How is multi-tasking performed?,"Multi-task learning is performed across DAs, FRs, and ARs using a seq2seq approach, where each task is modelled as a separate problem with task-specific labels assigned to each token of the sentence according to the IOB2 notation.","The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. | Each of the shared representations is then fed to the primary attention mechanism"
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What are the datasets used for training?,clean or incorrect data,SemEval 2016 Task 6 BIBREF7 | Stance Sentiment Emotion Corpus (SSEC) BIBREF15
Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis,What is the previous state-of-the-art model?,post-hoc text generation,"BIBREF7 | BIBREF39 | BIBREF37 | LitisMind | Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN"
Mapping (Dis-)Information Flow about the MH17 Plane Crash,How can the classifier facilitate the annotation task for human annotators?,Not mentioned in the given text.,"quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets"
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What recommendations are made to improve the performance in future?,Aggressive (word) dropout and tuning other hyperparameters.,applying reasoning BIBREF36 or irony detection methods BIBREF37
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What type of errors do the classifiers use?,"Category I, Category II, and Category III errors.","correct class can be directly inferred from the text content easily, even without background knowledge | correct class can be inferred from the text content, given that event-specific knowledge is provided | orrect class can be inferred from the text content if the text is interpreted correctly"
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What neural classifiers are used?,Bidirectional transformers and multilayer perceptrons., convolutional neural network (CNN) BIBREF29
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What languages are included in the dataset?,Wiktionary languages,English
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What dataset is used for this study?,"SearchSnippets, StackOverﬂow, Biomedical",MH17 Twitter dataset
Mapping (Dis-)Information Flow about the MH17 Plane Crash,What proxies for data annotation were used in previous datasets?,gtts-witai and macsay-witai,"widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet | Natural Language Processing (NLP) models can be used to automatically label text content"
Predictive Embeddings for Hate Speech Detection on Twitter,Which publicly available datasets are used?,Project Gutenberg,BIBREF3 | BIBREF4 | BIBREF9
Predictive Embeddings for Hate Speech Detection on Twitter,What embedding algorithm and dimension size are used?,"Token embeddings, segmentation embeddings, and position embeddings. The dimension sizes are not specified in the given text snippet.",300 Dimensional Glove
Predictive Embeddings for Hate Speech Detection on Twitter,What data are the embeddings trained on?,Both incomplete and complete data during training.,Common Crawl 
Predictive Embeddings for Hate Speech Detection on Twitter,how much was the parameter difference between their model and previous methods?,0.5,"our model requires 100k parameters , while BIBREF8 requires 250k parameters"
Predictive Embeddings for Hate Speech Detection on Twitter,how many parameters did their model use?,2,"Excluding the embedding weights, our model requires 100k parameters"
Predictive Embeddings for Hate Speech Detection on Twitter,which datasets were used?,Project Gutenberg titles.,Sexist/Racist (SR) data set | HATE dataset | HAR
Predictive Embeddings for Hate Speech Detection on Twitter,what was their system's f1 performance?,0.8769,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively."
Predictive Embeddings for Hate Speech Detection on Twitter,what was the baseline?,There is no mention of a baseline in the provided context.,logistic regression
An Analysis of Visual Question Answering Algorithms,From when are many VQA datasets collected?,2014,late 2014
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What non-contextual properties do they refer to?,Plausibility and faithfulness.,These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What is the baseline?,"There is no mention of a ""baseline"" in the provided context.",random method  | LSTM 
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What are their proposed features?,They propose two features: linguistic characteristic of a claim and its context.,Non-contextual properties of a word | Word usage in an OP or PC (two groups) | How a word connects an OP and PC. | General OP/PC properties
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What are overall baseline results on new this new task?,No specific numbers are provided in the given snippet.,all of our models outperform the random baseline by a wide margin | he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What metrics are used in evaluation of this task?,"SLOR, WP-SLOR, ROUGE-LM, and word-overlap metrics.",F1 score
"What Gets Echoed? Understanding the""Pointers""in Explanations of Persuasive Arguments",What features are proposed?,Linguistic characteristic of a claim and its context length (Cl),Non-contextual properties of a word | Word usage in an OP or PC (two groups) | How a word connects an OP and PC | General OP/PC properties
Automatic Reminiscence Therapy for Dementia.,How is performance of this system measured?,"SLOR, a LM score which accounts for the effects of sentence length and individual unigram probabilities.",using the BLEU score as a quantitative metric and human evaluation for quality
Automatic Reminiscence Therapy for Dementia.,How many questions per image on average are available in dataset?,244,5 questions per image
Automatic Reminiscence Therapy for Dementia.,How big dataset is used for training this system?,3-gram,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues."
Lattice CNNs for Matching Based Chinese Question Answering,How do they obtain word lattices from words?,"They construct a directed graph G = ⟨V,E⟩ where V represents a node set of all possible substrings that can be considered as words, and E represents an edge set connecting neighbor words according to their positions in the original sentence.",By considering words as vertices and generating directed edges between neighboring words within a sentence
Lattice CNNs for Matching Based Chinese Question Answering,Which metrics do they use to evaluate matching?,"BLEU, ROUGE and METEOR.",Precision@1 | Mean Average Precision | Mean Reciprocal Rank
Lattice CNNs for Matching Based Chinese Question Answering,Which dataset(s) do they evaluate on?,They evaluate on a dataset that is not specified.,DBQA | KBRE
Speaker-independent classification of phonetic segments from raw ultrasound in child speech,What are the characteristics of the dataset?,"The dataset is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections, with two intents (Departure Time and Find Connection), containing 100 train samples and 106 test samples. It also includes a few German station and street names, despite English being the main language.","synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male) | data was aligned at the phone-level | 121fps with a 135 field of view | single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)"
Speaker-independent classification of phonetic segments from raw ultrasound in child speech,What type of models are used for classification?,"MLP with 3 hidden layers of sizes [300,100,50] respectively; Random Forest with 50 estimators or trees.",feedforward neural networks (DNNs) | convolutional neural networks (CNNs)
Speaker-independent classification of phonetic segments from raw ultrasound in child speech,How many instances does their dataset have?,16,10700
Speaker-independent classification of phonetic segments from raw ultrasound in child speech,What model do they use to classify phonetic segments? ,DeBERT,feedforward neural networks | convolutional neural networks
Speaker-independent classification of phonetic segments from raw ultrasound in child speech,How many speakers do they have in the dataset?,2,58
Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data,What is the baseline model for the agreement-based mode?,RNNG,"PCFGLA-based parser, viz. Berkeley parser BIBREF5 | minimal span-based neural parser BIBREF6"
Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data,Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?,No.,syntax-based system may generate correct syntactic analyses for partial grammatical fragments
Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data,Who manually annotated the semantic roles for the set of learner texts?,Not specified in the given information.,Authors
Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining,By how much do they outperform existing state-of-the-art VQA models?,"They don't. Simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.",the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X
Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining,How do they measure the correlation between manual groundings and model generated ones?,They use the same criteria as region-level groundings to score each match.,rank-correlation BIBREF25
Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining,How do they obtain region descriptions and object annotations?,"They mine visual groundings from Visual Genome (VG) using heuristics. For region descriptions, they enumerate all region descriptions of an image I and pick the description Di that has the most overlapped informative words with question Q and answer A. For object annotations, they select the bounding box of an object as a valid grounding label if the object name matches one of the informative nouns in Q or A.",they are available in the Visual Genome dataset
Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,Which training dataset allowed for the best generalization to benchmark sets?,None of the six models trained on three different NLI datasets were able to generalize well across test sets taken from different NLI benchmarks.,MultiNLI
Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,Which model generalized the best?,SLOR,BERT
Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,Which models were compared?,BERT model without context and with flat context representation.,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT"
Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,Which datasets were used?,Project Gutenberg titles.,"SNLI, MultiNLI and SICK"
Joint Learning of Sentence Embeddings for Relevance and Entailment,what were the baselines?,the linguistic characteristic of a claim,RNN model | CNN model  | RNN-CNN model | attn1511 model | Deep Averaging Network model | avg mean of word embeddings in the sentence with projection matrix
Joint Learning of Sentence Embeddings for Relevance and Entailment,what is the state of the art for ranking mc test answers?,Attention-based deep neural architectures.,ensemble of hand-crafted syntactic and frame-semantic features BIBREF16
Joint Learning of Sentence Embeddings for Relevance and Entailment,what datasets did they use?,They used Project Gutenberg titles and modern narratives.,Argus Dataset | AI2-8grade/CK12 Dataset | MCTest Dataset
Aspect Term Extraction with History Attention and Selective Transformation,How do they determine the opinion summary?,Opinion summary is determined as the linear combination of the opinion representations generated from LSTM.,"the weighted sum of the new opinion representations, according to their associations with the current aspect representation"
Aspect Term Extraction with History Attention and Selective Transformation,Which dataset(s) do they use to train the model?,They simply take a given training dataset.,"INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain."
Aspect Term Extraction with History Attention and Selective Transformation,By how much do they outperform state-of-the-art methods?,There is no mention of performance comparison with state-of-the-art methods in the given text.,"Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively."
Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset,What is the average number of turns per dialog?,23,The average number of utterances per dialog is about 23 
Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset,What baseline models are offered?,Baseline model,3-gram and 4-gram conditional language model | Convolution | LSTM models BIBREF27 with and without attention BIBREF28 | Transformer | GPT-2
Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset,Which six domains are covered in the dataset?,"News, Government Reports, Magazine Articles, Books, Conversational Speeches, and Miscellaneous","ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations"
Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,How are the auxiliary signals from the morphology table incorporated in the decoder?,Through an extra output channel.,an additional morphology table including target-side affixes. | We inject the decoder with morphological properties of the target language.
Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,"What type of morphological information is contained in the ""morphology table""?","Morphemes that are clearly separable from each other, specifically for agglutinative languages.",target-side affixes
Learning Twitter User Sentiments on Climate Change with Limited Labeled Data,Which machine learning models are used?,LSTM (Long Short Term Memory),RNNs | CNNs | Naive Bayes with Laplace Smoothing | k-clustering | SVM with linear kernel
Learning Twitter User Sentiments on Climate Change with Limited Labeled Data,What methodology is used to compensate for limited labelled data?,3-class representation for impact labels.,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.
Learning Twitter User Sentiments on Climate Change with Limited Labeled Data,Which five natural disasters were examined?,None.,"the East Coast Bomb Cyclone |  the Mendocino, California wildfires | Hurricane Florence | Hurricane Michael | the California Camp Fires"
Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization,What is the size of the dataset?,Not specified.," 9,892 stories of sexual harassment incidents"
Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization,What model did they use?,They used post-hoc text generation.,joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)
Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization,What patterns were discovered from the stories?,None.,"we demonstrate that harassment occurred more frequently during the night time than the day time | it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives | we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s)  | We also found that the majority of young perpetrators engaged in harassment behaviors on the streets | we found that adult perpetrators of sexual harassment are more likely to act alone | we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location  | commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."
Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding,Does the performance increase using their method?,No.,The multi-task model outperforms the single-task model at all data sizes | but none have an overall benefit from the open vocabulary system
Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding,What tasks are they experimenting with in this paper?,Evaluating the impact of claims.,"Slot filling | we consider the actions that a user might perform via apps on their phone | The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant"
What we write about when we write about causality: Features of causal statements across large-scale social discourse,How do they extract causality from text?,"They use various methods such as iterative topic modeling with time series feedback, mining causal relations, and visual adaptation of perception.","They identify documents that contain the unigrams 'caused', 'causing', or 'causes'"
What we write about when we write about causality: Features of causal statements across large-scale social discourse,"What is the source of the ""control"" corpus?",Not specified in the snippet.,"Randomly selected from a Twitter dump, temporally matched to causal documents"
What we write about when we write about causality: Features of causal statements across large-scale social discourse,"What are the selection criteria for ""causal statements""?",Statements that are with high certainty causal statements.,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'"
What we write about when we write about causality: Features of causal statements across large-scale social discourse,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",They use both expert annotations (from linguistics publications) and crowdsourcing.,Only automatic methods
What we write about when we write about causality: Features of causal statements across large-scale social discourse,how do they collect the comparable corpus?,They don't.,Randomly from a Twitter dump
What we write about when we write about causality: Features of causal statements across large-scale social discourse,How do they collect the control corpus?,"They randomly sample sentences from the remaining sentences (excluding the input sentence), compute the controls for each of these ""sibling"" sentences, and generate new sentences using the original input sentence content features.",Randomly from Twitter
Multi-style Generative Reading Comprehension,How do they measure the quality of summaries?,"They use five criteria: Grammaticality, Non redundancy, Referential Clarity, Focus, and Structure & Coherence.",Rouge-L | Bleu-1
Multi-style Generative Reading Comprehension,What do they mean by answer styles?,"They refer to different ways of answering questions, such as short answers with pronouns (NQA style) versus longer answers (NLG style).",well-formed sentences vs concise answers
Multi-style Generative Reading Comprehension,What are the baselines that Masque is compared against?,Wu et al. (2018) and Hu et al. (2018),"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"
Multi-style Generative Reading Comprehension,What is the performance achieved on NarrativeQA?,"The performance achieved on NarrativeQA is not explicitly stated in the provided snippet. However, it mentions that the model's detailed setup and output examples are in the supplementary material.","Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"
Multi-style Generative Reading Comprehension,"What is an ""answer style""?","An ""answer style"" refers to the format or characteristics of an answer, such as its length, tone, or language use. In this context, it appears that the NarrativeQA dataset has a distinct answer style compared to the MS MARCO dataset, with answers being shorter and containing more pronouns.",well-formed sentences vs concise answers
A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading,What was the previous state of the art model for this task?,"There is no mention of a specific ""previous state of the art model"" in the provided text snippet. However, it mentions that available methods have successfully converted natural language to programming language within fixed or tightly bounded linguistic paradigms.",WAS | LipCH-Net-seq | CSSMCM-w/o video
A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading,What syntactic structure is used to model tones?,Syntax,syllables
A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading,What visual information characterizes tones?,Visible movements.,video sequence is first fed into the VGG model BIBREF9 to extract visual feature
Dissecting Content and Context in Argumentative Relation Analysis,How do they demonstrate the robustness of their results?,"They use 3-class representation for impact labels to decrease sparsity issues, and consider only claims with more than 60% agreement for a more reliable assignment of impact labels.",performances of a purely content-based model naturally stays stable
Dissecting Content and Context in Argumentative Relation Analysis,What baseline and classification systems are used in experiments?,"Naive Bayes, SVM, Maximum Entropy classiﬁers.",BIBREF13 | majority baseline
Dissecting Content and Context in Argumentative Relation Analysis,How are the EAU text spans annotated?,"Only the clause is annotated as an EAU span (e.g., only ""people should not smoke"" is annotated as EAU in the sentence ""Therefore, people should not smoke."").",Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.
Efficient Attention using a Fixed-Size Memory Representation,Which baseline methods are used?,BiDAF,standard parametrized attention and a non-attention baseline
Efficient Attention using a Fixed-Size Memory Representation,How much is the BLEU score?,100.00,Ranges from 44.22 to 100.00 depending on K and the sequence length.
Efficient Attention using a Fixed-Size Memory Representation,Which datasets are used in experiments?,"SearchSnippets, StackOverﬂow, Biomedical",Sequence Copy Task and WMT'17
Team Papelo: Transformer Networks at FEVER,What baseline do they compare to?,They don't mention what baseline they compare to.,"For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 ."
Team Papelo: Transformer Networks at FEVER,Which pre-trained transformer do they use?,They don't specify which pre-trained Transformer model they used.,BIBREF5
Team Papelo: Transformer Networks at FEVER,What is the FEVER task?,"The FEVER task involves resolving conflicts between supporting and refuting information in favor of the supporting information, and determining whether a sentence supports a claim.","tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem"
Automatic Differentiation in ROOT,How is correctness of automatic derivation proved?,It is not mentioned in the given text.,"empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)"
Spectral decomposition method of dialog state tracking via collective matrix factorization,What state-of-the-art models are compared against?,BERT model,a deep neural network (DNN) architecture proposed in BIBREF24  | maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model
Torch-Struct: Deep Structured Prediction Library,Is this library implemented into Torch or is framework agnostic?,Framework agnostic.,It uses deep learning framework (pytorch)
Torch-Struct: Deep Structured Prediction Library,What baselines are used in experiments?,HELP,"Typical implementations of dynamic programming algorithms are serial in the length of the sequence | Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized | Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient"
Torch-Struct: Deep Structured Prediction Library,What general-purpose optimizations are included?,None.,Parallel Scan Inference | Vectorized Parsing | Semiring Matrix Operations
Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study,what baseline do they compare to?,They use 3-class representation for the impact labels.,VecMap | Muse | Barista
Improving Open Information Extraction via Iterative Rank-Aware Learning,How does this compare to traditional calibration methods like Platt Scaling?,Not mentioned in the snippet.,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.
Improving Open Information Extraction via Iterative Rank-Aware Learning,What's the input representation of OpenIE tuples into the model?,tf-idf,word embeddings
Detecting Online Hate Speech Using Context Aware Models,How do they combine the models?,"They use multi-task learning to consider RC, passage ranking, and answer possibility classification together.",maximum of two scores assigned by the two separate models | average score
Detecting Online Hate Speech Using Context Aware Models,What is their baseline?,They do not have one.,Logistic regression model with character-level n-gram features
Detecting Online Hate Speech Using Context Aware Models,What context do they use?,They use the argument path starting from the thesis until the claim C.,title of the news article | screen name of the user
Detecting Online Hate Speech Using Context Aware Models,What is their definition of hate speech?,"Hate speech is defined as publications that attack communities, which may be determined by the context of a publication, including the combination of text and images, rather than just the presence of offensive terms.","language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation"
Detecting Online Hate Speech Using Context Aware Models,What architecture has the neural network?,Dynamic Convolutional Neural Network (DCNN),three parallel LSTM BIBREF21 layers
"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",How is human interaction consumed by the model?,Human interaction is consumed by the model through user input that is lowercased and tokenized to match the model training data via spaCy.,"displays three different versions of a story written by three distinct models for a human to compare | human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages"
"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",How do they evaluate generated stories?,"They evaluate generated stories based on four criteria: Relevance (is the story relevant to the title?), Creativity (is the story unusual and interesting?), Overall Quality (how good is the story?), and Event Coherence (do the events in the story make sense together and are they in the right order?).",separate set of Turkers to rate the stories for overall quality and the three improvement areas
"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",What are the baselines?,There is no mention of baselines in the provided context.,Title-to-Story system
Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,What is used a baseline?,Nothing.,"As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12"
Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,What contextual features are used?,"Contextual keywords that co-occur with malicious file names, such as ""download"", ""malware"", and ""malicious"".",The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.
Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,Where are the cybersecurity articles used in the model sourced from?,DBLP, from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018
Polysemy Detection in Distributed Representation of Word Sense,How is the fluctuation in the sense of the word and its neighbors measured?,Volatility Vct(w) measures the fluctuation in the sense of the word and its neighbors.,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word."
MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,How big are improvements of MMM over state of the art?,16.9%,"test accuracy of 88.9%, which exceeds the previous best by 16.9%"
MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,What out of domain datasets authors used for coarse-tuning stage?,hotels domain,MultiNLI BIBREF15 and SNLI BIBREF16 
MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,What are state of the art methods MMM is compared to?,"BERT-Base, BERT-Large, RoBERTA-Large.","FTLM++, BERT-large, XLNet"
MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,What four representative datasets are used for bechmark?,"MCTest Dataset, Argus task, bAbI dataset, English Wikipedia","DREAM, MCTest, TOEFL, and SemEval-2018 Task 11"
Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,How do they measure the diversity of inferences?,They don't mention how they measure the diversity of inferences.,by number of distinct n-grams
Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,By how much do they improve the accuracy of inferences over state-of-the-art methods?,They improve the accuracy by 4.3%,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively."
Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,Which models do they use as baselines on the Atomic dataset?,They use Conditional Variational Autoencoder (CVAE) as their baseline model.,RNN-based Seq2Seq | Variational Seq2Seq | VRNMT  | CWVAE-Unpretrained
Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,How does the context-aware variational autoencoder learn event background information?,"It learns event background information by using an extra context-aware latent variable in the pretrain stage, where it is trained on an auxiliary dataset consisting of three narrative story corpora that contain rich event background knowledge."," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target."
Comparing Human and Machine Errors in Conversational Speech Transcription,what standard speech transcription pipeline was used?,None.,pipeline that is used at Microsoft for production data
Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources,How is speed measured?,Speed is not mentioned in the given context.,time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred
Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources,What is the architecture of their model?,"Their model consists of two components: an interpreter and a generator. The interpreter provides an explanation for the model's decision, while the generator outputs a textual explanation of the model's decision, learned with supervision of textual explanations.","we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources,What are the nine types?,"Determiners, negation, verbs, nouns, adverbs, prepositions, conditionals, conjunctions, and disjunctions.",agreement | answer | appreciation | disagreement | elaboration | humor | negative reaction | question | other
Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing,How many paraphrases are generated per question?,No specific number is mentioned in the given context.,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans"
Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing,What latent variables are modeled in the PCFG?,Syntactic and semantic/topical information.,syntactic information | semantic and topical information
Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing,What are the baselines?,There is no mention of baselines in the provided context.,GraphParser without paraphrases | monolingual machine translation based model for paraphrase generation
"Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter",How strong was the correlation between exercise and diabetes?,No p-value is given for this correlation.,weak correlation with p-value of 0.08
"Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter",How were topics of interest about DDEO identified?,Semantic and linguistic analyses were used to disclose health characteristics of opinions in tweets containing DDEO words.,using topic modeling model Latent Dirichlet Allocation (LDA)
FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,What is the performance difference between proposed method and state-of-the-arts on these datasets?,"The proposed method (weGAN) outperforms word2vec in terms of clustering results (Rand index) by 0.18% and classification accuracy by 0.97%. Compared to deGAN, weGAN has a slightly lower classification accuracy (-0.62%).",Difference is around 1 BLEU score lower on average than state of the art methods.
FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,What non autoregressive NMT models are used for comparison?,"NAT w/ Fertility (Gu et al., 2018), NAT-IR (Lee et al. , 2018), NAT-REG (Wang et al. , 2019), LV NAR ( Shu et al. , 2019), CTC Loss ( Libovick`y and Helcl , 2018), and CMLM (Ghazvininejad et al., 2019).",NAT w/ Fertility | NAT-IR | NAT-REG | LV NAR | CTC Loss | CMLM
FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,What are three neural machine translation (NMT) benchmark datasets used for evaluation?,"IWSLT14, Korean–English dataset","WMT2014, WMT2016 and IWSLT-2014"
On Leveraging the Visual Modality for Neural Machine Translation,What is result of their attention distribution analysis?,They found that the three attention maps are fairly similar despite the completely different decisions.,visual attention is very sparse |  visual component of the attention hasn't learnt any variation over the source encodings
On Leveraging the Visual Modality for Neural Machine Translation,What is result of their Principal Component Analysis?,No information about the results of their Principal Component Analysis.,existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT
On Leveraging the Visual Modality for Neural Machine Translation,What are 3 novel fusion techniques that are proposed?,None.,Step-Wise Decoder Fusion | Multimodal Attention Modulation | Visual-Semantic (VS) Regularizer
Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,What are two models' architectures in proposed solution?,"Two models' architectures are not explicitly mentioned in the provided snippet. However, it mentions an additional ""generator"" component for post-hoc text generation.","Reasoner model, also implemented with the MatchLSTM architecture | Ranker model"
Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,How do two models cooperate to select the most confident chains?,They play a cooperative game and are rewarded when they find a consistent chain.,"Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards"
Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,What benchmarks are created?,"ORIGINAL , MT, NAIVE , PPDB and BILAYERED",Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples
A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What empricial investigations do they reference?,Miller (2018),empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation
A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What languages do they investigate for machine translation?,Natural Language to programming language (primarily Python),English  | Chinese 
A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What recommendations do they offer?,They recommend considering both linguistic characteristics of a claim and its context to determine its impact.," Choose professional translators as raters |  Evaluate documents, not sentences | Evaluate fluency in addition to adequacy | Do not heavily edit reference translations for fluency | Use original source texts"
A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What percentage fewer errors did professional translations make?,No specific percentage is mentioned in the given text snippet.,36%
A Set of Recommendations for Assessing Human-Machine Parity in Language Translation,What was the weakness in Hassan et al's evaluation design?,Not explicitly stated.,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
"
Effective Use of Transformer Networks for Entity Tracking,What evidence do they present that the model attends to shallow context clues?,"They mention that accuracy on conjunction was opposite to that on disjunction, suggesting that the model relies on superficial cues.",Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues
Effective Use of Transformer Networks for Entity Tracking,In what way is the input restructured?,into coding expressions,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level"
Recognizing Musical Entities in User-generated Content,What are their results on the entity recognition task?,"Their model performs surprisingly well, nearly as well as the first occurrence baseline.","With both test sets performances decrease, varying between 94-97%"
Recognizing Musical Entities in User-generated Content,What task-specific features are used?,API arguments for each type of conversation.,"6)Contributor first names | 7)Contributor last names | 8)Contributor types (""soprano"", ""violinist"", etc.) | 9)Classical work types (""symphony"", ""overture"", etc.) | 10)Musical instruments | 11)Opus forms (""op"", ""opus"") | 12)Work number forms (""no"", ""number"") | 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp"") | 14)Work Modes (""major"", ""minor"", ""m"")"
Recognizing Musical Entities in User-generated Content,What kind of corpus-based features are taken into account?,Aligned parallel paraphrase corpora.,"standard linguistic features, such as Part-Of-Speech (POS) and chunk tag | series of features representing tokens' left and right context"
Recognizing Musical Entities in User-generated Content,Which machine learning algorithms did the explore?,LSTM (Long Short-Term Memory),biLSTM-networks
Recognizing Musical Entities in User-generated Content,What language is the Twitter content in?,English,English
MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge,What is the architecture of the siamese neural network?,"Symmetrical component to extract high level features from different input channels, which share parameters and map inputs to the same vector space.","two parallel convolutional networks, INLINEFORM0 , that share the same set of weights"
MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge,Which are the four Arabic dialects?,"arabic–classical-syriac, arabic–maltese, arabic–turkmen",Egyptian (EGY) | Levantine (LEV) | Gulf (GLF) | North African (NOR)
Bias in Semantic and Discourse Interpretation,What factors contribute to interpretive biases according to this research?,The framework developed in this research provides tools for understanding and analyzing the range of interpretive biases and the factors that contribute to them.,"Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march"
Bias in Semantic and Discourse Interpretation,Which interpretative biases are analyzed in this paper?,The range of interpretive biases.,"in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury"
QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships,How does the QuaSP+Zero model work?,It uses an entity-linking approach applied to properties.,"does not just consider the question tokens, but also the relationship between those tokens and the properties"
QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships,Which off-the-shelf tools do they use on QuaRel?,They use BILSTM.,information retrieval system | word-association method |  CCG-style rule-based semantic parser written specifically for friction questions | state-of-the-art neural semantic parser
QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships,How do they obtain the logical forms of their questions in their dataset?,"They elicit the logical forms using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism."," workers were given a seed qualitative relation | asked to enter two objects, people, or situations to compare | created a question, guided by a large number of examples | LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions"
Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts,What accuracy does CNN model achieve?,0.373,Combined per-pixel accuracy for character line segments is 74.79
Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts,How many documents are in the Indiscapes dataset?,508,508
Evaluating Rewards for Question Generation Models,What human evaluation metrics were used in the paper?,Human-targeted metrics for machine translation.,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context
Deep contextualized word representations for detecting sarcasm and irony,What are the 7 different datasets?,"There is only one dataset mentioned in the given context information. However, it consists of three subsets:

1. SearchSnippets
2. StackOverﬂow
3. Biomedical",SemEval 2018 Task 3 | BIBREF20 | BIBREF4 | SARC 2.0 | SARC 2.0 pol | Sarcasm Corpus V1 (SC-V1) | Sarcasm Corpus V2 (SC-V2)
Deep contextualized word representations for detecting sarcasm and irony,What are the three different sources of data?,There is no mention of three different sources of data in the given context.,Twitter | Reddit | Online Dialogues
Deep contextualized word representations for detecting sarcasm and irony,What type of model are the ELMo representations used in?,A one layer Bi-directional RNN (16 units) with GRU cells.,A bi-LSTM with max-pooling on top of it
Deep contextualized word representations for detecting sarcasm and irony,Which morphosyntactic features are thought to indicate irony or sarcasm?,Specific linguistic cues and combinations of such.,all caps | quotation marks | emoticons | emojis | hashtags
Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data,Which frozen acoustic model do they use?,None.,"a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13"
Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data,By how much does using phonetic feedback improve state-of-the-art systems?,No specific improvement value is mentioned in the provided snippet.,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9
Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction,What features are used?,linguistic characteristic of a claim and its context length (Cl),"Sociodemographics: gender, age, marital status, etc. | Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc. | Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc."
Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction,How do they incorporate sentiment analysis?,"They use subjectivity classification to filter out fairly objective tweets and reduce the size of the training set, as most subjective sentences have a clear inclination towards either positive or negative sentiment.",features per admission were extracted as inputs to the readmission risk classifier
Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction,What is the dataset used?,the dataset,"EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA"
Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction,How do they extract topics?,"They don't. They extract entities such as characters, locations, and objects by asking questions to a QA model.", automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15
Analysing Coreference in Transformer Outputs,What translationese effects are seen in the analysis?,"Translationese source texts exhibit less lexical variety than original Chinese text, making them simpler and easier for machine translation (MT) systems to score well on. The difference between human and machine translation quality is smaller when source texts are translated from English (translationese) rather than original Chinese.",potentially indicating a shining through effect | explicitation effect
Analysing Coreference in Transformer Outputs,What languages are seen in the news and TED datasets?,English,English | German
Analysing Coreference in Transformer Outputs,How are the (possibly incorrect) coreference chains in the MT outputs annotated?,Chain members in MT output are annotated as to whether or not they are correct.,"allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause) | The mentions referring to the same discourse item are linked between each other. | chain members are annotated for their correctness"
Analysing Coreference in Transformer Outputs,Which three neural machine translation systems are analyzed?,None.,first two systems are transformer models trained on different amounts of data | The third system includes a modification to consider the information of full coreference chains
Analysing Coreference in Transformer Outputs,Which coreference phenomena are analyzed?,Cross-lingual coreference resolution.,shining through | explicitation
NumNet: Machine Reading Comprehension with Numerical Reasoning,what are the existing models they compared with?,BERT model without context and BERT model with flat context representation.,Syn Dep | OpenIE | SRL | BiDAF | QANet | BERT | NAQANet | NAQANet+
Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks,What model achieves state of the art performance on this task?,BERT-Base,BIBREF16
Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks,Which multitask annotated corpus is used?,Natural Language Decathlon,IEMOCAP
Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks,What are the tasks in the multitask learning setup?,Multi-task learning with the source dataset RACE and various smaller target datasets.,"set of related tasks are learned (e.g., emotional activation) | primary task (e.g., emotional valence)"
Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities,What is the relationship between the co-voting and retweeting patterns?,"There is a relationship between the co-voting and retweeting patterns, as indicated by the increased use of Twitter by MEPs on days with a roll-call vote session and the similar structure of the co-voting network and the retweet network on the left-to-center side of the political spectrum.","we observe a positive correlation between retweeting and co-voting | strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets | Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union | significantly negative coefficient, is the area Economic and monetary system"
Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities,What insights does the analysis give about the cohesion of political groups in the European parliament?,"The analysis shows that S&D, EPP, and Greens-EFA have high cohesion, while NI has the lowest cohesion. EFDD and ENL are also among the least cohesive groups. The results from ERGM confirm these findings, indicating homophilous matching between MEPs with the same political affiliation.","Greens-EFA, S&D, and EPP exhibit the highest cohesion | non-aligned members NI have the lowest cohesion, followed by EFDD and ENL | two methods disagree is the level of cohesion of GUE-NGL"
QA4IE: A Question Answering based Framework for Information Extraction,What QA models were used?,Scoremul and Scoreavg,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer."
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,What baseline is used to compare the experimental results against?,No specific baseline is mentioned in the provided context.,Transformer generation model
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,How does counterfactual data augmentation aim to tackle bias?,"Counterfactual data augmentation aims to mitigate gender stereotypes in languages with rich morphology by generating alternative versions of sentences that differ only in terms of gender, thereby reducing the impact of biased language representations.",The training dataset is augmented by swapping all gendered words by their other gender counterparts
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"In the targeted data collection approach, what type of data is targetted?",Argument impact data.,Gendered characters in the dataset
Microsoft Research Asia's Systems for WMT19,How does soft contextual data augmentation work?,"Soft contextual data augmentation (SCA) replaces a randomly chosen word in a sentence with its soft version, which is a distribution over the vocabulary of words. The embedding of the soft version of a word w is calculated as the expectation of word embeddings over this distribution, using a pre-trained language model to compute the probability of each word in the vocabulary appearing after the sequence preceding w.",softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words | replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary
Microsoft Research Asia's Systems for WMT19,How does muli-agent dual learning work?,Multi-agent dual learning (MADL) leverages the duality between the primal task (mapping from domain X to domain Y) and dual task (mapping from domain Y to X) to boost the performances of both tasks by introducing multiple primal and dual models.,"MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models."
Microsoft Research Asia's Systems for WMT19,Which language directions are machine translation systems of WMT evaluated on?,"German ↔English, German ↔French, Chinese↔English, English →Lithuanian, English→Finnish, Russian →English, Lithuanian →English, Finnish→English, and English→Kazakh.","German$\leftrightarrow $English, German$\leftrightarrow $French, Chinese$\leftrightarrow $English, English$\rightarrow $Lithuanian, English$\rightarrow $Finnish, and Russian$\rightarrow $English | Lithuanian$\rightarrow $English, Finnish$\rightarrow $English, and English$\rightarrow $Kazakh"
Few-shot Natural Language Generation for Task-Oriented Dialog,What was the criteria for human evaluation?,"RE Match, Fluency, Sentiment",to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness
Few-shot Natural Language Generation for Task-Oriented Dialog,What automatic metrics are used to measure performance of the system?,ROUGE-LM (a combination of WPSLOR and ROUGE),BLEU scores and the slot error rate (ERR)
Few-shot Natural Language Generation for Task-Oriented Dialog,What existing methods is SC-GPT compared to?,"SC-LSTM, GPT-2, HDSA",$({1})$ SC-LSTM BIBREF3 | $({2})$ GPT-2 BIBREF6  | $({3})$ HDSA BIBREF7
Finding Street Gang Members on Twitter,What are the differences in the use of emojis between gang member and the rest of the Twitter population?,Fig. 5 shows that gang members use emojis differently than non-gang members.,"32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members | only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them | gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior"
Finding Street Gang Members on Twitter,What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?,"Gang members share more YouTube links related to hip-hop music, gangster rap, and its surrounding culture (76.58% of shared links), with an average of 8 YouTube links shared per gang member, whereas non-gang members do not exhibit this behavior. Additionally, the top terms used in YouTube videos shared by gang members differ from those used by non-gang members.","76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre"
Finding Street Gang Members on Twitter,What are the differences in the use of images between gang member and the rest of the Twitter population?,Textual features extracted from images shared by gang members differ from those shared by non-gang members.,"user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash"
Finding Street Gang Members on Twitter,What are the differences in language use between gang member and the rest of the Twitter population?,"Gang members more frequently use curse words, talk about drugs and money, and focus on material things, whereas ordinary users tend to vocalize their feelings and hardly speak about finances and drugs.","Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word | gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us"
Finding Street Gang Members on Twitter,How is gang membership verified?,Gang membership is verified through Twitter profile images.,Manual verification
A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,What is English mixed with in the TRAC dataset?,German station and street names.,Hindi
A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,Which psycholinguistic and basic linguistic features are used?,Inﬂectional morphology and part of speech (POS),"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features"
A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,"The text does not mention Facebook, but it mentions that social media messages are generally short, often length-limited by design as in Twitter.",Systems do not perform well both in Facebook and Twitter texts
A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,None.,None
An Emotional Analysis of False Information in Social Media and News Articles,What is the baseline?,"There is no mention of a ""baseline"" in the provided context.",Majority Class baseline (MC)  | Random selection baseline (RAN)
An Emotional Analysis of False Information in Social Media and News Articles,What datasets did they use?,"datasets about geography (Zelle and Mooney 1996), travel booking (Dahl et al. 1994), factoid QA over knowledge bases (Berant et al. 2013), Wikipedia tables (Pasupat and Liang 2015)",News Articles | Twitter
STransE: a novel embedding model of entities and relationships in knowledge bases,What scoring function does the model use to score triples?,"fr(h,t) = ∥Wr,1h + r −Wr,2t∥ℓ1/2","$ f_r(h, t) & = & \Vert \textbf {W}_{r,1}\textbf {h} + \textbf {r} - \textbf {W}_{r,2}\textbf {t}\Vert _{\ell _{1/2}} $"
STransE: a novel embedding model of entities and relationships in knowledge bases,What datasets are used to evaluate the model?,Chatbot Natural Language Understanding (NLU) Evaluation Corpus,"WN18, FB15k"
Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks,What are the differences with previous applications of neural networks for this task?,Simple models can outperform or closely match performance of complex architectures; all the models considered are task-independent and were successfully used in different contexts than Hypothesis Evaluation.,This approach considers related images
