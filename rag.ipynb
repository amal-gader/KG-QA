{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1f335e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67730c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Load the PDF\n",
    "loader = PyPDFLoader(\"pdfs/RAG.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: Create the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Step 3: Split the documents\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58ee9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Use a supported HF model\n",
    "embed = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# Now this will work\n",
    "db = FAISS.from_documents(texts, embed)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "490ce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Based on the {context} provided answer the query asked by the user in a best possible way.\"\n",
    "Question:{input}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae63d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model=OllamaLLM(model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94addc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d42ecbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, here are the basics of a RAG (Reformer-based Architecture for Generative models) system:\n",
      "\n",
      "1. Access to both parametric and non-parametric components.\n",
      "2. Ability to generate answers by aggregating content from multiple pieces of retrieved content.\n",
      "3. Learning latent retrieval techniques.\n",
      "4. Retrieving evidence documents instead of related training pairs.\n",
      "\n",
      "Note that RAG is a hybrid generation model, which means it combines different techniques (parametric and non-parametric) to achieve its goals.\n"
     ]
    }
   ],
   "source": [
    "result=retrieval_chain.invoke({'input':\"What are the basics of a  RAG system?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fa4393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, some potential downsides of a RAG (Reinforced Augmented Generator) system include:\n",
      "\n",
      "1. **Factual inaccuracies and bias**: Since RAG relies on external knowledge sources like Wikipedia, it may inherit their biases and inaccuracies.\n",
      "2. **Potential for abuse or misleading content generation**: Like GPT-2, RAG can be used to generate faked or misleading content, which is a concern given its language model capabilities.\n",
      "\n",
      "These are the main pitfalls mentioned in the text as potential downsides of using a RAG system.\n"
     ]
    }
   ],
   "source": [
    "result=retrieval_chain.invoke({'input':\"What are the pitfalls of a  RAG system?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d83b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proposed future work of the paper is not explicitly mentioned. However, based on the context and content of the paper, we can infer that some potential directions for future research could be:\n",
      "\n",
      "* Investigating how RAG models can be adapted or extended to handle more complex generation tasks, such as long-form text generation or multi-modal generation.\n",
      "* Exploring ways to improve the performance and efficiency of RAG models on downstream tasks, such as using different architectures or training methods.\n",
      "* Developing new applications or use cases for RAG models, such as in areas like question answering, dialogue systems, or content creation.\n",
      "\n",
      "However, it's worth noting that the paper does mention some acknowledgments and funding sources at the end, which might indicate that the authors are currently working on related research projects.\n"
     ]
    }
   ],
   "source": [
    "result=retrieval_chain.invoke({'input':\"What is the proposed Future Work of the paper?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c10d5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "576e61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"pdfs/RAG.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77d5e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    api_key=\"not-needed\",\n",
    "    api_base=\"http://localhost:8000/v1\",\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25344050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f1e841403b0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7f1f0c37b920>, completion_to_prompt=<function default_completion_to_prompt at 0x7f1f0c11ef20>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Llama-2-7b-hf', temperature=0.0, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='not-needed', api_base='http://localhost:8000/v1', api_version='', strict=False, reasoning_effort=None, modalities=None, audio_config=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cee9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 6.86 GiB (4.53 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 40\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 40\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 5120\n",
      "print_info: n_embd_v_gqa     = 5120\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 13B\n",
      "print_info: model params     = 13.02 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_0) (and 82 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  6806.25 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  7023.90 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.36.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.36.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.37.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.37.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.37.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.38.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.38.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.38.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.39.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.39.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.39.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.39.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.39.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.39.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.39.ffn_up.weight with q4_0_8x8\n",
      "...\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 5000\n",
      "llama_context: n_ctx_per_seq = 5000\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (5000) > n_ctx_train (4096) -- possible training context overflow\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 5024 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified: layer  36: dev = CPU\n",
      "llama_kv_cache_unified: layer  37: dev = CPU\n",
      "llama_kv_cache_unified: layer  38: dev = CPU\n",
      "llama_kv_cache_unified: layer  39: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  3925.00 MiB\n",
      "llama_kv_cache_unified: size = 3925.00 MiB (  5024 cells,  40 layers,  1 seqs), K (f16): 1962.50 MiB, V (f16): 1962.50 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   442.32 MiB\n",
      "llama_context: graph nodes  = 1446\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "# llm = LlamaCPP(\n",
    "#     model_url=model_url,\n",
    "#     model_path=None,\n",
    "#     temperature=0.1,\n",
    "#     max_new_tokens=256,\n",
    "#     context_window=5000,\n",
    "#     generate_kwargs={},\n",
    "#     model_kwargs={\"n_gpu_layers\": 1},\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# llm = LlamaCPP(\n",
    "#      model_url=model_url,\n",
    "#     temperature=0.1,\n",
    "#     max_new_tokens=256,\n",
    "#     context_window=5000,\n",
    "#     generate_kwargs={},\n",
    "#     model_kwargs={\"n_gpu_layers\": 80},  # as high as VRAM allows\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a41215c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model 'meta-llama/Llama-2-7b-hf'. Please provide a valid OpenAI model name in: o1, o1-2024-12-17, o1-pro, o1-pro-2025-03-19, o1-preview, o1-preview-2024-09-12, o1-mini, o1-mini-2024-09-12, o3-mini, o3-mini-2025-01-31, o3, o3-2025-04-16, o3-pro, o3-pro-2025-06-10, o4-mini, o4-mini-2025-04-16, gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-1106-vision-preview, gpt-4-turbo-2024-04-09, gpt-4-turbo, gpt-4o, gpt-4o-audio-preview, gpt-4o-audio-preview-2024-12-17, gpt-4o-audio-preview-2024-10-01, gpt-4o-mini-audio-preview, gpt-4o-mini-audio-preview-2024-12-17, gpt-4o-2024-05-13, gpt-4o-2024-08-06, gpt-4o-2024-11-20, gpt-4.5-preview, gpt-4.5-preview-2025-02-27, chatgpt-4o-latest, gpt-4o-mini, gpt-4o-mini-2024-07-18, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-4.1-2025-04-14, gpt-4.1-mini-2025-04-14, gpt-4.1-nano-2025-04-14, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m documents = SimpleDirectoryReader(\u001b[33m\"\u001b[39m\u001b[33mpdfs\u001b[39m\u001b[33m\"\u001b[39m).load_data()\n\u001b[32m     18\u001b[39m index = VectorStoreIndex.from_documents(documents)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m query_engine = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:514\u001b[39m, in \u001b[36mBaseIndex.as_query_engine\u001b[39m\u001b[34m(self, llm, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m retriever = \u001b[38;5;28mself\u001b[39m.as_retriever(**kwargs)\n\u001b[32m    508\u001b[39m llm = (\n\u001b[32m    509\u001b[39m     resolve_llm(llm, callback_manager=\u001b[38;5;28mself\u001b[39m._callback_manager)\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m Settings.llm\n\u001b[32m    512\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrieverQueryEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:106\u001b[39m, in \u001b[36mRetrieverQueryEngine.from_args\u001b[39m\u001b[34m(cls, retriever, llm, response_synthesizer, node_postprocessors, callback_manager, response_mode, text_qa_template, refine_template, summary_template, simple_template, output_cls, use_async, streaming, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mInitialize a RetrieverQueryEngine object.\".\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m llm = llm \u001b[38;5;129;01mor\u001b[39;00m Settings.llm\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m response_synthesizer = response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_response_synthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimple_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimple_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m callback_manager = callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings.callback_manager\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    121\u001b[39m     retriever=retriever,\n\u001b[32m    122\u001b[39m     response_synthesizer=response_synthesizer,\n\u001b[32m    123\u001b[39m     callback_manager=callback_manager,\n\u001b[32m    124\u001b[39m     node_postprocessors=node_postprocessors,\n\u001b[32m    125\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/factory.py:63\u001b[39m, in \u001b[36mget_response_synthesizer\u001b[39m\u001b[34m(llm, prompt_helper, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[39m\n\u001b[32m     57\u001b[39m callback_manager = callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings.callback_manager\n\u001b[32m     58\u001b[39m llm = llm \u001b[38;5;129;01mor\u001b[39;00m Settings.llm\n\u001b[32m     59\u001b[39m prompt_helper = (\n\u001b[32m     60\u001b[39m     prompt_helper\n\u001b[32m     61\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m Settings._prompt_helper\n\u001b[32m     62\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m PromptHelper.from_llm_metadata(\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m,\n\u001b[32m     64\u001b[39m     )\n\u001b[32m     65\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_mode == ResponseMode.REFINE:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Refine(\n\u001b[32m     69\u001b[39m         llm=llm,\n\u001b[32m     70\u001b[39m         callback_manager=callback_manager,\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m         verbose=verbose,\n\u001b[32m     79\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/llms/openai/base.py:367\u001b[39m, in \u001b[36mOpenAI.metadata\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> LLMMetadata:\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m         context_window=\u001b[43mopenai_modelname_to_contextsize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    368\u001b[39m         num_output=\u001b[38;5;28mself\u001b[39m.max_tokens \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m1\u001b[39m,\n\u001b[32m    369\u001b[39m         is_chat_model=is_chat_model(model=\u001b[38;5;28mself\u001b[39m._get_model_name()),\n\u001b[32m    370\u001b[39m         is_function_calling_model=is_function_calling_model(\n\u001b[32m    371\u001b[39m             model=\u001b[38;5;28mself\u001b[39m._get_model_name()\n\u001b[32m    372\u001b[39m         ),\n\u001b[32m    373\u001b[39m         model_name=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    374\u001b[39m         \u001b[38;5;66;03m# TODO: Temp for O1 beta\u001b[39;00m\n\u001b[32m    375\u001b[39m         system_role=MessageRole.USER\n\u001b[32m    376\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01min\u001b[39;00m O1_MODELS\n\u001b[32m    377\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m MessageRole.SYSTEM,\n\u001b[32m    378\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/llms/openai/utils.py:288\u001b[39m, in \u001b[36mopenai_modelname_to_contextsize\u001b[39m\u001b[34m(modelname)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    284\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpenAI model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has been discontinued. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease choose another model.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     )\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modelname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ALL_AVAILABLE_MODELS:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m. Please provide a valid OpenAI model name in:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(ALL_AVAILABLE_MODELS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ALL_AVAILABLE_MODELS[modelname]\n",
      "\u001b[31mValueError\u001b[39m: Unknown model 'meta-llama/Llama-2-7b-hf'. Please provide a valid OpenAI model name in: o1, o1-2024-12-17, o1-pro, o1-pro-2025-03-19, o1-preview, o1-preview-2024-09-12, o1-mini, o1-mini-2024-09-12, o3-mini, o3-mini-2025-01-31, o3, o3-2025-04-16, o3-pro, o3-pro-2025-06-10, o4-mini, o4-mini-2025-04-16, gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-1106-vision-preview, gpt-4-turbo-2024-04-09, gpt-4-turbo, gpt-4o, gpt-4o-audio-preview, gpt-4o-audio-preview-2024-12-17, gpt-4o-audio-preview-2024-10-01, gpt-4o-mini-audio-preview, gpt-4o-mini-audio-preview-2024-12-17, gpt-4o-2024-05-13, gpt-4o-2024-08-06, gpt-4o-2024-11-20, gpt-4.5-preview, gpt-4.5-preview-2025-02-27, chatgpt-4o-latest, gpt-4o-mini, gpt-4o-mini-2024-07-18, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-4.1-2025-04-14, gpt-4.1-mini-2025-04-14, gpt-4.1-nano-2025-04-14, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "Settings.embed_model=embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c1ddf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  236262.67 ms\n",
      "llama_perf_context_print: prompt eval time =  236261.65 ms /  1633 tokens (  144.68 ms per token,     6.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1487.31 ms /    13 runs   (  114.41 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  237770.42 ms /  1646 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please provide the answer based on the given context information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the difficulties when applying RAG?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5379e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1622 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  236262.67 ms\n",
      "llama_perf_context_print: prompt eval time =   56455.91 ms /    12 tokens ( 4704.66 ms per token,     0.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2093.41 ms /    19 runs   (  110.18 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   58562.26 ms /    31 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='\\n\\nPlease provide a concise and accurate answer to the query based on the given context.', source_nodes=[NodeWithScore(node=TextNode(id_='185ea783-c595-429b-a0d2-a0c99817df32', embedding=None, metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7d0041da-6db5-4f97-b77d-0f0f61dfb9f9', node_type='4', metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='fd3c75b4dd3bc0c5fc4825151385d0277f02f556fb2245b11079b805a13935b9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='614a707a-8103-4b48-8262-947e90a425b0', node_type='1', metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='8dc3af52c614d38f7623f968492939996ffff6316a61c578a19e9d4f1d15ca27')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation exibility of the closed-book (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized salient span masking pre-training [20]. It is worth noting that RAGs\\nretriever is initialized using DPRs retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based cross-\\nencoder to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5', mimetype='text/plain', start_char_idx=3469, end_char_idx=4556, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8134793950030017), NodeWithScore(node=TextNode(id_='36b12576-97ee-4374-bb32-2a016ca6a49a', embedding=None, metadata={'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9fad4fab-c4b0-40f4-a47d-1a9712e2cf5c', node_type='4', metadata={'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='c4304c9eefa42dca9125b0faaaa3acbecccca51313ea363b99ca4a1183fe435c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Broader Impact\\nThis work offers several positive societal benets over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it hallucinate less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct benet to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi and Jan ediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 15331544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 18701879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10', mimetype='text/plain', start_char_idx=0, end_char_idx=3778, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8118443606460757)], metadata={'185ea783-c595-429b-a0d2-a0c99817df32': {'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, '36b12576-97ee-4374-bb32-2a016ca6a49a': {'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What are briefly the basics of RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de5484dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 15 prefix-match hit, remaining 1084 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  236262.67 ms\n",
      "llama_perf_context_print: prompt eval time =  337308.01 ms /  1084 tokens (  311.17 ms per token,     3.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28172.92 ms /    12 runs   ( 2347.74 ms per token,     0.43 tokens per second)\n",
      "llama_perf_context_print:       total time =  365487.42 ms /  1096 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='\\n\\nPlease provide the answer based on the given context.', source_nodes=[NodeWithScore(node=TextNode(id_='159059fe-56bc-4785-9349-73f2140c3861', embedding=None, metadata={'page_label': '17', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8114c2d7-e17c-449c-9e89-9d5b3c22f4a3', node_type='4', metadata={'page_label': '17', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='26cd052b59efaa78db0d7558da50fb5ffd9b609ada9c4f42a6b898dbc08a93d1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not nd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\noating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We nd that doing Maximum Inner Product Search\\nwith FAISS is sufciently fast on CPU, so we store document index vectors on CPU, requiring100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISSs compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=2558, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8473159706460062), NodeWithScore(node=TextNode(id_='6f9102de-95d3-4616-94a3-9f23daf726ab', embedding=None, metadata={'page_label': '7', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d4baf79-6ffa-46f3-a293-d1fcb0e594d6', node_type='4', metadata={'page_label': '7', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='cee843ba86bac680bfceb6724966fabb17abdc60af8de097f088a91453f1b162'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='40c082d3-948d-4407-9505-fec69868b797', node_type='1', metadata={'page_label': '7', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='ef03b69a1d44e260b2b53d652f2c1b2284024d170aba6ec131d952bc0a9b0106')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAGs dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAGs retriever with a xed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7', mimetype='text/plain', start_char_idx=3077, end_char_idx=4331, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8463596511722716)], metadata={'159059fe-56bc-4785-9349-73f2140c3861': {'page_label': '17', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, '6f9102de-95d3-4616-94a3-9f23daf726ab': {'page_label': '7', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What are briefly the implementation details of RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "47eb7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 15 prefix-match hit, remaining 1618 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  236262.67 ms\n",
      "llama_perf_context_print: prompt eval time =  462468.72 ms /  1618 tokens (  285.83 ms per token,     3.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19617.55 ms /    13 runs   ( 1509.04 ms per token,     0.66 tokens per second)\n",
      "llama_perf_context_print:       total time =  482134.46 ms /  1631 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='\\n\\nPlease provide the answer based on the given context information.', source_nodes=[NodeWithScore(node=TextNode(id_='185ea783-c595-429b-a0d2-a0c99817df32', embedding=None, metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7d0041da-6db5-4f97-b77d-0f0f61dfb9f9', node_type='4', metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='fd3c75b4dd3bc0c5fc4825151385d0277f02f556fb2245b11079b805a13935b9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='614a707a-8103-4b48-8262-947e90a425b0', node_type='1', metadata={'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='8dc3af52c614d38f7623f968492939996ffff6316a61c578a19e9d4f1d15ca27')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation exibility of the closed-book (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized salient span masking pre-training [20]. It is worth noting that RAGs\\nretriever is initialized using DPRs retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based cross-\\nencoder to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5', mimetype='text/plain', start_char_idx=3469, end_char_idx=4556, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8170206239020039), NodeWithScore(node=TextNode(id_='36b12576-97ee-4374-bb32-2a016ca6a49a', embedding=None, metadata={'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9fad4fab-c4b0-40f4-a47d-1a9712e2cf5c', node_type='4', metadata={'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, hash='c4304c9eefa42dca9125b0faaaa3acbecccca51313ea363b99ca4a1183fe435c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Broader Impact\\nThis work offers several positive societal benets over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it hallucinate less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct benet to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi and Jan ediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 15331544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 18701879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10', mimetype='text/plain', start_char_idx=0, end_char_idx=3778, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8159068564409367)], metadata={'185ea783-c595-429b-a0d2-a0c99817df32': {'page_label': '5', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}, '36b12576-97ee-4374-bb32-2a016ca6a49a': {'page_label': '10', 'file_name': 'RAG.pdf', 'file_path': '/root/kg-qa/pdfs/RAG.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-07-07', 'last_modified_date': '2025-07-07'}})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What are the difficulties when applying RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "326d0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "documents = SimpleDirectoryReader(input_files=[\"pdfs/RAG.pdf\"]).load_data()\n",
    "#api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "#Settings.llm = llm\n",
    "#Gemini(api_key=api_key, model=\"models/gemini-1.5-flash\")\n",
    "#Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ef1a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b6b4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5fe07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to dataset\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the dataset.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402af47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7cd1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 155 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   53400.24 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1357.49 ms /    13 runs   (  104.42 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1362.22 ms /    14 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: '\\nPlease note that the input document is a JSON object.'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m query_engine = RouterQueryEngine(\n\u001b[32m      6\u001b[39m     selector=LLMSingleSelector.from_defaults(),\n\u001b[32m      7\u001b[39m     query_engine_tools=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# First query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response1 = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the summary of the document? \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(response1.response.replace(\u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Second query\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m dispatcher.event(\n\u001b[32m     54\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/query_engine/router_query_engine.py:164\u001b[39m, in \u001b[36mRouterQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    162\u001b[39m         CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    163\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result.inds) > \u001b[32m1\u001b[39m:\n\u001b[32m    167\u001b[39m             responses = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/base/base_selector.py:88\u001b[39m, in \u001b[36mBaseSelector.select\u001b[39m\u001b[34m(self, choices, query)\u001b[39m\n\u001b[32m     86\u001b[39m metadatas = [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[32m     87\u001b[39m query_bundle = _wrap_query(query)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/selectors/llm_selectors.py:117\u001b[39m, in \u001b[36mLLMSingleSelector._select\u001b[39m\u001b[34m(self, choices, query)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prompt.output_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m parse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kg-qa/venv/lib/python3.12/site-packages/llama_index/core/output_parsers/selection.py:97\u001b[39m, in \u001b[36mSelectionOutputParser.parse\u001b[39m\u001b[34m(self, output)\u001b[39m\n\u001b[32m     94\u001b[39m     json_obj = [json_obj]\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_obj, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m json_output = \u001b[38;5;28mself\u001b[39m._format_output(json_obj)\n\u001b[32m    100\u001b[39m answers = [Answer.from_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[31mValueError\u001b[39m: Failed to convert output to JSON: '\\nPlease note that the input document is a JSON object.'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# First query\n",
    "response1 = query_engine.query(\"What is the summary of the document? \\n\")\n",
    "print(response1.response.replace(\". \", \".\\n\"))\n",
    "\n",
    "# Second query\n",
    "response2 = query_engine.query(\"What is RAG? \\n\")\n",
    "print(response2.response.replace(\". \", \".\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
