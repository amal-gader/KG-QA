[
  {
    "id": "1909.00694",
    "title": "Minimally Supervised Learning of Affective Events Using Discourse Relations",
    "qas": [
      {
        "question": "What is the seed lexicon?",
        "question_id": "753990d0b621d390ed58f20c4d9e4f065f0dc672",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "a vocabulary of positive and negative predicates that helps determine the polarity score of an event",
            "evidence": [
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types."
            ],
            "highlighted_evidence": [
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.",
              "It is a "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "seed lexicon consists of positive and negative predicates"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types."
            ],
            "highlighted_evidence": [
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event."
            ]
          }
        ]
      },
      {
        "question": "What are the results?",
        "question_id": "9d578ddccc27dd849244d632dd0f6bf27348ad81",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",
            "evidence": [
              "FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.",
              "FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.",
              "As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.",
              "We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.",
              "FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.",
              "As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ",
              "We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$."
            ]
          }
        ]
      },
      {
        "question": "How are relations used to propagate polarity?",
        "question_id": "02e4bf719b1a504e385c35c6186742e720bcb281",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",
            "evidence": [
              "In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event."
            ],
            "highlighted_evidence": [
              "As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity",
            "evidence": [
              "In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.",
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types."
            ],
            "highlighted_evidence": [
              "As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.",
              "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation."
            ]
          }
        ]
      },
      {
        "question": "How big is the Japanese data?",
        "question_id": "44c4bd6decc86f1091b5fc0728873d9324cdde4e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",
            "evidence": [
              "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.",
              "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.",
              "FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.",
              "We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:",
              "Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.",
              "FLOAT SELECTED: Table 2: Details of the ACP dataset."
            ],
            "highlighted_evidence": [
              "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ",
              "From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.",
              "FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.",
              "We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well.",
              "Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.",
              "FLOAT SELECTED: Table 2: Details of the ACP dataset."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The ACP corpus has around 700k events split into positive and negative polarity ",
            "evidence": [
              "FLOAT SELECTED: Table 2: Details of the ACP dataset."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Details of the ACP dataset."
            ]
          }
        ]
      },
      {
        "question": "What are labels available in dataset for supervision?",
        "question_id": "86abeff85f3db79cf87a8c993e5e5aa61226dc98",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "negative",
              "positive"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive)."
            ],
            "highlighted_evidence": [
              "In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive)."
            ]
          }
        ]
      },
      {
        "question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?",
        "question_id": "c029deb7f99756d2669abad0a349d917428e9c12",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "3%",
            "evidence": [
              "FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data."
            ]
          }
        ]
      },
      {
        "question": "How does their model learn using mostly raw data?",
        "question_id": "39f8db10d949c6b477fa4b51e7c184016505884f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity",
            "evidence": [
              "In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event."
            ],
            "highlighted_evidence": [
              "In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive)."
            ]
          }
        ]
      },
      {
        "question": "How big is seed lexicon used for training?",
        "question_id": "d0bc782961567dc1dd7e074b621a6d6be44bb5b4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "30 words",
            "evidence": [
              "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16."
            ],
            "highlighted_evidence": [
              "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. "
            ]
          }
        ]
      },
      {
        "question": "How large is raw corpus used for training?",
        "question_id": "a592498ba2fac994cd6fad7372836f0adb37e22a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "100 million sentences"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.",
              "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16."
            ],
            "highlighted_evidence": [
              "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ",
              "From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/SaitoMK19",
    "dblp_title": "Minimally Supervised Learning of Affective Events Using Discourse Relations.",
    "year": "2019"
  },
  {
    "id": "2003.07723",
    "title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry",
    "qas": [
      {
        "question": "Does the paper report macro F1?",
        "question_id": "3a9d391d25cde8af3334ac62d478b36b30079d74",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We find that the multilingual model cannot handle infrequent categories, i.e., Awe/Sublime, Suspense and Humor. However, increasing the dataset with English data improves the results, suggesting that the classification would largely benefit from more annotated data. The best model overall is DBMDZ (.520), showing a balanced response on both validation and test set. See Table TABREF37 for a breakdown of all emotions as predicted by the this model. Precision is mostly higher than recall. The labels Awe/Sublime, Suspense and Humor are harder to predict than the other labels.",
              "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
            ],
            "highlighted_evidence": [
              "See Table TABREF37 for a breakdown of all emotions as predicted by the this model.",
              "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels."
            ]
          }
        ]
      },
      {
        "question": "How is the annotation experiment evaluated?",
        "question_id": "8d8300d88283c73424c8f301ad9fdd733845eb47",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "confusion matrices of labels between annotators"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We find that Cohen $\\kappa $ agreement ranges from .84 for Uneasiness in the English data, .81 for Humor and Nostalgia, down to German Suspense (.65), Awe/Sublime (.61) and Vitality for both languages (.50 English, .63 German). Both annotators have a similar emotion frequency profile, where the ranking is almost identical, especially for German. However, for English, Annotator 2 annotates more Vitality than Uneasiness. Figure FIGREF18 shows the confusion matrices of labels between annotators as heatmaps. Notably, Beauty/Joy and Sadness are confused across annotators more often than other labels. This is topical for poetry, and therefore not surprising: One might argue that the beauty of beings and situations is only beautiful because it is not enduring and therefore not to divorce from the sadness of the vanishing of beauty BIBREF48. We also find considerable confusion of Sadness with Awe/Sublime and Vitality, while the latter is also regularly confused with Beauty/Joy."
            ],
            "highlighted_evidence": [
              "Figure FIGREF18 shows the confusion matrices of labels between annotators as heatmaps."
            ]
          }
        ]
      },
      {
        "question": "What are the aesthetic emotions formalized?",
        "question_id": "48b12eb53e2d507343f19b8a667696a39b719807",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking)",
              "Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To emotionally move readers is considered a prime goal of literature since Latin antiquity BIBREF1, BIBREF2, BIBREF3. Deeply moved readers shed tears or get chills and goosebumps even in lab settings BIBREF4. In cases like these, the emotional response actually implies an aesthetic evaluation: narratives that have the capacity to move readers are evaluated as good and powerful texts for this very reason. Similarly, feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking). Emotions that exhibit this dual capacity have been defined as “aesthetic emotions” BIBREF2. Contrary to the negativity bias of classical emotion catalogues, emotion terms used for aesthetic evaluation purposes include far more positive than negative emotions. At the same time, many overall positive aesthetic emotions encompass negative or mixed emotional ingredients BIBREF2, e.g., feelings of suspense include both hopeful and fearful anticipations."
            ],
            "highlighted_evidence": [
              "Deeply moved readers shed tears or get chills and goosebumps even in lab settings BIBREF4. In cases like these, the emotional response actually implies an aesthetic evaluation: narratives that have the capacity to move readers are evaluated as good and powerful texts for this very reason. Similarly, feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking). Emotions that exhibit this dual capacity have been defined as “aesthetic emotions” BIBREF2."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/HaiderEKKM20",
    "dblp_title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry.",
    "year": "2020"
  },
  {
    "id": "1705.09665",
    "title": "Community Identity and User Engagement in a Multi-Community Landscape",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "003f884d3893532f8c302431c9f70be6f64d9be8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 )."
            ],
            "highlighted_evidence": [
              "We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. "
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How do the various social phenomena examined manifest in different types of communities?",
        "question_id": "bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n",
            "evidence": [
              "We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right).",
              "As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content."
            ],
            "highlighted_evidence": [
              "We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right).",
              "As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content."
            ]
          }
        ]
      },
      {
        "question": "What patterns do they observe about how user engagement varies with the characteristics of a community?",
        "question_id": "eea089baedc0ce80731c8fdcb064b82f584f483a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members",
              "within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Engagement and community identity. We apply our framework to understand how two important aspects of user engagement in a community—the community's propensity to retain its users (Section SECREF3 ), and its permeability to new members (Section SECREF4 )—vary according to the type of collective identity it fosters. We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members.",
              "More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities."
            ],
            "highlighted_evidence": [
              "We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members.",
              "More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). "
            ]
          }
        ]
      },
      {
        "question": "How did the select the 300 Reddit communities for comparison?",
        "question_id": "edb2d24d6d10af13931b3a47a6543bd469752f0c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.",
            "evidence": [
              "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 )."
            ],
            "highlighted_evidence": [
              "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They collect subreddits from January 2013 to December 2014,2 for which there are at\nleast 500 words in the vocabulary used to estimate the measures,\nin at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.",
            "evidence": [
              "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 )."
            ],
            "highlighted_evidence": [
              "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 )."
            ]
          }
        ]
      },
      {
        "question": "How do the authors measure how temporally dynamic a community is?",
        "question_id": "938cf30c4f1d14fa182e82919e16072fdbcf2a82",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the average volatility of all utterances"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Dynamicity. A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . We refer to a community whose language is relatively consistent throughout time as being stable."
            ],
            "highlighted_evidence": [
              ". A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . "
            ]
          }
        ]
      },
      {
        "question": "How do the authors measure how distinctive a community is?",
        "question_id": "93f4ad6568207c9bd10d712a52f8de25b3ebadd4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " the average specificity of all utterances"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Distinctiveness. A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 . We refer to a community with a less distinctive identity as being generic."
            ],
            "highlighted_evidence": [
              "A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icwsm/ZhangHDJL17",
    "dblp_title": "Community Identity and User Engagement in a Multi-Community Landscape.",
    "year": "2017"
  },
  {
    "id": "1908.06606",
    "title": "Question Answering based Clinical Text Structuring Using Pre-trained Language Model",
    "qas": [
      {
        "question": "What data is the language model pretrained on?",
        "question_id": "71a7153e12879defa186bfb6dbafe79c74265e10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Chinese general corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts."
            ],
            "highlighted_evidence": [
              "Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts."
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What baselines is the proposed model compared against?",
        "question_id": "85d1831c28d3c19c84472589a252e28e9884500f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BERT-Base",
              "QANet"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Experimental Studies ::: Comparison with State-of-the-art Methods",
              "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23."
            ],
            "highlighted_evidence": [
              "Experimental Studies ::: Comparison with State-of-the-art Methods\nSince BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "QANet BIBREF39",
              "BERT-Base BIBREF26"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.",
              "FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL"
            ],
            "highlighted_evidence": [
              "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. ",
              "FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL"
            ]
          }
        ]
      },
      {
        "question": "How is the clinical text structuring task defined?",
        "question_id": "1959e0ebc21fafdf1dd20c6ea054161ba7446f61",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained.",
              "Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task.",
              "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.",
              "To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task.",
              "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.",
              "Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.",
            "evidence": [
              "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.",
              "However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost.",
              "To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows."
            ],
            "highlighted_evidence": [
              "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly.",
              "However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size).",
              "To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text."
            ]
          }
        ]
      },
      {
        "question": "What are the specific tasks being unified?",
        "question_id": "77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " three types of questions, namely tumor size, proximal resection margin and distal resection margin"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.",
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.",
              "In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different datasets. A novel model is also proposed to integrate named entity information into a pre-trained language model and adapt it to QA-CTS task. Initially, sequential results of named entity recognition on both paragraph and query texts are integrated together. Contextualized representation on both paragraph and query texts are transformed by a pre-trained language model. Then, the integrated named entity information and contextualized representation are integrated together and fed into a feed forward network for final prediction. Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. The shared task and shared model introduced by QA-CTS task has also been proved to be useful for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset."
            ],
            "highlighted_evidence": [
              "Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data.",
              "All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. ",
              "Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks."
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Is all text in this dataset a question, or are there unrelated sentences in between questions?",
        "question_id": "06095a4dee77e9a570837b35fc38e77228664f91",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ",
            "evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20."
            ],
            "highlighted_evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. "
            ]
          }
        ]
      },
      {
        "question": "How many questions are in the dataset?",
        "question_id": "19c9cfbc4f29104200393e848b7b9be41913a7ac",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "2,714 ",
            "evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20."
            ],
            "highlighted_evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs."
            ]
          }
        ]
      },
      {
        "question": "What is the perWhat are the tasks evaluated?",
        "question_id": "6743c1dd7764fc652cfe2ea29097ea09b5544bc3",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Are there privacy concerns with clinical data?",
        "question_id": "14323046220b2aea8f15fba86819cbccc389ed8b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How they introduce domain-specific features into pre-trained language model?",
        "question_id": "08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "integrate clinical named entity information into pre-trained language model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model.",
              "In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word."
            ],
            "highlighted_evidence": [
              "We also propose an effective model to integrate clinical named entity information into pre-trained language model.",
              "In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text."
            ]
          }
        ]
      },
      {
        "question": "How big is QA-CTS task dataset?",
        "question_id": "975a4ac9773a4af551142c324b64a0858670d06e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20."
            ],
            "highlighted_evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. "
            ]
          }
        ]
      },
      {
        "question": "How big is dataset of pathology reports collected from Ruijing Hospital?",
        "question_id": "326e08a0f5753b90622902bd4a9c94849a24b773",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20."
            ],
            "highlighted_evidence": [
              "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs."
            ]
          }
        ]
      },
      {
        "question": "What are strong baseline models in specific tasks?",
        "question_id": "bd78483a746fda4805a7678286f82d9621bc45cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23."
            ],
            "highlighted_evidence": [
              "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/bibm/QiuZMRLS19",
    "dblp_title": "Question Answering based Clinical Text Structuring Using Pre-trained Language Model.",
    "year": "2019"
  },
  {
    "id": "1811.00942",
    "title": "Progress and Tradeoffs in Neural Language Models",
    "qas": [
      {
        "question": "What aspects have been compared between various language models?",
        "question_id": "dd155f01f6f4a14f9d25afc97504aefdc6d29c13",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",
            "evidence": [
              "For each model, we examined word-level perplexity, R@3 in next-word prediction, latency (ms/q), and energy usage (mJ/q). To explore the perplexity–recall relationship, we collected individual perplexity and recall statistics for each sentence in the test set."
            ],
            "highlighted_evidence": [
              "For each model, we examined word-level perplexity, R@3 in next-word prediction, latency (ms/q), and energy usage (mJ/q). To explore the perplexity–recall relationship, we collected individual perplexity and recall statistics for each sentence in the test set."
            ]
          }
        ]
      },
      {
        "question": "what classic language models are mentioned in the paper?",
        "question_id": "a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Kneser–Ney smoothing"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we examine the quality–performance tradeoff in the shift from non-neural to neural language models. In particular, we compare Kneser–Ney smoothing, widely accepted as the state of the art prior to NLMs, to the best NLMs today. The decrease in perplexity on standard datasets has been well documented BIBREF3 , but to our knowledge no one has examined the performances tradeoffs. With deployment on a mobile device in mind, we evaluate energy usage and inference latency on a Raspberry Pi (which shares the same ARM architecture as nearly all smartphones today). We find that a 2.5 $\\times $ reduction in perplexity on PTB comes at a staggering cost in terms of performance: inference with NLMs takes 49 $\\times $ longer and requires 32 $\\times $ more energy. Furthermore, we find that impressive reductions in perplexity translate into at best modest improvements in next-word prediction, which is arguable a better metric for evaluating software keyboards on a smartphone. The contribution of this paper is the first known elucidation of this quality–performance tradeoff. Note that we refrain from prescriptive recommendations: whether or not a tradeoff is worthwhile depends on the application. Nevertheless, NLP engineers should arguably keep these tradeoffs in mind when selecting a particular operating point."
            ],
            "highlighted_evidence": [
              "Kneser–Ney smoothing",
              "In particular, we compare Kneser–Ney smoothing, widely accepted as the state of the art prior to NLMs, to the best NLMs today."
            ]
          }
        ]
      },
      {
        "question": "What is a commonly used evaluation metric for language models?",
        "question_id": "e07df8f613dbd567a35318cd6f6f4cb959f5c82d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "perplexity"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 ."
            ],
            "highlighted_evidence": [
              "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 ."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "perplexity"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 ."
            ],
            "highlighted_evidence": [
              "recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1811-00942",
    "dblp_title": "Progress and Tradeoffs in Neural Language Models.",
    "year": "2018"
  },
  {
    "id": "1805.02400",
    "title": "Stay On-Topic: Generating Context-specific Fake Restaurant Reviews",
    "qas": [
      {
        "question": "Which dataset do they use a starting point in generating fake reviews?",
        "question_id": "1a43df221a567869964ad3b275de30af2ac35598",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the Yelp Challenge dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the Yelp Challenge dataset BIBREF2 for our fake review generation. The dataset (Aug 2017) contains 2.9 million 1 –5 star restaurant reviews. We treat all reviews as genuine human-written reviews for the purpose of this work, since wide-scale deployment of machine-generated review attacks are not yet reported (Sep 2017) BIBREF19 . As preprocessing, we remove non-printable (non-ASCII) characters and excessive white-space. We separate punctuation from words. We reserve 15,000 reviews for validation and 3,000 for testing, and the rest we use for training. NMT models require a parallel corpus of source and target sentences, i.e. a large set of (source, target)-pairs. We set up a parallel corpus by constructing (context, review)-pairs from the dataset. Next, we describe how we created our input context."
            ],
            "highlighted_evidence": [
              "We use the Yelp Challenge dataset BIBREF2 for our fake review generation. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Yelp Challenge dataset BIBREF2"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the Yelp Challenge dataset BIBREF2 for our fake review generation. The dataset (Aug 2017) contains 2.9 million 1 –5 star restaurant reviews. We treat all reviews as genuine human-written reviews for the purpose of this work, since wide-scale deployment of machine-generated review attacks are not yet reported (Sep 2017) BIBREF19 . As preprocessing, we remove non-printable (non-ASCII) characters and excessive white-space. We separate punctuation from words. We reserve 15,000 reviews for validation and 3,000 for testing, and the rest we use for training. NMT models require a parallel corpus of source and target sentences, i.e. a large set of (source, target)-pairs. We set up a parallel corpus by constructing (context, review)-pairs from the dataset. Next, we describe how we created our input context."
            ],
            "highlighted_evidence": [
              "We use the Yelp Challenge dataset BIBREF2 for our fake review generation."
            ]
          }
        ]
      },
      {
        "question": "Do they use a pretrained NMT model to help generating reviews?",
        "question_id": "98b11f70239ef0e22511a3ecf6e413ecb726f954",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How does using NMT ensure generated reviews stay on topic?",
        "question_id": "d4d771bcb59bab4f3eb9026cda7d182eb582027d",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What kind of model do they use for detection?",
        "question_id": "12f1919a3e8ca460b931c6cacc268a926399dff4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AdaBoost-based classifier"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We developed an AdaBoost-based classifier to detect our new fake reviews, consisting of 200 shallow decision trees (depth 2). The features we used are recorded in Table~\\ref{table:features_adaboost} (Appendix)."
            ],
            "highlighted_evidence": [
              "We developed an AdaBoost-based classifier to detect our new fake reviews, consisting of 200 shallow decision trees (depth 2)."
            ]
          }
        ]
      },
      {
        "question": "Does their detection tool work better than human detection?",
        "question_id": "cd1034c183edf630018f47ff70b48d74d2bb1649",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We noticed some variation in the detection of different fake review categories. The respondents in our MTurk survey had most difficulties recognizing reviews of category $(b=0.3, \\lambda=-5)$, where true positive rate was $40.4\\%$, while the true negative rate of the real class was $62.7\\%$. The precision were $16\\%$ and $86\\%$, respectively. The class-averaged F-score is $47.6\\%$, which is close to random. Detailed classification reports are shown in Table~\\ref{table:MTurk_sub} in Appendix. Our MTurk-study shows that \\emph{our NMT-Fake reviews pose a significant threat to review systems}, since \\emph{ordinary native English-speakers have very big difficulties in separating real reviews from fake reviews}. We use the review category $(b=0.3, \\lambda=-5)$ for future user tests in this paper, since MTurk participants had most difficulties detecting these reviews. We refer to this category as NMT-Fake* in this paper.",
              "Figure~\\ref{fig:adaboost_matrix_b_lambda} shows our AdaBoost classifier's class-averaged F-score at detecting different kind of fake reviews. The classifier is very effective in detecting reviews that humans have difficulties detecting. For example, the fake reviews MTurk users had most difficulty detecting ($b=0.3, \\lambda=-5$) are detected with an excellent 97\\% F-score."
            ],
            "highlighted_evidence": [
              "The respondents in our MTurk survey had most difficulties recognizing reviews of category $(b=0.3, \\lambda=-5)$, where true positive rate was $40.4\\%$, while the true negative rate of the real class was $62.7\\%$. The precision were $16\\%$ and $86\\%$, respectively. The class-averaged F-score is $47.6\\%$, which is close to random. Detailed classification reports are shown in Table~\\ref{table:MTurk_sub} in Appendix. Our MTurk-study shows that \\emph{our NMT-Fake reviews pose a significant threat to review systems}, since \\emph{ordinary native English-speakers have very big difficulties in separating real reviews from fake reviews}. We use the review category $(b=0.3, \\lambda=-5)$ for future user tests in this paper, since MTurk participants had most difficulties detecting these reviews. We refer to this category as NMT-Fake* in this paper.",
              "The classifier is very effective in detecting reviews that humans have difficulties detecting. For example, the fake reviews MTurk users had most difficulty detecting ($b=0.3, \\lambda=-5$) are detected with an excellent 97\\% F-score."
            ]
          }
        ]
      },
      {
        "question": "How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?",
        "question_id": "bd9930a613dd36646e2fc016b6eb21ab34c77621",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1,006 fake reviews and 994 real reviews"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We first investigated overall detection of any NMT-Fake reviews (1,006 fake reviews and 994 real reviews). We found that the participants had big difficulties in detecting our fake reviews. In average, the reviews were detected with class-averaged \\emph{F-score of only 56\\%}, with 53\\% F-score for fake review detection and 59\\% F-score for real review detection. The results are very close to \\emph{random detection}, where precision, recall and F-score would each be 50\\%. Results are recorded in Table~\\ref{table:MTurk_super}. Overall, the fake review generation is very successful, since human detection rate across categories is close to random."
            ],
            "highlighted_evidence": [
              "We first investigated overall detection of any NMT-Fake reviews (1,006 fake reviews and 994 real reviews)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/esorics/JuutiSMA18",
    "dblp_title": "Stay On-Topic: Generating Context-Specific Fake Restaurant Reviews.",
    "year": "2018"
  },
  {
    "id": "1907.05664",
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "qas": [
      {
        "question": "Which baselines did they compare?",
        "question_id": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it.",
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ],
            "highlighted_evidence": [
              "We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset.",
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ],
            "highlighted_evidence": [
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ]
          }
        ]
      },
      {
        "question": "How many attention layers are there in their model?",
        "question_id": "aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "one",
            "evidence": [
              "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ],
            "highlighted_evidence": [
              "The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. "
            ]
          }
        ]
      },
      {
        "question": "Is the explanation from saliency map correct?",
        "question_id": "710c1f8d4c137c8dad9972f5ceacdbf8004db208",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We showed that in some cases the saliency maps are truthful to the network's computation, meaning that they do highlight the input features that the network focused on. But we also showed that in some cases the saliency maps seem to not capture the important input features. This brought us to discuss the fact that these attributions are not sufficient by themselves, and that we need to define the counter-factual case and test it to measure how truthful the saliency maps are.",
              "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates. The saliency maps on Figure 3 correspond to the summary from Figure 1 , and we don't see the word “video\" highlighted in the input text, which seems to be important for the output."
            ],
            "highlighted_evidence": [
              "But we also showed that in some cases the saliency maps seem to not capture the important input features. ",
              "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1907-05664",
    "dblp_title": "Saliency Maps Generation for Automatic Text Summarization.",
    "year": "2019"
  },
  {
    "id": "1910.14497",
    "title": "Probabilistic Bias Mitigation in Word Embeddings",
    "qas": [
      {
        "question": "How is embedding quality assessed?",
        "question_id": "47726be8641e1b864f17f85db9644ce676861576",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate our framework on fastText embeddings trained on Wikipedia (2017), UMBC webbase corpus and statmt.org news dataset (16B tokens) BIBREF11. For simplicity, only the first 22000 words are used in all embeddings, though preliminary results indicate the findings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then embedded and for which a loss value is calculated, allowing back-propagation to adjust the embeddings. For each of the models, a fixed number of iterations is used to prevent overfitting, which can eventually hurt performance on the embedding benchmarks (See Figure FIGREF12). We evaluated the embedding after 1000 iterations, and stopped training if performance on a benchmark decreased significantly.",
              "We construct a list of candidate words to debias, taken from the words used in the WEAT gender bias statistics. Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix). We note that this list can easily be expanded to include a greater proportion of words in the corpus. For example, BIBREF4 suggested a method for identifying inappropriately gendered words using unsupervised learning.",
              "We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics."
            ],
            "highlighted_evidence": [
              "We evaluate our framework on fastText embeddings trained on Wikipedia (2017), UMBC webbase corpus and statmt.org news dataset (16B tokens) BIBREF11. For simplicity, only the first 22000 words are used in all embeddings, though preliminary results indicate the findings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then embedded and for which a loss value is calculated, allowing back-propagation to adjust the embeddings. For each of the models, a fixed number of iterations is used to prevent overfitting, which can eventually hurt performance on the embedding benchmarks (See Figure FIGREF12). We evaluated the embedding after 1000 iterations, and stopped training if performance on a benchmark decreased significantly.",
              "We construct a list of candidate words to debias, taken from the words used in the WEAT gender bias statistics. Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix). We note that this list can easily be expanded to include a greater proportion of words in the corpus. For example, BIBREF4 suggested a method for identifying inappropriately gendered words using unsupervised learning.",
              "We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics."
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are the three measures of bias which are reduced in experiments?",
        "question_id": "8958465d1eaf81c8b781ba4d764a4f5329f026aa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "RIPA, Neighborhood Metric, WEAT",
            "evidence": [
              "Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0. This method implicitly defines bias as a geometric asymmetry between words when projected onto a subspace, such as the gender subspace constructed from a set of gender pairs such as $\\mathcal {P} = \\lbrace (he,she),(man,woman),(king,queen)...\\rbrace $. The projection of a vector $v$ onto $B$ (the subspace) is defined by $v_B = \\sum _{j=1}^{k} (v \\cdot b_j) b_j$ where a subspace $B$ is defined by k orthogonal unit vectors $B = {b_1,...,b_k}$.",
              "The WEAT statistic BIBREF1 demonstrates the presence of biases in word embeddings with an effect size defined as the mean test statistic across the two word sets:",
              "Where $s$, the test statistic, is defined as: $s(w,A,B) = mean_{a \\in A} cos(w,a) - mean_{b \\in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured. Possible values range from $-2$ to 2 depending on the association of the words groups, and a value of zero indicates $X$ and $Y$ are equally associated with $A$ and $B$. See BIBREF4 for further details on WEAT.",
              "The RIPA (relational inner product association) metric was developed as an alternative to WEAT, with the critique that WEAT is likely to overestimate the bias of a target attribute BIBREF4. The RIPA metric formalizes the measure of bias used in geometric bias mitigation as the inner product association of a word vector $v$ with respect to a relation vector $b$. The relation vector is constructed from the first principal component of the differences between gender word pairs. We report the absolute value of the RIPA metric as the value can be positive or negative according to the direction of the bias. A value of zero indicates a lack of bias, and the value is bound by $[-||w||,||w||]$.",
              "The neighborhood bias metric proposed by BIBREF5 quantifies bias as the proportion of male socially-biased words among the $k$ nearest socially-biased male and female neighboring words, whereby biased words are obtained by projecting neutral words onto a gender relation vector. As we only examine the target word among the 1000 most socially-biased words in the vocabulary (500 male and 500 female), a word’s bias is measured as the ratio of its neighborhood of socially-biased male and socially-biased female words, so that a value of 0.5 in this metric would indicate a perfectly unbiased word, and values closer to 0 and 1 indicate stronger bias.",
              "FLOAT SELECTED: Table 1: Remaining Bias (as measured by RIPA and Neighborhood metrics) in fastText embeddings for baseline (top two rows) and our (bottom three) methods. Figure 2: Remaining Bias (WEAT score)"
            ],
            "highlighted_evidence": [
              "Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0.",
              "The WEAT statistic BIBREF1 demonstrates the presence of biases in word embeddings with an effect size defined as the mean test statistic across the two word sets:\n\nWhere $s$, the test statistic, is defined as: $s(w,A,B) = mean_{a \\in A} cos(w,a) - mean_{b \\in B} cos(w,a)$, and $X$,$Y$,$A$, and $B$ are groups of words for which the association is measured.",
              "The RIPA (relational inner product association) metric was developed as an alternative to WEAT, with the critique that WEAT is likely to overestimate the bias of a target attribute BIBREF4. ",
              "The neighborhood bias metric proposed by BIBREF5 quantifies bias as the proportion of male socially-biased words among the $k$ nearest socially-biased male and female neighboring words, whereby biased words are obtained by projecting neutral words onto a gender relation vector.",
              "FLOAT SELECTED: Table 1: Remaining Bias (as measured by RIPA and Neighborhood metrics) in fastText embeddings for baseline (top two rows) and our (bottom three) methods. Figure 2: Remaining Bias (WEAT score)"
            ]
          }
        ]
      },
      {
        "question": "What are the probabilistic observations which contribute to the more robust algorithm?",
        "question_id": "31b6544346e9a31d656e197ad01756813ee89422",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-14497",
    "dblp_title": "Probabilistic Bias Mitigation in Word Embeddings.",
    "year": "2019"
  },
  {
    "id": "1810.04528",
    "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
    "qas": [
      {
        "question": "Does this paper target European or Brazilian Portuguese?",
        "question_id": "519db0922376ce1e87fcdedaa626d665d9f3e8ce",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What were the word embeddings trained on?",
        "question_id": "99a10823623f78dbff9ccecb210f187105a196e9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "large Portuguese corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words. The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word. The latter was chosen for application in the present proposal.",
              "Using the word2vec model available in a public repository BIBREF14 , the proposal involves the analysis of the most similar analogies generated before and after the application of the BIBREF3 . The work is focused on the analysis of gender bias associated with professions in word embeddings. So therefore into the evaluation of the accuracy of the associations generated, aiming at achieving results as good as possible without prejudicing the evaluation metrics."
            ],
            "highlighted_evidence": [
              "In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. ",
              "Using the word2vec model available in a public repository BIBREF14 , the proposal involves the analysis of the most similar analogies generated before and after the application of the BIBREF3 . "
            ]
          }
        ]
      },
      {
        "question": "Which word embeddings are analysed?",
        "question_id": "09f0dce416a1e40cc6a24a8b42a802747d2c9363",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Continuous Bag-of-Words (CBOW)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words. The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word. The latter was chosen for application in the present proposal."
            ],
            "highlighted_evidence": [
              "The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1810-04528",
    "dblp_title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
    "year": "2018"
  },
  {
    "id": "2002.02224",
    "title": "Citation Data of Czech Apex Courts",
    "qas": [
      {
        "question": "Did they experiment on this dataset?",
        "question_id": "ac706631f2b3fa39bf173cd62480072601e44f66",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
              "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ],
            "highlighted_evidence": [
              "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
              "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ]
          }
        ]
      },
      {
        "question": "How is quality of the citation measured?",
        "question_id": "8b71ede8170162883f785040e8628a97fc6b5bcb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
              "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ],
            "highlighted_evidence": [
              "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10.",
              "At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ]
          }
        ]
      },
      {
        "question": "How big is the dataset?",
        "question_id": "fa2a384a23f5d0fe114ef6a39dced139bddac20e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "903019 references",
            "evidence": [
              "Overall, through the process described in Section SECREF3, we have retrieved three datasets of extracted references - one dataset per each of the apex courts. These datasets consist of the individual pairs containing the identification of the decision from which the reference was retrieved, and the identification of the referred documents. As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3."
            ],
            "highlighted_evidence": [
              "As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2002-02224",
    "dblp_title": "Citation Data of Czech Apex Courts.",
    "year": "2020"
  },
  {
    "id": "2003.07433",
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "qas": [
      {
        "question": "Do they evaluate only on English datasets?",
        "question_id": "53712f0ce764633dbb034e550bb6604f15c0cacd",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do the authors mention any possible confounds in this study?",
        "question_id": "0bffc3d82d02910d4816c16b390125e5df55fd01",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is the intensity of the PTSD established?",
        "question_id": "bdd8368debcb1bdad14c454aaf96695ac5186b09",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",
            "evidence": [
              "To provide an initial results, we take 50% of users' last week's (the week they responded of having PTSD) data to develop PTSD Linguistic dictionary and apply LAXARY framework to fill up surveys on rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition. In terms of intensity, Coppersmith et. al. totally fails to provide any idea however LAXARY provides extremely accurate measures of intensity estimation for PTSD sufferers (as shown in Fig FIGREF31) which can be explained simply providing LAXARY model filled out survey details. Table TABREF29 shows the details of accuracies of both PTSD detection and intensity estimation. Fig FIGREF32 shows the classification accuracy changes over the training sample sizes for each survey which shows that DOSPERT scale outperform other surveys. Fig FIGREF33 shows that if we take previous weeks (instead of only the week diagnosis of PTSD was taken), there are no significant patterns of PTSD detection."
            ],
            "highlighted_evidence": [
              " Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "defined into four categories from high risk, moderate risk, to low risk",
            "evidence": [
              "There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15. This is a psychometric scale that assesses risk taking in five content domains: financial decisions (separately for investing versus gambling), health/safety, recreational, ethical, and social decisions. Respondents rate the likelihood that they would engage in domain-specific risky activities (Part I). An optional Part II assesses respondents' perceptions of the magnitude of the risks and expected benefits of the activities judged in Part I. There are more scales that are used in risky behavior analysis of individual's daily activities such as, The Berlin Social Support Scales (BSSS) BIBREF16 and Values In Action Scale (VIAS) BIBREF17. Dryhootch America BIBREF18, BIBREF19, a veteran peer support community organization, chooses 5, 6 and 5 questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample DOSPERT scale demographic chosen by dryhootch. The threshold (in Table TABREF13) is used to calculate the risky behavior limits. For example, if one individual's weekly DOSPERT score goes over 28, he is in critical situation in terms of risk taking symptoms of PTSD. Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )",
              "High risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life.",
              "Moderate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression.",
              "Low risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD.",
              "No PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD."
            ],
            "highlighted_evidence": [
              "Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )\n\nHigh risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life.\n\nModerate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression.\n\nLow risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD.\n\nNo PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD."
            ]
          }
        ]
      },
      {
        "question": "How is LIWC incorporated into this system?",
        "question_id": "3334f50fe1796ce0df9dd58540e9c08be5856c23",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " For each user, we calculate the proportion of tweets scored positively by each LIWC category."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A threshold of 1 for $s-score$ divides scores into positive and negative classes. In a multi-class setting, the algorithm minimizes the cross entropy, selecting the model with the highest probability. For each user, we calculate the proportion of tweets scored positively by each LIWC category. These proportions are used as a feature vector in a loglinear regression model BIBREF20. Prior to training, we preprocess the text of each tweet: we replace all usernames with a single token (USER), lowercase all text, and remove extraneous whitespace. We also exclude any tweet that contained a URL, as these often pertain to events external to the user."
            ],
            "highlighted_evidence": [
              "For each user, we calculate the proportion of tweets scored positively by each LIWC category. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "to calculate the possible scores of each survey question using PTSD Linguistic Dictionary "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "LAXARY includes a modified LIWC model to calculate the possible scores of each survey question using PTSD Linguistic Dictionary to fill out the PTSD assessment surveys which provides a practical way not only to determine fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment."
            ],
            "highlighted_evidence": [
              "LAXARY includes a modified LIWC model to calculate the possible scores of each survey question using PTSD Linguistic Dictionary to fill out the PTSD assessment surveys which provides a practical way not only to determine fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment."
            ]
          }
        ]
      },
      {
        "question": "How many twitter users are surveyed using the clinically validated survey?",
        "question_id": "7081b6909cb87b58a7b85017a2278275be58bf60",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "210"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets. Fig FIGREF16 shows each of the 210 veteran twitter users' monthly average tweets. We categorize these Tweets into two groups: Tweets related to work and Tweets not related to work. That is, only the Tweets that use a form of the word “work*” (e.g. work,worked, working, worker, etc.) or “job*” (e.g. job, jobs, jobless, etc.) are identified as work-related Tweets, with the remaining categorized as non-work-related Tweets. This categorization method increases the likelihood that most Tweets in the work group are indeed talking about work or job; for instance, “Back to work. Projects are firing back up and moving ahead now that baseball is done.” This categorization results in 456 work-related Tweets, about 5.4% of all Tweets written in English (and 75 unique Twitter users). To conduct weekly-level analysis, we consider three categorizations of Tweets (i.e. overall Tweets, work-related Tweets, and non work-related Tweets) on a daily basis, and create a text file for each week for each group."
            ],
            "highlighted_evidence": [
              "We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets. "
            ]
          }
        ]
      },
      {
        "question": "Which clinically validated survey tools are used?",
        "question_id": "1870f871a5bcea418c44f81f352897a2f53d0971",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DOSPERT, BSSS and VIAS"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use an automated regular expression based searching to find potential veterans with PTSD in twitter, and then refine the list manually. First, we select different keywords to search twitter users of different categories. For example, to search self-claimed diagnosed PTSD sufferers, we select keywords related to PTSD for example, post trauma, post traumatic disorder, PTSD etc. We use a regular expression to search for statements where the user self-identifies as being diagnosed with PTSD. For example, Table TABREF27 shows a self-identified tweet posts. To search veterans, we mostly visit to different twitter accounts of veterans organizations such as \"MA Women Veterans @WomenVeterans\", \"Illinois Veterans @ILVetsAffairs\", \"Veterans Benefits @VAVetBenefits\" etc. We define an inclusion criteria as follows: one twitter user will be part of this study if he/she describes himself/herself as a veteran in the introduction and have at least 25 tweets in last week. After choosing the initial twitter users, we search for self-identified PTSD sufferers who claim to be diagnosed with PTSD in their twitter posts. We find 685 matching tweets which are manually reviewed to determine if they indicate a genuine statement of a diagnosis for PTSD. Next, we select the username that authored each of these tweets and retrieve last week's tweets via the Twitter API. We then filtered out users with less than 25 tweets and those whose tweets were not at least 75% in English (measured using an automated language ID system.) This filtering left us with 305 users as positive examples. We repeated this process for a group of randomly selected users. We randomly selected 3,000 twitter users who are veterans as per their introduction and have at least 25 tweets in last one week. After filtering (as above) in total 2,423 users remain, whose tweets are used as negative examples developing a 2,728 user's entire weeks' twitter posts where 305 users are self-claimed PTSD sufferers. We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses. Among these responses, 92 users were diagnosed as PTSD by any of the three surveys and rest of the 118 users are diagnosed with NO PTSD. Among the clinically diagnosed PTSD sufferers, 17 of them were not self-identified before. However, 7 of the self-identified PTSD sufferers are assessed with no PTSD by PTSD assessment tools. The response rates of PTSD and NO PTSD users are 27% and 12%. In summary, we have collected one week of tweets from 2,728 veterans where 305 users claimed to have diagnosed with PTSD. After distributing Dryhootch surveys, we have a dataset of 210 veteran twitter users among them 92 users are assessed with PTSD and 118 users are diagnosed with no PTSD using clinically validated surveys. The severity of the PTSD are estimated as Non-existent, light, moderate and high PTSD based on how many surveys support the existence of PTSD among the participants according to dryhootch manual BIBREF18, BIBREF19.",
              "There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15. This is a psychometric scale that assesses risk taking in five content domains: financial decisions (separately for investing versus gambling), health/safety, recreational, ethical, and social decisions. Respondents rate the likelihood that they would engage in domain-specific risky activities (Part I). An optional Part II assesses respondents' perceptions of the magnitude of the risks and expected benefits of the activities judged in Part I. There are more scales that are used in risky behavior analysis of individual's daily activities such as, The Berlin Social Support Scales (BSSS) BIBREF16 and Values In Action Scale (VIAS) BIBREF17. Dryhootch America BIBREF18, BIBREF19, a veteran peer support community organization, chooses 5, 6 and 5 questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample DOSPERT scale demographic chosen by dryhootch. The threshold (in Table TABREF13) is used to calculate the risky behavior limits. For example, if one individual's weekly DOSPERT score goes over 28, he is in critical situation in terms of risk taking symptoms of PTSD. Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )"
            ],
            "highlighted_evidence": [
              "We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses. ",
              "Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/smartcomp/AlamK20",
    "dblp_title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment.",
    "year": "2020"
  },
  {
    "id": "2003.12218",
    "title": "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision",
    "qas": [
      {
        "question": "Did they experiment with the dataset?",
        "question_id": "ce6201435cc1196ad72b742db92abd709e0f9e8d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In Figure FIGREF28, we show some examples of the annotation results in CORD-19-NER. We can see that our distantly- or weakly supervised methods achieve high quality recognizing the new entity types, requiring only several seed examples as the input. For example, we recognized “SARS-CoV-2\" as the “CORONAVIRUS\" type, “bat\" and “pangolins\" as the “WILDLIFE\" type and “Van der Waals forces\" as the “PHYSICAL_SCIENCE\" type. This NER annotation results help downstream text mining tasks in discovering the origin and the physical nature of the virus. Our NER methods are domain-independent that can be applied to corpus in different domains. In addition, we show another example of NER annotation on New York Times with our system in Figure FIGREF29.",
              "In Figure FIGREF30, we show the comparison of our annotation results with existing NER/BioNER systems. In Figure FIGREF30, we can see that only our method can identify “SARS-CoV-2\" as a coronavirus. In Figure FIGREF30, we can see that our method can identify many more entities such as “pylogenetic\" as a evolution term and “bat\" as a wildlife. In Figure FIGREF30, we can also see that our method can identify many more entities such as “racism\" as a social behavior. In summary, our distantly- and weakly-supervised NER methods are reliable for high-quality entity recognition without requiring human effort for training data annotation."
            ],
            "highlighted_evidence": [
              "In Figure FIGREF28, we show some examples of the annotation results in CORD-19-NER. We can see that our distantly- or weakly supervised methods achieve high quality recognizing the new entity types, requiring only several seed examples as the input. For example, we recognized “SARS-CoV-2\" as the “CORONAVIRUS\" type, “bat\" and “pangolins\" as the “WILDLIFE\" type and “Van der Waals forces\" as the “PHYSICAL_SCIENCE\" type. This NER annotation results help downstream text mining tasks in discovering the origin and the physical nature of the virus. Our NER methods are domain-independent that can be applied to corpus in different domains. In addition, we show another example of NER annotation on New York Times with our system in Figure FIGREF29.\n\nIn Figure FIGREF30, we show the comparison of our annotation results with existing NER/BioNER systems. In Figure FIGREF30, we can see that only our method can identify “SARS-CoV-2\" as a coronavirus. In Figure FIGREF30, we can see that our method can identify many more entities such as “pylogenetic\" as a evolution term and “bat\" as a wildlife. In Figure FIGREF30, we can also see that our method can identify many more entities such as “racism\" as a social behavior. In summary, our distantly- and weakly-supervised NER methods are reliable for high-quality entity recognition without requiring human effort for training data annotation."
            ]
          }
        ]
      },
      {
        "question": "What is the size of this dataset?",
        "question_id": "928828544e38fe26c53d81d1b9c70a9fb1cc3feb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "29,500 documents"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Named entity recognition (NER) is a fundamental step in text mining system development to facilitate the COVID-19 studies. There is critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation. We created this CORD-19-NER dataset with comprehensive named entity annotation on the CORD-19 corpus (2020-03-13). This dataset covers 75 fine-grained named entity types. CORD-19-NER is automatically generated by combining the annotation results from four sources. In the following sections, we introduce the details of CORD-19-NER dataset construction. We also show some NER annotation results in this dataset.",
              "The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13). We first merge all the meta-data (all_sources_metadata_2020-03-13.csv) with their corresponding full-text papers. Then we create a tokenized corpus (CORD-19-corpus.json) for further NER annotations."
            ],
            "highlighted_evidence": [
              "We created this CORD-19-NER dataset with comprehensive named entity annotation on the CORD-19 corpus (2020-03-13). ",
              "The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13). "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "29,500 documents in the CORD-19 corpus (2020-03-13)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13). We first merge all the meta-data (all_sources_metadata_2020-03-13.csv) with their corresponding full-text papers. Then we create a tokenized corpus (CORD-19-corpus.json) for further NER annotations."
            ],
            "highlighted_evidence": [
              "The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13). We first merge all the meta-data (all_sources_metadata_2020-03-13.csv) with their corresponding full-text papers. Then we create a tokenized corpus (CORD-19-corpus.json) for further NER annotations.\n\n"
            ]
          }
        ]
      },
      {
        "question": "Do they list all the named entity types present?",
        "question_id": "4f243056e63a74d1349488983dc1238228ca76a7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2003-12218",
    "dblp_title": "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision.",
    "year": "2020"
  },
  {
    "id": "1904.09678",
    "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
    "qas": [
      {
        "question": "how is quality measured?",
        "question_id": "8f87215f4709ee1eb9ddcc7900c6c054c970160b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.",
            "evidence": [
              "FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting."
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "how many languages exactly is the sentiment lexica for?",
        "question_id": "b04098f7507efdffcbabd600391ef32318da28b3",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "what sentiment sources do they compare with?",
        "question_id": "8fc14714eb83817341ada708b9a0b6b4c6ab5023",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . These lexica contain general domain words (as opposed to Twitter or Bible). As gold standard for twitter domain we use emoticon dataset and perform emoticon sentiment prediction BIBREF16 , BIBREF17 ."
            ],
            "highlighted_evidence": [
              "As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/AsgariBRRM20",
    "dblp_title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages.",
    "year": "2020"
  },
  {
    "id": "2003.06651",
    "title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only",
    "qas": [
      {
        "question": "Is the method described in this work a clustering-based method?",
        "question_id": "d94ac550dfdb9e4bbe04392156065c072b9d75e1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The task of Word Sense Induction (WSI) can be seen as an unsupervised version of WSD. WSI aims at clustering word senses and does not require to map each cluster to a predefined sense. Instead of that, word sense inventories are induced automatically from the clusters, treating each cluster as a single sense of a word. WSI approaches fall into three main groups: context clustering, word ego-network clustering and synonyms (or substitute) clustering.",
              "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”. Thus, our algorithm is based on graph-based word sense induction, but it also relies on vector-based operations between word embeddings to perform filtering of graph nodes. Analogously to the work of Pelevina:16, we construct a semantic relatedness graph from a list of nearest neighbours, but we filter this list using the following procedure:"
            ],
            "highlighted_evidence": [
              "The task of Word Sense Induction (WSI) can be seen as an unsupervised version of WSD. WSI aims at clustering word senses and does not require to map each cluster to a predefined sense. Instead of that, word sense inventories are induced automatically from the clusters, treating each cluster as a single sense of a word.",
              "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”. Thus, our algorithm is based on graph-based word sense induction, but it also relies on vector-based operations between word embeddings to perform filtering of graph nodes. Analogously to the work of Pelevina:16, we construct a semantic relatedness graph from a list of nearest neighbours, but we filter this list using the following procedure:"
            ],
            "highlighted_evidence": [
              "We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space."
            ]
          }
        ]
      },
      {
        "question": "How are the different senses annotated/labeled? ",
        "question_id": "eeb6e0caa4cf5fdd887e1930e22c816b99306473",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The contexts are manually labelled with WordNet senses of the target words"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.",
              "The task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9."
            ],
            "highlighted_evidence": [
              "The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.\n\nThe task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9."
            ]
          }
        ]
      },
      {
        "question": "Was any extrinsic evaluation carried out?",
        "question_id": "3c0eaa2e24c1442d988814318de5f25729696ef5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task."
            ],
            "highlighted_evidence": [
              "We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/LogachevaTSRUKA20",
    "dblp_title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only.",
    "year": "2020"
  },
  {
    "id": "1910.04269",
    "title": "Spoken Language Identification using ConvNets",
    "qas": [
      {
        "question": "Does the model use both spectrogram images and raw waveforms as features?",
        "question_id": "dc1fe3359faa2d7daa891c1df33df85558bc461b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 4: Results of the two models and all its variations"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Results of the two models and all its variations"
            ]
          }
        ]
      },
      {
        "question": "Is the performance compared against a baseline model?",
        "question_id": "922f1b740f8b13fdc8371e2a275269a44c86195e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel)."
            ],
            "highlighted_evidence": [
              "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel)."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the accuracy reported by state-of-the-art methods?",
        "question_id": "b39f2249a1489a2cef74155496511cc5d1b2a73d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Answer with content missing: (Table 1)\nPrevious state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",
            "evidence": [
              "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel)."
            ],
            "highlighted_evidence": [
              "In Table TABREF1, we summarize the quantitative results of the above previous studies.",
              "In Table TABREF1, we summarize the quantitative results of the above previous studies."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ami/SarthakSM19",
    "dblp_title": "Spoken Language Identification Using ConvNets.",
    "year": "2019"
  },
  {
    "id": "1906.00378",
    "title": "Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data",
    "qas": [
      {
        "question": "Which vision-based approaches does this approach outperform?",
        "question_id": "591231d75ff492160958f8aa1e6bfcbbcd85a776",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CNN-mean",
              "CNN-avgmax"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:",
              "CNN-mean: taking the similarity score of the averaged feature of the two image sets.",
              "CNN-avgmax: taking the average of the maximum similarity scores of two image sets."
            ],
            "highlighted_evidence": [
              "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets."
            ]
          }
        ]
      },
      {
        "question": "What baseline is used for the experimental setup?",
        "question_id": "9e805020132d950b54531b1a2620f61552f06114",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CNN-mean",
              "CNN-avgmax"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:",
              "CNN-mean: taking the similarity score of the averaged feature of the two image sets.",
              "CNN-avgmax: taking the average of the maximum similarity scores of two image sets."
            ],
            "highlighted_evidence": [
              "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets."
            ]
          }
        ]
      },
      {
        "question": "Which languages are used in the multi-lingual caption model?",
        "question_id": "95abda842c4df95b4c5e84ac7d04942f1250b571",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "German-English, French-English, and Japanese-English"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:"
            ],
            "highlighted_evidence": [
              "We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "multiple language pairs including German-English, French-English, and Japanese-English."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:"
            ],
            "highlighted_evidence": [
              "We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/ChenJH19",
    "dblp_title": "Unsupervised Bilingual Lexicon Induction from Mono-Lingual Multimodal Data.",
    "year": "2019"
  },
  {
    "id": "1912.13072",
    "title": "AraNet: A Deep Learning Toolkit for Arabic Social Media",
    "qas": [
      {
        "question": "Did they experiment on all the tasks?",
        "question_id": "2419b38624201d678c530eba877c0c016cccd49f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Implementation & Models Parameters. For all our tasks, we use the BERT-Base Multilingual Cased model released by the authors . The model is trained on 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 15 epochs and choose the best model based on performance on a development set. We use the same hyper-parameters in all of our BERT models. We fine-tune BERT on each respective labeled dataset for each task. For BERT input, we apply WordPiece tokenization, setting the maximal sequence length to 50 words/WordPieces. For all tasks, we use a TensorFlow implementation. An exception is the sentiment analysis task, where we used a PyTorch implementation with the same hyper-parameters but with a learning rate $2e-6$.",
              "We presented AraNet, a deep learning toolkit for a host of Arabic social media processing. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. AraNet has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions. Our toolkit can be used to make important discoveries on the wide region of the Arab world, and can enhance our understating of Arab online communication. AraNet will be publicly available upon acceptance."
            ],
            "highlighted_evidence": [
              "For all tasks, we use a TensorFlow implementation.",
              "AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. "
            ]
          }
        ]
      },
      {
        "question": "What models did they compare to?",
        "question_id": "b99d100d17e2a121c3c8ff789971ce66d1d40a4d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet ."
            ],
            "highlighted_evidence": [
              "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets."
            ]
          }
        ]
      },
      {
        "question": "What datasets are used in training?",
        "question_id": "578d0b23cb983b445b1a256a34f969b34d332075",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Arap-Tweet BIBREF19 ",
              "an in-house Twitter dataset for gender",
              "the MADAR shared task 2 BIBREF20",
              "the LAMA-DINA dataset from BIBREF22",
              "LAMA-DIST",
              "Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24",
              "BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <برشة> /barsha/ ‘many’ for Tunisian Arabic and <وايد> /wayed/ ‘many’ for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data.",
              "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 “male\", 528 “female\", and 215 unknown users. We remove the “unknown\" category and balance the dataset to have 528 from each of the two `male\" and “female\" categories. We ended with 69,509 tweets for `male\" and 67,511 tweets for “female\". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST.",
              "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data.",
              "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun انا> (Eng. “I\") + a seed word expressing an emotion, e.g., فرحان> (Eng. “happy\"). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score.",
              "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony\"). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.",
              "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\\lbrace `positive^{\\prime }, `negative^{\\prime }\\rbrace $ by following rules:"
            ],
            "highlighted_evidence": [
              "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet.",
              "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries.",
              "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels.",
              "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels.",
              "The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. ",
              "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24.",
              "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              " Arap-Tweet ",
              "UBC Twitter Gender Dataset",
              "MADAR ",
              "LAMA-DINA ",
              "IDAT@FIRE2019",
              "15 datasets related to sentiment analysis of Arabic, including MSA and dialects"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <برشة> /barsha/ ‘many’ for Tunisian Arabic and <وايد> /wayed/ ‘many’ for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data.",
              "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 “male\", 528 “female\", and 215 unknown users. We remove the “unknown\" category and balance the dataset to have 528 from each of the two `male\" and “female\" categories. We ended with 69,509 tweets for `male\" and 67,511 tweets for “female\". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST.",
              "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data.",
              "Data and Models ::: Emotion",
              "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun انا> (Eng. “I\") + a seed word expressing an emotion, e.g., فرحان> (Eng. “happy\"). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score.",
              "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony\"). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.",
              "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\\lbrace `positive^{\\prime }, `negative^{\\prime }\\rbrace $ by following rules:"
            ],
            "highlighted_evidence": [
              "For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet.",
              "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. ",
              "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. ",
              "Emotion\nWe make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. ",
              "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24.",
              "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-13072",
    "dblp_title": "AraNet: A Deep Learning Toolkit for Arabic Social Media.",
    "year": "2019"
  },
  {
    "id": "1712.09127",
    "title": "Generative Adversarial Nets for Multiple Text Corpora",
    "qas": [
      {
        "question": "Which GAN do they use?",
        "question_id": "6548db45fc28e8a8b51f114635bad14a13eaec5b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 ."
            ],
            "highlighted_evidence": [
              "We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 ."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "weGAN",
              "deGAN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Suppose we have a number of different corpora INLINEFORM0 , which for example can be based on different categories or sentiments of text documents. We suppose that INLINEFORM1 , INLINEFORM2 , where each INLINEFORM3 represents a document. The words in all corpora are collected in a dictionary, and indexed from 1 to INLINEFORM4 . We name the GAN model to train cross-corpus word embeddings as “weGAN,” where “we” stands for “word embeddings,” and the GAN model to generate document embeddings for multiple corpora as “deGAN,” where “de” stands for “document embeddings.”"
            ],
            "highlighted_evidence": [
              "We name the GAN model to train cross-corpus word embeddings as “weGAN,” where “we” stands for “word embeddings,” and the GAN model to generate document embeddings for multiple corpora as “deGAN,” where “de” stands for “document embeddings.”"
            ]
          }
        ]
      },
      {
        "question": "Do they evaluate grammaticality of generated text?",
        "question_id": "4c4f76837d1329835df88b0921f4fe8bda26606f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We hypothesize that because weGAN takes into account document labels in a semi-supervised way, the embeddings trained from weGAN can better incorporate the labeling information and therefore, produce document embeddings which are better separated. The results are shown in Table 1 and averaged over 5 randomized runs. Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. Because the Rand index captures matching accuracy, we observe from the Table 1 that weGAN tends to improve both metrics."
            ],
            "highlighted_evidence": [
              "Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level. "
            ]
          }
        ]
      },
      {
        "question": "Which corpora do they use?",
        "question_id": "819d2e97f54afcc7cdb3d894a072bcadfba9b747",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CNN, TIME, 20 Newsgroups, and Reuters-21578"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the experiments, we consider four data sets, two of them newly created and the remaining two already public: CNN, TIME, 20 Newsgroups, and Reuters-21578. The code and the two new data sets are available at github.com/baiyangwang/emgan. For the pre-processing of all the documents, we transformed all characters to lower case, stemmed the documents, and ran the word2vec model on each corpora to obtain word embeddings with a size of 300. In all subsequent models, we only consider the most frequent INLINEFORM0 words across all corpora in a data set."
            ],
            "highlighted_evidence": [
              "In the experiments, we consider four data sets, two of them newly created and the remaining two already public: CNN, TIME, 20 Newsgroups, and Reuters-21578. The code and the two new data sets are available at github.com/baiyangwang/emgan. For the pre-processing of all the documents, we transformed all characters to lower case, stemmed the documents, and ran the word2vec model on each corpora to obtain word embeddings with a size of 300. In all subsequent models, we only consider the most frequent INLINEFORM0 words across all corpora in a data set."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcnn/EbrahimiKW21",
    "dblp_title": "Generative Adversarial Nets for Multiple Text Corpora.",
    "year": "2021"
  },
  {
    "id": "2001.00137",
    "title": "Stacked DeBERT: All Attention in Incomplete Data for Text Classification",
    "qas": [
      {
        "question": "Do they report results only on English datasets?",
        "question_id": "637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.",
              "The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names."
            ],
            "highlighted_evidence": [
              "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12.",
              "The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names."
            ]
          }
        ]
      },
      {
        "question": "How do the authors define or exemplify 'incorrect words'?",
        "question_id": "4b8257cdd9a60087fa901da1f4250e7d910896df",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "typos in spellings or ungrammatical words",
            "evidence": [
              "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. Panko BIBREF0 goes as far as claiming that human accuracy regarding research paper writing is none when considering the entire document. This has been aggravated with the advent of internet and social networks, which allowed language and modern communication to be been rapidly transformed BIBREF1, BIBREF2. Take Twitter for instance, where information is expected to be readily communicated in short and concise sentences with little to no regard to correct sentence grammar or word spelling BIBREF3."
            ],
            "highlighted_evidence": [
              "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences. "
            ]
          }
        ]
      },
      {
        "question": "How many vanilla transformers do they use after applying an embedding layer?",
        "question_id": "7e161d9facd100544fa339b06f656eb2fc64ed28",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they test their approach on a dataset without incomplete data?",
        "question_id": "abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. The incomplete sentences with STT error are obtained in a 2-step process shown in Fig. FIGREF22. The first step is to apply a TTS module to the available complete sentence. Here, we apply gtts , a Google Text-to-Speech python library, and macsay , a terminal command available in Mac OS as say. The second step consists of applying an STT module to the obtained audio files in order to obtain text containing STT errors. The STT module used here was witai , freely available and maintained by Wit.ai. The mentioned TTS and STT modules were chosen according to code availability and whether it's freely available or has high daily usage limitations."
            ],
            "highlighted_evidence": [
              "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11.",
              "In the intent classification task, we are presented with a corpus that suffers from the opposite problem of the Twitter sentiment classification corpus. In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words. Analysis on this dataset adds value to our work by enabling evaluation of our model's robustness to different rates of data incompleteness."
            ],
            "highlighted_evidence": [
              "For this reason, we choose Kaggle's two-class Sentiment140 dataset BIBREF18, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table TABREF11.",
              "In the intent classification corpus, we have the complete sentences and intent labels but lack their corresponding incomplete sentences, and since our task revolves around text classification in incomplete or incorrect data, it is essential that we obtain this information. To remedy this issue, we apply a Text-to-Speech (TTS) module followed by a Speech-to-Text (STT) module to the complete sentences in order to obtain incomplete sentences with STT error. Due to TTS and STT modules available being imperfect, the resulting sentences have a reasonable level of noise in the form of missing or incorrectly transcribed words."
            ]
          }
        ]
      },
      {
        "question": "Should their approach be applied only when dealing with incomplete data?",
        "question_id": "4debd7926941f1a02266b1a7be2df8ba6e79311a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences."
            ],
            "highlighted_evidence": [
              "We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We propose Stacked Denoising BERT (DeBERT) as a novel encoding scheming for the task of incomplete intent classification and sentiment classification from incorrect sentences, such as tweets and text with STT error. The proposed model, illustrated in Fig. FIGREF4, is structured as a stacking of embedding layers and vanilla transformer layers, similarly to the conventional BERT BIBREF10, followed by layers of novel denoising transformers. The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. By reconstructing these hidden embeddings, we are able to improve the encoding scheme in BERT."
            ],
            "highlighted_evidence": [
              "The main purpose of this model is to improve the robustness and efficiency of BERT when applied to incomplete data by reconstructing hidden embeddings from sentences with missing words. "
            ]
          }
        ]
      },
      {
        "question": "By how much do they outperform other models in the sentiment in intent classification tasks?",
        "question_id": "3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average",
            "evidence": [
              "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences.",
              "Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.",
              "FLOAT SELECTED: Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5))."
            ],
            "highlighted_evidence": [
              "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. ",
              "Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.",
              "FLOAT SELECTED: Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5))."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/nn/SergioL21",
    "dblp_title": "Stacked DeBERT: All attention in incomplete data for text classification.",
    "year": "2021"
  },
  {
    "id": "1910.03042",
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "qas": [
      {
        "question": "What is the sample size of people used to measure user satisfaction?",
        "question_id": "44c7c1fbac80eaea736622913d65fe6453d72828",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "34,432 user conversations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets)."
            ],
            "highlighted_evidence": [
              " Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\")."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "34,432 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Amazon Alexa Prize BIBREF0 provides a platform to collect real human-machine conversation data and evaluate performance on speech-based social conversational systems. Our system, Gunrock BIBREF1 addresses several limitations of prior chatbots BIBREF2, BIBREF3, BIBREF4 including inconsistency and difficulty in complex sentence understanding (e.g., long utterances) and provides several contributions: First, Gunrock's multi-step language understanding modules enable the system to provide more useful information to the dialog manager, including a novel dialog act scheme. Additionally, the natural language understanding (NLU) module can handle more complex sentences, including those with coreference. Second, Gunrock interleaves actions to elicit users' opinions and provide responses to create an in-depth, engaging conversation; while a related strategy to interleave task- and non-task functions in chatbots has been proposed BIBREF5, no chatbots to our knowledge have employed a fact/opinion interleaving strategy. Finally, we use an extensive persona database to provide coherent profile information, a critical challenge in building social chatbots BIBREF3. Compared to previous systems BIBREF4, Gunrock generates more balanced conversations between human and machine by encouraging and understanding more human inputs (see Table TABREF2 for an example).",
              "From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets)."
            ],
            "highlighted_evidence": [
              "Amazon Alexa Prize BIBREF0 provides a platform to collect real human-machine conversation data and evaluate performance on speech-based social conversational systems. Our system, Gunrock BIBREF1 addresses several limitations of prior chatbots BIBREF2, BIBREF3, BIBREF4 including inconsistency and difficulty in complex sentence understanding (e.g., long utterances) and provides several contributions: First, Gunrock's multi-step language understanding modules enable the system to provide more useful information to the dialog manager, including a novel dialog act scheme. ",
              "From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock.",
              "We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations."
            ]
          }
        ]
      },
      {
        "question": "What are all the metrics to measure user engagement?",
        "question_id": "3e0c9469821cb01a75e1818f2acb668d071fcf40",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "overall rating",
              "mean number of turns"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions."
            ],
            "highlighted_evidence": [
              " We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "overall rating",
              "mean number of turns"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions."
            ],
            "highlighted_evidence": [
              "We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions."
            ]
          }
        ]
      },
      {
        "question": "What the system designs introduced?",
        "question_id": "a725246bac4625e6fe99ea236a96ccb21b5f30c6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Amazon Conversational Bot Toolkit",
              "natural language understanding (NLU) (nlu) module",
              "dialog manager",
              "knowledge bases",
              "natural language generation (NLG) (nlg) module",
              "text to speech (TTS) (tts)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1."
            ],
            "highlighted_evidence": [
              "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts)."
            ]
          }
        ]
      },
      {
        "question": "Do they specify the model they use for Gunrock?",
        "question_id": "516626825e51ca1e8a3e0ac896c538c9d8a747c8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF3 provides an overview of Gunrock's architecture. We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1."
            ],
            "highlighted_evidence": [
              "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response.",
              "While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1."
            ]
          }
        ]
      },
      {
        "question": "Do they gather explicit user satisfaction data on Gunrock?",
        "question_id": "77af93200138f46bb178c02f710944a01ed86481",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). Users engaged with Gunrock for an average of 20.92 overall turns (median 13.0), with an average of 6.98 words per utterance, and had an average conversation time of 7.33 minutes (median: 2.87 min.). We conducted three principal analyses: users' response depth (wordcount), backstory queries (backstorypersona), and interleaving of personal and factual responses (pets)."
            ],
            "highlighted_evidence": [
              "Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\")."
            ]
          }
        ]
      },
      {
        "question": "How do they correlate user backstory queries to user satisfaction?",
        "question_id": "71538776757a32eee930d297f6667cd0ec2e9231",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We assessed the degree of conversational depth by measuring users' mean word count. Prior work has found that an increase in word count has been linked to improved user engagement (e.g., in a social dialog system BIBREF13). For each user conversation, we extracted the overall rating, the number of turns of the interaction, and the user's per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.",
              "Results showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts."
            ],
            "highlighted_evidence": [
              "We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.\n\nResults showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/YuCYCWZZJCBISDB19",
    "dblp_title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations.",
    "year": "2019"
  },
  {
    "id": "2002.06644",
    "title": "Towards Detection of Subjective Bias using Contextualized Word Embeddings",
    "qas": [
      {
        "question": "Do the authors report only on English?",
        "question_id": "830de0bd007c4135302138ffa8f4843e4915e440",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the baseline for the experiments?",
        "question_id": "680dc3e56d1dc4af46512284b9996a1056f89ded",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "FastText",
              "BiLSTM",
              "BERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
              "BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.",
              "BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset."
            ],
            "highlighted_evidence": [
              "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
              "BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.",
              "BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "FastText",
              "BERT ",
              "two-layer BiLSTM architecture with GloVe word embeddings"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Baselines and Approach",
              "In this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection.",
              "Baselines and Approach ::: Baselines",
              "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
              "BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.",
              "BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.",
              "FLOAT SELECTED: Table 1: Experimental Results for the Subjectivity Detection Task"
            ],
            "highlighted_evidence": [
              "Baselines and Approach\nIn this section, we outline baseline models like $BERT_{large}$. We further propose three approaches: optimized BERT-based models, distilled pretrained models, and the use of ensemble methods for the task of subjectivity detection.\n\n",
              "Baselines and Approach ::: Baselines\nFastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.\n\nBiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline.\n\nBERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset.",
              "FLOAT SELECTED: Table 1: Experimental Results for the Subjectivity Detection Task"
            ]
          }
        ]
      },
      {
        "question": "Which experiments are perfomed?",
        "question_id": "bd5379047c2cf090bea838c67b6ed44773bcd56f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They used BERT-based models to detect subjective language in the WNC corpus",
            "evidence": [
              "In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints. Writers and editors of texts like news and textbooks try to avoid the use of biased language, yet subjective bias is pervasive in these texts. More than $56\\%$ of Americans believe that news sources do not report the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language.",
              "In this work, we investigate the application of BERT-based models for the task of subjective language detection. We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\\%$ of Accuracy.",
              "Experiments ::: Dataset and Experimental Settings",
              "We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019. We randomly shuffled these sentences and split this dataset into two parts in a $90:10$ Train-Test split and perform the evaluation on the held-out test dataset."
            ],
            "highlighted_evidence": [
              "In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints.",
              "In this work, we investigate the application of BERT-based models for the task of subjective language detection.",
              "Experiments ::: Dataset and Experimental Settings\nWe perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/www/PantDM20",
    "dblp_title": "Towards Detection of Subjective Bias using Contextualized Word Embeddings.",
    "year": "2020"
  },
  {
    "id": "1809.08731",
    "title": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",
    "qas": [
      {
        "question": "Is ROUGE their only baseline?",
        "question_id": "7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.",
              "We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.",
              "We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as",
              "Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:",
              "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments."
            ],
            "highlighted_evidence": [
              "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.",
              "We compare to the best n-gram-overlap metrics from toutanova2016dataset;",
              "We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length.",
              "Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:",
              "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",
            "evidence": [
              "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.",
              "We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.",
              "We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as",
              "Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:",
              "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments."
            ],
            "highlighted_evidence": [
              "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks.",
              "We compare to the best n-gram-overlap metrics from toutanova2016dataset;",
              "We further compare to the negative LM cross-entropy",
              "Our next baseline is perplexity, ",
              "Due to its popularity, we also performed initial experiments with BLEU BIBREF17 "
            ]
          }
        ]
      },
      {
        "question": "what language models do they use?",
        "question_id": "3ac30bd7476d759ea5d9a5abf696d4dfc480175b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LSTM LMs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.",
              "We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data."
            ],
            "highlighted_evidence": [
              "We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus.",
              "We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data."
            ]
          }
        ]
      },
      {
        "question": "what questions do they ask human judges?",
        "question_id": "0e57a0983b4731eba9470ba964d131045c8c7ea7",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/conll/KannRF18",
    "dblp_title": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!",
    "year": "2018"
  },
  {
    "id": "1707.00995",
    "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation",
    "qas": [
      {
        "question": "What misbehavior is identified?",
        "question_id": "f0317e48dafe117829e88e54ed2edab24b86edb1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:"
            ],
            "highlighted_evidence": [
              "Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:"
            ],
            "highlighted_evidence": [
              "Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations."
            ]
          }
        ]
      },
      {
        "question": "What is the baseline used?",
        "question_id": "ec91b87c3f45df050e4e16018d2bf5b62e4ca298",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which attention mechanisms do they compare?",
        "question_id": "f129c97a81d81d32633c94111018880a7ffe16d1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Soft attention",
              "Hard Stochastic attention",
              "Local Attention"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image.",
              "Soft attention",
              "Hard Stochastic attention",
              "Local Attention"
            ],
            "highlighted_evidence": [
              "We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 .",
              "Soft attention",
              "Hard Stochastic attention",
              "Local Attention"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/DelbrouckD17",
    "dblp_title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation.",
    "year": "2017"
  },
  {
    "id": "1809.04960",
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "qas": [
      {
        "question": "Which paired corpora did they use in the other experiment?",
        "question_id": "100cf8b72d46da39fedfe77ec939fb44f25de77f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In addition to the unsupervised training, we explore a semi-supervised training framework to combine the proposed unsupervised model and the supervised model. In this scenario we have a paired dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1 . The supervised model is trained on INLINEFORM2 so that we can learn the matching or mapping between articles and comments. By sharing the encoder of the supervised model and the unsupervised model, we can jointly train both the models with a joint objective function: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              " In this scenario we have a paired dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1 . The supervised model is trained on INLINEFORM2 so that we can learn the matching or mapping between articles and comments. By sharing the encoder of the supervised model and the unsupervised model, we can jointly train both the models with a joint objective function: DISPLAYFORM0"
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Chinese dataset BIBREF0"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words.",
              "We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M). It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model."
            ],
            "highlighted_evidence": [
              "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model.",
              "We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M)."
            ]
          }
        ]
      },
      {
        "question": "By how much does their system outperform the lexicon-based models?",
        "question_id": "8cc56fc44136498471754186cfa04056017b4e54",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029",
            "evidence": [
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",
              "Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information.",
              "FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)",
              "Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.",
              "FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)"
            ],
            "highlighted_evidence": [
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",
              "Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. ",
              "FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)",
              "Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation.",
              "FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)"
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.",
            "evidence": [
              "TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model.",
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",
              "Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information.",
              "Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.",
              "FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)",
              "FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)"
            ],
            "highlighted_evidence": [
              "TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline.",
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",
              "Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation.",
              "Table TABREF32 shows the performance for our models and the baselines in generative evaluation.",
              "FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)",
              "FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)"
            ]
          }
        ]
      },
      {
        "question": "Which lexicon-based models did they compare with?",
        "question_id": "5fa431b14732b3c47ab6eec373f51f2bca04f614",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "TF-IDF",
              "NVDM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model.",
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic."
            ],
            "highlighted_evidence": [
              "TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline.",
              "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 ."
            ]
          }
        ]
      },
      {
        "question": "How many comments were used?",
        "question_id": "33ccbc401b224a48fba4b167e86019ffad1787fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "from 50K to 4.8M"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We analyze the performance of the proposed method under the semi-supervised setting. We train the supervised IR model with different numbers of paired data. Figure FIGREF39 shows the curve (blue) of the recall1 score. As expected, the performance grows as the paired dataset becomes larger. We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M). It shows that IR+Proposed can outperform the supervised IR model given the same paired dataset. It concludes that the proposed model can exploit the unpaired data to further improve the performance of the supervised model."
            ],
            "highlighted_evidence": [
              "We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M)."
            ]
          }
        ]
      },
      {
        "question": "How many articles did they have?",
        "question_id": "cca74448ab0c518edd5fc53454affd67ac1a201c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "198,112"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words."
            ],
            "highlighted_evidence": [
              "The dataset consists of 198,112 news articles."
            ]
          }
        ]
      },
      {
        "question": "What news comment dataset was used?",
        "question_id": "b69ffec1c607bfe5aa4d39254e0770a3433a191b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Chinese dataset BIBREF0"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words."
            ],
            "highlighted_evidence": [
              "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1809-04960",
    "dblp_title": "Unsupervised Machine Commenting with Neural Variational Topic Model.",
    "year": "2018"
  },
  {
    "id": "1909.08402",
    "title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification",
    "qas": [
      {
        "question": "By how much do they outperform standard BERT?",
        "question_id": "f5cf8738e8d211095bb89350ed05ee7f9997eb19",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "up to four percentage points in accuracy"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper we presented a way of enriching BERT with knowledge graph embeddings and additional metadata. Exploiting the linked knowledge that underlies Wikidata improves performance for our task of document classification. With this approach we improve the standard BERT models by up to four percentage points in accuracy. Furthermore, our results reveal that with task-specific information such as author names and publication metadata improves the classification task essentially compared a text-only approach. Especially, when metadata feature engineering is less trivial, adding additional task-specific information from an external knowledge source such as Wikidata can help significantly. The source code of our experiments and the trained models are publicly available."
            ],
            "highlighted_evidence": [
              "With this approach we improve the standard BERT models by up to four percentage points in accuracy."
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use?",
        "question_id": "bed527bcb0dd5424e69563fba4ae7e6ea1fca26a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2019 GermEval shared task on hierarchical text classification"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we work with the dataset of the 2019 GermEval shared task on hierarchical text classification BIBREF0 and use the predefined set of labels to evaluate our approach to this classification task."
            ],
            "highlighted_evidence": [
              "hierarchical",
              "In this paper, we work with the dataset of the 2019 GermEval shared task on hierarchical text classification BIBREF0 and use the predefined set of labels to evaluate our approach to this classification task."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "GermEval 2019 shared task"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our experiments are modelled on the GermEval 2019 shared task and deal with the classification of books. The dataset contains 20,784 German books. Each record has:"
            ],
            "highlighted_evidence": [
              "Our experiments are modelled on the GermEval 2019 shared task and deal with the classification of books. The dataset contains 20,784 German books."
            ]
          }
        ]
      },
      {
        "question": "How do they combine text representations with the knowledge graph embeddings?",
        "question_id": "aeab5797b541850e692f11e79167928db80de1ea",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "all three representations are concatenated and passed into a MLP"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The BERT architecture uses 12 hidden layers, each layer consists of 768 units. To derive contextualized representations from textual features, the book title and blurb are concatenated and then fed through BERT. To minimize the GPU memory consumption, we limit the input length to 300 tokens (which is shorter than BERT's hard-coded limit of 512 tokens). Only 0.25% of blurbs in the training set consist of more than 300 words, so this cut-off can be expected to have minor impact.",
              "The non-text features are generated in a separate preprocessing step. The metadata features are represented as a ten-dimensional vector (two dimensions for gender, see Section SECREF10). Author embedding vectors have a length of 200 (see Section SECREF22). In the next step, all three representations are concatenated and passed into a MLP with two layers, 1024 units each and ReLu activation function. During training, the MLP is supposed to learn a non-linear combination of its input representations. Finally, the output layer does the actual classification. In the SoftMax output layer each unit corresponds to a class label. For sub-task A the output dimension is eight. We treat sub-task B as a standard multi-label classification problem, i. e., we neglect any hierarchical information. Accordingly, the output layer for sub-task B has 343 units. When the value of an output unit is above a given threshold the corresponding label is predicted, whereby thresholds are defined separately for each class. The optimum was found by varying the threshold in steps of $0.1$ in the interval from 0 to 1."
            ],
            "highlighted_evidence": [
              "To derive contextualized representations from textual features, the book title and blurb are concatenated and then fed through BERT",
              "The non-text features are generated in a separate preprocessing step. The metadata features are represented as a ten-dimensional vector (two dimensions for gender, see Section SECREF10). Author embedding vectors have a length of 200 (see Section SECREF22). In the next step, all three representations are concatenated and passed into a MLP with two layers, 1024 units each and ReLu activation function."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/konvens/OstendorffBBSRG19",
    "dblp_title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification.",
    "year": "2019"
  },
  {
    "id": "1909.11189",
    "title": "Diachronic Topics in New High German Poetry",
    "qas": [
      {
        "question": "What is the algorithm used for the classification tasks?",
        "question_id": "bfa3776c30cb30e0088e185a5908e5172df79236",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Random Forest Ensemble classifiers"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slots (instead of 25) to ease the task."
            ],
            "highlighted_evidence": [
              "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. "
            ]
          }
        ]
      },
      {
        "question": "Is the outcome of the LDA analysis evaluated in any way?",
        "question_id": "a2a66726a5dca53af58aafd8494c4de833a06f14",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%."
            ],
            "highlighted_evidence": [
              "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%."
            ]
          }
        ]
      },
      {
        "question": "What is the corpus used in the study?",
        "question_id": "ee87608419e4807b9b566681631a8cd72197a71a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "TextGrid Repository"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work). We find that around 51k texts are annotated with the label ’verse’ (TGRID-V), not distinguishing between ’lyric verse’ and ’epic verse’. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Also, the poems are distributed over 229 authors, where the average author contributed 240 poems (median 131 poems). A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French, Dutch and Latin (over 400 texts). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work."
            ],
            "highlighted_evidence": [
              "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "The Digital Library in the TextGrid Repository"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work). We find that around 51k texts are annotated with the label ’verse’ (TGRID-V), not distinguishing between ’lyric verse’ and ’epic verse’. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Also, the poems are distributed over 229 authors, where the average author contributed 240 poems (median 131 poems). A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French, Dutch and Latin (over 400 texts). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work."
            ],
            "highlighted_evidence": [
              "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-11189",
    "dblp_title": "Diachronic Topics in New High German Poetry.",
    "year": "2019"
  },
  {
    "id": "1810.05320",
    "title": "Important Attribute Identification in Knowledge Graph",
    "qas": [
      {
        "question": "What are the traditional methods to identifying important attributes?",
        "question_id": "cda4612b4bda3538d19f4b43dde7bc30c1eda4e5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "automated attribute-value extraction",
              "score the attributes using the Bayes model",
              "evaluate their importance with several different frequency metrics",
              "aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model",
              "OntoRank algorithm"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model. Those approaches typically suffer from the poor quality of the pattern rules, and the ranking process is used to identify relatively more precise attributes from all attribute candidates.",
              "As for an already existing knowledge graph, there is plenty of work in literature dealing with ranking entities by relevance without or with a query. In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine. In BIBREF6 , Hogan et al. presented an approach that adapted the well-known PageRank/HITS algorithms to semantic web data, which took advantage of property values to rank entities. In BIBREF7 , BIBREF8 , authors also focused on ranking entities, sorting the semantic web resources based on importance, relevance and query length, and aggregating the features together with an overall ranking model."
            ],
            "highlighted_evidence": [
              "In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model.",
              "In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "TextRank",
              "Word2vec BIBREF19",
              "GloVe BIBREF20"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The existing co-occurrence methods do not suit our application scenario at all, since exact string matching is too strong a requirement and initial trial has shown its incompetency. In stead we implemented an improved version of their method based on TextRank as our baseline. In addition, we also tested multiple semantic matching algorithms for comparison with our chosen method.",
              "TextRank: TextRank is a graph-based ranking model for text processing. BIBREF18 It is an unsupervised algorithm for keyword extraction. Since product attributes are usually the keywords in enquiries, we can compare these keywords with the category attributes and find the most important attributes. This method consists of three steps. The first step is to merge all enquiries under one category as an article. The second step is to extract the top 50 keywords for each category. The third step is to find the most important attributes by comparing top keywords with category attributes.",
              "Word2vec BIBREF19 : We use the word vector trained by BIBREF19 as the distributed representation of words. Then we get the enquiry sentence representation and category attribute representation. Finally we collect the statistics about the matched attributes of each category, and select the most frequent attributes under the same category.",
              "GloVe BIBREF20 : GloVe is a global log-bilinear regression model for the unsupervised learning of word representations, which utilizes the ratios of word-word co-occurrence probabilities. We use the GloVe method to train the distributed representation of words. And attribute selection procedure is the same as word2vec."
            ],
            "highlighted_evidence": [
              "The existing co-occurrence methods do not suit our application scenario at all, since exact string matching is too strong a requirement and initial trial has shown its incompetency. In stead we implemented an improved version of their method based on TextRank as our baseline. In addition, we also tested multiple semantic matching algorithms for comparison with our chosen method.\n\nTextRank: TextRank is a graph-based ranking model for text processing. BIBREF18 It is an unsupervised algorithm for keyword extraction. Since product attributes are usually the keywords in enquiries, we can compare these keywords with the category attributes and find the most important attributes. This method consists of three steps. The first step is to merge all enquiries under one category as an article. The second step is to extract the top 50 keywords for each category. The third step is to find the most important attributes by comparing top keywords with category attributes.\n\nWord2vec BIBREF19 : We use the word vector trained by BIBREF19 as the distributed representation of words. Then we get the enquiry sentence representation and category attribute representation. Finally we collect the statistics about the matched attributes of each category, and select the most frequent attributes under the same category.\n\nGloVe BIBREF20 : GloVe is a global log-bilinear regression model for the unsupervised learning of word representations, which utilizes the ratios of word-word co-occurrence probabilities. We use the GloVe method to train the distributed representation of words. And attribute selection procedure is the same as word2vec."
            ]
          }
        ]
      },
      {
        "question": "What do you use to calculate word/sub-word embeddings",
        "question_id": "e12674f0466f8c0da109b6076d9939b30952c7da",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "FastText"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best. We sample and analyze the category attributes and find that many self-filled attributes contain misspellings. The FastText algorithm represents words by a sum of its character n-grams and it is much robust against problems like misspellings. In summary, FastText has greater advantages in dealing with natural language corpus usually with spelling mistakes."
            ],
            "highlighted_evidence": [
              "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best."
            ]
          }
        ]
      },
      {
        "question": "What user generated text data do you use?",
        "question_id": "9fe6339c7027a1a0caffa613adabe8b5bb6a7d4a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1810-05320",
    "dblp_title": "Important Attribute Identification in Knowledge Graph.",
    "year": "2018"
  },
  {
    "id": "2003.08529",
    "title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections",
    "qas": [
      {
        "question": "Did they propose other metrics?",
        "question_id": "b5c3787ab3784214fc35f230ac4926fe184d86ba",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions."
            ],
            "highlighted_evidence": [
              "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions."
            ]
          }
        ]
      },
      {
        "question": "Which real-world datasets did they use?",
        "question_id": "9174aded45bc36915f2e2adb6f352f3c7d9ada8b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SST-2 (Stanford Sentiment Treebank, version 2)",
              "Snips"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative.",
              "The second task involves two essential problems in SLU, which are intent classification (IC) and slot labeling (SL). In IC, the model needs to detect the intention of a text input (i.e., utterance, conveys). For example, for an input of I want to book a flight to Seattle, the intention is to book a flight ticket, hence the intent class is bookFlight. In SL, the model needs to extract the semantic entities that are related to the intent. From the same example, Seattle is a slot value related to booking the flight, i.e., the destination. Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents."
            ],
            "highlighted_evidence": [
              "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative.",
              "Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "SST-2",
              "Snips"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative.",
              "The second task involves two essential problems in SLU, which are intent classification (IC) and slot labeling (SL). In IC, the model needs to detect the intention of a text input (i.e., utterance, conveys). For example, for an input of I want to book a flight to Seattle, the intention is to book a flight ticket, hence the intent class is bookFlight. In SL, the model needs to extract the semantic entities that are related to the intent. From the same example, Seattle is a slot value related to booking the flight, i.e., the destination. Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents."
            ],
            "highlighted_evidence": [
              "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments.",
              "Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research."
            ]
          }
        ]
      },
      {
        "question": "How did they obtain human intuitions?",
        "question_id": "a8f1029f6766bffee38a627477f61457b2d6ed5c",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/LaiZZD20",
    "dblp_title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections.",
    "year": "2020"
  },
  {
    "id": "1708.05873",
    "title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016",
    "qas": [
      {
        "question": "What are the country-specific drivers of international development rhetoric?",
        "question_id": "a2103e7fe613549a9db5e65008f33cf2ee0403bd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "wealth ",
              "democracy ",
              "population",
              "levels of ODA",
              "conflict "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Yet surprisingly little is known about the agenda-setting process for international development in global governance institutions. This is perhaps best demonstrated by the lack of information on how the different goals and targets of the MDGs were decided, which led to much criticism and concern about the global governance of development BIBREF1 . More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.",
              "The analysis of discussion of international development in annual UN General Debate statements therefore uncovers two principle development topics: economic development and sustainable development. We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). However, we find that the extent to which countries discuss sustainable development (Topic 7) in their annual GD statements varies considerably according to these different structural factors. The results suggest that broadly-speaking we do not observe linear trends in the relationship between these country-specific factors and discussion of Topic 7. Instead, we find that there are significant fluctuations in the relationship between factors such as wealth, democracy, etc., and the extent to which these states discuss sustainable development in their GD statements. These relationships require further analysis and exploration."
            ],
            "highlighted_evidence": [
              " More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.",
              " We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects). "
            ]
          }
        ]
      },
      {
        "question": "Is the dataset multilingual?",
        "question_id": "13b36644357870008d70e5601f394ec3c6c07048",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements."
            ],
            "highlighted_evidence": [
              "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements.",
              "FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model."
            ],
            "highlighted_evidence": [
              "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . ",
              "FLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model."
            ]
          }
        ]
      },
      {
        "question": "How are the main international development topics that states raise identified?",
        "question_id": "e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": " They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",
            "evidence": [
              "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in Amazon's Mechanical Turk experiments.",
              "Exclusivity scores for each topic follows BIBREF7 . Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. Cohesive and exclusive topics are more semantically useful. Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 ."
            ],
            "highlighted_evidence": [
              "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures.",
              "Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. ",
              "Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1708-05873",
    "dblp_title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016.",
    "year": "2017"
  },
  {
    "id": "2003.08553",
    "title": "QnAMaker: Data to Bot in 2 Minutes",
    "qas": [
      {
        "question": "What experiments do the authors present to validate their system?",
        "question_id": "fd0ef5a7b6f62d07776bf672579a99c67e61a568",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " we measure our system's performance for datasets across various domains",
              "evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "QnAMaker is not domain-specific and can be used for any type of data. To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). Hybrid of deep learning(CDSSM) and machine learning features give our ranking model low computation cost, high explainability and significant F1/AUC score. Based on QnAMaker usage, we observed these trends:"
            ],
            "highlighted_evidence": [
              " To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19)."
            ]
          }
        ]
      },
      {
        "question": "How does the conversation layer work?",
        "question_id": "071bcb4b054215054f17db64bfd21f17fd9e1a80",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What components is the QnAMaker composed of?",
        "question_id": "f399d5a8dbeec777a858f81dc4dd33a83ba341a2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "QnAMaker Portal",
              "QnaMaker Management APIs",
              "Azure Search Index",
              "QnaMaker WebApp",
              "Bot"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "System description ::: Architecture",
              "As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:",
              "QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.",
              "QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.",
              "Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.",
              "QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.",
              "Bot: Calls the WebApp with the User's query to get results."
            ],
            "highlighted_evidence": [
              "System description ::: Architecture",
              "The components involved in the process are:",
              "QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. ",
              "QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. ",
              "Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.",
              "QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. ",
              "Bot: Calls the WebApp with the User's query to get results."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "QnAMaker Portal",
              "QnaMaker Management APIs",
              "Azure Search Index",
              "QnaMaker WebApp",
              "Bot"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:",
              "QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.",
              "QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.",
              "Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.",
              "QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.",
              "Bot: Calls the WebApp with the User's query to get results."
            ],
            "highlighted_evidence": [
              "The components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.\n\nBot: Calls the WebApp with the User's query to get results."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/www/AgrawalMKNCSKSK20",
    "dblp_title": "QnAMaker: Data to Bot in 2 Minutes.",
    "year": "2020"
  },
  {
    "id": "1909.09491",
    "title": "A simple discriminative training method for machine translation with large-scale features",
    "qas": [
      {
        "question": "How they measure robustness in experiments?",
        "question_id": "d28260b5565d9246831e8dbe594d4f6211b60237",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We empirically provide a formula to measure the richness in the scenario of machine translation."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of “rich” is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation.",
              "The greater, the richer. In practice, we find a rough threshold of r is 5."
            ],
            "highlighted_evidence": [
              "The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of “rich” is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation.",
              "The greater, the richer. In practice, we find a rough threshold of r is 5"
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "boost the training BLEU very greatly",
              "the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "First, the Plackett-Luce models boost the training BLEU very greatly, even up to 2.5 points higher than MIRA. This verifies our assumption, richer features benefit BLEU, though they are optimized towards a different objective.",
              "Second, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$. In PL(1), the over-fitting is quite obvious, the portion in which the curve overpasses MIRA is the smallest compared to other $k$, and its convergent performance is below the baseline. When $k$ is not smaller than 5, the curves are almost above the MIRA line. After 500 L-BFGS iterations, their performances are no less than the baseline, though only by a small margin."
            ],
            "highlighted_evidence": [
              "First, the Plackett-Luce models boost the training BLEU very greatly, even up to 2.5 points higher than MIRA. This verifies our assumption, richer features benefit BLEU, though they are optimized towards a different objective.\n\nSecond, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$. In PL(1), the over-fitting is quite obvious, the portion in which the curve overpasses MIRA is the smallest compared to other $k$, and its convergent performance is below the baseline. When $k$ is not smaller than 5, the curves are almost above the MIRA line."
            ]
          }
        ]
      },
      {
        "question": "Is new method inferior in terms of robustness to MIRAs in experiments?",
        "question_id": "8670989ca39214eda6c1d1d272457a3f3a92818b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What experiments with large-scale features are performed?",
        "question_id": "923b12c0a50b0ee22237929559fad0903a098b7b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Plackett-Luce Model for SMT Reranking"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluation ::: Plackett-Luce Model for SMT Reranking",
              "After being de-duplicated, the N-best list has an average size of around 300, and with 7491 features. Refer to Formula DISPLAY_FORM9, this is ideal to use the Plackett-Luce model. Results are shown in Figure FIGREF12. We observe some interesting phenomena.",
              "This experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree."
            ],
            "highlighted_evidence": [
              " Plackett-Luce Model for SMT Reranking\nAfter being de-duplicated, the N-best list has an average size of around 300, and with 7491 features.",
              "This experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-09491",
    "dblp_title": "A simple discriminative training method for machine translation with large-scale features.",
    "year": "2019"
  },
  {
    "id": "2001.05284",
    "title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses",
    "qas": [
      {
        "question": "Which ASR system(s) is used in this work?",
        "question_id": "67131c15aceeb51ae1d3b2b8241c8750a19cca8e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Oracle "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The preliminary architecture is shown in Fig. FIGREF4. For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.",
              "For convenience, we simplify the whole process in Fig.FIGREF4 as a mapping $BM$ (Baseline Mapping) from the input utterance $S$ to an estimated tag's probability $p(\\tilde{t})$, where $p(\\tilde{t}) \\leftarrow BM(S)$. The $Baseline$ is trained on transcription and evaluated on ASR 1st best hypothesis ($S=\\text{ASR}\\ 1^{st}\\ \\text{best})$. The $Oracle$ is trained on transcription and evaluated on transcription ($S = \\text{Transcription}$). We name it Oracle simply because we assume that hypotheses are noisy versions of transcription."
            ],
            "highlighted_evidence": [
              "For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.",
              "We name it Oracle simply because we assume that hypotheses are noisy versions of transcription."
            ]
          }
        ]
      },
      {
        "question": "What are the series of simple models?",
        "question_id": "579a0603ec56fc2b4aa8566810041dbb0cd7b5e7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "perform experiments to utilize ASR $n$-best hypotheses during evaluation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation. The models evaluating with $n$-bests and a BM (pre-trained on transcription) are called Direct Models (in Fig. FIGREF7):"
            ],
            "highlighted_evidence": [
              "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation."
            ]
          }
        ]
      },
      {
        "question": "Over which datasets/corpora is this work evaluated?",
        "question_id": "c9c85eee41556c6993f40e428fa607af4abe80a9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$\\sim $ 8.7M annotated anonymised user utterances"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
            ],
            "highlighted_evidence": [
              "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "on $\\sim $ 8.7M annotated anonymised user utterances"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
            ],
            "highlighted_evidence": [
              "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2001-05284",
    "dblp_title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses.",
    "year": "2020"
  },
  {
    "id": "1909.12140",
    "title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German",
    "qas": [
      {
        "question": "Is the semantic hierarchy representation used for any task?",
        "question_id": "f8281eb49be3e8ea0af735ad3bec955a5dedf5b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Yes, Open IE",
            "evidence": [
              "An extrinsic evaluation was carried out on the task of Open IE BIBREF7. It revealed that when applying DisSim as a preprocessing step, the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall, i.e. leading to a lower information loss and a higher accuracy of the extracted relations. For details, the interested reader may refer to niklaus-etal-2019-transforming."
            ],
            "highlighted_evidence": [
              "An extrinsic evaluation was carried out on the task of Open IE BIBREF7."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Moreover, most current Open IE approaches output only a loose arrangement of extracted tuples that are hard to interpret as they ignore the context under which a proposition is complete and correct and thus lack the expressiveness needed for a proper interpretation of complex assertions BIBREF8. As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks."
            ],
            "highlighted_evidence": [
              "As illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks."
            ]
          }
        ]
      },
      {
        "question": "What are the corpora used for the task?",
        "question_id": "a5ee9b40a90a6deb154803bef0c71c2628acb571",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains",
              "The evaluation of the German version is in progress."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress."
            ],
            "highlighted_evidence": [
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains ",
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains ",
              "The evaluation of the German version is in progress."
            ]
          }
        ]
      },
      {
        "question": "Is the model evaluated?",
        "question_id": "e286860c41a4f704a3a08e45183cb8b14fa2ad2f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "the English version is evaluated. The German version evaluation is in progress ",
            "evidence": [
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress."
            ],
            "highlighted_evidence": [
              "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-12140",
    "dblp_title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German.",
    "year": "2019"
  },
  {
    "id": "1709.00947",
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "qas": [
      {
        "question": "What new metrics are suggested to track progress?",
        "question_id": "982979cb3c71770d8d7d2d1be8f92b66223dec85",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "It is now clear that a key aspect for future work will be developing additional performance metrics based on topological properties. We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. Future work will necessarily include developing this type of metrics."
            ],
            "highlighted_evidence": [
              "We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine."
            ]
          }
        ]
      },
      {
        "question": "What intrinsic evaluation metrics are used?",
        "question_id": "5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Class Membership Tests",
              "Class Distinction Test",
              "Word Equivalence Test"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Tests and Gold-Standard Data for Intrinsic Evaluation",
              "Using the gold standard data (described below), we performed three types of tests:",
              "Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year\", “Portuguese Cities\", “Smileys\") should be close, since they are supposed to be found in mostly the same contexts.",
              "Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts.",
              "Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque\" abbreviated by “pq\") and partial references (e.g. “slb and benfica\") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).",
              "Therefore, in our tests, two words are considered:",
              "distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80).",
              "to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80).",
              "equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95)."
            ],
            "highlighted_evidence": [
              "Tests and Gold-Standard Data for Intrinsic Evaluation\nUsing the gold standard data (described below), we performed three types of tests:\n\nClass Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year\", “Portuguese Cities\", “Smileys\") should be close, since they are supposed to be found in mostly the same contexts.\n\nClass Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts.\n\nWord Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque\" abbreviated by “pq\") and partial references (e.g. “slb and benfica\") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).\n\nTherefore, in our tests, two words are considered:\n\ndistinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80).\n\nto belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80).\n\nequivalent if the cosine of the embeddings is higher that 0.85 (or 0.95)."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "coverage metric",
              "being distinct (cosine INLINEFORM0 0.7 or 0.8)",
              "belonging to the same class (cosine INLINEFORM1 0.7 or 0.8)",
              "being equivalent (cosine INLINEFORM2 0.85 or 0.95)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95)."
            ],
            "highlighted_evidence": [
              "For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced.",
              "Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95)."
            ]
          }
        ]
      },
      {
        "question": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?",
        "question_id": "7ce7edd06925a943e32b59f3e7b5159ccb7acaf6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "consistent increase in the validation loss after about 15 epochs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model."
            ],
            "highlighted_evidence": [
              "The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 )."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/epia/SaleiroSRSO17",
    "dblp_title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of Some Practical Aspects.",
    "year": "2017"
  },
  {
    "id": "1909.08859",
    "title": "Procedural Reasoning Networks for Understanding Multimodal Procedures",
    "qas": [
      {
        "question": "What multimodality is available in the dataset?",
        "question_id": "a883bb41449794e0a63b716d9766faea034eb359",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "context is a procedural text, the question and the multiple choice answers are composed of images"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the following, we explain our Procedural Reasoning Networks model. Its architecture is based on a bi-directional attention flow (BiDAF) model BIBREF6, but also equipped with an explicit reasoning module that acts on entity-specific relational memory units. Fig. FIGREF4 shows an overview of the network architecture. It consists of five main modules: An input module, an attention module, a reasoning module, a modeling module, and an output module. Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images."
            ],
            "highlighted_evidence": [
              "Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "images and text"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. To this end, inspired from BIBREF5, we propose Procedural Reasoning Networks (PRN) that incorporates entities into the comprehension process and allows to keep track of entities, understand their interactions and accordingly update their states across time. We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. We further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps."
            ],
            "highlighted_evidence": [
              "In particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. ",
              "We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. "
            ]
          }
        ]
      },
      {
        "question": "What are previously reported models?",
        "question_id": "5d83b073635f5fd8cd1bdb1895d3f13406583fbd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Hasty Student",
              "Impatient Reader",
              "BiDAF",
              "BiDAF w/ static memory"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.",
              "Hasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.",
              "Impatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.",
              "BiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context. Here, we adapt it to work in a multimodal setting and answer multiple choice questions instead.",
              "BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities. However, it does not make any updates on the memory cells. That is, it uses the static entity embeeddings initialized with GloVe word vectors. We propose this baseline to test the significance of the use of relational memory updates."
            ],
            "highlighted_evidence": [
              "We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.\n\nHasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.\n\nImpatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.\n\nBiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context.",
              "BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities."
            ]
          }
        ]
      },
      {
        "question": "How better is accuracy of new model compared to previously reported models?",
        "question_id": "171ebfdc9b3a98e4cdee8f8715003285caeb2f39",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59",
            "evidence": [
              "Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models. Moreover, it achieves the best performance on average. These results demonstrate the importance of having a dynamic memory and keeping track of entities extracted from the recipe. In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20.",
              "FLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines."
            ],
            "highlighted_evidence": [
              "Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.",
              "In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF.",
              "FLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/conll/AmacYEE19",
    "dblp_title": "Procedural Reasoning Networks for Understanding Multimodal Procedures.",
    "year": "2019"
  },
  {
    "id": "1908.08419",
    "title": "Active Learning for Chinese Word Segmentation in Medical Text",
    "qas": [
      {
        "question": "How does the scoring model work?",
        "question_id": "3c3cb51093b5fd163e87a773a857496a4ae71f03",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."
            ],
            "highlighted_evidence": [
              "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              " the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."
            ],
            "highlighted_evidence": [
              "The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. "
            ]
          }
        ]
      },
      {
        "question": "How does the active learning model work?",
        "question_id": "53a0763eff99a8148585ac642705637874be69d4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",
            "evidence": [
              "Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively."
            ],
            "highlighted_evidence": [
              "Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively."
            ]
          }
        ]
      },
      {
        "question": "Which neural network architectures are employed?",
        "question_id": "0bfed6f9cfe93617c5195c848583e3945f2002ff",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "gated neural network "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."
            ],
            "highlighted_evidence": [
              "A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-08419",
    "dblp_title": "Active Learning for Chinese Word Segmentation in Medical Text.",
    "year": "2019"
  },
  {
    "id": "1703.05260",
    "title": "InScript: Narrative texts annotated with script information",
    "qas": [
      {
        "question": "What are the key points in the role of script knowledge that can be studied?",
        "question_id": "352c081c93800df9654315e13a880d6387b91919",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Did the annotators agreed and how much?",
        "question_id": "18fbf9c08075e3b696237d22473c463237d153f5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.",
            "evidence": [
              "FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics.",
              "In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.",
              "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics.",
              " The results are shown in Figure 4 and indicate moderate to substantial agreement",
              "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Moderate agreement of 0.64-0.68 Fleiss’ Kappa over event type labels, 0.77 Fleiss’ Kappa over participant labels, and good agreement of 90.5% over coreference information.",
            "evidence": [
              "In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.",
              "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement.",
              "FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics."
            ],
            "highlighted_evidence": [
              "The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 .",
              "We take the result of 90.5% between annotators to be a good agreement.",
              "FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics."
            ]
          }
        ]
      },
      {
        "question": "How many subjects have been used to create the annotations?",
        "question_id": "a37ef83ab6bcc6faff3c70a481f26174ccd40489",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " four different annotators"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used the WebAnno annotation tool BIBREF2 for our project. The stories from each scenario were distributed among four different annotators. In a calibration phase, annotators were presented with some sample texts for test annotations; the results were discussed with the authors. Throughout the whole annotation phase, annotators could discuss any emerging issues with the authors. All annotations were done by undergraduate students of computational linguistics. The annotation was rather time-consuming due to the complexity of the task, and thus we decided for single annotation mode. To assess annotation quality, a small sample of texts was annotated by all four annotators and their inter-annotator agreement was measured (see Section \"Inter-Annotator Agreement\" ). It was found to be sufficiently high."
            ],
            "highlighted_evidence": [
              "The stories from each scenario were distributed among four different annotators. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/ModiA0P17",
    "dblp_title": "InScript: Narrative texts annotated with script information.",
    "year": "2017"
  },
  {
    "id": "1905.00563",
    "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",
    "qas": [
      {
        "question": "What datasets are used to evaluate this approach?",
        "question_id": "bc9c31b3ce8126d1d148b1025c66f270581fde10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": " Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",
            "evidence": [
              "FLOAT SELECTED: Table 2: Data Statistics of the benchmarks."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Data Statistics of the benchmarks."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "WN18 and YAGO3-10"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since the setting is quite different from traditional adversarial attacks, search for link prediction adversaries brings up unique challenges. To find these minimal changes for a target link, we need to identify the fact that, when added into or removed from the graph, will have the biggest impact on the predicted score of the target fact. Unfortunately, computing this change in the score is expensive since it involves retraining the model to recompute the embeddings. We propose an efficient estimate of this score change by approximating the change in the embeddings using Taylor expansion. The other challenge in identifying adversarial modifications for link prediction, especially when considering addition of fake facts, is the combinatorial search space over possible facts, which is intractable to enumerate. We introduce an inverter of the original embedding model, to decode the embeddings to their corresponding graph components, making the search of facts tractable by performing efficient gradient-based continuous optimization. We evaluate our proposed methods through following experiments. First, on relatively small KGs, we show that our approximations are accurate compared to the true change in the score. Second, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10. We also explore the utility of adversarial modifications in explaining the model predictions by presenting rule-like descriptions of the most influential neighbors. Finally, we use adversaries to detect errors in the KG, obtaining up to $55\\%$ accuracy in detecting errors."
            ],
            "highlighted_evidence": [
              "WN18 and YAGO3-10",
              "Second, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10. "
            ]
          }
        ]
      },
      {
        "question": "How is this approach used to detect incorrect facts?",
        "question_id": "185841e979373808d99dccdade5272af02b98774",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph."
            ],
            "highlighted_evidence": [
              "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data.",
              "Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph."
            ]
          }
        ]
      },
      {
        "question": "Can this adversarial approach be used to directly improve model accuracy?",
        "question_id": "d427e3d41c4c9391192e249493be23926fc5d2e9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To evaluate this application, we inject random triples into the graph, and measure the ability of to detect the errors using our optimization. We consider two types of incorrect triples: 1) incorrect triples in the form of $\\langle s^{\\prime }, r, o\\rangle $ where $s^{\\prime }$ is chosen randomly from all of the entities, and 2) incorrect triples in the form of $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ where $s^{\\prime }$ and $r^{\\prime }$ are chosen randomly. We choose 100 random triples from the observed graph, and for each of them, add an incorrect triple (in each of the two scenarios) to its neighborhood. Then, after retraining DistMult on this noisy training data, we identify error triples through a search over the neighbors of the 100 facts. The result of choosing the neighbor with the least influence on the target is provided in the Table 7 . When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors."
            ],
            "highlighted_evidence": [
              "When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/akbc/PezeshkpourTS19",
    "dblp_title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.",
    "year": "2019"
  },
  {
    "id": "1808.05902",
    "title": "Learning Supervised Topic Models for Classification and Regression from Crowds",
    "qas": [
      {
        "question": "what are the advantages of the proposed model?",
        "question_id": "330f2cdeab689670b68583fc4125f5c0b26615a8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "he proposed model outperforms all the baselines, being the svi version the one that performs best.",
              "the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For all the experiments the hyper-parameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 were set using a simple grid search in the collection INLINEFORM3 . The same approach was used to optimize the hyper-parameters of the all the baselines. For the svi algorithm, different mini-batch sizes and forgetting rates INLINEFORM4 were tested. For the 20-Newsgroup dataset, the best results were obtained with a mini-batch size of 500 and INLINEFORM5 . The INLINEFORM6 was kept at 1. The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.",
              "In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
            ],
            "highlighted_evidence": [
              "The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.",
              "In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
            ]
          }
        ]
      },
      {
        "question": "what are the state of the art approaches?",
        "question_id": "c87b2dd5c439d5e68841a705dd81323ec0d64c97",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Bosch 2006 (mv)",
              "LDA + LogReg (mv)",
              "LDA + Raykar",
              "LDA + Rodrigues",
              "Blei 2003 (mv)",
              "sLDA (mv)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:",
              "Bosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).",
              "The results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.",
              "Analyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.",
              "Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:",
              "[itemsep=0.02cm]",
              "LDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .",
              "LDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.",
              "LDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.",
              "Blei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).",
              "sLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers."
            ],
            "highlighted_evidence": [
              "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.",
              "Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers."
            ]
          }
        ]
      },
      {
        "question": "what datasets were used?",
        "question_id": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Reuters-21578 BIBREF30",
              " LabelMe BIBREF31",
              "20-Newsgroups benchmark corpus BIBREF29 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .",
              "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: “computers\", “science\", “politics\" and “recreative\". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing."
            ],
            "highlighted_evidence": [
              "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .",
              "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              " 20-Newsgroups benchmark corpus ",
              "Reuters-21578",
              "LabelMe"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: “computers\", “science\", “politics\" and “recreative\". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing.",
              "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 ."
            ],
            "highlighted_evidence": [
              "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. ",
              "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1808-05902",
    "dblp_title": "Learning Supervised Topic Models for Classification and Regression from Crowds.",
    "year": "2018"
  },
  {
    "id": "2002.11893",
    "title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset",
    "qas": [
      {
        "question": "How was the dataset collected?",
        "question_id": "2376c170c343e2305dac08ba5f5bda47c370357f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. ",
              "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context.",
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.",
              "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.",
              "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.",
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.",
              "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."
            ],
            "highlighted_evidence": [
              "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database.",
              "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. ",
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. ",
              "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",
            "evidence": [
              "Our corpus is to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing. Domains include hotel, attraction, restaurant, metro, and taxi. The data collection process is summarized as below:",
              "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.",
              "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.",
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.",
              "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."
            ],
            "highlighted_evidence": [
              "The data collection process is summarized as below:\n\nDatabase Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.\n\nGoal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.\n\nDialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."
            ]
          }
        ]
      },
      {
        "question": "What are the benchmark models?",
        "question_id": "0137ecebd84a03b224eb5ca51d189283abb5f6d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BERTNLU from ConvLab-2",
              "a rule-based model (RuleDST) ",
              "TRADE (Transferable Dialogue State Generator) ",
              "a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Model: We adapted BERTNLU from ConvLab-2. BERT BIBREF22 has shown strong performance in many NLP tasks. We use Chinese pre-trained BERT BIBREF23 for initialization and then fine-tune the parameters on CrossWOZ. We obtain word embeddings and the sentence representation (embedding of [CLS]) from BERT. Since there may exist more than one intent in an utterance, we modify the traditional method accordingly. For dialogue acts of inform and recommend intents such as (intent=Inform, domain=Attraction, slot=fee, value=free) whose values appear in the sentence, we perform sequential labeling using an MLP which takes word embeddings (\"free\") as input and outputs tags in BIO schema (\"B-Inform-Attraction-fee\"). For each of the other dialogue acts (e.g., (intent=Request, domain=Attraction, slot=fee)) that do not have actual values, we use another MLP to perform binary classification on the sentence representation to predict whether the sentence should be labeled with this dialogue act. To incorporate context information, we use the same BERT to get the embedding of last three utterances. We separate the utterances with [SEP] tokens and insert a [CLS] token at the beginning. Then each original input of the two MLP is concatenated with the context embedding (embedding of [CLS]), serving as the new input. We also conducted an ablation test by removing context information. We trained models with both system-side and user-side utterances.",
              "Model: We implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment. RuleDST takes as input the previous system state and the last user dialogue acts. Then, the system state is updated according to hand-crafted rules. For example, If one of user dialogue acts is (intent=Inform, domain=Attraction, slot=fee, value=free), then the value of the \"fee\" slot in the attraction domain will be filled with \"free\". TRADE generates the system state directly from all the previous utterances using a copy mechanism. As mentioned in Section SECREF18, the first query of the system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work.",
              "Model: We adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy). The state $s$ consists of the last system dialogue acts, last user dialogue acts, system state of the current turn, the number of entities that satisfy the constraints in the current domain, and a terminal signal indicating whether the user goal is completed. The action $a$ is delexicalized dialogue acts of current turn which ignores the exact values of the slots, where the values will be filled back after prediction."
            ],
            "highlighted_evidence": [
              "We adapted BERTNLU from ConvLab-2. ",
              "We implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment. ",
              "We adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy). "
            ]
          }
        ]
      },
      {
        "question": "How was the corpus annotated?",
        "question_id": "5f6fbd57cce47f20a0fda27d954543c00c4344c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The workers were also asked to annotate both user states and system states",
              "we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.",
              "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."
            ],
            "highlighted_evidence": [
              "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/tacl/ZhuHZZH20",
    "dblp_title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset.",
    "year": "2020"
  },
  {
    "id": "1910.07181",
    "title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance",
    "qas": [
      {
        "question": "What models other than standalone BERT is new model compared to?",
        "question_id": "d6e2b276390bdc957dfa7e878de80cee1f41fbca",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Only Bert base and Bert large are compared to proposed approach.",
            "evidence": [
              "Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin."
            ],
            "highlighted_evidence": [
              "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin."
            ]
          }
        ]
      },
      {
        "question": "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?",
        "question_id": "32537fdf0d4f76f641086944b413b2f756097e5e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin."
            ],
            "highlighted_evidence": [
              "Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking."
            ]
          }
        ]
      },
      {
        "question": "What are three downstream task datasets?",
        "question_id": "ef081d78be17ef2af792e7e919d15a235b8d7275",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MNLI BIBREF21",
              "AG's News BIBREF22",
              "DBPedia BIBREF23"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35."
            ],
            "highlighted_evidence": [
              "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "MNLI",
              "AG's News",
              "DBPedia"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35."
            ],
            "highlighted_evidence": [
              "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23."
            ]
          }
        ]
      },
      {
        "question": "What is dataset for word probing task?",
        "question_id": "537b2d7799124d633892a1ef1a485b3b071b303d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WNLaMPro dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evalute Bertram on the WNLaMPro dataset of BIBREF0. This dataset consists of cloze-style phrases like"
            ],
            "highlighted_evidence": [
              "We evalute Bertram on the WNLaMPro dataset of BIBREF0."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/SchickS20",
    "dblp_title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance.",
    "year": "2020"
  },
  {
    "id": "1902.00330",
    "title": "Joint Entity Linking with Deep Reinforcement Learning",
    "qas": [
      {
        "question": "How fast is the model compared to baselines?",
        "question_id": "9aca4b89e18ce659c905eccc78eda76af9f0072a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How big is the performance difference between this method and the baseline?",
        "question_id": "b0376a7f67f1568a7926eff8ff557a93f434a253",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",
            "evidence": [
              "FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1."
            ]
          }
        ]
      },
      {
        "question": "What datasets used for evaluation?",
        "question_id": "dad8cc543a87534751f9f9e308787e1af06f0627",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AIDA-B",
              "ACE2004",
              "MSNBC",
              "AQUAINT",
              "WNED-CWEB",
              "WNED-WIKI"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.",
              "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
              "ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.",
              "MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)",
              "AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.",
              "WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.",
              "WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation."
            ],
            "highlighted_evidence": [
              "In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. ",
              "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "AIDA-CoNLL",
              "ACE2004",
              "MSNBC",
              "AQUAINT",
              "WNED-CWEB",
              "WNED-WIKI",
              "OURSELF-WIKI"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.",
              "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
              "ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.",
              "MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)",
              "AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.",
              "WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.",
              "WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.",
              "OURSELF-WIKI is crawled by ourselves from Wikipedia pages."
            ],
            "highlighted_evidence": [
              "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. ",
              "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nOURSELF-WIKI is crawled by ourselves from Wikipedia pages."
            ]
          }
        ]
      },
      {
        "question": "what are the mentioned cues?",
        "question_id": "0481a8edf795768d062c156875d20b8fb656432c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Figure 2: The overall structure of our RLEL model. It contains three parts: Local Encoder, Global Encoder and Entity Selector. In this framework, (Vmt ,Vekt ) denotes the concatenation of the mention context vector Vmt and one candidate entity vector Vekt . The policy network selects one entity from the candidate set, and Vat denotes the concatenation of the mention context vector Vmt and the selected entity vector Ve∗t . ht represents the hidden status of Vat , and it will be fed into St+1.",
              "Where $\\oplus $ indicates vector concatenation. The $V_{m_i}^t$ and $V_{e_i}^t$ respectively denote the vector of $m_i$ and $e_i$ at time $t$ . For each mention, there are multiple candidate entities correspond to it. With the purpose of comparing the semantic relevance between the mention and each candidate entity at the same time, we copy multiple copies of the mention vector. Formally, we extend $V_{m_i}^t \\in \\mathbb {R}^{1\\times {n}}$ to $V_{m_i}^t{^{\\prime }} \\in \\mathbb {R}^{k\\times {n}}$ and then combine it with $V_{e_i}^t \\in \\mathbb {R}^{k\\times {n}}$ . Since $V_{m_i}^t$ and $V_{m_i}^t$0 are mainly to represent semantic information, we add feature vector $V_{m_i}^t$1 to enrich lexical and statistical features. These features mainly include the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc. After getting these feature values, we combine them into a vector and add it to the current state. In addition, the global vector $V_{m_i}^t$2 is also added to $V_{m_i}^t$3 . As mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 . Thus, the state $V_{m_i}^t$8 contains current information and previous decisions, while also covering the semantic representations and a variety of statistical features. Next, the concatenated vector will be fed into the policy network to generate action."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 2: The overall structure of our RLEL model. It contains three parts: Local Encoder, Global Encoder and Entity Selector. In this framework, (Vmt ,Vekt ) denotes the concatenation of the mention context vector Vmt and one candidate entity vector Vekt . The policy network selects one entity from the candidate set, and Vat denotes the concatenation of the mention context vector Vmt and the selected entity vector Ve∗t . ht represents the hidden status of Vat , and it will be fed into St+1.",
              "As mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/www/FangC0ZZL19",
    "dblp_title": "Joint Entity Linking with Deep Reinforcement Learning.",
    "year": "2019"
  },
  {
    "id": "1909.00542",
    "title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b",
    "qas": [
      {
        "question": "How did the author's work rank among other submissions on the challenge?",
        "question_id": "b6a4ab009e6f213f011320155a7ce96e713c11cf",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What approaches without reinforcement learning have been tried?",
        "question_id": "cfffc94518d64cb3c8789395707e4336676e0345",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "classification, regression, neural methods",
            "evidence": [
              "We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.",
              "The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that “NNC SU4 F1” outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer."
            ],
            "highlighted_evidence": [
              "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.",
              "The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              " Support Vector Regression (SVR) and Support Vector Classification (SVC)",
              "deep learning regression models of BIBREF2 to convert them to classification models"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.",
              "Based on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models. In particular, we add a sigmoid activation to the final layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28."
            ],
            "highlighted_evidence": [
              "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.",
              "Based on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models."
            ]
          }
        ]
      },
      {
        "question": "What classification approaches were experimented for this task?",
        "question_id": "f60629c01f99de3f68365833ee115b95a3388699",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "NNC SU4 F1",
              "NNC top 5",
              "Support Vector Classification (SVC)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.",
              "The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that “NNC SU4 F1” outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer."
            ],
            "highlighted_evidence": [
              "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.",
              "The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence."
            ]
          }
        ]
      },
      {
        "question": "Did classification models perform better than previous regression one?",
        "question_id": "a7cb4f8e29fd2f3d1787df64cd981a6318b65896",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels."
            ],
            "highlighted_evidence": [
              "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-00542",
    "dblp_title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b.",
    "year": "2019"
  },
  {
    "id": "1810.06743",
    "title": "Marrying Universal Dependencies and Universal Morphology",
    "qas": [
      {
        "question": "What are the main sources of recall errors in the mapping?",
        "question_id": "642c4704a71fd01b922a0ef003f234dcc7b223cd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "irremediable annotation discrepancies",
              "differences in choice of attributes to annotate",
              "The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them",
              "the two annotations encode distinct information",
              "incorrectly applied UniMorph annotation",
              "cross-lingual inconsistency in both resources"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We present the intrinsic task's recall scores in tab:recall. Bear in mind that due to annotation errors in the original corpora (like the vas example from sec:resources), the optimal score is not always $100\\%$ . Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.",
              "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation."
            ],
            "highlighted_evidence": [
              "irremediable annotation discrepancies",
              "Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.",
              "We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. "
            ]
          }
        ]
      },
      {
        "question": "Do they look for inconsistencies between different languages' annotations in UniMorph?",
        "question_id": "e477e494fe15a978ff9c0a5f1c88712cdaec0c5c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, cross-lingual annotation."
            ],
            "highlighted_evidence": [
              "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources."
            ]
          }
        ]
      },
      {
        "question": "Do they look for inconsistencies between different UD treebanks?",
        "question_id": "04495845251b387335bf2e77e2c423130f43c7d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The contributions of this work are:"
            ],
            "highlighted_evidence": [
              "The contributions of this work are:"
            ]
          }
        ]
      },
      {
        "question": "Which languages do they validate on?",
        "question_id": "564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",
            "evidence": [
              "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "We apply this conversion to the 31 languages",
              "Arabic, Hindi, Lithuanian, Persian, and Russian. ",
              "Dutch",
              "Spanish"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 4: Tagging F1 using UD sentences annotated with either original UD MSDs or UniMorph-converted MSDs",
              "A dataset-by-dataset problem demands a dataset-by-dataset solution; our task is not to translate a schema, but to translate a resource. Starting from the idealized schema, we create a rule-based tool for converting UD-schema annotations to UniMorph annotations, incorporating language-specific post-edits that both correct infelicities and also increase harmony between the datasets themselves (rather than the schemata). We apply this conversion to the 31 languages with both UD and UniMorph data, and we report our method's recall, showing an improvement over the strategy which just maps corresponding schematic features to each other. Further, we show similar downstream performance for each annotation scheme in the task of morphological tagging.",
              "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.",
              "There are three other transformations for which we note no improvement here. Because of the problem in Basque argument encoding in the UniMorph dataset—which only contains verbs—we note no improvement in recall on Basque. Irish also does not improve: UD marks gender on nouns, while UniMorph marks case. Adjectives in UD are also underspecified. The verbs, though, are already correct with the simple mapping. Finally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge.",
              "We present the intrinsic task's recall scores in tab:recall. Bear in mind that due to annotation errors in the original corpora (like the vas example from sec:resources), the optimal score is not always $100\\%$ . Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.",
              "For the extrinsic task, the performance is reasonably similar whether UniMorph or UD; see tab:tagging. A large fluctuation would suggest that the two annotations encode distinct information. On the contrary, the similarities suggest that the UniMorph-mapped MSDs have similar content. We recognize that in every case, tagging F1 increased—albeit by amounts as small as $0.16$ points. This is in part due to the information that is lost in the conversion. UniMorph's schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Tagging F1 using UD sentences annotated with either original UD MSDs or UniMorph-converted MSDs",
              "We apply this conversion to the 31 languages",
              "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.",
              "Finally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge.",
              "Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.",
              "UniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acludw/McCarthySCHY18",
    "dblp_title": "Marrying Universal Dependencies and Universal Morphology.",
    "year": "2018"
  },
  {
    "id": "1909.02764",
    "title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning",
    "qas": [
      {
        "question": "Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features?",
        "question_id": "f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is face and audio data analysis evaluated?",
        "question_id": "9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "confusion matrices",
              "$\\text{F}_1$ score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF16 shows the confusion matrices for facial and audio emotion recognition on our complete AMMER data set and Table TABREF17 shows the results per class for each method, including facial and audio data and micro and macro averages. The classification from facial expressions yields a macro-averaged $\\text{F}_1$ score of 33 % across the three emotions joy, insecurity, and annoyance (P=0.31, R=0.35). While the classification results for joy are promising (R=43 %, P=57 %), the distinction of insecurity and annoyance from the other classes appears to be more challenging."
            ],
            "highlighted_evidence": [
              "Table TABREF16 shows the confusion matrices for facial and audio emotion recognition on our complete AMMER data set and Table TABREF17 shows the results per class for each method, including facial and audio data and micro and macro averages. The classification from facial expressions yields a macro-averaged $\\text{F}_1$ score of 33 % across the three emotions joy, insecurity, and annoyance (P=0.31, R=0.35)."
            ]
          }
        ]
      },
      {
        "question": "What is the baseline method for the task?",
        "question_id": "00bcdffff7e055f99aaf1b05cf41c98e2748e948",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For the emotion recognition from text they use described neural network as baseline.\nFor audio and face there is no baseline.",
            "evidence": [
              "For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model."
            ],
            "highlighted_evidence": [
              "For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model."
            ]
          }
        ]
      },
      {
        "question": "What are the emotion detection tools used for audio and face input?",
        "question_id": "f92ee3c5fce819db540bded3cfcc191e21799cb1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored.",
              "We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as “no emotion”. We accept the emotion with the highest score as the discrete prediction otherwise."
            ],
            "highlighted_evidence": [
              " We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear.",
              "We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "cannot be disclosed due to licensing restrictions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored."
            ],
            "highlighted_evidence": [
              "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/konvens/CevherZK19",
    "dblp_title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning.",
    "year": "2019"
  },
  {
    "id": "1905.11901",
    "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
    "qas": [
      {
        "question": "what amounts of size were used on german-english?",
        "question_id": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",
            "evidence": [
              "We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.",
              "To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see SECREF7 ). Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary.",
              "FLOAT SELECTED: Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE→EN data, and for KO→EN data."
            ],
            "highlighted_evidence": [
              "We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.",
              "Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary.",
              "FLOAT SELECTED: Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE→EN data, and for KO→EN data."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6–7 BLEU in both data conditions.",
              "In the ultra-low data condition, reducing the BPE vocabulary size is very effective (+4.9 BLEU). Reducing the batch size to 1000 token results in a BLEU gain of 0.3, and the lexical model yields an additional +0.6 BLEU. However, aggressive (word) dropout (+3.4 BLEU) and tuning other hyperparameters (+0.7 BLEU) has a stronger effect than the lexical model, and adding the lexical model (9) on top of the optimized configuration (8) does not improve performance. Together, the adaptations to the ultra-low data setting yield 9.4 BLEU (7.2 INLINEFORM2 16.6). The model trained on full IWSLT data is less sensitive to our changes (31.9 INLINEFORM3 32.8 BLEU), and optimal hyperparameters differ depending on the data condition. Subsequently, we still apply the hyperparameters that were optimized to the ultra-low data condition (8) to other data conditions, and Korean INLINEFORM4 English, for simplicity.",
              "FLOAT SELECTED: Table 2: German→English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported."
            ],
            "highlighted_evidence": [
              "Table TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6–7 BLEU in both data conditions.\n\nIn the ultra-low data condition, reducing the BPE vocabulary size is very effecti",
              "FLOAT SELECTED: Table 2: German→English IWSLT results for training corpus size of 100k words and 3.2M words (full corpus). Mean and standard deviation of three training runs reported."
            ]
          }
        ]
      },
      {
        "question": "what were their experimental results in the low-resource dataset?",
        "question_id": "07d7652ad4a0ec92e6b44847a17c378b0d9f57f5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "10.37 BLEU"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
            ],
            "highlighted_evidence": [
              "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
            ]
          }
        ]
      },
      {
        "question": "what are the methods they compare with in the korean-english dataset?",
        "question_id": "9f3444c9fb2e144465d63abf58520cddd4165a01",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "gu-EtAl:2018:EMNLP1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
            ],
            "highlighted_evidence": [
              "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
            ]
          }
        ]
      },
      {
        "question": "what pitfalls are mentioned in the paper?",
        "question_id": "2348d68e065443f701d8052018c18daa4ecc120e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "highly data-inefficient",
              "underperform phrase-based statistical machine translation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows:"
            ],
            "highlighted_evidence": [
              "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/SennrichZ19",
    "dblp_title": "Revisiting Low-Resource Neural Machine Translation: A Case Study.",
    "year": "2019"
  },
  {
    "id": "1912.01252",
    "title": "Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian",
    "qas": [
      {
        "question": "Does the paper report the results of previous models applied to the same tasks?",
        "question_id": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Technical and theoretical questions related to the proposed method and infrastructure for the exploration and facilitation of debates will be discussed in three sections. The first section concerns notions of how to define what constitutes a belief or opinion and how these can be mined from texts. To this end, an approach based on the automated extraction of semantic frames expressing causation is proposed. The observatory thus builds on the theoretical premise that expressions of causation such as `global warming causes rises in sea levels' can be revelatory for a person or group's underlying belief systems. Through a further technical description of the observatory's data-analytical components, section two of the paper deals with matters of spatially modelling the output of the semantic frame extractor and how this might be achieved without sacrificing nuances of meaning. The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
            ],
            "highlighted_evidence": [
              "The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is the quality of the discussion evaluated?",
        "question_id": "a939a53cabb4893b2fd82996f3dbe8688fdb7bbb",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the technique used for text analysis and mining?",
        "question_id": "8b99767620fd4efe51428b68841cc3ec06699280",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are the causal mapping methods employed?",
        "question_id": "312417675b3dc431eb7e7b16a917b7fed98d4376",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Axelrod's causal mapping method"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network’s directed edges, notably assertions of whether the causal linkage is positive or negative). These causes and effects are to be extracted from relevant sources by means of a series of heuristics and an encoding scheme (it should be noted that for this task Axelrod had human readers in mind). The graphs resulting from these efforts provide a structural overview of the relations among causal assertions (and thus beliefs):"
            ],
            "highlighted_evidence": [
              "Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network’s directed edges, notably assertions of whether the causal linkage is positive or negative)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-01252",
    "dblp_title": "Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian.",
    "year": "2019"
  },
  {
    "id": "1912.13109",
    "title": "\"Hinglish\"Language -- Modeling a Messy Code-Mixed Language",
    "qas": [
      {
        "question": "What is the previous work's model?",
        "question_id": "792d7b579cbf7bfad8fe125b0d66c2059a174cf9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Ternary Trans-CNN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.",
              "The approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work."
            ],
            "highlighted_evidence": [
              "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work."
            ]
          }
        ]
      },
      {
        "question": "What dataset is used?",
        "question_id": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "HEOT ",
              "A labelled dataset for a corresponding english tweets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
            ],
            "highlighted_evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "HEOT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
            ],
            "highlighted_evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small."
            ]
          }
        ]
      },
      {
        "question": "How big is the dataset?",
        "question_id": "5908d7fb6c48f975c5dfc5b19bb0765581df2b25",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "3189 rows of text messages"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. Prior work addresses this concern by using Transfer Learning on an architecture learnt on about 14,500 messages with an accuracy of 83.90. We addressed this concern using data augmentation techniques applied on text data."
            ],
            "highlighted_evidence": [
              "Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Resulting dataset was 7934 messages for train and 700 messages for test.",
            "evidence": [
              "Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below:",
              "FLOAT SELECTED: Table 3: Train-test split"
            ],
            "highlighted_evidence": [
              "The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below:",
              "FLOAT SELECTED: Table 3: Train-test split"
            ]
          }
        ]
      },
      {
        "question": "How is the dataset collected?",
        "question_id": "cca3301f20db16f82b5d65a102436bebc88a2026",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al",
              "HEOT obtained from one of the past studies done by Mathur et al"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
            ],
            "highlighted_evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
            ]
          }
        ]
      },
      {
        "question": "Was each text augmentation technique experimented individually?",
        "question_id": "cfd67b9eeb10e5ad028097d192475d21d0b6845b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What models do previous work use?",
        "question_id": "e1c681280b5667671c7f78b1579d0069cba72b0e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Ternary Trans-CNN ",
              "Hybrid multi-channel CNN and LSTM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Related Work ::: Transfer learning based approaches",
              "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.",
              "The approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work.",
              "Related Work ::: Hybrid models",
              "In another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture."
            ],
            "highlighted_evidence": [
              " Transfer learning based approaches\nMathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work.\n\nRelated Work ::: Hybrid models\nIn another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture."
            ]
          }
        ]
      },
      {
        "question": "Does the dataset contain content from various social media platforms?",
        "question_id": "58d50567df71fa6c3792a0964160af390556757d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Hinglish is a linguistic blend of Hindi (very widely spoken language in India) and English (an associate language of urban areas) and is spoken by upwards of 350 million people in India. While the name is based on the Hindi language, it does not refer exclusively to Hindi, but is used in India, with English words blending with Punjabi, Gujarati, Marathi and Hindi. Sometimes, though rarely, Hinglish is used to refer to Hindi written in English script and mixing with English words or phrases. This makes analyzing the language very interesting. Its rampant usage in social media like Twitter, Facebook, Online blogs and reviews has also led to its usage in delivering hate and abuses in similar platforms. We aim to find such content in the social media focusing on the tweets. Hypothetically, if we can classify such tweets, we might be able to detect them and isolate them for further analysis before it reaches public. This will a great application of AI to the social cause and thus is motivating. An example of a simple, non offensive message written in Hinglish could be:"
            ],
            "highlighted_evidence": [
              "We aim to find such content in the social media focusing on the tweets."
            ]
          }
        ]
      },
      {
        "question": "What dataset is used?",
        "question_id": "07c79edd4c29635dbc1c2c32b8df68193b7701c6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "HEOT ",
              "A labelled dataset for a corresponding english tweets "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
            ],
            "highlighted_evidence": [
              "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-13109",
    "dblp_title": "&quot;Hinglish&quot; Language - Modeling a Messy Code-Mixed Language.",
    "year": "2019"
  },
  {
    "id": "1911.03310",
    "title": "How Language-Neutral is Multilingual BERT?",
    "qas": [
      {
        "question": "How they demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment?",
        "question_id": "66125cfdf11d3bf8e59728428e02021177142c3a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.",
              "We use a pre-trained mBERT model that was made public with the BERT release. The model dimension is 768, hidden layer dimension 3072, self-attention uses 12 heads, the model has 12 layers. It uses a vocabulary of 120k wordpieces that is shared for all languages.",
              "To train the language identification classifier, for each of the BERT languages we randomly selected 110k sentences of at least 20 characters from Wikipedia, and keep 5k for validation and 5k for testing for each language. The training data are also used for estimating the language centroids.",
              "Results ::: Word Alignment.",
              "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.",
              "FLOAT SELECTED: Table 4: Maximum F1 score for word alignment across layers compared with FastAlign baseline."
            ],
            "highlighted_evidence": [
              "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.",
              "We use a pre-trained mBERT model that was made public with the BERT release. The model dimension is 768, hidden layer dimension 3072, self-attention uses 12 heads, the model has 12 layers. It uses a vocabulary of 120k wordpieces that is shared for all languages.\n\nTo train the language identification classifier, for each of the BERT languages we randomly selected 110k sentences of at least 20 characters from Wikipedia, and keep 5k for validation and 5k for testing for each language. The training data are also used for estimating the language centroids.",
              "Results ::: Word Alignment.\nTable TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.",
              "FLOAT SELECTED: Table 4: Maximum F1 score for word alignment across layers compared with FastAlign baseline."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "explicit projection had a negligible effect on the performance"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance."
            ],
            "highlighted_evidence": [
              "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance."
            ]
          }
        ]
      },
      {
        "question": "Are language-specific and language-neutral components disjunctive?",
        "question_id": "222b2469eede9a0448e0226c6c742e8c91522af3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings."
            ],
            "highlighted_evidence": [
              "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space."
            ]
          }
        ]
      },
      {
        "question": "How they show that mBERT representations can be split into a language-specific component and a language-neutral component?",
        "question_id": "6f8386ad64dce3a20bc75165c5c7591df8f419cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.",
              "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings.",
              "We then analyze the semantic properties of both the original and the centered representations using a range of probing tasks. For all tasks, we test all layers of the model. For tasks utilizing a single-vector sentence representation, we test both the vector corresponding to the [cls] token and mean-pooled states."
            ],
            "highlighted_evidence": [
              "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.\n\nWe thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.",
              "We then analyze the semantic properties of both the original and the centered representations using a range of probing tasks."
            ]
          }
        ]
      },
      {
        "question": "What challenges this work presents that must be solved to build better language-neutral representations?",
        "question_id": "81dc39ee6cdacf90d5f0f62134bf390a29146c65",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks."
            ],
            "highlighted_evidence": [
              "Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-03310",
    "dblp_title": "How Language-Neutral is Multilingual BERT?",
    "year": "2019"
  },
  {
    "id": "1907.12108",
    "title": "CAiRE: An End-to-End Empathetic Chatbot",
    "qas": [
      {
        "question": "What is the performance of their system?",
        "question_id": "b1ced2d6dcd1d7549be2594396cbda34da6c3bca",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What evaluation metrics are used?",
        "question_id": "f3be1a27df2e6ad12eed886a8cd2dfe09b9e2b30",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the source of the dialogues?",
        "question_id": "a45a86b6a02a98d3ab11f1d04acd3446e95f5a16",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What pretrained LM is used?",
        "question_id": "1f1a9f2dd8c4c10b671cb8affe56e181948e229e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Generative Pre-trained Transformer (GPT)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model. GPT is a multi-layer Transformer decoder with a causal self-attention which is pre-trained, unsupervised, on the BooksCorpus dataset. BooksCorpus dataset contains over 7,000 unique unpublished books from a variety of genres. Pre-training on such large contiguous text corpus enables the model to capture long-range dialogue context information. Furthermore, as existing EmpatheticDialogue dataset BIBREF4 is relatively small, fine-tuning only on such dataset will limit the chitchat topic of the model. Hence, we first integrate persona into CAiRE, and pre-train the model on PersonaChat BIBREF3 , following a previous transfer-learning strategy BIBREF1 . This pre-training procedure allows CAiRE to have a more consistent persona, thus improving the engagement and consistency of the model. We refer interested readers to the code repository recently released by HuggingFace. Finally, in order to optimize empathy in CAiRE, we fine-tune this pre-trained model using EmpatheticDialogue dataset to help CAiRE understand users' feeling."
            ],
            "highlighted_evidence": [
              "We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Generative Pre-trained Transformer (GPT)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In contrast to such modularized dialogue system, end-to-end systems learn all components as a single model in a fully data-driven manner, and mitigate the lack of labeled data by sharing representations among different modules. In this paper, we build an end-to-end empathetic chatbot by fine-tuning BIBREF1 the Generative Pre-trained Transformer (GPT) BIBREF2 on the PersonaChat dataset BIBREF3 and the Empathetic-Dialogue dataset BIBREF4 . We establish a web-based user interface which allows multiple users to asynchronously chat with CAiRE online. CAiRE can also collect user feedback and continuously improve its response quality and discard undesirable generation behaviors (e.g. unethical responses) via active learning and negative training."
            ],
            "highlighted_evidence": [
              "In this paper, we build an end-to-end empathetic chatbot by fine-tuning BIBREF1 the Generative Pre-trained Transformer (GPT) BIBREF2 on the PersonaChat dataset BIBREF3 and the Empathetic-Dialogue dataset BIBREF4 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/LinXWSLSF20",
    "dblp_title": "CAiRE: An End-to-End Empathetic Chatbot.",
    "year": "2020"
  },
  {
    "id": "2004.03685",
    "title": "Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?",
    "qas": [
      {
        "question": "What approaches they propose?",
        "question_id": "eeaceee98ef1f6c971dac7b0b8930ee8060d71c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks.",
              "Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We argue that a way out of this standstill is in a more practical and nuanced methodology for defining and evaluating faithfulness. We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.",
              "We note two possible approaches to this end:",
              "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.",
              "For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.",
              "Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only."
            ],
            "highlighted_evidence": [
              "We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only."
            ]
          }
        ]
      },
      {
        "question": "What faithfulness criteria does they propose?",
        "question_id": "3371d586a3a81de1552d90459709c57c0b1a2594",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks.",
              "Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We argue that a way out of this standstill is in a more practical and nuanced methodology for defining and evaluating faithfulness. We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.",
              "We note two possible approaches to this end:",
              "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.",
              "For example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.",
              "Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only."
            ],
            "highlighted_evidence": [
              "We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only."
            ]
          }
        ]
      },
      {
        "question": "Which are three assumptions in current approaches for defining faithfulness?",
        "question_id": "d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Two models will make the same predictions if and only if they use the same reasoning process.",
              "On similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Defining Faithfulness ::: Assumption 1 (The Model Assumption).",
              "Two models will make the same predictions if and only if they use the same reasoning process.",
              "Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).",
              "On similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).",
              "Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ],
            "highlighted_evidence": [
              "Defining Faithfulness ::: Assumption 1 (The Model Assumption).\nTwo models will make the same predictions if and only if they use the same reasoning process.",
              "Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).\nOn similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).\nCertain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Two models will make the same predictions if and only if they use the same reasoning process.",
              "On similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Defining Faithfulness ::: Assumption 1 (The Model Assumption).",
              "Two models will make the same predictions if and only if they use the same reasoning process.",
              "Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).",
              "On similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).",
              "Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ],
            "highlighted_evidence": [
              "Defining Faithfulness ::: Assumption 1 (The Model Assumption).\nTwo models will make the same predictions if and only if they use the same reasoning process.",
              "Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).\nOn similar inputs, the model makes similar decisions if and only if its reasoning is similar.",
              "Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).\nCertain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ]
          }
        ]
      },
      {
        "question": "Which are key points in guidelines for faithfulness evaluation?",
        "question_id": "2a859e80d8647923181cb2d8f9a2c67b1c3f4608",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Be explicit in what you evaluate.",
              "Faithfulness evaluation should not involve human-judgement on the quality of interpretation.",
              "Faithfulness evaluation should not involve human-provided gold labels.",
              "Do not trust “inherent interpretability” claims.",
              "Faithfulness evaluation of IUI systems should not rely on user performance."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Guidelines for Evaluating Faithfulness",
              "We propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.",
              "Guidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.",
              "Conflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques—be clear about which properties are being prioritized.",
              "Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.",
              "We note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.",
              "Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.",
              "We should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.",
              "Guidelines for Evaluating Faithfulness ::: Do not trust “inherent interpretability” claims.",
              "Inherent interpretability is a claim until proven otherwise. Explanations provided by “inherently interpretable” models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.",
              "Guidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.",
              "End-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness."
            ],
            "highlighted_evidence": [
              "Guidelines for Evaluating Faithfulness\nWe propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.\n\nGuidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.\nConflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques—be clear about which properties are being prioritized.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.\nWe note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.\nWe should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.\n\nGuidelines for Evaluating Faithfulness ::: Do not trust “inherent interpretability” claims.\nInherent interpretability is a claim until proven otherwise. Explanations provided by “inherently interpretable” models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.\nEnd-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/JacoviG20",
    "dblp_title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?",
    "year": "2020"
  },
  {
    "id": "1808.03894",
    "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
    "qas": [
      {
        "question": "Did they use the state-of-the-art model to analyze the attention?",
        "question_id": "aceac4ad16ffe1af0f01b465919b1d4422941a6b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we provide an extensive analysis of the state-of-the-art model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We make two main contributions. First, we introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency."
            ],
            "highlighted_evidence": [
              "Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency."
            ]
          }
        ]
      },
      {
        "question": "What is the performance of their model?",
        "question_id": "f7070b2e258beac9b09514be2bfcc5a528cc3a0e",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many layers are there in their model?",
        "question_id": "2efdcebebeb970021233553104553205ce5d6567",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "two LSTM layers"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Instead of considering individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference."
            ],
            "highlighted_evidence": [
              "Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference."
            ]
          }
        ]
      },
      {
        "question": "Did they compare with gradient-based methods?",
        "question_id": "4fa851d91388f0803e33f6cfae519548598cd37c",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/GhaeiniFT18",
    "dblp_title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.",
    "year": "2018"
  },
  {
    "id": "1703.04617",
    "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering",
    "qas": [
      {
        "question": "What MC abbreviate for?",
        "question_id": "a891039441e008f1fd0a227dbed003f76c140737",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "machine comprehension",
            "evidence": [
              "Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs."
            ],
            "highlighted_evidence": [
              "machine comprehension ",
              "Nelufar "
            ]
          }
        ]
      },
      {
        "question": "how much of improvement the adaptation model can get?",
        "question_id": "73738e42d488b32c9db89ac8adefc75403fa2653",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " 69.10%/78.38%"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set."
            ],
            "highlighted_evidence": [
              "69.10%/78.38%"
            ]
          }
        ]
      },
      {
        "question": "what is the architecture of the baseline model?",
        "question_id": "6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "word embedding, input encoder, alignment, aggregation, and prediction."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details."
            ],
            "highlighted_evidence": [
              "word embedding, input encoder, alignment, aggregation, and prediction"
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details.",
              "FLOAT SELECTED: Figure 1: A high level view of our basic model."
            ],
            "highlighted_evidence": [
              "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction.",
              "FLOAT SELECTED: Figure 1: A high level view of our basic model."
            ]
          }
        ]
      },
      {
        "question": "What is the exact performance on SQUAD?",
        "question_id": "fa218b297d9cdcae238cef71096752ce27ca8f4a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Our model achieves a 68.73% EM score and 77.39% F1 score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling)."
            ],
            "highlighted_evidence": [
              "Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/ZhangZCDWJ17",
    "dblp_title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering.",
    "year": "2017"
  },
  {
    "id": "1909.00578",
    "title": "SUM-QE: a BERT-based Summary Quality Estimation Model",
    "qas": [
      {
        "question": "What are their correlation results?",
        "question_id": "ff28d34d1aaa57e7ad553dba09fc924dc21dd728",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "High correlation results range from 0.472 to 0.936",
            "evidence": [
              "FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years."
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use?",
        "question_id": "ae8354e67978b7c333094c36bf9d561ca0c2d286",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems)."
            ],
            "highlighted_evidence": [
              "We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems)."
            ]
          }
        ]
      },
      {
        "question": "What simpler models do they look at?",
        "question_id": "02348ab62957cb82067c589769c14d798b1ceec7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BiGRU s with attention",
              "ROUGE",
              "Language model (LM)",
              "Next sentence prediction"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Methods ::: Baselines ::: BiGRU s with attention:",
              "This is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).",
              "Methods ::: Baselines ::: ROUGE:",
              "This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.",
              "Methods ::: Baselines ::: Language model (LM):",
              "For a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.",
              "Methods ::: Baselines ::: Next sentence prediction:",
              "BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:",
              "where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary."
            ],
            "highlighted_evidence": [
              "Methods ::: Baselines ::: BiGRU s with attention:\nThis is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n\nMethods ::: Baselines ::: ROUGE:\nThis baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.\n\nMethods ::: Baselines ::: Language model (LM):\nFor a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.\n\nMethods ::: Baselines ::: Next sentence prediction:\nBERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:\n\nwhere $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",
            "evidence": [
              "Methods ::: Baselines ::: BiGRU s with attention:",
              "This is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).",
              "Methods ::: Baselines ::: ROUGE:",
              "This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.",
              "Methods ::: Baselines ::: Language model (LM):",
              "For a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.",
              "Methods ::: Baselines ::: Next sentence prediction:",
              "BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:",
              "where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary."
            ],
            "highlighted_evidence": [
              "Methods ::: Baselines ::: BiGRU s with attention:\nThis is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n\nMethods ::: Baselines ::: ROUGE:\nThis baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.\n\nMethods ::: Baselines ::: Language model (LM):\nFor a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.\n\nMethods ::: Baselines ::: Next sentence prediction:\nBERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:\n\nwhere $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary."
            ]
          }
        ]
      },
      {
        "question": "What linguistic quality aspects are addressed?",
        "question_id": "3748787379b3a7d222c3a6254def3f5bfb93a60e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",
            "evidence": [
              "FLOAT SELECTED: Figure 1: SUM-QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 1: SUM-QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/XenouleasMAA19",
    "dblp_title": "SUM-QE: a BERT-based Summary Quality Estimation Model.",
    "year": "2019"
  },
  {
    "id": "1911.09419",
    "title": "Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction",
    "qas": [
      {
        "question": "What benchmark datasets are used for the link prediction task?",
        "question_id": "6852217163ea678f2009d4726cb6bd03cf6a8f78",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WN18RR",
              "FB15k-237",
              "YAGO3-10"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18."
            ],
            "highlighted_evidence": [
              "We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "WN18RR BIBREF26",
              "FB15k-237 BIBREF18",
              "YAGO3-10 BIBREF27"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18.",
              "WN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 BIBREF8, FB15k BIBREF8, and YAGO3 BIBREF27, respectively. As pointed out by BIBREF26 and BIBREF18, WN18 and FB15k suffer from the test set leakage problem. One can attain the state-of-the-art results even using a simple rule based model. Therefore, we use WN18RR and FB15k-237 as the benchmark datasets."
            ],
            "highlighted_evidence": [
              "We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. Details of these datasets are summarized in Table TABREF18.\n\nWN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 BIBREF8, FB15k BIBREF8, and YAGO3 BIBREF27, respectively."
            ]
          }
        ]
      },
      {
        "question": "What are state-of-the art models for this task?",
        "question_id": "cd1ad7e18d8eef8f67224ce47f3feec02718ea1a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "TransE",
              "DistMult",
              "ComplEx",
              "ConvE",
              "RotatE"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7."
            ],
            "highlighted_evidence": [
              "In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7."
            ]
          }
        ]
      },
      {
        "question": "How better does HAKE model peform than state-of-the-art methods?",
        "question_id": "9c9e90ceaba33242342a5ae7568e89fe660270d5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively",
              "doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10",
              "HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively. The superior performance of HAKE compared with RotatE implies that our proposed model can better model different levels in the hierarchy.",
              "FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets. However, the results also show that our models can gain better performance so long as there exists semantic hierarchies in knowledge graphs. As almost all knowledge graphs have such hierarchy structures, our model is widely applicable.",
              "YAGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively."
            ],
            "highlighted_evidence": [
              "WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively.",
              "FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets.",
              "AGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively."
            ]
          }
        ]
      },
      {
        "question": "How are entities mapped onto polar coordinate system?",
        "question_id": "2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively. That is, HAKE maps an entity $h$ to $[\\textbf {h}_m;\\textbf {h}_p]$, where $\\textbf {h}_m$ and $\\textbf {h}_p$ are generated by the modulus part and the phase part, respectively, and $[\\,\\cdot \\,; \\,\\cdot \\,]$ denotes the concatenation of two vectors. Obviously, $([\\textbf {h}_m]_i,[\\textbf {h}_p]_i)$ is a 2D point in the polar coordinate system. Specifically, we formulate HAKE as follows:"
            ],
            "highlighted_evidence": [
              "Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/nca/ZhangSZ24a",
    "dblp_title": "Learning hierarchy-aware complex knowledge graph embeddings for link prediction.",
    "year": "2024"
  },
  {
    "id": "1910.11471",
    "title": "Machine Translation from Natural Language to Code using Long-Short Term Memory",
    "qas": [
      {
        "question": "What additional techniques are incorporated?",
        "question_id": "db9021ddd4593f6fadf172710468e2fdcea99674",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "incorporating coding syntax tree model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–",
              "\"define the method tzname with 2 arguments: self and dt.\"",
              "is translated into–",
              "def __init__ ( self , regex ) :.",
              "The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax."
            ],
            "highlighted_evidence": [
              "Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–\n\n\"define the method tzname with 2 arguments: self and dt.\"\n\nis translated into–\n\ndef __init__ ( self , regex ) :.\n\nThe translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax."
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use?",
        "question_id": "8ea4bd4c1d8a466da386d16e4844ea932c44a412",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A parallel corpus where the source is an English expression of code and the target is Python code.",
            "evidence": [
              "SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."
            ],
            "highlighted_evidence": [
              "SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              " text-code parallel corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."
            ],
            "highlighted_evidence": [
              "A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it ."
            ]
          }
        ]
      },
      {
        "question": "Do they compare to other models?",
        "question_id": "92240eeab107a4f636705b88f00cefc4f0782846",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the architecture of the system?",
        "question_id": "4196d329061f5a9d147e1e77aeed6a6bd9b35d18",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "seq2seq translation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to train the translation model between text-to-code an open source Neural Machine Translation (NMT) - OpenNMT implementation is utilized BIBREF11. PyTorch is used as Neural Network coding framework. For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation."
            ],
            "highlighted_evidence": [
              "For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation."
            ]
          }
        ]
      },
      {
        "question": "How long are expressions in layman's language?",
        "question_id": "a37e4a21ba98b0259c36deca0d298194fa611d2f",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What additional techniques could be incorporated to further improve accuracy?",
        "question_id": "321429282557e79061fe2fe02a9467f3d0118cdd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "phrase-based word embedding",
              "Abstract Syntax Tree(AST)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial."
            ],
            "highlighted_evidence": [
              "In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial."
            ]
          }
        ]
      },
      {
        "question": "What programming language is target language?",
        "question_id": "891cab2e41d6ba962778bda297592c916b432226",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Python"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."
            ],
            "highlighted_evidence": [
              "In target data, the code is written in Python programming language."
            ]
          }
        ]
      },
      {
        "question": "What dataset is used to measure accuracy?",
        "question_id": "1eeabfde99594b8d9c6a007f50b97f7f527b0a17",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "validation data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Training parallel corpus had 18805 lines of annotated code in it. The training model is executed several times with different training parameters. During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. We run the training with epoch value of 10 with a batch size of 64. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% (Fig. FIGREF17)."
            ],
            "highlighted_evidence": [
              "During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data.",
              "After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40%"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-11471",
    "dblp_title": "Machine Translation from Natural Language to Code using Long-Short Term Memory.",
    "year": "2019"
  },
  {
    "id": "1910.09399",
    "title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis",
    "qas": [
      {
        "question": "Is text-to-image synthesis trained is suppervized or unsuppervized manner?",
        "question_id": "e96adf8466e67bd19f345578d5a6dc68fd0279a1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "unsupervised "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Following the above definition, the $\\min \\max $ objective function in Eq. (DISPLAY_FORM10) aims to learn parameters for the discriminator ($\\theta _d$) and generator ($\\theta _g$) to reach an optimization goal: The discriminator intends to differentiate true vs. fake images with maximum capability $\\max _{\\theta _d}$ whereas the generator intends to minimize the difference between a fake image vs. a true image $\\min _{\\theta _g}$. In other words, the discriminator sets the characteristics and the generator produces elements, often images, iteratively until it meets the attributes set forth by the discriminator. GANs are often used with images and other visual elements and are notoriously efficient in generating compelling and convincing photorealistic images. Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. The following sections go into further detail regarding how the generator and discriminator are trained in GANs."
            ],
            "highlighted_evidence": [
              "Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24. "
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "black Deep learning shed some light to some of the most sophisticated advances in natural language representation, image synthesis BIBREF7, BIBREF8, BIBREF43, BIBREF35, and classification of generic data BIBREF44. However, a bulk of the latest breakthroughs in deep learning and computer vision were related to supervised learning BIBREF8. Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis BIBREF45, BIBREF14, BIBREF8, BIBREF46, BIBREF47. These subproblems are typically subdivided as focused research areas. DC-GAN's contributions are mainly driven by these two research areas. In order to generate plausible images from natural language, DC-GAN contributions revolve around developing a straightforward yet effective GAN architecture and training strategy that allows natural text to image synthesis. These contributions are primarily tested on the Caltech-UCSD Birds and Oxford-102 Flowers datasets. Each image in these datasets carry five text descriptions. These text descriptions were created by the research team when setting up the evaluation environment. The DC-GANs model is subsequently trained on several subcategories. Subcategories in this research represent the training and testing sub datasets. The performance shown by these experiments display a promising yet effective way to generate images from textual natural language descriptions BIBREF8."
            ],
            "highlighted_evidence": [
              "Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis BIBREF45, BIBREF14, BIBREF8, BIBREF46, BIBREF47."
            ]
          }
        ]
      },
      {
        "question": "What challenges remain unresolved?",
        "question_id": "c1477a6c86bd1670dd17407590948000c9a6b7c6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "give more independence to the several learning methods (e.g. less human intervention) involved in the studies",
              "increasing the size of the output images"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples. In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images."
            ],
            "highlighted_evidence": [
              "Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images."
            ]
          }
        ]
      },
      {
        "question": "What is the conclusion of comparison of proposed solution?",
        "question_id": "e020677261d739c35c6f075cde6937d0098ace7f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset",
              "In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor",
              "text to image synthesis is continuously improving the results for better visual perception and interception"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While we gathered all the data we could find on scores for each model on the CUB, Oxford, and COCO datasets using IS, FID, FCN, and human classifiers, we unfortunately were unable to find certain data for AttnGAN and HDGAN (missing in Table TABREF48). The best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers. In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset. This is evidence that the attentional model and DAMSM introduced by AttnGAN are very effective in producing high-quality images. Examples of the best results of birds and plates of vegetables generated by each model are presented in Figures FIGREF50 and FIGREF51, respectively.",
              "blackIn terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis. However, StackGAN++ did introduce a very worthy enhancement for unconditional image generation by organizing the generators and discriminators in a “tree-like” structure. This indicates that revising the structures of the discriminators and/or generators can bring a moderate level of improvement in text-to-image synthesis.",
              "blackIn addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception. Technical wise, DM-GAN BIBREF53 is a model using dynamic memory to refine fuzzy image contents initially generated from the GAN networks. A memory writing gate is used for DM-GAN to select important text information and generate images based on he selected text accordingly. On the other hand, Obj-GAN BIBREF81 focuses on object centered text-to-image synthesis. The proposed framework of Obj-GAN consists of a layout generation, including a bounding box generator and a shape generator, and an object-driven attentive image generator. The designs and advancement of DM-GAN and Obj-GAN indicate that research in text-to-image synthesis is advancing to put more emphasis on the image details and text semantics for better understanding and perception."
            ],
            "highlighted_evidence": [
              "In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset.",
              "In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis.",
              "In addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception."
            ]
          }
        ]
      },
      {
        "question": "What is typical GAN architecture for each text-to-image synhesis group?",
        "question_id": "6389d5a152151fb05aae00b53b521c117d7b5e54",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Semantic Enhancement GANs: DC-GANs, MC-GAN\nResolution Enhancement GANs: StackGANs, AttnGAN, HDGAN\nDiversity Enhancement GANs: AC-GAN, TAC-GAN etc.\nMotion Enhancement GAGs: T2S, T2V, StoryGAN",
            "evidence": [
              "In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24. The taxonomy organizes GAN frameworks into four categories, including Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. Following the proposed taxonomy, each subsection will introduce several typical frameworks and address their techniques of using GANS to solve certain aspects of the text-to-mage synthesis challenges.",
              "FLOAT SELECTED: Figure 9. A Taxonomy and categorization of advanced GAN frameworks for Text-to-Image Synthesis. We categorize advanced GAN frameworks into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. The relationship between relevant frameworks and their publication date are also outlined as a reference."
            ],
            "highlighted_evidence": [
              "In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24.",
              "FLOAT SELECTED: Figure 9. A Taxonomy and categorization of advanced GAN frameworks for Text-to-Image Synthesis. We categorize advanced GAN frameworks into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. The relationship between relevant frameworks and their publication date are also outlined as a reference."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-09399",
    "dblp_title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis.",
    "year": "2019"
  },
  {
    "id": "1904.05584",
    "title": "Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study",
    "qas": [
      {
        "question": "Where do they employ feature-wise sigmoid gating?",
        "question_id": "7fe48939ce341212c1d801095517dc552b98e7b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "gating mechanism acts upon each dimension of the word and character-level vectors"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism."
            ],
            "highlighted_evidence": [
              "The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism."
            ]
          }
        ]
      },
      {
        "question": "Which model architecture do they use to obtain representations?",
        "question_id": "65ad17f614b7345f0077424c04c94971c831585b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BiLSTM with max pooling"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ."
            ],
            "highlighted_evidence": [
              "To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ."
            ]
          }
        ]
      },
      {
        "question": "Which downstream sentence-level tasks do they evaluate on?",
        "question_id": "323e100a6c92d3fe503f7a93b96d821408f92109",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF13 , BIBREF18"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .",
              "table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not necessarily to the URLs where SentEval got the data from.",
              "FLOAT SELECTED: Table B.2: Sentence representation evaluation datasets. SST5 was obtained from a GitHub repository with no associated peer-reviewed work."
            ],
            "highlighted_evidence": [
              "Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .",
              "table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper.",
              "FLOAT SELECTED: Table B.2: Sentence representation evaluation datasets. SST5 was obtained from a GitHub repository with no associated peer-reviewed work."
            ]
          }
        ]
      },
      {
        "question": "Which similarity datasets do they use?",
        "question_id": "9f89bff89cea722debc991363f0826de945bc582",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MEN",
              "MTurk287",
              "MTurk771",
              "RG",
              "RW",
              "SimLex999",
              "SimVerb3500",
              "WS353",
              "WS353R",
              "WS353S"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "table:word-similarity-dataset lists the word-similarity datasets and their corresponding reference. As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each.",
              "MEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets.",
              "MTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times.",
              "MTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 .",
              "RG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”.",
              "RW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words.",
              "SimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs.",
              "SimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness.",
              "WS353 contains 353 word pairs annotated with similarity scores from 0 to 10.",
              "WS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs.",
              "WS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously."
            ],
            "highlighted_evidence": [
              "As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each.\n\nMEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets.\n\nMTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times.\n\nMTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 .\n\nRG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”.\n\nRW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words.\n\nSimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs.\n\nSimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness.\n\nWS353 contains 353 word pairs annotated with similarity scores from 0 to 10.\n\nWS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs.\n\nWS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "WS353S",
              "SimLex999",
              "SimVerb3500"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). We also included the RareWords (RW) dataset for evaluating the quality of rare word representations. See appendix:datasets for a more complete description of the datasets we used."
            ],
            "highlighted_evidence": [
              "To face the previous problem, we tested our methods in a wide variety of datasets, including some that explicitly model relatedness (WS353R), some that explicitly consider similarity (WS353S, SimLex999, SimVerb3500), and some where the distinction is not clear (MEN, MTurk287, MTurk771, RG, WS353). "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/BalazsM19",
    "dblp_title": "Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study.",
    "year": "2019"
  },
  {
    "id": "1911.09886",
    "title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction",
    "qas": [
      {
        "question": "Are there datasets with relation tuples annotated, how big are datasets available?",
        "question_id": "735f58e28d84ee92024a36bc348cfac2ee114409",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11."
            ],
            "highlighted_evidence": [
              "This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations."
            ]
          }
        ]
      },
      {
        "question": "Which one of two proposed approaches performed better in experiments?",
        "question_id": "710fa8b3e74ee63d2acc20af19f95f7702b7ce5e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WordDecoding (WDec) model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively."
            ],
            "highlighted_evidence": [
              "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively."
            ]
          }
        ]
      },
      {
        "question": "What is previous work authors reffer to?",
        "question_id": "56123dd42cf5c77fc9a88fc311ed2e1eb672126e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SPTree",
              "Tagging",
              "CopyR",
              "HRL",
              "GraphR",
              "N-gram Attention"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our model with the following state-of-the-art joint entity and relation extraction models:",
              "(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities.",
              "(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities.",
              "(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple.",
              "(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities.",
              "(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations.",
              "(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17)."
            ],
            "highlighted_evidence": [
              "We compare our model with the following state-of-the-art joint entity and relation extraction models:\n\n(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM.",
              "(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder.",
              "(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations.",
              "(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction.",
              "(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them.",
              "(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data."
            ]
          }
        ]
      },
      {
        "question": "How higher are F1 scores compared to previous work?",
        "question_id": "1898f999626f9a6da637bd8b4857e5eddf2fc729",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively",
              "PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively."
            ],
            "highlighted_evidence": [
              "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively",
              "In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively."
            ],
            "highlighted_evidence": [
              "Among the baselines, HRL achieves significantly higher F1 scores on the two datasets.",
              "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.",
              "In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/NayakN20",
    "dblp_title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction.",
    "year": "2020"
  },
  {
    "id": "1611.01400",
    "title": "Learning to Rank Scientific Documents from the Crowd",
    "qas": [
      {
        "question": "what were the baselines?",
        "question_id": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Rank by the number of times a citation is mentioned in the document",
              " Rank by the number of times the citation is cited in the literature (citation impact). ",
              "Rank using Google Scholar Related Articles.",
              "Rank by the TF*IDF weighted cosine similarity. ",
              "ank using a learning-to-rank model trained on text similarity rankings"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
            ],
            "highlighted_evidence": [
              "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "(1) Rank by the number of times a citation is mentioned in the document.",
              "(2) Rank by the number of times the citation is cited in the literature (citation impact).",
              "(3) Rank using Google Scholar Related Articles.",
              "(4) Rank by the TF*IDF weighted cosine similarity.",
              "(5) Rank using a learning-to-rank model trained on text similarity rankings."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
            ],
            "highlighted_evidence": [
              "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings."
            ]
          }
        ]
      },
      {
        "question": "what is the supervised model they developed?",
        "question_id": "c10f38ee97ed80484c1a70b8ebba9b1fb149bc91",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SVMRank"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Support Vector Machine (SVM) ( BIBREF25 ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in ( BIBREF27 ) for a more traditional short query task and has been shown to be a top-performing system for ranking."
            ],
            "highlighted_evidence": [
              "SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. "
            ]
          }
        ]
      },
      {
        "question": "what is the size of this built corpus?",
        "question_id": "340501f23ddc0abe344a239193abbaaab938cc3a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We asked authors to rank documents by how “close to your work” they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."
            ],
            "highlighted_evidence": [
              "The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."
            ]
          }
        ]
      },
      {
        "question": "what crowdsourcing platform is used?",
        "question_id": "fbb85cbd41de6d2818e77e8f8d4b91e431931faa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "asked the authors to rank by closeness five citations we selected from their paper"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors' willingness to participate in an unpaid annotation task declines with the age of publication ( BIBREF23 ). We then extracted the abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation's abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation."
            ],
            "highlighted_evidence": [
              "Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/LingemanY16",
    "dblp_title": "Learning to Rank Scientific Documents from the Crowd.",
    "year": "2016"
  },
  {
    "id": "1808.05077",
    "title": "Exploiting Deep Learning for Persian Sentiment Analysis",
    "qas": [
      {
        "question": "Which deep learning model performed better?",
        "question_id": "1951cde612751410355610074c3c69cec94824c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "autoencoders"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1"
            ],
            "highlighted_evidence": [
              "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "CNN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the literature, deep learning based automated feature extraction has been shown to outperform state-of-the-art manual feature engineering based classifiers such as Support Vector Machine (SVM), Naive Bayes (NB) or Multilayer Perceptron (MLP) etc. One of the important techniques in deep learning is the autoencoder that generally involves reducing the number of feature dimensions under consideration. The aim of dimensionality reduction is to obtain a set of principal variables to improve the performance of the approach. Similarly, CNNs have been proven to be very effective in sentiment analysis. However, little work has been carried out to exploit deep learning based feature representation for Persian sentiment analysis BIBREF8 BIBREF9 . In this paper, we present two deep learning models (deep autoencoders and CNNs) for Persian sentiment analysis. The obtained deep learning results are compared with MLP.",
              "To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1"
            ],
            "highlighted_evidence": [
              "In this paper, we present two deep learning models (deep autoencoders and CNNs) for Persian sentiment analysis. ",
              "To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices.",
              "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%.",
              "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%."
            ]
          }
        ]
      },
      {
        "question": "By how much did the results improve?",
        "question_id": "4140d8b5a78aea985546aa1e323de12f63d24add",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What was their performance on the dataset?",
        "question_id": "61272b1d0338ed7708cf9ed9c63060a6a53e97a2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "accuracy of 82.6%"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1"
            ],
            "highlighted_evidence": [
              "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%."
            ]
          }
        ]
      },
      {
        "question": "How large is the dataset?",
        "question_id": "53b02095ba7625d85721692fce578654f66bbdf0",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/bics/DashtipourGAILH18",
    "dblp_title": "Exploiting Deep Learning for Persian Sentiment Analysis.",
    "year": "2018"
  },
  {
    "id": "1807.03367",
    "title": "Talk the Walk: Navigating New York City through Grounded Dialogue",
    "qas": [
      {
        "question": "Did the authors use crowdsourcing platforms?",
        "question_id": "0cd0755ac458c3bafbc70e4268c1e37b87b9721b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
            ],
            "highlighted_evidence": [
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We introduce the Talk the Walk dataset, where the aim is for two agents, a “guide” and a “tourist”, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location. The guide has access to a map and knows the target location, but does not know where the tourist is; the tourist has a 360-degree view of the world, but knows neither the target location on the map nor the way to it. The agents need to work together through communication in order to successfully solve the task. An example of the task is given in Figure FIGREF3 .",
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
            ],
            "highlighted_evidence": [
              "We introduce the Talk the Walk dataset, where the aim is for two agents, a “guide” and a “tourist”, to interact with each other via natural language in order to achieve a common goal: having the tourist navigate towards the correct location.",
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
            ]
          }
        ]
      },
      {
        "question": "How was the dataset collected?",
        "question_id": "c1ce652085ef9a7f02cb5c363ce2b8757adbe213",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."
            ],
            "highlighted_evidence": [
              "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)."
            ]
          }
        ]
      },
      {
        "question": "What language do the agents talk in?",
        "question_id": "96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "English",
            "evidence": [
              "Tourist: I can't go straight any further.",
              "Guide: ok. turn so that the theater is on your right.",
              "Guide: then go straight",
              "Tourist: That would be going back the way I came",
              "Guide: yeah. I was looking at the wrong bank",
              "Tourist: I'll notify when I am back at the brooks brothers, and the bank.",
              "Tourist: ACTION:TURNRIGHT ACTION:TURNRIGHT",
              "Guide: make a right when the bank is on your left",
              "Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNRIGHT",
              "Tourist: Making the right at the bank.",
              "Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT ACTION:TURNLEFT",
              "Tourist: I can't go that way.",
              "Tourist: ACTION:TURNLEFT ACTION:TURNLEFT ACTION:TURNLEFT",
              "Tourist: Bank is ahead of me on the right",
              "Tourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT",
              "Guide: turn around on that intersection",
              "Tourist: I can only go to the left or back the way I just came.",
              "Guide: you're in the right place. do you see shops on the corners?",
              "Guide: If you're on the corner with the bank, cross the street",
              "Tourist: I'm back where I started by the shop and the bank."
            ],
            "highlighted_evidence": [
              "Tourist: I can't go straight any further.\n\nGuide: ok. turn so that the theater is on your right.\n\nGuide: then go straight\n\nTourist: That would be going back the way I came\n\nGuide: yeah. I was looking at the wrong bank\n\nTourist: I'll notify when I am back at the brooks brothers, and the bank.\n\nTourist: ACTION:TURNRIGHT\n\nGuide: make a right when the bank is on your left\n\nTourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNRIGHT\n\nTourist: Making the right at the bank.\n\nTourist: ACTION:FORWARD ACTION:FORWARD\n\nTourist: I can't go that way.\n\nTourist: ACTION:TURNLEFT\n\nTourist: Bank is ahead of me on the right\n\nTourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT\n\nGuide: turn around on that intersection\n\nTourist: I can only go to the left or back the way I just came.\n\nTourist: ACTION:TURNLEFT\n\nGuide: you're in the right place. do you see shops on the corners?\n\nGuide: If you're on the corner with the bank, cross the street\n\nTourist: I'm back where I started by the shop and the bank.\n\nTourist: ACTION:TURNRIGHT"
            ]
          }
        ]
      },
      {
        "question": "What evaluation metrics did the authors look at?",
        "question_id": "b85ab5f862221fac819cf2fef239bcb08b9cafc6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "localization accuracy"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section, we describe the findings of various experiments. First, we analyze how much information needs to be communicated for accurate localization in the Talk The Walk environment, and find that a short random path (including actions) is necessary. Next, for emergent language, we show that the MASC architecture can achieve very high localization accuracy, significantly outperforming the baseline that does not include this mechanism. We then turn our attention to the natural language experiments, and find that localization from human utterances is much harder, reaching an accuracy level that is below communicating a single landmark observation. We show that generated utterances from a conditional language model leads to significantly better localization performance, by successfully grounding the utterance on a single landmark observation (but not yet on multiple observations and actions). Finally, we show performance of the localization baseline on the full task, which can be used for future comparisons to this work."
            ],
            "highlighted_evidence": [
              "Next, for emergent language, we show that the MASC architecture can achieve very high localization accuracy, significantly outperforming the baseline that does not include this mechanism."
            ]
          }
        ]
      },
      {
        "question": "What data did they use?",
        "question_id": "7e34501255b89d64b9598b409d73f96489aafe45",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " dataset on Mechanical Turk involving human perception, action and communication"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Talk The Walk is the first task to bring all three aspects together: perception for the tourist observing the world, action for the tourist to navigate through the environment, and interactive dialogue for the tourist and guide to work towards their common goal. To collect grounded dialogues, we constructed a virtual 2D grid environment by manually capturing 360-views of several neighborhoods in New York City (NYC). As the main focus of our task is on interactive dialogue, we limit the difficulty of the control problem by having the tourist navigating a 2D grid via discrete actions (turning left, turning right and moving forward). Our street view environment was integrated into ParlAI BIBREF6 and used to collect a large-scale dataset on Mechanical Turk involving human perception, action and communication."
            ],
            "highlighted_evidence": [
              " Our street view environment was integrated into ParlAI BIBREF6 and used to collect a large-scale dataset on Mechanical Turk involving human perception, action and communication."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1807-03367",
    "dblp_title": "Talk the Walk: Navigating New York City through Grounded Dialogue.",
    "year": "2018"
  },
  {
    "id": "1907.02030",
    "title": "Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks",
    "qas": [
      {
        "question": "Do the authors report results only on English data?",
        "question_id": "e854edcc5e9111922e6e120ae17d062427c27ec1",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is the accuracy of the system measured?",
        "question_id": "bd6cec2ab620e67b3e0e7946fc045230e6906020",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates",
              "distances between duplicate and non-duplicate questions using different embedding systems"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems. The X axis shows the euclidean distance between vectors and the Y axis frequency. A perfect result would be a blue peak to the left and an entirely disconnected orange spike to the right, showing that all non-duplicate questions have a greater euclidean distance than the least similar duplicate pair of questions. As can be clearly seen in the figure above, Elmo BIBREF23 and Infersent BIBREF13 show almost no separation and therefore cannot be considered good models for this problem. A much greater disparity is shown by the Google USE models BIBREF14 , and even more for the Google USE Large model. In fact the Google USE Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates.",
              "In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate. We had no original data on whether sentences were semantically similar, but we did have a corpus of articles clustered into stories. Working on the assumption that similar claims would be more likely to be in the same story, we developed an equation to judge how well our corpus of sentences was clustered, rewarding clustering which matches the article clustering and the total number of claims clustered. The precise formula is given below, where INLINEFORM0 is the proportion of claims in clusters from one story cluster, INLINEFORM1 is the proportion of claims in the correct claim cluster, where they are from the most common story cluster, and INLINEFORM2 is the number of claims placed in clusters. A,B and C are parameters to tune. INLINEFORM3"
            ],
            "highlighted_evidence": [
              "The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems.",
              "Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates.",
              "In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate."
            ]
          }
        ]
      },
      {
        "question": "How is an incoming claim used to retrieve similar factchecked claims?",
        "question_id": "4b0ba460ae3ba7a813f204abd16cf631b871baca",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "text clustering on the embeddings of texts"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Traditional text clustering methods, using TFIDF and some clustering algorithm, are poorly suited to the problem of clustering and comparing short texts, as they can be semantically very similar but use different words. This is a manifestation of the the data sparsity problem with Bag-of-Words (BoW) models. BIBREF16 . Dimensionality reduction methods such as Latent Dirichlet Allocation (LDA) can help solve this problem by giving a dense approximation of this sparse representation BIBREF17 . More recently, efforts in this area have used text embedding-based systems in order to capture dense representation of the texts BIBREF18 . Much of this recent work has relied on the increase of focus in word and text embeddings. Text embeddings have been an increasingly popular tool in NLP since the introduction of Word2Vec BIBREF19 , and since then the number of different embeddings has exploded. While many focus on giving a vector representation of a word, an increasing number now exist that will give a vector representation of a entire sentence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time"
            ],
            "highlighted_evidence": [
              "While many focus on giving a vector representation of a word, an increasing number now exist that will give a vector representation of a entire sentence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time"
            ]
          }
        ]
      },
      {
        "question": "What existing corpus is used for comparison in these experiments?",
        "question_id": "63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Quora duplicate question dataset BIBREF22"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to choose an embedding, we sought a dataset to represent our problem. Although no perfect matches exist, we decided upon the Quora duplicate question dataset BIBREF22 as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions."
            ],
            "highlighted_evidence": [
              "Although no perfect matches exist, we decided upon the Quora duplicate question dataset BIBREF22 as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions."
            ]
          }
        ]
      },
      {
        "question": "What are the components in the factchecking algorithm? ",
        "question_id": "d27f23bcd80b12f6df8e03e65f9b150444925ecf",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/sigir/AdlerB19",
    "dblp_title": "Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks.",
    "year": "2019"
  },
  {
    "id": "1910.04601",
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "qas": [
      {
        "question": "What is the baseline?",
        "question_id": "b11ee27f3de7dd4a76a1f158dc13c2331af37d9f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " path ranking-based KGC (PRKGC)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "RC-QED$^{\\rm E}$ can be naturally solved by path ranking-based KGC (PRKGC), where the query triplet and the sampled paths correspond to a question and derivation steps, respectively. PRKGC meets our purposes because of its glassboxness: we can trace the derivation steps of the model easily."
            ],
            "highlighted_evidence": [
              "RC-QED$^{\\rm E}$ can be naturally solved by path ranking-based KGC (PRKGC), where the query triplet and the sampled paths correspond to a question and derivation steps, respectively."
            ]
          }
        ]
      },
      {
        "question": "What dataset was used in the experiment?",
        "question_id": "7aba5e4483293f5847caad144ee0791c77164917",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WikiHop"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural language statements (Macchu Picchu is located in Peru) using a simple conversion dictionary."
            ],
            "highlighted_evidence": [
              "Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used."
            ]
          }
        ]
      },
      {
        "question": "Did they use any crowdsourcing platform?",
        "question_id": "565d668947ffa6d52dad019af79289420505889b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
            ],
            "highlighted_evidence": [
              "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance."
            ],
            "highlighted_evidence": [
              "We deployed the task on Amazon Mechanical Turk (AMT)"
            ]
          }
        ]
      },
      {
        "question": "How was the dataset annotated?",
        "question_id": "d83304c70fe66ae72e78aa1d183e9f18b7484cd6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable)",
              "why they are unsure from two choices (“Not stated in the article” or “Other”)",
              "The “summary” text boxes"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Given a statement and articles, workers are asked to judge whether the statement can be derived from the articles at three grades: True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable). If a worker selects Unsure, we ask workers to tell us why they are unsure from two choices (“Not stated in the article” or “Other”).",
              "If a worker selects True or Likely in the judgement task, we first ask which sentences in the given articles are justification explanations for a given statement, similarly to HotpotQA BIBREF2. The “summary” text boxes (i.e. NLDs) are then initialized with these selected sentences. We give a ¢6 bonus to those workers who select True or Likely. To encourage an abstraction of selected sentences, we also introduce a gamification scheme to give a bonus to those who provide shorter NLDs. Specifically, we probabilistically give another ¢14 bonus to workers according to a score they gain. The score is always shown on top of the screen, and changes according to the length of NLDs they write in real time. To discourage noisy annotations, we also warn crowdworkers that their work would be rejected for noisy submissions. We periodically run simple filtering to exclude noisy crowdworkers (e.g. workers who give more than 50 submissions with the same answers)."
            ],
            "highlighted_evidence": [
              "Given a statement and articles, workers are asked to judge whether the statement can be derived from the articles at three grades: True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable). If a worker selects Unsure, we ask workers to tell us why they are unsure from two choices (“Not stated in the article” or “Other”).",
              "If a worker selects True or Likely in the judgement task, we first ask which sentences in the given articles are justification explanations for a given statement, similarly to HotpotQA BIBREF2. The “summary” text boxes (i.e. NLDs) are then initialized with these selected sentences."
            ]
          }
        ]
      },
      {
        "question": "What is the source of the proposed dataset?",
        "question_id": "e90ac9ee085dc2a9b6fe132245302bbce5f3f5ab",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-04601",
    "dblp_title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension.",
    "year": "2019"
  },
  {
    "id": "1912.05066",
    "title": "Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds",
    "qas": [
      {
        "question": "How many label options are there in the multi-label task?",
        "question_id": "5b029ad0d20b516ec11967baaf7d2006e8d7199f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " two labels "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the sentiment analysis, we look at our problem in a multi-label setting, our two labels being sentiment polarity and the candidate/category in consideration. We test both single-label classifiers and multi-label ones on the problem and as intuition suggests, the multi-label classifier RaKel performs better. A combination of document-embedding features BIBREF3 and topic features (essentially the document-topic probabilities) BIBREF4 is shown to give the best results. These features make sense intuitively because the document-embedding features take context of the text into account, which is important for sentiment polarity classification, and topic features take into account the topic of the tweet (who/what is it about)."
            ],
            "highlighted_evidence": [
              "For the sentiment analysis, we look at our problem in a multi-label setting, our two labels being sentiment polarity and the candidate/category in consideration."
            ]
          }
        ]
      },
      {
        "question": "What is the interannotator agreement of the crowd sourced users?",
        "question_id": "79bd2ad4cb5c630ce69d5a859ed118132cae62d7",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Who are the experts?",
        "question_id": "d3a1a53521f252f869fdae944db986931d9ffe48",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "political pundits of the Washington Post"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd."
            ],
            "highlighted_evidence": [
              "Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "the experts in the field"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "This paper presents a study that compares the opinions of users on microblogs, which is essentially the crowd wisdom, to that of the experts in the field. Specifically, we explore three datasets: US Presidential Debates 2015-16, Grammy Awards 2013, Super Bowl 2013. We determined if the opinions of the crowd and the experts match by using the sentiments of the tweets to predict the outcomes of the debates/Grammys/Super Bowl. We observed that in most of the cases, the predictions were right indicating that crowd wisdom is indeed worth looking at and mining sentiments in microblogs is useful. In some cases where there were disagreements, however, we observed that the opinions of the experts did have some influence on the opinions of the users. We also find that the features that were most useful in our case of multi-label classification was a combination of the document-embedding and topic features."
            ],
            "highlighted_evidence": [
              "This paper presents a study that compares the opinions of users on microblogs, which is essentially the crowd wisdom, to that of the experts in the field. Specifically, we explore three datasets: US Presidential Debates 2015-16, Grammy Awards 2013, Super Bowl 2013. "
            ]
          }
        ]
      },
      {
        "question": "Who is the crowd in these experiments?",
        "question_id": "38e11663b03ac585863742044fd15a0e875ae9ab",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " peoples' sentiments expressed over social media"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd."
            ],
            "highlighted_evidence": [
              "The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd."
            ]
          }
        ]
      },
      {
        "question": "How do you establish the ground truth of who won a debate?",
        "question_id": "14421b7ae4459b647033b3ccba635d4ba7bb114b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "experts in Washington Post"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Trend Analysis: We also analyze some certain trends of the debates. Firstly, we look at the change in sentiments of the users towards the candidates over time (hours, days, months). This is done by computing the sentiment scores for each candidate in each of the debates and seeing how it varies over time, across debates. Secondly, we examine the effect of Washington Post on the views of the users. This is done by looking at the sentiments of the candidates (to predict winners) of a debate before and after the winners are announced by the experts in Washington Post. This way, we can see if Washington Post has had any effect on the sentiments of the users. Besides that, to study the behavior of the users, we also look at the correlation of the tweet volume with the number of viewers as well as the variation of tweet volume over time (hours, days, months) for debates.",
              "Next, we investigate how the sentiments of the users towards the candidates change before and after the debate. In essence, we examine how the debate and the results of the debates given by the experts affects the sentiment of the candidates. Figure FIGREF25 shows the sentiments of the users towards the candidate during the 5th Republican Debate, 15th December 2015. It can be seen that the sentiments of the users towards the candidates does indeed change over the course of two days. One particular example is that of Jeb Bush. It seems that the populace are generally prejudiced towards the candidates, which is reflected in their sentiments of the candidates on the day of the debate. The results of the Washington Post are released in the morning after the debate. One can see the winners suggested by the Washington Post in Table TABREF35. One of the winners in that debate according to them is Jeb Bush. Coincidentally, Figure FIGREF25 suggests that the sentiment of Bush has gone up one day after the debate (essentially, one day after the results given by the experts are out)."
            ],
            "highlighted_evidence": [
              "Secondly, we examine the effect of Washington Post on the views of the users. This is done by looking at the sentiments of the candidates (to predict winners) of a debate before and after the winners are announced by the experts in Washington Post. This way, we can see if Washington Post has had any effect on the sentiments of the users.",
              "One can see the winners suggested by the Washington Post in Table TABREF35. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-05066",
    "dblp_title": "Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds.",
    "year": "2019"
  },
  {
    "id": "1910.03891",
    "title": "Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding",
    "qas": [
      {
        "question": "How much better is performance of proposed method than state-of-the-art methods in experiments?",
        "question_id": "52f7e42fe8f27d800d1189251dfec7446f0e1d3b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",
            "evidence": [
              "Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power.",
              "FLOAT SELECTED: Table 2: Entity classification results in accuracy. We run all models 10 times and report mean ± standard deviation. KANE significantly outperforms baselines on FB24K, DBP24K and Game30K."
            ],
            "highlighted_evidence": [
              "Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets.",
              "FLOAT SELECTED: Table 2: Entity classification results in accuracy. We run all models 10 times and report mean ± standard deviation. KANE significantly outperforms baselines on FB24K, DBP24K and Game30K."
            ]
          }
        ]
      },
      {
        "question": "What further analysis is done?",
        "question_id": "00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we use t-SNE tool BIBREF27 to visualize the learned embedding"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF30 shows the test accuracy with increasing epoch on DBP24K and Game30K. We can see that test accuracy first rapidly increased in the first ten iterations, but reaches a stable stages when epoch is larger than 40. Figure FIGREF31 shows test accuracy with different embedding size and training data proportions. We can note that too small embedding size or training data proportions can not generate sufficient global information. In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding. Figure FIGREF32 shows the visualization of 256 dimensional entity's embedding on Game30K learned by KANE, R-GCN, PransE and TransE. We observe that our method can learn more discriminative entity's embedding than other other methods."
            ],
            "highlighted_evidence": [
              "In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding."
            ]
          }
        ]
      },
      {
        "question": "What seven state-of-the-art methods are used for comparison?",
        "question_id": "aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "TransE, TransR and TransH",
              "PTransE, and ALL-PATHS",
              "R-GCN BIBREF24 and KR-EAR BIBREF26"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines. For TransE, the dissimilarity measure is implemented with L1-norm, and relation as well as entity are replaced during negative sampling. For TransR, we directly use the source codes released in BIBREF9. In order for better performance, the replacement of relation in negative sampling is utilized according to the suggestion of author.",
              "2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18. PTransE is the first method to model relation path in KG embedding task, and ALL-PATHS improve the PTransE through a dynamic programming algorithm which can incorporate all relation paths of bounded length.",
              "3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets."
            ],
            "highlighted_evidence": [
              "1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines.",
              "2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18.",
              "3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets."
            ]
          }
        ]
      },
      {
        "question": "What three datasets are used to measure performance?",
        "question_id": "cf0085c1d7bd9bc9932424e4aba4e6812d27f727",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "FB24K",
              "DBP24K",
              "Game30K"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24."
            ],
            "highlighted_evidence": [
              "First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K."
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [
              "Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24."
            ],
            "highlighted_evidence": [
              "In this study, we evaluate our model on three real KG including two typical large-scale knowledge graph: Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph. First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. The statistics of datasets are listed in Table TABREF24."
            ]
          }
        ]
      },
      {
        "question": "How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?",
        "question_id": "586b7470be91efe246c3507b05e30651ea6b9832",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "To capture both high-order structural information of KGs, we used an attention-based embedding propagation method."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The process of KANE is illustrated in Figure FIGREF2. We introduce the architecture of KANE from left to right. As shown in Figure FIGREF2, the whole triples of knowledge graph as input. The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method. This method can recursively propagate the embeddings of entities from an entity's neighbors, and aggregate the neighbors with different weights. The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification."
            ],
            "highlighted_evidence": [
              "The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.",
              "The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification."
            ]
          }
        ]
      },
      {
        "question": "What are recent works on knowedge graph embeddings authors mention?",
        "question_id": "31b20a4bab09450267dfa42884227103743e3426",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "entity types or concepts BIBREF13",
              "relations paths BIBREF17",
              " textual descriptions BIBREF11, BIBREF12",
              "logical rules BIBREF23",
              "deep neural network models BIBREF24"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to address this issue, TransH BIBREF8 models a relation as a relation-specific hyperplane together with a translation on it, allowing entities to have distinct representation in different relations. TransR BIBREF9 models entities and relations in separate spaces, i.e., entity space and relation spaces, and performs translation from entity spaces to relation spaces. TransD BIBREF22 captures the diversity of relations and entities simultaneously by defining dynamic mapping matrix. Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24."
            ],
            "highlighted_evidence": [
              "Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-03891",
    "dblp_title": "Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding.",
    "year": "2019"
  },
  {
    "id": "1610.00879",
    "title": "A Computational Approach to Automatic Prediction of Drunk Texting",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "45306b26447ea4b120655d6bb2e3636079d3d6e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Figure 1: Word cloud for drunk tweets"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 1: Word cloud for drunk tweets"
            ]
          }
        ]
      },
      {
        "question": "Do the authors mention any confounds to their study?",
        "question_id": "0c08af6e4feaf801185f2ec97c4da04c8b767ad6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "A key challenge is to obtain an annotated dataset. We use hashtag-based supervision so that the authors of the tweets mention if they were drunk at the time of posting a tweet. We create three datasets by using different strategies that are related to the use of hashtags. We then present SVM-based classifiers that use N-gram and stylistic features such as capitalisation, spelling errors, etc. Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. To the best of our knowledge, this is a first study that shows the feasibility of text-based analysis for drunk-texting prediction."
            ],
            "highlighted_evidence": [
              "Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work."
            ]
          }
        ]
      },
      {
        "question": "What baseline model is used?",
        "question_id": "6412e97373e8e9ae3aa20aa17abef8326dc05450",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Human evaluators",
            "evidence": [
              "FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set"
            ]
          }
        ]
      },
      {
        "question": "What stylistic features are used to detect drunk texts?",
        "question_id": "957bda6b421ef7d2839c3cec083404ac77721f14",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio",
            "evidence": [
              "FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction"
            ]
          },
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.",
            "evidence": [
              "FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction"
            ]
          }
        ]
      },
      {
        "question": "Is the data acquired under distant supervision verified by humans at any stage?",
        "question_id": "368317b4fd049511e00b441c2e9550ded6607c37",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Using held-out dataset H, we evaluate how our system performs in comparison to humans. Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. Table TABREF19 shows a moderate agreement between our annotators (for example, it is 0.42 for A1 and A2). Table TABREF20 compares our classifier with humans. Our human annotators perform the task with an average accuracy of 68.8%, while our classifier (with all features) trained on Dataset 2 reaches 64%. The classifier trained on Dataset 2 is better than which is trained on Dataset 1."
            ],
            "highlighted_evidence": [
              "Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober."
            ]
          }
        ]
      },
      {
        "question": "What hashtags are used for distant supervision?",
        "question_id": "b3ec918827cd22b16212265fcdd5b3eadee654ae",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do the authors equate drunk tweeting with drunk texting? ",
        "question_id": "387970ebc7ef99f302f318d047f708274c0e8f21",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The ubiquity of communication devices has made social media highly accessible. The content on these media reflects a user's day-to-day activities. This includes content created under the influence of alcohol. In popular culture, this has been referred to as `drunk-texting'. In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. We refer to tweets written under the influence of alcohol as `drunk tweets', and the opposite as `sober tweets'."
            ],
            "highlighted_evidence": [
              "In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/JoshiMABC16",
    "dblp_title": "A Computational Approach to Automatic Prediction of Drunk Texting.",
    "year": "2016"
  },
  {
    "id": "1704.05572",
    "title": "Answering Complex Questions Using Open Information Extraction",
    "qas": [
      {
        "question": "What corpus was the source of the OpenIE extractions?",
        "question_id": "2fffff59e57b8dbcaefb437a6b3434fc137f813b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).",
              "We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams."
            ],
            "highlighted_evidence": [
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. ",
              "Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).",
              "The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. "
            ]
          },
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the accuracy of the proposed technique?",
        "question_id": "eb95af36347ed0e0808e19963fe4d058e2ce3c9f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge",
            "evidence": [
              "FLOAT SELECTED: Table 2: TUPLEINF is significantly better at structured reasoning than TABLEILP.9"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: TUPLEINF is significantly better at structured reasoning than TABLEILP.9"
            ]
          }
        ]
      },
      {
        "question": "Is an entity linking process used?",
        "question_id": "cd1792929b9fa5dd5b1df0ae06fc6aece4c97424",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.",
              "Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $ &\\textit {tf}(x, q)=1\\; \\textmd {if x} \\in q ; \\textit {idf}(x) = log(1 + N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $"
            ],
            "highlighted_evidence": [
              "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.",
              "Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ ."
            ]
          }
        ]
      },
      {
        "question": "Are the OpenIE extractions all triples?",
        "question_id": "65d34041ffa4564385361979a08706b10b92ebc7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We create an additional table in TableILP with all the tuples in $T$ . Since TableILP uses fixed-length $(subject; predicate; object)$ triples, we need to map tuples with multiple objects to this format. For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table."
            ],
            "highlighted_evidence": [
              "For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table."
            ]
          }
        ]
      },
      {
        "question": "What method was used to generate the OpenIE extractions?",
        "question_id": "e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S",
              "take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ],
            "highlighted_evidence": [
              "For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ]
          }
        ]
      },
      {
        "question": "Can the method answer multi-hop questions?",
        "question_id": "a8545f145d5ea2202cb321c8f93e75ad26fcf4aa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
            ],
            "highlighted_evidence": [
              "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
            ]
          }
        ]
      },
      {
        "question": "What was the textual source to which OpenIE was applied?",
        "question_id": "417dabd43d6266044d38ed88dbcb5fdd7a426b22",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams.",
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ],
            "highlighted_evidence": [
              "The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining.",
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. ",
              "We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ]
          }
        ]
      },
      {
        "question": "What OpenIE method was used to generate the extractions?",
        "question_id": "fed230cef7c130f6040fb04304a33bbc17ca3a36",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S",
              "take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ],
            "highlighted_evidence": [
              "Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T)."
            ]
          }
        ]
      },
      {
        "question": "Is their method capable of multi-hop reasoning?",
        "question_id": "7917d44e952b58ea066dc0b485d605c9a1fe3dda",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
            ],
            "highlighted_evidence": [
              "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/KhotSC17",
    "dblp_title": "Answering Complex Questions Using Open Information Extraction.",
    "year": "2017"
  },
  {
    "id": "1804.10686",
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "qas": [
      {
        "question": "Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?",
        "question_id": "7d5ba230522df1890619dedcfb310160958223c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We use two different unsupervised approaches for word sense disambiguation. The first, called `sparse model', uses a straightforward sparse vector space model, as widely used in Information Retrieval, to represent contexts and synsets. The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.",
              "In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. We transform every synset into its dense vector representation by averaging the word embeddings corresponding to each constituent word: DISPLAYFORM0",
              "We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size."
            ],
            "highlighted_evidence": [
              "We use two different unsupervised approaches for word sense disambiguation. ",
              "The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.",
              "In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. ",
              "We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. "
            ]
          }
        ]
      },
      {
        "question": "What evaluation is conducted?",
        "question_id": "a48cc6d3d322a7b159ff40ec162a541bf74321eb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Word Sense Induction & Disambiguation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 . In the gold standard, each word is provided with a set of instances, i.e., the sentences containing the word. Each instance is manually annotated with the single sense identifier according to a pre-defined sense inventory. Each participating system estimates the sense labels for these ambiguous words, which can be viewed as a clustering of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation."
            ],
            "highlighted_evidence": [
              "We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 . "
            ]
          }
        ]
      },
      {
        "question": "Which corpus of synsets are used?",
        "question_id": "2bc0bb7d3688fdd2267c582ca593e2ce72718a91",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Wiktionary"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The following different sense inventories have been used during the evaluation:",
              "Watlink, a word sense network constructed automatically. It uses the synsets induced in an unsupervised way by the Watset[CWnolog, MCL] method BIBREF2 and the semantic relations from such dictionaries as Wiktionary referred as Joint INLINEFORM0 Exp INLINEFORM1 SWN in Ustalov:17:dialogue. This is the only automatically built inventory we use in the evaluation."
            ],
            "highlighted_evidence": [
              "The following different sense inventories have been used during the evaluation:",
              "Watlink, a word sense network constructed automatically. It uses the synsets induced in an unsupervised way by the Watset[CWnolog, MCL] method BIBREF2 and the semantic relations from such dictionaries as Wiktionary referred as Joint INLINEFORM0 Exp INLINEFORM1 SWN in Ustalov:17:dialogue."
            ]
          }
        ]
      },
      {
        "question": "What measure of semantic similarity is used?",
        "question_id": "8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "cosine similarity"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/UstalovTPCBP18",
    "dblp_title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages.",
    "year": "2018"
  },
  {
    "id": "1707.03904",
    "title": "Quasar: Datasets for Question Answering by Search and Reading",
    "qas": [
      {
        "question": "Which retrieval system was used for baselines?",
        "question_id": "dcb18516369c3cf9838e83168357aed6643ae1b8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.",
            "evidence": [
              "Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant.",
              "The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question.",
              "Several baselines rely on the retrieved context to extract the answer to a question. For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy. The performance of the baseline among these instances is referred to as the Reading Accuracy, and the overall performance (which is a product of the two) is referred to as the Overall Accuracy. In Figure 4 we compare how these three vary as the number of context documents is varied. Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents. Hence, simply retrieving more documents is not sufficient – finding the few most relevant ones will allow the reader to work best."
            ],
            "highlighted_evidence": [
              "Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution.",
              "The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question.",
              "Several baselines rely on the retrieved context to extract the answer to a question. For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy.",
              "Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/DhingraMC17",
    "dblp_title": "Quasar: Datasets for Question Answering by Search and Reading.",
    "year": "2017"
  },
  {
    "id": "1911.07228",
    "title": "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models",
    "qas": [
      {
        "question": "What word embeddings were used?",
        "question_id": "f46a907360d75ad566620e7f6bf7746497b6e4a9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Kyubyong Park",
              "Edouard Grave et al BIBREF11"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:",
              "Kyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. His word embedding is the vector of 100 dimension and it has about 10k words.",
              "Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words"
            ],
            "highlighted_evidence": [
              "We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:\n\nKyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps.",
              "Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia."
            ]
          }
        ]
      },
      {
        "question": "What type of errors were produced by the BLSTM-CNN-CRF system?",
        "question_id": "79d999bdf8a343ce5b2739db3833661a1deab742",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Step 2: Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2)."
            ],
            "highlighted_evidence": [
              "Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2)."
            ]
          }
        ]
      },
      {
        "question": "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?",
        "question_id": "71d59c36225b5ee80af11d3568bdad7425f17b0c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ",
            "evidence": [
              "Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.",
              "FLOAT SELECTED: Table 2. F1 score of two models with different pre-trained word embeddings"
            ],
            "highlighted_evidence": [
              "Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.",
              "FLOAT SELECTED: Table 2. F1 score of two models with different pre-trained word embeddings"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-07228",
    "dblp_title": "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models.",
    "year": "2019"
  },
  {
    "id": "1603.07044",
    "title": "Recurrent Neural Network Encoder with Attention for Community Question Answering",
    "qas": [
      {
        "question": "What supplemental tasks are used for multitask learning?",
        "question_id": "efc65e5032588da4a134d121fe50d49fe8fe5e8c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",
            "evidence": [
              "Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval BIBREF0 could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include informal usage of language, highly diverse content of comments, and variation in the length of both questions and comments.",
              "In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships using RNNs. Parallel LSTMs encode two objects independently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classification.",
              "For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC)."
            ],
            "highlighted_evidence": [
              "Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C).",
              "In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant.",
              "For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC)."
            ]
          }
        ]
      },
      {
        "question": "Is the improvement actually coming from using an RNN?",
        "question_id": "a30958c7123d1ad4723dcfd19d8346ccedb136d5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How much performance gap between their approach and the strong handcrafted method?",
        "question_id": "08333e4dd1da7d6b5e9b645d40ec9d502823f5d7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",
            "evidence": [
              "FLOAT SELECTED: Table 4: Compared with other systems (bold is best)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Compared with other systems (bold is best)."
            ]
          }
        ]
      },
      {
        "question": "What is a strong feature-based method?",
        "question_id": "bc1bc92920a757d5ec38007a27d0f49cb2dde0d1",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Did they experimnet in other languages?",
        "question_id": "942eb1f7b243cdcfd47f176bcc71de2ef48a17c4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Moreover, our approach is also language independent. We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a hand-tuned strong baseline from SemEval-2015."
            ],
            "highlighted_evidence": [
              "We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/HsuZG16",
    "dblp_title": "Recurrent Neural Network Encoder with Attention for Community Question Answering.",
    "year": "2016"
  },
  {
    "id": "1902.09314",
    "title": "Attentional Encoder Network for Targeted Sentiment Classification",
    "qas": [
      {
        "question": "Do they use multi-attention heads?",
        "question_id": "9bffc9a9c527e938b2a95ba60c483a916dbd1f6b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
            ],
            "highlighted_evidence": [
              "This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
            ]
          }
        ]
      },
      {
        "question": "How big is their model?",
        "question_id": "8434974090491a3c00eed4f22a878f0b70970713",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed model has 1.16 million parameters and 11.04 MB.",
            "evidence": [
              "To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU .",
              "FLOAT SELECTED: Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold."
            ],
            "highlighted_evidence": [
              "Statistical results are reported in Table TABREF37 .",
              "FLOAT SELECTED: Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold."
            ]
          }
        ]
      },
      {
        "question": "How is their model different from BERT?",
        "question_id": "b67420da975689e47d3ea1c12b601851018c4071",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT."
            ],
            "highlighted_evidence": [
              "Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1902-09314",
    "dblp_title": "Attentional Encoder Network for Targeted Sentiment Classification.",
    "year": "2019"
  },
  {
    "id": "1904.03339",
    "title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples",
    "qas": [
      {
        "question": "What datasets were used?",
        "question_id": "01d91d356568fca79e47873bd0541bd22ba66ec0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "datasets given on the shared task, without using any additional external data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "JESSI is trained using only the datasets given on the shared task, without using any additional external data. Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%."
            ],
            "highlighted_evidence": [
              "JESSI is trained using only the datasets given on the shared task, without using any additional external data."
            ]
          }
        ]
      },
      {
        "question": "How did they do compared to other teams?",
        "question_id": "37e45a3439b048a80c762418099a183b05772e6a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "second on Subtask A with an F1 score of 77.78% among 33 other team submissions",
              "performs well on Subtask B with an F1 score of 79.59%"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "JESSI is trained using only the datasets given on the shared task, without using any additional external data. Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%."
            ],
            "highlighted_evidence": [
              "Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/semeval/ParkKLAKSL19",
    "dblp_title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples.",
    "year": "2019"
  },
  {
    "id": "1910.11769",
    "title": "DENS: A Dataset for Multi-class Emotion Analysis",
    "qas": [
      {
        "question": "Which tested technique was the worst performer?",
        "question_id": "a4e66e842be1438e5cd8d7cb2a2c589f494aee27",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Depeche + SVM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation)",
              "We computed bag-of-words-based benchmarks using the following methods:",
              "Classification with TF-IDF + Linear SVM (TF-IDF + SVM)",
              "Classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)",
              "Classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)",
              "Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation)",
              "We computed bag-of-words-based benchmarks using the following methods:\n\nClassification with TF-IDF + Linear SVM (TF-IDF + SVM)\n\nClassification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)\n\nClassification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)\n\nCombination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)"
            ]
          }
        ]
      },
      {
        "question": "How many emotions do they look at?",
        "question_id": "cb78e280e3340b786e81636431834b75824568c3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "9",
            "evidence": [
              "The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral."
            ],
            "highlighted_evidence": [
              "The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral"
            ]
          }
        ]
      },
      {
        "question": "What are the baseline benchmarks?",
        "question_id": "2941874356e98eb2832ba22eae9cb08ec8ce0308",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "TF-IDF + SVM",
              "Depeche + SVM",
              "NRC + SVM",
              "TF-NRC + SVM",
              "Doc2Vec + SVM",
              " Hierarchical RNN",
              "BiRNN + Self-Attention",
              "ELMo + BiRNN",
              " Fine-tuned BERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We computed bag-of-words-based benchmarks using the following methods:",
              "Classification with TF-IDF + Linear SVM (TF-IDF + SVM)",
              "Classification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)",
              "Classification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)",
              "Combination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)",
              "Benchmarks ::: Doc2Vec + SVM",
              "We also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier.",
              "Benchmarks ::: Hierarchical RNN",
              "For this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs.",
              "The outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training.",
              "Benchmarks ::: Bi-directional RNN and Self-Attention (BiRNN + Self-Attention)",
              "One challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations.",
              "Self-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark.",
              "The benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function.",
              "Note that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments.",
              "Benchmarks ::: ELMo embedding and Bi-directional RNN (ELMo + BiRNN)",
              "Deep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words.",
              "We used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function.",
              "Benchmarks ::: Fine-tuned BERT",
              "Bidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification.",
              "We used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%."
            ],
            "highlighted_evidence": [
              "We computed bag-of-words-based benchmarks using the following methods:\n\nClassification with TF-IDF + Linear SVM (TF-IDF + SVM)\n\nClassification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)\n\nClassification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)\n\nCombination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)\n\nBenchmarks ::: Doc2Vec + SVM\nWe also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier.\n\nBenchmarks ::: Hierarchical RNN\nFor this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs.\n\nThe outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training.\n\nBenchmarks ::: Bi-directional RNN and Self-Attention (BiRNN + Self-Attention)\nOne challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations.\n\nSelf-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark.\n\nThe benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function.\n\nNote that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments.\n\nBenchmarks ::: ELMo embedding and Bi-directional RNN (ELMo + BiRNN)\nDeep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words.\n\nWe used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function.\n\nBenchmarks ::: Fine-tuned BERT\nBidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification.\n\nWe used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%."
            ]
          }
        ]
      },
      {
        "question": "What is the size of this dataset?",
        "question_id": "4e50e9965059899d15d3c3a0c0a2d73e0c5802a0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words."
            ],
            "highlighted_evidence": [
              "The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words."
            ]
          }
        ]
      },
      {
        "question": "How many annotators were there?",
        "question_id": "67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "3 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We required all annotators have a `master' MTurk qualification. Each passage was labelled by 3 unique annotators. Only passages with a majority agreement between annotators were accepted as valid. This is equivalent to a Fleiss's $\\kappa $ score of greater than $0.4$."
            ],
            "highlighted_evidence": [
              " Each passage was labelled by 3 unique annotators."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/LiuOA19",
    "dblp_title": "DENS: A Dataset for Multi-class Emotion Analysis.",
    "year": "2019"
  },
  {
    "id": "1702.06378",
    "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
    "qas": [
      {
        "question": "Can SCRF be used to pretrain the model?",
        "question_id": "aecb485ea7d501094e50ad022ade4f0c93088d80",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder. The joint model by multitask learning is slightly more expensive than the stand-alone SRNN model. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model. This is analogous to sequence training of HMM acoustic models, where the network is usually pretrained by the frame-level CE criterion. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model."
            ],
            "highlighted_evidence": [
              "One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder.",
              "To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model.",
              "Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/LuKDS17",
    "dblp_title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition.",
    "year": "2017"
  },
  {
    "id": "1903.03467",
    "title": "Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection",
    "qas": [
      {
        "question": "What conclusions are drawn from the syntactic analysis?",
        "question_id": "2fea3c955ff78220b2c31a8ad1322bc77f6706f8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and number information that is not available in the source. We propose a method for injecting such information into a pre-trained NMT model in a black-box setting. We demonstrate the effectiveness of this method by showing an improvement of 2.3 BLEU in an English-to-Hebrew translation setting where the speaker and audience gender can be inferred. We also perform a fine-grained syntactic analysis that shows how our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them. In future work we would like to explore automatic generation of the injected context, or the use of cross-sentence context to infer the injected information."
            ],
            "highlighted_evidence": [
              "We also perform a fine-grained syntactic analysis that shows how our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them."
            ]
          }
        ]
      },
      {
        "question": "What type of syntactic analysis is performed?",
        "question_id": "faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Speaker's Gender Effects",
              "Interlocutors' Gender and Number Effects"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser. In addition to the parse structure, the parser also performs morphological analysis and tagging of the individual tokens. We then perform the following analysis.",
              "Speaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix.",
              "Interlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms."
            ],
            "highlighted_evidence": [
              "We then perform the following analysis.\n\nSpeaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix.\n\nInterlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms."
            ]
          }
        ]
      },
      {
        "question": "How is it demonstrated that the correct gender and number information is injected using this system?",
        "question_id": "da068b20988883bc324e55c073fb9c1a5c39be33",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline",
              "Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare the different conditions by comparing BLEU BIBREF5 with respect to the reference Hebrew translations. We use the multi-bleu.perl script from the Moses toolkit BIBREF6 . Table shows BLEU scores for the different prefixes. The numbers match our expectations: Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline. We note the BLEU score improves in all cases, even when given the wrong gender of either the speaker or the audience. We hypothesise this improvement stems from the addition of the word “said” which hints the model to generate a more “spoken” language which matches the tested scenario. Providing correct information for both speaker and audience usually helps more than providing correct information to either one of them individually. The one outlier is providing “She” for the speaker and “her” for the audience. While this is not the correct scenario, we hypothesise it gives an improvement in BLEU as it further reinforces the female gender in the sentence.",
              "Results: Speaker. Figure FIGREF3 shows the result for controlling the morphological properties of the speaker ({he, she, I} said). It shows the proportion of gender-inflected verbs for the various conditions and the reference. We see that the baseline system severely under-predicts the feminine form of verbs as compared to the reference. The “He said” conditions further decreases the number of feminine verbs, while the “I said” conditions bring it back to the baseline level. Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference (though still under-predicting some of the feminine cases)."
            ],
            "highlighted_evidence": [
              " Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline.",
              "We see that the baseline system severely under-predicts the feminine form of verbs as compared to the reference. The “He said” conditions further decreases the number of feminine verbs, while the “I said” conditions bring it back to the baseline level. Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference (though still under-predicting some of the feminine cases)."
            ]
          }
        ]
      },
      {
        "question": "Which neural machine translation system is used?",
        "question_id": "0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Google's machine translation system (GMT)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API. To test the method on real-world sentences, we consider a monologue from the stand-up comedy show “Sarah Silverman: A Speck of Dust”. The monologue consists of 1,244 English sentences, all by a female speaker conveyed to a plural, gender-neutral audience. Our parallel corpora consists of the 1,244 English sentences from the transcript, and their corresponding Hebrew translations based on the Hebrew subtitles. We translate the monologue one sentence at a time through the Google Cloud API. Eyeballing the results suggest that most of the translations use the incorrect, but default, masculine and singular forms for the speaker and the audience, respectively. We expect that by adding the relevant condition of “female speaking to an audience” we will get better translations, affecting both the gender of the speaker and the number of the audience."
            ],
            "highlighted_evidence": [
              "To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API."
            ]
          }
        ]
      },
      {
        "question": "What are the components of the black-box context injection system?",
        "question_id": "edcde2b675cf8a362a63940b2bbdf02c150fe01f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our goal is to supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences, in order to produce the desired target-side morphology when the information is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by vanmassenhove-hardmeier-way:2018:EMNLP.",
              "To verify this, we experiment with translating the sentences with the following variations: No Prefix—The baseline translation as returned by the GMT system. “He said:”—Signaling a male speaker. We expect to further skew the system towards masculine forms. “She said:”—Signaling a female speaker and unknown audience. As this matches the actual speaker's gender, we expect an improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. “I said to them:”—Signaling an unknown speaker and plural audience. “He said to them:”—Masculine speaker and plural audience. “She said to them:”—Female speaker and plural audience—the complete, correct condition. We expect the best translation accuracy on this setup. “He/she said to him/her”—Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology."
            ],
            "highlighted_evidence": [
              "Our goal is to supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences, in order to produce the desired target-side morphology when the information is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by vanmassenhove-hardmeier-way:2018:EMNLP.",
              "To verify this, we experiment with translating the sentences with the following variations: No Prefix—The baseline translation as returned by the GMT system. “He said:”—Signaling a male speaker. We expect to further skew the system towards masculine forms. “She said:”—Signaling a female speaker and unknown audience. As this matches the actual speaker's gender, we expect an improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. “I said to them:”—Signaling an unknown speaker and plural audience. “He said to them:”—Masculine speaker and plural audience. “She said to them:”—Female speaker and plural audience—the complete, correct condition. We expect the best translation accuracy on this setup. “He/she said to him/her”—Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1903-03467",
    "dblp_title": "Filling Gender &amp; Number Gaps in Neural Machine Translation with Black-box Context Injection.",
    "year": "2019"
  },
  {
    "id": "1807.00868",
    "title": "Exploring End-to-End Techniques for Low-Resource Speech Recognition",
    "qas": [
      {
        "question": "What normalization techniques are mentioned?",
        "question_id": "d20d6c8ecd7cb0126479305d27deb0c8b642b09f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "FBanks with cepstral mean normalization (CMN)",
              "variance with mean normalization (CMVN)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks."
            ],
            "highlighted_evidence": [
              "We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks."
            ]
          }
        ]
      },
      {
        "question": "What features do they experiment with?",
        "question_id": "11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window",
              "deltas and delta-deltas (120 features in vector)",
              "spectrogram"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "All models are trained with CTC-loss. Input features are 40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector). We also tried to use spectrogram and experimented with different normalization techniques."
            ],
            "highlighted_evidence": [
              "Input features are 40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector). We also tried to use spectrogram and experimented with different normalization techniques."
            ]
          }
        ]
      },
      {
        "question": "Which architecture is their best model?",
        "question_id": "2677b88c2def3ed94e25a776599555a788d197f2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "6-layer bLSTM with 1024 hidden units"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To train our best model we chose the best network from our experiments (6-layer bLSTM with 1024 hidden units), trained it with Adam optimizer and fine-tuned with SGD with momentum using exponential learning rate decay. The best model trained with speed and volume perturbation BIBREF24 achieved 45.8% WER, which is the best published end-to-end result on Babel Turkish dataset using in-domain data. For comparison, WER of model trained using in-domain data in BIBREF18 is 53.1%, using 4 additional languages (including English Switchboard dataset) – 48.7%. It is also not far from Kaldi DNN-HMM system BIBREF22 with 43.8% WER."
            ],
            "highlighted_evidence": [
              "To train our best model we chose the best network from our experiments (6-layer bLSTM with 1024 hidden units), trained it with Adam optimizer and fine-tuned with SGD with momentum using exponential learning rate decay."
            ]
          }
        ]
      },
      {
        "question": "What kind of spontaneous speech is used?",
        "question_id": "8ca31caa34cc5b65dc1d01d0d1f36bf8c4928805",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/specom/BataevKMZ18",
    "dblp_title": "Exploring End-to-End Techniques for Low-Resource Speech Recognition.",
    "year": "2018"
  },
  {
    "id": "1909.13375",
    "title": "Tag-based Multi-Span Extraction in Reading Comprehension",
    "qas": [
      {
        "question": "What approach did previous models use for multi-span questions?",
        "question_id": "9ab43f941c11a4b09a0e4aea61b4a5b4612e7933",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span",
            "evidence": [
              "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable."
            ],
            "highlighted_evidence": [
              "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable"
            ]
          }
        ]
      },
      {
        "question": "How they use sequence tagging to answer multi-span questions?",
        "question_id": "5a02a3dd26485a4e4a77411b50b902d2bda3731b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans."
            ],
            "highlighted_evidence": [
              "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans."
            ]
          }
        ]
      },
      {
        "question": "What is difference in peformance between proposed model and state-of-the art on other question types?",
        "question_id": "579941de2838502027716bae88e33e79e69997a6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",
            "evidence": [
              "FLOAT SELECTED: Table 2. Performance of different models on DROP’s development set in terms of Exact Match (EM) and F1."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2. Performance of different models on DROP’s development set in terms of Exact Match (EM) and F1."
            ]
          }
        ]
      },
      {
        "question": "What is the performance of proposed model on entire DROP dataset?",
        "question_id": "9a65cfff4d99e4f9546c72dece2520cae6231810",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",
            "evidence": [
              "Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions.",
              "FLOAT SELECTED: Table 3. Comparing test and development set results of models from the official DROP leaderboard"
            ],
            "highlighted_evidence": [
              "Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions.",
              "FLOAT SELECTED: Table 3. Comparing test and development set results of models from the official DROP leaderboard"
            ]
          }
        ]
      },
      {
        "question": "What is the previous model that attempted to tackle multi-span questions as a part of its design?",
        "question_id": "a9def7958eac7b9a780403d4f136927f756bab83",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MTMSN BIBREF4"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable."
            ],
            "highlighted_evidence": [
              "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-13375",
    "dblp_title": "Tag-based Multi-Span Extraction in Reading Comprehension.",
    "year": "2019"
  },
  {
    "id": "1909.00430",
    "title": "Transfer Learning Between Related Tasks Using Expected Label Proportions",
    "qas": [
      {
        "question": "How much more data does the model trained using XR loss have access to, compared to the fully supervised model?",
        "question_id": "547be35cff38028648d199ad39fb48236cfb99ee",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Does the system trained only using XR loss outperform the fully supervised neural system?",
        "question_id": "47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
            ]
          }
        ]
      },
      {
        "question": "How accurate is the aspect based sentiment classifier trained only using the XR loss?",
        "question_id": "e42fbf6c183abf1c6c2321957359c7683122b48e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n",
            "evidence": [
              "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
            ]
          }
        ]
      },
      {
        "question": "How is the expectation regularization loss defined?",
        "question_id": "e574f0f733fb98ecef3c64044004aa7a320439be",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DISPLAYFORM0"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/NoachG19",
    "dblp_title": "Transfer Learning Between Related Tasks Using Expected Label Proportions.",
    "year": "2019"
  },
  {
    "id": "1910.11493",
    "title": "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection",
    "qas": [
      {
        "question": "What were the non-neural baselines used for the task?",
        "question_id": "b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The Lemming model in BIBREF17",
            "evidence": [
              "BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. The model is globally normalized with the use of a second order linear-chain CRF. To efficiently calculate the partition function, the choice of lemmata are pruned with the use of pre-extracted edit trees."
            ],
            "highlighted_evidence": [
              "BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-11493",
    "dblp_title": "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection.",
    "year": "2019"
  },
  {
    "id": "1910.00912",
    "title": "Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU",
    "qas": [
      {
        "question": "Which publicly available NLU dataset is used?",
        "question_id": "bd3ccb63fd8ce5575338d7332e96def7a3fabad6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "ROMULUS dataset",
              "NLU-Benchmark dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We tested the system on two datasets, different in size and complexity of the addressed language.",
              "Experimental Evaluation ::: Datasets ::: NLU-Benchmark dataset",
              "The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. For example, “schedule a call with Lisa on Monday morning” is labelled to contain a calendar scenario, where the set_event action is instantiated through the entities [event_name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and action labels (e.g., calendar_set_event). This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot BIBREF7.",
              "Experimental Evaluation ::: Datasets ::: ROMULUS dataset",
              "The second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., “enter the bedroom slowly, turn left and turn the lights off ”) to complex requests for information (e.g., “good morning I want to buy a new mobile phone is there any shop nearby?”) or open-domain chit-chat (e.g., “nope thanks let's talk about cinema”). A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain ($\\approx $$70\\%$), though a small portion has been synthetically generated for balancing the frame distribution."
            ],
            "highlighted_evidence": [
              "We tested the system on two datasets, different in size and complexity of the addressed language.\n\nExperimental Evaluation ::: Datasets ::: NLU-Benchmark dataset\nThe first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities.",
              "Experimental Evaluation ::: Datasets ::: ROMULUS dataset\nThe second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation."
            ]
          }
        ]
      },
      {
        "question": "What metrics other than entity tagging are compared?",
        "question_id": "7c794fa0b2818d354ca666969107818a2ffdda0c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We also report the metrics in BIBREF7 for consistency",
              "we report the span F1",
              " Exact Match (EM) accuracy of the entire sequence of labels",
              "metric that combines intent and entities"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems. The statistical analysis shows a significant improvement over Rasa $[Z=-2.803, p = .005]$, Dialogflow $[Z=-2.803, p = .005]$, LUIS $[Z=-2.803, p = .005]$ and Watson $[Z=-2.803, p = .005]$.",
              "FLOAT SELECTED: Table 4: Comparison of HERMIT with the results in (Liu et al., 2019) by combining Intent and Entity.",
              "In this section we report the experiments performed on the ROMULUS dataset (Table TABREF27). Together with the evaluation metrics used in BIBREF7, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions – e.g., a match is when all the three sequences are correct.",
              "Results in terms of EM reflect the complexity of the different tasks, motivating their position within the hierarchy. Specifically, dialogue act identification is the easiest task ($89.31\\%$) with respect to frame ($82.60\\%$) and frame element ($79.73\\%$), due to the shallow semantics it aims to catch. However, when looking at the span F1, its score ($89.42\\%$) is lower than the frame element identification task ($92.26\\%$). What happens is that even though the label set is smaller, dialogue act spans are supposed to be longer than frame element ones, sometimes covering the whole sentence. Frame elements, instead, are often one or two tokens long, that contribute in increasing span based metrics. Frame identification is the most complex task for several reasons. First, lots of frame spans are interlaced or even nested; this contributes to increasing the network entropy. Second, while the dialogue act label is highly related to syntactic structures, frame identification is often subject to the inherent ambiguity of language (e.g., get can evoke both Commerce_buy and Arriving). We also report the metrics in BIBREF7 for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks. However, the frame element scores are comparable to the benchmark, since the task is very similar."
            ],
            "highlighted_evidence": [
              "Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems.",
              "FLOAT SELECTED: Table 4: Comparison of HERMIT with the results in (Liu et al., 2019) by combining Intent and Entity.",
              "Together with the evaluation metrics used in BIBREF7, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels. It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions – e.g., a match is when all the three sequences are correct.",
              "We also report the metrics in BIBREF7 for consistency. For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels. In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/sigdial/VanzoBL19",
    "dblp_title": "Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU.",
    "year": "2019"
  },
  {
    "id": "1908.10449",
    "title": "Interactive Machine Comprehension with Information Seeking Agents",
    "qas": [
      {
        "question": "Do they provide decision sequences as supervision while training models?",
        "question_id": "1ef5fc4473105f1c72b4d35cf93d312736833d3d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ],
            "highlighted_evidence": [
              "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ]
          }
        ]
      },
      {
        "question": "What are the models evaluated on?",
        "question_id": "5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)",
            "evidence": [
              "We build the iSQuAD and iNewsQA datasets based on SQuAD v1.1 BIBREF0 and NewsQA BIBREF1. Both original datasets share similar properties. Specifically, every data-point consists of a tuple, $\\lbrace p, q, a\\rbrace $, where $p$ represents a paragraph, $q$ a question, and $a$ is the answer. The answer is a word span defined by head and tail positions in $p$. NewsQA is more difficult than SQuAD because it has a larger vocabulary, more difficult questions, and longer source documents.",
              "iMRC: Making MRC Interactive ::: Evaluation Metric",
              "Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance ."
            ],
            "highlighted_evidence": [
              "We build the iSQuAD and iNewsQA datasets based on SQuAD v1.1 BIBREF0 and NewsQA BIBREF1.",
              "iMRC: Making MRC Interactive ::: Evaluation Metric\nSince iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance ."
            ]
          }
        ]
      },
      {
        "question": "How do they train models in this setup?",
        "question_id": "b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ],
            "highlighted_evidence": [
              "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ]
          }
        ]
      },
      {
        "question": "What commands does their setup provide to models seeking information?",
        "question_id": "e9cf1b91f06baec79eb6ddfd91fc5d434889f652",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "previous",
              "next",
              "Ctrl+F $<$query$>$",
              "stop"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Information Gathering: At step $t$ of the information gathering phase, the agent can issue one of the following four actions to interact with the paragraph $p$, where $p$ consists of $n$ sentences and where the current observation corresponds to sentence $s_k,~1 \\le k \\le n$:",
              "previous: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_n & \\text{if $k = 1$,}\\\\ s_{k-1} & \\text{otherwise;} \\end{array}\\right.} $",
              "next: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_1 & \\text{if $k = n$,}\\\\ s_{k+1} & \\text{otherwise;} \\end{array}\\right.} $",
              "Ctrl+F $<$query$>$: jump to the sentence that contains the next occurrence of “query”;",
              "stop: terminate information gathering phase."
            ],
            "highlighted_evidence": [
              "Information Gathering: At step $t$ of the information gathering phase, the agent can issue one of the following four actions to interact with the paragraph $p$, where $p$ consists of $n$ sentences and where the current observation corresponds to sentence $s_k,~1 \\le k \\le n$:\n\nprevious: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_n & \\text{if $k = 1$,}\\\\ s_{k-1} & \\text{otherwise;} \\end{array}\\right.} $\n\nnext: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_1 & \\text{if $k = n$,}\\\\ s_{k+1} & \\text{otherwise;} \\end{array}\\right.} $\n\nCtrl+F $<$query$>$: jump to the sentence that contains the next occurrence of “query”;\n\nstop: terminate information gathering phase."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/YuanFCTPT20",
    "dblp_title": "Interactive Machine Comprehension with Information Seeking Agents.",
    "year": "2020"
  },
  {
    "id": "1910.03814",
    "title": "Exploring Hate Speech Detection in Multimodal Publications",
    "qas": [
      {
        "question": "What models do they propose?",
        "question_id": "6976296126e4a5c518e6b57de70f8dc8d8fde292",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Feature Concatenation Model (FCM)",
              "Spatial Concatenation Model (SCM)",
              "Textual Kernels Model (TKM)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any)."
            ],
            "highlighted_evidence": [
              "To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM)"
            ]
          }
        ]
      },
      {
        "question": "Are all tweets in English?",
        "question_id": "53640834d68cf3b86cf735ca31f1c70aa0006b72",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How large is the dataset?",
        "question_id": "b2b0321b0aaf58c3aa9050906ade6ef35874c5c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " $150,000$ tweets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps."
            ],
            "highlighted_evidence": [
              "We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. "
            ]
          }
        ]
      },
      {
        "question": "What is the results of multimodal compared to unimodal models?",
        "question_id": "4e9684fd68a242cb354fa6961b0e3b5c35aae4b6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ",
            "evidence": [
              "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.",
              "FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time."
            ],
            "highlighted_evidence": [
              "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.",
              "FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time."
            ]
          }
        ]
      },
      {
        "question": "What is author's opinion on why current multimodal models cannot outperform models analyzing only text?",
        "question_id": "2e632eb5ad611bbd16174824de0ae5efe4892daf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Noisy data",
              "Complexity and diversity of multimodal relations",
              "Small set of multimodal examples"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Despite the model trained only with images proves that they are useful for hate speech detection, the proposed multimodal models are not able to improve the detection compared to the textual models. Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data:",
              "[noitemsep,leftmargin=*]",
              "Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.",
              "Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.",
              "Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate."
            ],
            "highlighted_evidence": [
              "Next, we analyze why they do not perform well in this task and with this data:\n\n[noitemsep,leftmargin=*]\n\nNoisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.\n\nComplexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.\n\nSmall set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate."
            ]
          }
        ]
      },
      {
        "question": "What metrics are used to benchmark the results?",
        "question_id": "d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "F-score",
              "Area Under the ROC Curve (AUC)",
              "mean accuracy (ACC)",
              "Precision vs Recall plot",
              "ROC curve (which plots the True Positive Rate vs the False Positive Rate)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models."
            ],
            "highlighted_evidence": [
              "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.",
              "Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models."
            ]
          }
        ]
      },
      {
        "question": "How is data collected, manual collection or Twitter api?",
        "question_id": "24c0f3d6170623385283dfda7f2b6ca2c7169238",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Twitter API"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter."
            ],
            "highlighted_evidence": [
              "We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9."
            ]
          }
        ]
      },
      {
        "question": "How many tweats does MMHS150k contains, 150000?",
        "question_id": "21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$150,000$ tweets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps."
            ],
            "highlighted_evidence": [
              "We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps."
            ]
          }
        ]
      },
      {
        "question": "What unimodal detection models were used?",
        "question_id": "a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. Since our dataset is not big enough to train a GloVe word embedding model, we used a pre-trained model that has been trained in two billion tweets. This ensures that the model will be able to produce word embeddings for slang and other words typically used in Twitter. To process the tweets text before generating the word embeddings, we use the same pipeline as the model authors, which includes generating symbols to encode Twitter special interactions such as user mentions (@user) or hashtags (#hashtag). To encode the tweet text and input it later to multimodal models, we use the LSTM hidden state after processing the last tweet word. Since the LSTM has been trained for hate speech classification, it extracts the most useful information for this task from the text, which is encoded in the hidden state after inputting the last tweet word."
            ],
            "highlighted_evidence": [
              "We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations."
            ]
          }
        ]
      },
      {
        "question": "What different models for multimodal detection were proposed?",
        "question_id": "b0799e26152197aeb3aa3b11687a6cc9f6c31011",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Feature Concatenation Model (FCM)",
              "Spatial Concatenation Model (SCM)",
              "Textual Kernels Model (TKM)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any)."
            ],
            "highlighted_evidence": [
              "To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM)."
            ]
          }
        ]
      },
      {
        "question": "What annotations are available in the dataset - tweat used hate speach or not?",
        "question_id": "4ce4db7f277a06595014db181342f8cb5cb94626",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "No attacks to any community",
              " racist",
              "sexist",
              "homophobic",
              "religion based attacks",
              "attacks to other communities"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We annotate the gathered tweets using the crowdsourcing platform Amazon Mechanical Turk. There, we give the workers the definition of hate speech and show some examples to make the task clearer. We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. Each one of the $150,000$ tweets is labeled by 3 different workers to palliate discrepancies among workers."
            ],
            "highlighted_evidence": [
              "We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-03814",
    "dblp_title": "Exploring Hate Speech Detection in Multimodal Publications.",
    "year": "2019"
  },
  {
    "id": "1701.00185",
    "title": "Self-Taught Convolutional Neural Networks for Short Text Clustering",
    "qas": [
      {
        "question": "What were the evaluation metrics used?",
        "question_id": "62a6382157d5f9c1dce6e6c24ac5994442053002",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "accuracy",
              "normalized mutual information"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . "
            ]
          }
        ]
      },
      {
        "question": "What were their performance results?",
        "question_id": "9e04730907ad728d62049f49ac828acb4e0a1a2a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",
            "evidence": [
              "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
              "FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
              "FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models."
            ]
          }
        ]
      },
      {
        "question": "By how much did they outperform the other methods?",
        "question_id": "5a0841cc0628e872fe473874694f4ab9411a1d10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",
            "evidence": [
              "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
              "FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
              "FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models."
            ]
          }
        ]
      },
      {
        "question": "Which popular clustering methods did they experiment with?",
        "question_id": "a5dd569e6d641efa86d2c2b2e970ce5871e0963f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . We further compare our approach with some other non-biased neural networks, such as bidirectional RNN. More details are listed as follows:"
            ],
            "highlighted_evidence": [
              "In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 . "
            ]
          }
        ]
      },
      {
        "question": "What datasets did they use?",
        "question_id": "785c054f6ea04701f4ab260d064af7d124260ccc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SearchSnippets",
              "StackOverflow",
              "Biomedical"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We test our proposed approach on three public short text datasets. The summary statistics and semantic topics of these datasets are described in Table TABREF24 and Table TABREF25 .",
              "SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 .",
              "StackOverflow. We use the challenge data published in Kaggle.com. The raw dataset consists 3,370,528 samples through July 31st, 2012 to August 14, 2012. In our experiments, we randomly select 20,000 question titles from 20 different tags as in Table TABREF25 .",
              "Biomedical. We use the challenge data published in BioASQ's official website. In our experiments, we randomly select 20, 000 paper titles from 20 different MeSH major topics as in Table TABREF25 . As described in Table TABREF24 , the max length of selected paper titles is 53."
            ],
            "highlighted_evidence": [
              "We test our proposed approach on three public short text datasets. ",
              "SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 .",
              "StackOverflow. We use the challenge data published in Kaggle.com. ",
              "Biomedical. We use the challenge data published in BioASQ's official website. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/nn/XuXWZTZX17",
    "dblp_title": "Self-Taught convolutional neural networks for short text clustering.",
    "year": "2017"
  },
  {
    "id": "1912.00871",
    "title": "Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations",
    "qas": [
      {
        "question": "Does pre-training on general text corpus improve performance?",
        "question_id": "3f6610d1d68c62eddc2150c460bf1b48a064e5e6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability.",
              "Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation."
            ],
            "highlighted_evidence": [
              "Our attempt at language pre-training fell short of our expectations in all but one tested dataset.",
              "Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported."
            ]
          }
        ]
      },
      {
        "question": "What neural configurations are explored?",
        "question_id": "4c854d33a832f3f729ce73b206ff90677e131e48",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "tried many configurations of our network models, but report results with only three configurations",
              "Transformer Type 1",
              "Transformer Type 2",
              "Transformer Type 3"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare medium-sized, small, and minimal networks to show if network size can be reduced to increase training and testing efficiency while retaining high accuracy. Networks over six layers have shown to be non-effective for this task. We tried many configurations of our network models, but report results with only three configurations of Transformers.",
              "Transformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024.",
              "Transformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024.",
              "Transformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512."
            ],
            "highlighted_evidence": [
              "We tried many configurations of our network models, but report results with only three configurations of Transformers.\n\nTransformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024.\n\nTransformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024.\n\nTransformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512."
            ]
          }
        ]
      },
      {
        "question": "Are the Transformers masked?",
        "question_id": "163c15da1aa0ba370a00c5a09294cd2ccdb4b96d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We calculate the loss in training according to a mean of the sparse categorical cross-entropy formula. Sparse categorical cross-entropy BIBREF23 is used for identifying classes from a feature set, which assumes a large target classification set. Evaluation between the possible translation classes (all vocabulary subword tokens) and the produced class (predicted token) is the metric of performance here. During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value. We adjust the model's loss according to the mean of the translation accuracy after predicting every determined subword in a translation."
            ],
            "highlighted_evidence": [
              "During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value."
            ]
          }
        ]
      },
      {
        "question": "How is this problem evaluated?",
        "question_id": "90dd5c0f5084a045fd6346469bc853c33622908f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BLEU-2",
              "average accuracies over 3 test trials on different randomly sampled test sets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Approach ::: Method: Training and Testing ::: Experiment 1: Representation",
              "Some of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability. Traditionally, a BLEU score is a metric of translation quality BIBREF24. Our presented BLEU scores represent an average of scores a given model received over each of the target test sets. We use a standard bi-gram weight to show how accurate translations are within a window of two adjacent terms. After testing translations, we calculate an average BLEU-2 score per test set, which is related to the success over that data. An average of the scores for each dataset become the presented value.",
              "Approach ::: Method: Training and Testing ::: Experiment 2: State-of-the-art",
              "This experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect\" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset. This calculation more accurately depicts the generalization of our networks."
            ],
            "highlighted_evidence": [
              "Approach ::: Method: Training and Testing ::: Experiment 1: Representation\nSome of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability.",
              "Approach ::: Method: Training and Testing ::: Experiment 2: State-of-the-art\nThis experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect\" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset."
            ]
          }
        ]
      },
      {
        "question": "What datasets do they use?",
        "question_id": "095888f6e10080a958d9cd3f779a339498f3a109",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AI2 BIBREF2",
              "CC BIBREF19",
              "IL BIBREF4",
              "MAWPS BIBREF20"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.",
              "AI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.",
              "CC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.",
              "IL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.",
              "MAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. The problems not used were more complex problems that generate systems of equations. We exclude such problems because generating systems of equations is not our focus."
            ],
            "highlighted_evidence": [
              "We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.\n\nAI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.\n\nCC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.\n\nIL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.\n\nMAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-00871",
    "dblp_title": "Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations.",
    "year": "2019"
  },
  {
    "id": "1912.03234",
    "title": "What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant",
    "qas": [
      {
        "question": "What evaluation metrics were used?",
        "question_id": "57e783f00f594e08e43a31939aedb235c9d5a102",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AUC-ROC"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept)."
            ],
            "highlighted_evidence": [
              " The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. "
            ]
          }
        ]
      },
      {
        "question": "Where did the real production data come from?",
        "question_id": "9646fa1abbe3102a0364f84e0a55d107d45c97f0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept)."
            ],
            "highlighted_evidence": [
              "The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)."
            ]
          }
        ]
      },
      {
        "question": "What feedback labels are used?",
        "question_id": "29983f4bc8a5513a198755e474361deee93d4ab6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "five-minute reuse and one-day return"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The proposed models use binary classifiers to perform point-wise ranking, and therefore require a labelled dataset. To generate it, we explore two implicit user-feedback labelling strategies: five-minute reuse and one-day return. Online A/B testing is used to determine if these labelling strategies are suited to optimize the desired user-satisfaction metrics, and offline data to evaluated and compared the system's performance."
            ],
            "highlighted_evidence": [
              "To generate it, we explore two implicit user-feedback labelling strategies: five-minute reuse and one-day return. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-03234",
    "dblp_title": "What Do You Mean I&apos;m Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant.",
    "year": "2019"
  },
  {
    "id": "1911.11750",
    "title": "A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient",
    "qas": [
      {
        "question": "What representations for textual documents do they use?",
        "question_id": "6c0f97807cd83a94a4d26040286c6f89c4a0f8e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "finite sequence of terms"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A document $d$ can be defined as a finite sequence of terms (independent textual entities within a document, for example, words), namely $d=(t_1,t_2,\\dots ,t_n)$. A general idea is to associate weight to each term $t_i$ within $d$, such that"
            ],
            "highlighted_evidence": [
              "A document $d$ can be defined as a finite sequence of terms (independent textual entities within a document, for example, words), namely $d=(t_1,t_2,\\dots ,t_n)$."
            ]
          }
        ]
      },
      {
        "question": "Which dataset(s) do they use?",
        "question_id": "13ca4bf76565564c8ec3238c0cbfacb0b41e14d2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "14 TDs",
              "BIBREF15"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We have used a dataset of 14 TDs to conduct our experiments. There are several subjects on which their content is based: (aliens, stories, law, news) BIBREF15."
            ],
            "highlighted_evidence": [
              "We have used a dataset of 14 TDs to conduct our experiments."
            ]
          }
        ]
      },
      {
        "question": "How do they evaluate knowledge extraction performance?",
        "question_id": "70797f66d96aa163a3bee2be30a328ba61c40a18",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SRCC"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The results in Table TABREF7 show that SRCC performs much better in knowledge extraction. The two documents' contents contain the same idea expressed by terms in a different order that John had asked Mary to marry him before she left. It is obvious that cosine similarity cannot recognize this association, but SRCC has successfully recognized it and produced a similarity value of -0.285714."
            ],
            "highlighted_evidence": [
              "The results in Table TABREF7 show that SRCC performs much better in knowledge extraction. The two documents' contents contain the same idea expressed by terms in a different order that John had asked Mary to marry him before she left. It is obvious that cosine similarity cannot recognize this association, but SRCC has successfully recognized it and produced a similarity value of -0.285714."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-11750",
    "dblp_title": "A Measure of Similarity in Textual Data Using Spearman&apos;s Rank Correlation Coefficient.",
    "year": "2019"
  },
  {
    "id": "1911.03894",
    "title": "CamemBERT: a Tasty French Language Model",
    "qas": [
      {
        "question": "What is CamemBERT trained on?",
        "question_id": "71f2b368228a748fd348f1abf540236568a61b07",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "unshuffled version of the French OSCAR corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens."
            ],
            "highlighted_evidence": [
              "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens."
            ]
          }
        ]
      },
      {
        "question": "Which tasks does CamemBERT not improve on?",
        "question_id": "d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Experiments ::: Results ::: Natural Language Inference: XNLI",
              "On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters)."
            ],
            "highlighted_evidence": [
              "Experiments ::: Results ::: Natural Language Inference: XNLI\nOn the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters)."
            ]
          }
        ]
      },
      {
        "question": "What is the state of the art?",
        "question_id": "63723c6b398100bba5dc21754451f503cb91c9b8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)",
            "evidence": [
              "We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.",
              "In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings.",
              "In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50."
            ],
            "highlighted_evidence": [
              "We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.",
              "In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings.",
              "In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50."
            ]
          }
        ]
      },
      {
        "question": "How much better was results of CamemBERT than previous results on these tasks?",
        "question_id": "5471766ca7c995dd7f0f449407902b32ac9db269",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2.36 point increase in the F1 score with respect to the best SEM architecture",
              "on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM)",
              "lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa",
              "For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT",
              "For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "CamemBERT also demonstrates higher performances than mBERT on those tasks. We observe a larger error reduction for parsing than for tagging. For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT.",
              "On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters).",
              "For named entity recognition, our experiments show that CamemBERT achieves a slightly better precision than the traditional CRF-based SEM architectures described above in Section SECREF25 (CRF and Bi-LSTM+CRF), but shows a dramatic improvement in finding entity mentions, raising the recall score by 3.5 points. Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB. One other important finding is the results obtained by mBERT. Previous work with this model showed increased performance in NER for German, Dutch and Spanish when mBERT is used as contextualised word embedding for an NER-specific model BIBREF48, but our results suggest that the multilingual setting in which mBERT was trained is simply not enough to use it alone and fine-tune it for French NER, as it shows worse performance than even simple CRF models, suggesting that monolingual models could be better at NER."
            ],
            "highlighted_evidence": [
              "For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT.",
              "On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M).",
              "However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa.",
              "Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB."
            ]
          }
        ]
      },
      {
        "question": "Was CamemBERT compared against multilingual BERT on these tasks?",
        "question_id": "dc49746fc98647445599da9d17bc004bafdc4579",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT). We then compare our models to UDify BIBREF36. UDify is a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS tagging and dependency parsing."
            ],
            "highlighted_evidence": [
              "To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT)."
            ]
          }
        ]
      },
      {
        "question": "How long was CamemBERT trained?",
        "question_id": "8720c096c8b990c7b19f956ee4930d5f2c019e2b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What data is used for training CamemBERT?",
        "question_id": "b573b36936ffdf1d70e66f9b5567511c989b46b2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "unshuffled version of the French OSCAR corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens."
            ],
            "highlighted_evidence": [
              "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2306-15550",
    "dblp_title": "CamemBERT-bio: a Tasty French Language Model Better for your Health.",
    "year": "2023"
  },
  {
    "id": "2001.09899",
    "title": "Vocabulary-based Method for Quantifying Controversy in Social Media",
    "qas": [
      {
        "question": "What are the state of the art measures?",
        "question_id": "bf25a202ac713a34e09bf599b3601058d9cace46",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Randomwalk",
              "Walktrap",
              "Louvain clustering"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As Garimella et al. BIBREF23 have made their code public , we reproduced their best method Randomwalk on our datasets and measured the AUC ROC, obtaining a score of 0.935. An interesting finding was that their method had a poor performance over their own datasets. This was due to the fact (already explained in Section SECREF4) that it was not possible to retrieve the complete discussions, moreover, in no case could we restore more than 50% of the tweets. So we decided to remove these discussions and measure again the AUC ROC of this method, obtaining a 0.99 value. Our hypothesis is that the performance of that method was seriously hurt by the incompleteness of the data. We also tested our method on these datasets, obtaining a 0.99 AUC ROC with Walktrap and 0.989 with Louvain clustering."
            ],
            "highlighted_evidence": [
              "As Garimella et al. BIBREF23 have made their code public , we reproduced their best method Randomwalk on our datasets and measured the AUC ROC, obtaining a score of 0.935. An interesting finding was that their method had a poor performance over their own datasets. This was due to the fact (already explained in Section SECREF4) that it was not possible to retrieve the complete discussions, moreover, in no case could we restore more than 50% of the tweets. So we decided to remove these discussions and measure again the AUC ROC of this method, obtaining a 0.99 value. Our hypothesis is that the performance of that method was seriously hurt by the incompleteness of the data. We also tested our method on these datasets, obtaining a 0.99 AUC ROC with Walktrap and 0.989 with Louvain clustering."
            ]
          }
        ]
      },
      {
        "question": "What controversial topics are experimented with?",
        "question_id": "abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "political events such as elections, corruption cases or justice decisions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To define the amount of data needed to run our method we established that the Fasttext model has to predict at least one user of each community with a probability greater or equal than 0.9 during ten different trainings. If that is not the case, we are not able to use DPC method. This decision made us consider only a subset of the datasets used in BIBREF23, because due to the time elapsed since their work, many tweets had been deleted and consequently the volume of the data was not enough for our framework. To enlarge our experiment base we added new debates, more detailed information about each one is shown in Table TABREF24 in UNKREF6. To select new discussions and to determine if they are controversial or not we looked for topics widely covered by mainstream media, and that have generated ample discussion, both online and offline. For non-controversy discussions we focused on “soft news\" and entertainment, but also to events that, while being impactful and/or dramatic, did not generate large controversies. To validate that intuition, we manually checked a sample of tweets, being unable to identify any clear instance of controversy On the other side, for controversial debates we focused on political events such as elections, corruption cases or justice decisions."
            ],
            "highlighted_evidence": [
              "To enlarge our experiment base we added new debates, more detailed information about each one is shown in Table TABREF24 in UNKREF6. To select new discussions and to determine if they are controversial or not we looked for topics widely covered by mainstream media, and that have generated ample discussion, both online and offline. For non-controversy discussions we focused on “soft news\" and entertainment, but also to events that, while being impactful and/or dramatic, did not generate large controversies. To validate that intuition, we manually checked a sample of tweets, being unable to identify any clear instance of controversy On the other side, for controversial debates we focused on political events such as elections, corruption cases or justice decisions."
            ]
          }
        ]
      },
      {
        "question": "What datasets did they use?",
        "question_id": "2df910c9806f0c379d7bb1bc2be2610438e487dc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF32, BIBREF23, BIBREF33",
              "discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The method we propose to measure the controversy equates in accuracy the one developed by Garimella et al.BIBREF23 and improves considerably computing time and robustness wrt the amount of data needed to effectively apply it. Our method is also based on a graph approach but it has its main focus on the vocabulary. We first train an NLP classifier that estimates opinion polarity of main users, then we run label-propagation BIBREF31 on the endorsement graph to get polarity of the whole network. Finally we compute the controversy score through a computation inspired in Dipole Moment, a measure used in physics to estimate electric polarity on a system. In our experiments we use the same data-sets from other works BIBREF32, BIBREF23, BIBREF33 as well as other datasets that we collected by us using a similar criterion (described in Section SECREF4).",
              "In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts."
            ],
            "highlighted_evidence": [
              "In our experiments we use the same data-sets from other works BIBREF32, BIBREF23, BIBREF33 as well as other datasets that we collected by us using a similar criterion (described in Section SECREF4).",
              "We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts."
            ]
          }
        ]
      },
      {
        "question": "What social media platform is observed?",
        "question_id": "a2a3af59f3f18a28eb2ca7055e1613948f395052",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Twitter"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Having this in mind and if we draw from the premise that when a discussion has a high controversy it is in general due to the presence of two principal communities fighting each other (or, conversely, that when there is no controversy there is just one principal community the members of which share a common point of view), we can measure the controversy by detecting if the discussion has one or two principal jargons in use. Our method is tested on Twitter datasets. This microblogging platform has been widely used to analyze discussions and polarization BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF2. It is a natural choice for these kind of problems, as it represents one of the main fora for public debate in online social media BIBREF15, it is a common destination for affiliative expressions BIBREF16 and is often used to report and read news about current events BIBREF17. An extra advantage of Twitter for this kind of studies is the availability of real-time data generated by millions of users. Other social media platforms offer similar data-sharing services, but few can match the amount of data and the accompanied documentation provided by Twitter. One last asset of Twitter for our work is given by retweets, whom typically indicate endorsement BIBREF18 and hence become a useful concept to model discussions as we can set “who is with who\". However, our method has a general approach and it could be used a priori in any social network. In this work we report excellent result tested on Twitter but in future work we are going to test it in other social networks."
            ],
            "highlighted_evidence": [
              "Our method is tested on Twitter datasets. This microblogging platform has been widely used to analyze discussions and polarization BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF2. It is a natural choice for these kind of problems, as it represents one of the main fora for public debate in online social media BIBREF15, it is a common destination for affiliative expressions BIBREF16 and is often used to report and read news about current events BIBREF17. An extra advantage of Twitter for this kind of studies is the availability of real-time data generated by millions of users. Other social media platforms offer similar data-sharing services, but few can match the amount of data and the accompanied documentation provided by Twitter. One last asset of Twitter for our work is given by retweets, whom typically indicate endorsement BIBREF18 and hence become a useful concept to model discussions as we can set “who is with who\". However, our method has a general approach and it could be used a priori in any social network. In this work we report excellent result tested on Twitter but in future work we are going to test it in other social networks."
            ]
          }
        ]
      },
      {
        "question": "How many languages do they experiment with?",
        "question_id": "d92f1c15537b33b32bfc436e6d017ae7d9d6c29a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "four different languages: English, Portuguese, Spanish and French"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts."
            ],
            "highlighted_evidence": [
              "In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts.\n\n"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iccs/ZarateF20",
    "dblp_title": "Vocabulary-Based Method for Quantifying Controversy in Social Media.",
    "year": "2020"
  },
  {
    "id": "1710.01492",
    "title": "Semantic Sentiment Analysis of Twitter Data",
    "qas": [
      {
        "question": "What is the current SOTA for sentiment analysis on Twitter at the time of writing?",
        "question_id": "fa3663567c48c27703e09c42930e51bacfa54905",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "deep convolutional networks BIBREF53 , BIBREF54"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Supervised learning. Traditionally, the above features were fed into classifiers such as Maximum Entropy (MaxEnt) and Support Vector Machines (SVM) with various kernels. However, observation over the SemEval Twitter sentiment task in recent years shows growing interest in, and by now clear dominance of methods based on deep learning. In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 . Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is at this point scarce. All these models are examples of supervised learning as they need labeled training data."
            ],
            "highlighted_evidence": [
              " In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54 "
            ]
          }
        ]
      },
      {
        "question": "What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",
        "question_id": "7997b9971f864a504014110a708f215c84815941",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",
            "evidence": [
              "Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 .",
              "Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 ."
            ],
            "highlighted_evidence": [
              " Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc.",
              "That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document"
            ]
          }
        ]
      },
      {
        "question": "What are the metrics to evaluate sentiment analysis on Twitter?",
        "question_id": "0d1408744651c3847469c4a005e4a9dccbd89cf1",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/kse/TalpadaHV19",
    "dblp_title": "An Analysis on Use of Deep Learning and Lexical-Semantic Based Sentiment Analysis Method on Twitter Data to Understand the Demographic Trend of Telemedicine.",
    "year": "2019"
  },
  {
    "id": "1912.01673",
    "title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations",
    "qas": [
      {
        "question": "How many sentence transformations on average are available per unique sentence in dataset?",
        "question_id": "a3d83c2a1b98060d609e7ff63e00112d36ce2607",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "27.41 transformation on average of single seed sentence is available in dataset.",
            "evidence": [
              "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics."
            ],
            "highlighted_evidence": [
              "After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset."
            ]
          }
        ]
      },
      {
        "question": "What annotations are available in the dataset?",
        "question_id": "aeda22ae760de7f5c0212dad048e4984cd613162",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",
            "evidence": [
              "We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.",
              "Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense."
            ],
            "highlighted_evidence": [
              "We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.\n\nSeveral modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense."
            ]
          }
        ]
      },
      {
        "question": "How are possible sentence transformations represented in dataset, as new sentences?",
        "question_id": "d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Yes, as new sentences.",
            "evidence": [
              "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics."
            ],
            "highlighted_evidence": [
              "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset."
            ]
          }
        ]
      },
      {
        "question": "What are all 15 types of modifications ilustrated in the dataset?",
        "question_id": "2d536961c6e1aec9f8491e41e383dc0aac700e0a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past",
            "evidence": [
              "We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions.",
              "FLOAT SELECTED: Table 2: Sentences transformations requested in the second round of annotation with the instructions to the annotators. The annotators were given no examples (with the exception of nonsense) not to be influenced as much as in the first round."
            ],
            "highlighted_evidence": [
              "We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions.",
              "FLOAT SELECTED: Table 2: Sentences transformations requested in the second round of annotation with the instructions to the annotators. The annotators were given no examples (with the exception of nonsense) not to be influenced as much as in the first round."
            ]
          }
        ]
      },
      {
        "question": "Is this dataset publicly available?",
        "question_id": "18482658e0756d69e39a77f8fcb5912545a72b9b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The corpus is freely available at the following link:",
              "http://hdl.handle.net/11234/1-3123"
            ],
            "highlighted_evidence": [
              "The corpus is freely available at the following link:\n\nhttp://hdl.handle.net/11234/1-3123"
            ]
          }
        ]
      },
      {
        "question": "Are some baseline models trained on this dataset?",
        "question_id": "9d336c4c725e390b6eba8bb8fe148997135ee981",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. Having browsed a number of 2D visualizations (PCA and t-SNE) of the space, we have to conclude that visually, LASER space does not seem to exhibit any of the desired topological properties discussed above, see fig:pca for one example."
            ],
            "highlighted_evidence": [
              "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019."
            ]
          }
        ]
      },
      {
        "question": "Do they do any analysis of of how the modifications changed the starting set of sentences?",
        "question_id": "016b59daa84269a93ce821070f4f5c1a71752a8a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The lack of semantic relations in the LASER space is also reflected in vector similarities, summarized in similarities. The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). Tense changes and some form of negation or banning also keep the vectors very similar."
            ],
            "highlighted_evidence": [
              "The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930)."
            ]
          }
        ]
      },
      {
        "question": "How do they introduce language variation?",
        "question_id": "771b373d09e6eb50a74fffbf72d059ad44e73ab0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " we were looking for original and uncommon sentence change suggestions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively."
            ],
            "highlighted_evidence": [
              "We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions."
            ]
          }
        ]
      },
      {
        "question": "Do they use external resources to make modifications to sentences?",
        "question_id": "efb52bda7366d2b96545cf927f38de27de3b5b77",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/BarancikovaB20",
    "dblp_title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations.",
    "year": "2020"
  },
  {
    "id": "1909.12231",
    "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
    "qas": [
      {
        "question": "How big is dataset domain-specific embedding are trained on?",
        "question_id": "1a7d28c25bb7e7202230e1b70a885a46dac8a384",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How big is unrelated corpus universal embedding is traned on?",
        "question_id": "6bc45d4f908672945192390642da5a2760971c40",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How better are state-of-the-art results than this model? ",
        "question_id": "48cc41c372d44b69a477998be449f8b81384786b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features",
              " RegSum achieves a similar ROUGE-2 score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF32 depicts models producing 100 words summaries, all depending on hand-crafted features. We use as baselines FreqSum BIBREF22 ; TsSum BIBREF23 ; traditional graph-based approaches such as Cont. LexRank BIBREF9 ; Centroid BIBREF24 ; CLASSY04 BIBREF25 ; its improved version CLASSY11 BIBREF26 and the greedy model GreedyKL BIBREF27. All of these models are significantly underperforming compared to SemSentSum. In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features. Nonetheless, our model is competitive and does not depend on hand-crafted features due to its full data-driven nature and thus, it is not limited to a single domain."
            ],
            "highlighted_evidence": [
              "In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-12231",
    "dblp_title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization.",
    "year": "2019"
  },
  {
    "id": "1706.08032",
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "qas": [
      {
        "question": "What were their results on the three datasets?",
        "question_id": "efb3a87845460655c53bd7365bcb8393c99358ec",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",
            "evidence": [
              "FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION"
            ]
          }
        ]
      },
      {
        "question": "What was the baseline?",
        "question_id": "0619fc797730a3e59ac146a5a4575c81517cc618",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN.",
              "we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.",
              "For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset."
            ],
            "highlighted_evidence": [
              "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.\n\nFor Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.\n\n"
            ]
          }
        ]
      },
      {
        "question": "Which datasets did they use?",
        "question_id": "846a1992d66d955fa1747bca9a139141c19908e8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus)",
              "Sanders - Twitter Sentiment Corpus",
              "Health Care Reform (HCR)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .",
              "Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.",
              "Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 ."
            ],
            "highlighted_evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 ."
            ]
          }
        ]
      },
      {
        "question": "Are results reported only on English datasets?",
        "question_id": "1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .",
              "Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.",
              "Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",
              "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.",
              "For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset."
            ],
            "highlighted_evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",
              "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. ",
              "For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). "
            ]
          }
        ]
      },
      {
        "question": "Which three Twitter sentiment classification datasets are used for experiments?",
        "question_id": "12d77ac09c659d2e04b5e3955a283101c3ad1058",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus)",
              "Sanders - Twitter Sentiment Corpus",
              "Health Care Reform (HCR)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .",
              "Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.",
              "Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 ."
            ],
            "highlighted_evidence": [
              "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 ."
            ]
          }
        ]
      },
      {
        "question": "What semantic rules are proposed?",
        "question_id": "d60a3887a0d434abc0861637bbcd9ad0c596caf4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "rules that compute polarity of words after POS tagging or parsing steps",
            "evidence": [
              "In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:",
              "@lonedog bwahahah...you are amazing! However, it was quite the letdown.",
              "@kirstiealley my dentist is great but she's expensive...=(",
              "In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.",
              "FLOAT SELECTED: Table I SEMANTIC RULES [12]"
            ],
            "highlighted_evidence": [
              "In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:\n\n@lonedog bwahahah...you are amazing! However, it was quite the letdown.\n\n@kirstiealley my dentist is great but she's expensive...=(\n\nIn two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.",
              "FLOAT SELECTED: Table I SEMANTIC RULES [12]"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/pacling/NguyenN17",
    "dblp_title": "A Deep Neural Architecture for Sentence-Level Sentiment Classification in Twitter Social Networking.",
    "year": "2017"
  },
  {
    "id": "1811.01399",
    "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
    "qas": [
      {
        "question": "Which knowledge graph completion tasks do they experiment with?",
        "question_id": "69a7a6675c59a4c5fb70006523b9fe0f01ca415c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "link prediction ",
              "triplet classification"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250. LSTM is used due to its large expressive capability BIBREF26 ."
            ],
            "highlighted_evidence": [
              "We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification."
            ]
          }
        ]
      },
      {
        "question": "Apart from using desired properties, do they evaluate their LAN approach in some other way?",
        "question_id": "60cb756d382b3594d9e1f4a5e2366db407e378ae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they evaluate existing methods in terms of desired properties?",
        "question_id": "352a1bf734b2d7f0618e9e2b0dbed4a3f1787160",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/WangHLP19",
    "dblp_title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding.",
    "year": "2019"
  },
  {
    "id": "1909.00124",
    "title": "Learning with Noisy Labels for Sentence-level Sentiment Classification",
    "qas": [
      {
        "question": "How does the model differ from Generative Adversarial Networks?",
        "question_id": "045dbdbda5d96a672e5c69442e30dbf21917a1ee",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the dataset used to train the model?",
        "question_id": "c20b012ad31da46642c553ce462bc0aad56912db",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " movie sentence polarity dataset from BIBREF19",
              "laptop and restaurant datasets collected from SemEval-201",
              "we collected 2,000 reviews for each domain from the same review source"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same.",
              "Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9."
            ],
            "highlighted_evidence": [
              "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 .",
              "Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source."
            ]
          }
        ]
      },
      {
        "question": "What is the performance of the model?",
        "question_id": "13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",
            "evidence": [
              "The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing.",
              "The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300.",
              "FLOAT SELECTED: Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.",
              "FLOAT SELECTED: Figure 2: Accuracy (ACC) on clean test data. For training, the labels of clean data are flipped with the noise rates [0, 0.1, 0.2, 0.3, 0.4, 0.5]. For example, 0.1means that 10% of the labels are flipped. (Color online)"
            ],
            "highlighted_evidence": [
              "The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN",
              "The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels.",
              "FLOAT SELECTED: Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.",
              "FLOAT SELECTED: Figure 2: Accuracy (ACC) on clean test data. For training, the labels of clean data are flipped with the noise rates [0, 0.1, 0.2, 0.3, 0.4, 0.5]. For example, 0.1means that 10% of the labels are flipped. (Color online)"
            ]
          }
        ]
      },
      {
        "question": "Is the model evaluated against a CNN baseline?",
        "question_id": "89b9a2389166b992c42ca19939d750d88c5fa79b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.",
              "The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300."
            ],
            "highlighted_evidence": [
              "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/WangLLYL19",
    "dblp_title": "Learning with Noisy Labels for Sentence-level Sentiment Classification.",
    "year": "2019"
  },
  {
    "id": "1909.00088",
    "title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange",
    "qas": [
      {
        "question": "Does the model proposed beat the baseline models for all the values of the masking parameter tested?",
        "question_id": "dccc3b182861fd19ccce5bd00ce9c3f40451ed6e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT",
              "FLOAT SELECTED: Table 9: Average results by MRT/RRT"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 4: Graph of average results by MRT/RRT",
              "FLOAT SELECTED: Table 9: Average results by MRT/RRT"
            ]
          }
        ]
      },
      {
        "question": "Has STES been previously used in the literature to evaluate similar tasks?",
        "question_id": "98ba7a7aae388b1a77dd6cab890977251d906359",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We propose a pipeline called SMERTI (pronounced `smarty') for STE. Combining entity replacement (ER), similarity masking (SM), and text infilling (TI), SMERTI can modify the semantic content of text. We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change."
            ],
            "highlighted_evidence": [
              "We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change."
            ]
          }
        ]
      },
      {
        "question": "What are the baseline models mentioned in the paper?",
        "question_id": "3da9a861dfa25ed486cff0ef657d398fdebf8a93",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Noun WordNet Semantic Text Exchange Model (NWN-STEM)",
              "General WordNet Semantic Text Exchange Model (GWN-STEM)",
              "Word2Vec Semantic Text Exchange Model (W2V-STEM)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate on three datasets: Yelp and Amazon reviews BIBREF1, and Kaggle news headlines BIBREF2. We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM)."
            ],
            "highlighted_evidence": [
              "We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/FengLH19",
    "dblp_title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange.",
    "year": "2019"
  },
  {
    "id": "1911.01799",
    "title": "CN-CELEB: a challenging Chinese speaker recognition dataset",
    "qas": [
      {
        "question": "What was the performance of both approaches on their dataset?",
        "question_id": "8c0a0747a970f6ea607ff9b18cfeb738502d9a95",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "ERR of 19.05 with i-vectors and 15.52 with x-vectors",
            "evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ]
          }
        ]
      },
      {
        "question": "What kind of settings do the utterances come from?",
        "question_id": "529dabe7b4a8a01b20ee099701834b60fb0c43b0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons.",
              "CN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging."
            ],
            "highlighted_evidence": [
              "CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons.\n\nCN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging."
            ]
          }
        ]
      },
      {
        "question": "What genres are covered?",
        "question_id": "a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",
            "evidence": [
              "FLOAT SELECTED: Table 1. The distribution over genres."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1. The distribution over genres."
            ]
          }
        ]
      },
      {
        "question": "Do they experiment with cross-genre setups?",
        "question_id": "5699996a7a2bb62c68c1e62e730cabf1e3186eef",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which of the two speech recognition models works better overall on CN-Celeb?",
        "question_id": "944d5dbe0cfc64bf41ea36c11b1d378c408d40b8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "x-vector",
            "evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ]
          }
        ]
      },
      {
        "question": "By how much is performance on CN-Celeb inferior to performance on VoxCeleb?",
        "question_id": "327e6c6609fbd4c6ae76284ca639951f03eb4a4c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",
            "evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icassp/FanKLLCCZZCW20",
    "dblp_title": "CN-Celeb: A Challenging Chinese Speaker Recognition Dataset.",
    "year": "2020"
  },
  {
    "id": "1812.06705",
    "title": "Conditional BERT Contextual Augmentation",
    "qas": [
      {
        "question": "On what datasets is the new model evaluated on?",
        "question_id": "df8cc1f395486a12db98df805248eb37c087458b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SST (Stanford Sentiment Treebank)",
              "Subj (Subjectivity dataset)",
              "MPQA Opinion Corpus",
              "RT is another movie review sentiment dataset",
              "TREC is a dataset for classification of the six question types"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).",
              "Subj BIBREF26 Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective.",
              "MPQA BIBREF27 MPQA Opinion Corpus is an opinion polarity detection dataset of short phrases rather than sentences, which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).",
              "RT BIBREF28 RT is another movie review sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes collected by Bo Pang and Lillian Lee.",
              "TREC BIBREF29 TREC is a dataset for classification of the six question types (whether the question is about person, location, numeric information, etc.).",
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ],
            "highlighted_evidence": [
              "SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).\n\nSubj BIBREF26 Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective.\n\nMPQA BIBREF27 MPQA Opinion Corpus is an opinion polarity detection dataset of short phrases rather than sentences, which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).\n\nRT BIBREF28 RT is another movie review sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes collected by Bo Pang and Lillian Lee.\n\nTREC BIBREF29 TREC is a dataset for classification of the six question types (whether the question is about person, location, numeric information, etc.).",
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ]
          }
        ]
      },
      {
        "question": "How do the authors measure performance?",
        "question_id": "6e97c06f998f09256be752fa75c24ba853b0db24",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Accuracy across six datasets",
            "evidence": [
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ]
          }
        ]
      },
      {
        "question": "Does the new objective perform better than the original objective bert is trained on?",
        "question_id": "de2d33760dc05f9d28e9dabc13bab2b3264cadb7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words predicted through BERT may not be compatible with original labels. The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
            ],
            "highlighted_evidence": [
              "The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
            ]
          }
        ]
      },
      {
        "question": "Are other pretrained language models also evaluated for contextual augmentation? ",
        "question_id": "63bb39fd098786a510147f8ebc02408de350cb7c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018)."
            ]
          }
        ]
      },
      {
        "question": "Do the authors report performance of conditional bert on tasks without data augmentation?",
        "question_id": "6333845facb22f862ffc684293eccc03002a4830",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. For we masked words randomly, the masked words may be label-sensitive or label-insensitive. If label-insensitive words are masked, words predicted through BERT may not be compatible with original labels. The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks."
            ],
            "highlighted_evidence": [
              "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iccS/WuLZHH19",
    "dblp_title": "Conditional BERT Contextual Augmentation.",
    "year": "2019"
  },
  {
    "id": "1905.08949",
    "title": "Recent Advances in Neural Question Generation",
    "qas": [
      {
        "question": "Do they cover data augmentation papers?",
        "question_id": "a12a08099e8193ff2833f79ecf70acf132eda646",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the latest paper covered by this survey?",
        "question_id": "999b20dc14cb3d389d9e3ba5466bc3869d2d6190",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Kim et al. (2019)",
            "evidence": [
              "FLOAT SELECTED: Table 2: Existing NQG models with their best-reported performance on SQuAD. Legend: QW: question word generation, PC: paragraph-level context, CP: copying mechanism, LF: linguistic features, PG: policy gradient."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Existing NQG models with their best-reported performance on SQuAD. Legend: QW: question word generation, PC: paragraph-level context, CP: copying mechanism, LF: linguistic features, PG: policy gradient."
            ]
          }
        ]
      },
      {
        "question": "Do they survey visual question generation work?",
        "question_id": "ca4b66ffa4581f9491442dcec78ca556253c8146",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded questions, i.e., all relevant information for the answer can be found in the input image BIBREF74 . A key purpose of grounded VQG is to support the dataset construction for VQA. To ensure the questions are grounded, existing systems rely on image captions to varying degrees. BIBREF75 and BIBREF76 simply convert image captions into questions using rule-based methods with textual patterns. BIBREF74 proposed a neural model that can generate questions with diverse types for a single image, using separate networks to construct dense image captions and to select question types.",
              "In contrast to grounded QG, humans ask higher cognitive level questions about what can be inferred rather than what can be seen from an image. Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image. These are deep questions that require high cognition such as analyzing and creation. With significant progress in deep generative models, marked by variational auto-encoders (VAEs) and GANs, such models are also used in open-ended VQG to bring “creativity” into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 ."
            ],
            "highlighted_evidence": [
              "Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image.",
              "Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image."
            ]
          }
        ]
      },
      {
        "question": "Do they survey multilingual aspects?",
        "question_id": "b3ff166bd480048e099d09ba4a96e2e32b42422b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What learning paradigms do they cover in this survey?",
        "question_id": "3703433d434f1913307ceb6a8cfb9a07842667dd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Considering \"What\" and \"How\" separately versus jointly optimizing for both.",
            "evidence": [
              "Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction. Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such QG architectures are limiting, as their representation is confined to the variety of intermediate representations, transformation rules or templates.",
              "In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework. The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language flexibility compared to question templates."
            ],
            "highlighted_evidence": [
              "Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction. ",
              "In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework. "
            ]
          }
        ]
      },
      {
        "question": "What are all the input modalities considered in prior work in question generation?",
        "question_id": "f7c34b128f8919e658ba4d5f1f3fc604fb7ff793",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Textual inputs, knowledge bases, and images.",
            "evidence": [
              "Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text generation BIBREF26 , traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs.",
              "Recently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 . This trend is also spurred by the remarkable success of neural models in feature representation, especially on image features BIBREF30 and knowledge representations BIBREF31 . We discuss adapting NQG models to other input modalities in Section \"Wider Input Modalities\" ."
            ],
            "highlighted_evidence": [
              "While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text generation BIBREF26 , traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs.\n\nRecently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 ."
            ]
          }
        ]
      },
      {
        "question": "Do they survey non-neural methods for question generation?",
        "question_id": "d42031893fd4ba5721c7d37e1acb1c8d229ffc21",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1905-08949",
    "dblp_title": "Recent Advances in Neural Question Generation.",
    "year": "2019"
  },
  {
    "id": "1909.00170",
    "title": "Open Named Entity Modeling from Embedding Distribution",
    "qas": [
      {
        "question": "What is their model?",
        "question_id": "a999761aa976458bbc7b4f330764796446d030ff",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "cross-lingual NE recognition"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Most annotated corpus based NE recognition tasks can benefit a great deal from a known NE dictionary, as NEs are those words which carry common sense knowledge quite differ from the rest ones in any language vocabulary. This work will focus on the NE recognition from plain text instead of corpus based NE recognition. For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition. Our study contains two main steps: First, we explore the NE distribution in monolingual case. Next, we learn a hypersphere mapping between embedding spaces of languages with minimal supervision."
            ],
            "highlighted_evidence": [
              "For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition."
            ]
          }
        ]
      },
      {
        "question": "Do they evaluate on NER data sets?",
        "question_id": "f229069bcb05c2e811e4786c89b0208af90d9a25",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0. Our results in Table 6 and Table 7 demonstrate the power of hypersphere features, which contribute to nearly all of the three types of entities as shown in Table 6, except for a slight drop in the PER type of BIBREF22 on a strong baseline. HS features stably enhance all strong state-of-the-art baselines, BIBREF22 , BIBREF21 and BIBREF23 by 0.33/0.72/0.23 $F_1$ point and 0.13/0.3/0.1 $F_1$ point on both benchmark datasets, CoNLL-2003 and ONTONOTES 5.0. We show that our HS feature is also comparable with previous much more complicated LS feature, and our model surpasses their baseline (without LS feature) by 0.58/0.78 $F_1$ point with only HS features. We establish a new state-of-the-art $F_1$ score of 89.75 on ONTONOTES 5.0, while matching state-of-the-art performance with a $F_1$ score of 92.95 on CoNLL-2003 dataset."
            ],
            "highlighted_evidence": [
              "To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/tkde/LuoZZT22",
    "dblp_title": "Open Named Entity Modeling From Embedding Distribution.",
    "year": "2022"
  },
  {
    "id": "1701.03051",
    "title": "Efficient Twitter Sentiment Classification using Subjective Distant Supervision",
    "qas": [
      {
        "question": "What previously proposed methods is this method compared against?",
        "question_id": "6b55b558ed581759425ede5d3a6fcdf44b8082ac",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Naive Bayes",
              "SVM",
              "Maximum Entropy classifiers"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment. Their feature vector is either composed of Unigrams, Bigrams, Unigrams + Bigrams, or Unigrams + POS tags."
            ],
            "highlighted_evidence": [
              "The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment."
            ]
          }
        ]
      },
      {
        "question": "How is effective word score calculated?",
        "question_id": "3e3f5254b729beb657310a5561950085fa690e83",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We define the Effective Word Score of score x as",
              "EFWS(x) = N(+x) - N(-x),",
              "where N(x) is the number of words in the tweet with polarity score x."
            ],
            "highlighted_evidence": [
              "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x."
            ]
          }
        ]
      },
      {
        "question": "How is tweet subjectivity measured?",
        "question_id": "5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/comsnets/SahniCCS17",
    "dblp_title": "Efficient Twitter sentiment classification using subjective distant supervision.",
    "year": "2017"
  },
  {
    "id": "1603.01417",
    "title": "Dynamic Memory Networks for Visual and Textual Question Answering",
    "qas": [
      {
        "question": "Why is supporting fact supervision necessary for DMN?",
        "question_id": "129c03acb0963ede3915415953317556a55f34ee",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We speculate that there are two main reasons for this performance disparity, all exacerbated by the removal of supporting facts. First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."
            ],
            "highlighted_evidence": [
              "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."
            ]
          }
        ]
      },
      {
        "question": "What does supporting fact supervision mean?",
        "question_id": "58b3b630a31fcb9bffb510390e1ec30efe87bfbf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " the facts that are relevant for answering a particular question) are labeled during training."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We analyze the DMN components, specifically the input module and memory module, to improve question answering. We propose a new input module which uses a two level encoder with a sentence reader and input fusion layer to allow for information flow between sentences. For the memory, we propose a modification to gated recurrent units (GRU) BIBREF7 . The new GRU formulation incorporates attention gates that are computed using global knowledge over the facts. Unlike before, the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training. The model learns to select the important facts from a larger set."
            ],
            "highlighted_evidence": [
              "the facts that are relevant for answering a particular question) are labeled during training."
            ]
          }
        ]
      },
      {
        "question": "What changes they did on input module?",
        "question_id": "141dab98d19a070f1ce7e7dc384001d49125d545",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader",
              "The second component is the input fusion layer"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, responsible only for encoding the words into a sentence embedding. The second component is the input fusion layer, allowing for interactions between sentences. This resembles the hierarchical neural auto-encoder architecture of BIBREF9 and allows content interaction between sentences. We adopt the bi-directional GRU for this input fusion layer because it allows information from both past and future sentences to be used. As gradients do not need to propagate through the words between sentences, the fusion layer also allows for distant supporting sentences to have a more direct interaction."
            ],
            "highlighted_evidence": [
              "replacing this single GRU with two different components",
              "first component is a sentence reader",
              "second component is the input fusion layer"
            ]
          }
        ]
      },
      {
        "question": "What improvements they did for DMN?",
        "question_id": "afdad4c9bdebf88630262f1a9a86ac494f06c4c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training.",
              "In addition, we introduce a new input module to represent images."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We analyze the DMN components, specifically the input module and memory module, to improve question answering. We propose a new input module which uses a two level encoder with a sentence reader and input fusion layer to allow for information flow between sentences. For the memory, we propose a modification to gated recurrent units (GRU) BIBREF7 . The new GRU formulation incorporates attention gates that are computed using global knowledge over the facts. Unlike before, the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training. The model learns to select the important facts from a larger set.",
              "In addition, we introduce a new input module to represent images. This module is compatible with the rest of the DMN architecture and its output is fed into the memory module. We show that the changes in the memory module that improved textual question answering also improve visual question answering. Both tasks are illustrated in Fig. 1 ."
            ],
            "highlighted_evidence": [
              "the new DMN+ model does not require that supporting facts",
              "In addition, we introduce a new input module to represent images."
            ]
          }
        ]
      },
      {
        "question": "How does the model circumvent the lack of supporting facts during training?",
        "question_id": "bfd4fc82ffdc5b2b32c37f4222e878106421ce2a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We have proposed new modules for the DMN framework to achieve strong results without supervision of supporting facts. These improvements include the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains."
            ],
            "highlighted_evidence": [
              " the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs."
            ]
          }
        ]
      },
      {
        "question": "Does the DMN+ model establish state-of-the-art ?",
        "question_id": "1ce26783f0ff38925bfc07bbbb65d206e52c2d21",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We have proposed new modules for the DMN framework to achieve strong results without supervision of supporting facts. These improvements include the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains."
            ],
            "highlighted_evidence": [
              "Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icml/XiongMS16",
    "dblp_title": "Dynamic Memory Networks for Visual and Textual Question Answering.",
    "year": "2016"
  },
  {
    "id": "1911.03385",
    "title": "Low-Level Linguistic Controls for Style Transfer and Content Preservation",
    "qas": [
      {
        "question": "Is this style generator compared to some baseline?",
        "question_id": "9213159f874b3bdd9b4de956a88c703aac988411",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. To generate in a specific style, we simply set the appropriate embedding. We use genre embeddings of size 850 which is equivalent to the total size of the $K$ feature embeddings in the StyleEQ model."
            ],
            "highlighted_evidence": [
              "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7."
            ]
          }
        ]
      },
      {
        "question": "How they perform manual evaluation, what is criteria?",
        "question_id": "5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "accuracy"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation."
            ],
            "highlighted_evidence": [
              "Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation."
            ]
          }
        ]
      },
      {
        "question": "What metrics are used for automatic evaluation?",
        "question_id": "a234bcbf2e41429422adda37d9e926b49ef66150",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "classification accuracy",
              "BLEU scores",
              "model perplexities of the reconstruction"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight. Beam candidates are ranked according to their length normalized log-likelihood. On these automatic measures we see that StyleEQ is better able to reconstruct the original sentences. In some sense this evaluation is mostly a sanity check, as the feature controls contain more locally specific information than the genre embeddings, which say very little about how many specific function words one should expect to see in the output.",
              "In table:fasttext-results we see the results. Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs."
            ],
            "highlighted_evidence": [
              "In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight.",
              "Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs."
            ]
          }
        ]
      },
      {
        "question": "How they know what are content words?",
        "question_id": "c383fa9170ae00a4a24a8e39358c38395c5f034b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " words found in the control word lists are then removed",
              "The remaining words, which represent the content"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "fig:sentenceinput illustrates the process. Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.",
              "In this way we encourage models to construct a sentence using content and style independently. This will allow us to vary the stylistic controls while keeping the content constant, and successfully perform style transfer. When generating a new sentence, the controls correspond to the counts of the corresponding syntactic features that we expect to be realized in the output."
            ],
            "highlighted_evidence": [
              "Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.\n\nIn this way we encourage models to construct a sentence using content and style independently."
            ]
          }
        ]
      },
      {
        "question": "How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?",
        "question_id": "83251fd4a641cea8b180b49027e74920bca2699a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples."
            ],
            "highlighted_evidence": [
              "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/inlg/GeroKRC19",
    "dblp_title": "Low Level Linguistic Controls for Style Transfer and Content Preservation.",
    "year": "2019"
  },
  {
    "id": "1902.06843",
    "title": "Fusing Visual, Textual and Connectivity Clues for Studying Mental Health",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "5d70c32137e82943526911ebdf78694899b3c28a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What insights into the relationship between demographics and mental health are provided?",
        "question_id": "97dac7092cf8082a6238aaa35f4b185343b914af",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age",
              "more women than men were given a diagnosis of depression"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Age Enabled Ground-truth Dataset: We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1 who disclose their age. From these 1061 users, 822 belong to depressed class and 239 belong to control class. From 3981 depressed users, 20.6% disclose their age in contrast with only 4% (239/4789) among control group. So self-disclosure of age is more prevalent among vulnerable users. Figure FIGREF18 depicts the age distribution in INLINEFORM2 . The general trend, consistent with the results in BIBREF42 , BIBREF49 , is biased toward young people. Indeed, according to Pew, 47% of Twitter users are younger than 30 years old BIBREF50 . Similar data collection procedure with comparable distribution have been used in many prior efforts BIBREF51 , BIBREF49 , BIBREF42 . We discuss our approach to mitigate the impact of the bias in Section 4.1. The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) BIBREF51",
              "Gender Enabled Ground-truth Dataset: We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. From 1464 users 64% belonged to the depressed group, and the rest (36%) to the control group. 23% of the likely depressed users disclose their gender which is considerably higher (12%) than that for the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed chi-square test (null hypothesis: gender and depression are two independent variables). Figure FIGREF19 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Figure FIGREF19 -A,D) show positive association among corresponding row and column variables while red circles (negative residuals, see Figure FIGREF19 -B,C) imply a repulsion. Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. In particular, the female-to-male ratio is 2.1 and 1.9 for Major Depressive Disorder and Dysthymic Disorder respectively. Our findings from Twitter data indicate there is a strong association (Chi-square: 32.75, p-value:1.04e-08) between being female and showing depressive behavior on Twitter."
            ],
            "highlighted_evidence": [
              "The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.)",
              "Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression."
            ]
          }
        ]
      },
      {
        "question": "What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?",
        "question_id": "195611926760d1ceec00bd043dfdc8eba2df5ad1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Random Forest classifier"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the above findings for predicting depressive behavior. Our model exploits early fusion BIBREF32 technique in feature space and requires modeling each user INLINEFORM0 in INLINEFORM1 as vector concatenation of individual modality features. As opposed to computationally expensive late fusion scheme where each modality requires a separate supervised modeling, this model reduces the learning effort and shows promising results BIBREF75 . To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm SECREF6 and Figure FIGREF45 ) BIBREF76 ."
            ],
            "highlighted_evidence": [
              "To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data."
            ]
          }
        ]
      },
      {
        "question": "How do this framework facilitate demographic inference from social media?",
        "question_id": "445e792ce7e699e960e2cb4fe217aeacdd88d392",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Demographic information is predicted using weighted lexicon of terms.",
            "evidence": [
              "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2",
              "where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset."
            ],
            "highlighted_evidence": [
              "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender.",
              "Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2\n\nwhere INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 ."
            ]
          }
        ]
      },
      {
        "question": "What types of features are used from each data type?",
        "question_id": "a3b1520e3da29d64af2b6e22ff15d330026d0b36",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "facial presence",
              "Facial Expression",
              "General Image Features",
              " textual content",
              "analytical thinking",
              "clout",
              "authenticity",
              "emotional tone",
              "Sixltr",
              " informal language markers",
              "1st person singular pronouns"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization. We identify facial presentation, emotion from facial expression, and demographic features from profile/posted images . Table TABREF21 illustrates facial presentation differences in both profile and posted images (media) for depressed and control users in INLINEFORM0 . With control class showing significantly higher in both profile and media (8%, 9% respectively) compared to that for the depressed class. In contrast with age and gender disclosure, vulnerable users are less likely to disclose their facial identity, possibly due to lack of confidence or fear of stigma.",
              "Facial Expression:",
              "Following BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images. Positive emotions are joy and surprise, and negative emotions are anger, disgust, fear, and sadness. In general, for each user u in INLINEFORM0 , we process profile/shared images for both the depressed and the control groups with at least one face from the shared images (Table TABREF23 ). For the photos that contain multiple faces, we measure the average emotion.",
              "General Image Features:",
              "The importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 . Color, as a pillar of the human vision system, has a strong association with conceptual ideas like emotion BIBREF58 , BIBREF59 . We measured the normalized red, green, blue and the mean of original colors, and brightness and contrast relative to variations of luminance. We represent images in Hue-Saturation-Value color space that seems intuitive for humans, and measure mean and variance for saturation and hue. Saturation is defined as the difference in the intensities of the different light wavelengths that compose the color. Although hue is not interpretable, high saturation indicates vividness and chromatic purity which are more appealing to the human eye BIBREF8 . Colorfulness is measured as a difference against gray background BIBREF60 . Naturalness is a measure of the degree of correspondence between images and the human perception of reality BIBREF60 . In color reproduction, naturalness is measured from the mental recollection of the colors of familiar objects. Additionally, there is a tendency among vulnerable users to share sentimental quotes bearing negative emotions. We performed optical character recognition (OCR) with python-tesseract to extract text and their sentiment score. As illustrated in Table TABREF26 , vulnerable users tend to use less colorful (higher grayscale) profile as well as shared images to convey their negative feelings, and share images that are less natural (Figure FIGREF15 ). With respect to the aesthetic quality of images (saturation, brightness, and hue), depressed users use images that are less appealing to the human eye. We employ independent t-test, while adopting Bonferroni Correction as a conservative approach to adjust the confidence intervals. Overall, we have 223 features, and choose Bonferroni-corrected INLINEFORM0 level of INLINEFORM1 (*** INLINEFORM2 , ** INLINEFORM3 ).",
              "Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.)"
            ],
            "highlighted_evidence": [
              "For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization.",
              "Facial Expression:\n\nFollowing BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images.",
              "General Image Features:\n\nThe importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 . ",
              "Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. ",
              "It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.)"
            ]
          }
        ]
      },
      {
        "question": "How is the data annotated?",
        "question_id": "2cf8825639164a842c3172af039ff079a8448592",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The data are self-reported by Twitter users and then verified by two human experts.",
            "evidence": [
              "Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., \"16 years old suicidal girl\"(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url."
            ],
            "highlighted_evidence": [
              "We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 ."
            ]
          }
        ]
      },
      {
        "question": "Where does the information on individual-level demographics come from?",
        "question_id": "36b25021464a9574bf449e52ae50810c4ac7b642",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "From Twitter profile descriptions of the users.",
            "evidence": [
              "Age Enabled Ground-truth Dataset: We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1 who disclose their age. From these 1061 users, 822 belong to depressed class and 239 belong to control class. From 3981 depressed users, 20.6% disclose their age in contrast with only 4% (239/4789) among control group. So self-disclosure of age is more prevalent among vulnerable users. Figure FIGREF18 depicts the age distribution in INLINEFORM2 . The general trend, consistent with the results in BIBREF42 , BIBREF49 , is biased toward young people. Indeed, according to Pew, 47% of Twitter users are younger than 30 years old BIBREF50 . Similar data collection procedure with comparable distribution have been used in many prior efforts BIBREF51 , BIBREF49 , BIBREF42 . We discuss our approach to mitigate the impact of the bias in Section 4.1. The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) BIBREF51",
              "Gender Enabled Ground-truth Dataset: We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. From 1464 users 64% belonged to the depressed group, and the rest (36%) to the control group. 23% of the likely depressed users disclose their gender which is considerably higher (12%) than that for the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed chi-square test (null hypothesis: gender and depression are two independent variables). Figure FIGREF19 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Figure FIGREF19 -A,D) show positive association among corresponding row and column variables while red circles (negative residuals, see Figure FIGREF19 -B,C) imply a repulsion. Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. In particular, the female-to-male ratio is 2.1 and 1.9 for Major Depressive Disorder and Dysthymic Disorder respectively. Our findings from Twitter data indicate there is a strong association (Chi-square: 32.75, p-value:1.04e-08) between being female and showing depressive behavior on Twitter."
            ],
            "highlighted_evidence": [
              "We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994).",
              "We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description."
            ]
          }
        ]
      },
      {
        "question": "What is the source of the user interaction data? ",
        "question_id": "98515bd97e4fae6bfce2d164659cd75e87a9fc89",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Sociability from ego-network on Twitter",
            "evidence": [
              "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ],
            "highlighted_evidence": [
              "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ]
          }
        ]
      },
      {
        "question": "What is the source of the textual data? ",
        "question_id": "53bf6238baa29a10f4ff91656c470609c16320e1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Users' tweets",
            "evidence": [
              "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ],
            "highlighted_evidence": [
              "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ]
          }
        ]
      },
      {
        "question": "What is the source of the visual data? ",
        "question_id": "b27f7993b1fe7804c5660d1a33655e424cea8d10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Profile pictures from the Twitter users' profiles.",
            "evidence": [
              "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ],
            "highlighted_evidence": [
              "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1902-06843",
    "dblp_title": "Fusing Visual, Textual and Connectivity Clues for Studying Mental Health.",
    "year": "2019"
  },
  {
    "id": "1905.06512",
    "title": "Incorporating Sememes into Chinese Definition Modeling",
    "qas": [
      {
        "question": "Is there an online demo of their system?",
        "question_id": "e21a8581cc858483a31c6133e53dd0cfda76ae4c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they perform manual evaluation?",
        "question_id": "9f6e877e3bde771595e8aee10c2656a0e7b9aeb2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word “旅馆” (hotel), the baseline model fails to generate definition containing the token “旅行者”(tourists). However, by incoporating sememes' information, especially the sememe “旅游” (tour), AAM and SAAM successfully generate “旅行者”(tourists). Manual inspection of others examples also supports our claim."
            ],
            "highlighted_evidence": [
              "Manual inspection of others examples also supports our claim."
            ]
          }
        ]
      },
      {
        "question": "Do they compare against Noraset et al. 2017?",
        "question_id": "a3783e42c2bf616c8a07bd3b3d503886660e4344",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework. Without utilizing the information of sememes, it learns a probabilistic mapping $P(y | x)$ from the word $x$ to be defined to a definition $y = [y_1, \\dots , y_T ]$ , in which $y_t$ is the $t$ -th word of definition $y$ ."
            ],
            "highlighted_evidence": [
              "The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework."
            ]
          }
        ]
      },
      {
        "question": "What is a sememe?",
        "question_id": "0d0959dba3f7c15ee4f5cdee51682656c4abbd8f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work, we introduce a new dataset for the Chinese definition modeling task that we call Chinese Definition Modeling Corpus cdm(CDM). CDM consists of 104,517 entries, where each entry contains a word, the sememes of a specific word sense, and the definition in Chinese of the same word sense. Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 . For a given word sense, CDM annotates the sememes according to HowNet BIBREF5 , and the definition according to Chinese Concept Dictionary (CCD) BIBREF6 . Since sememes have been widely used in improving word representation learning BIBREF7 and word similarity computation BIBREF8 , we argue that sememes can benefit the task of definition modeling."
            ],
            "highlighted_evidence": [
              "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/taslp/YangKCLFY20",
    "dblp_title": "Incorporating Sememes into Chinese Definition Modeling.",
    "year": "2020"
  },
  {
    "id": "2001.06286",
    "title": "RobBERT: a Dutch RoBERTa-based Language Model",
    "qas": [
      {
        "question": "What data did they use?",
        "question_id": "589be705a5cc73a23f30decba23ce58ec39d313b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the Dutch section of the OSCAR corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16. This Dutch corpus has 6.6 billion words, totalling 39 GB of text. It contains 126,064,722 lines of text, where each line can contain multiple sentences. Subsequent lines are however not related to each other, due to the shuffled nature of the OSCAR data set. For comparison, the French RoBERTa-based language model CamemBERT BIBREF7 has been trained on the French portion of OSCAR, which consists of 138 GB of scraped text."
            ],
            "highlighted_evidence": [
              "We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16."
            ]
          }
        ]
      },
      {
        "question": "What is the state of the art?",
        "question_id": "6e962f1f23061f738f651177346b38fd440ff480",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BERTje BIBREF8",
              "an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19.",
              "mBERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))",
              "We replicated the high-level sentiment analysis task used to evaluate BERTje BIBREF8 to be able to compare our methods. This task uses a dataset called Dutch Book Reviews Dataset (DBRD), in which book reviews scraped from hebban.nl are labeled as positive or negative BIBREF19. Although the dataset contains 118,516 reviews, only 22,252 of these reviews are actually labeled as positive or negative. The DBRD dataset is already split in a balanced 10% test and 90% train split, allowing us to easily compare to other models trained for solving this task. This dataset was released in a paper analysing the performance of an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))",
              "This dataset was released in a paper analysing the performance of an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19."
            ]
          }
        ]
      },
      {
        "question": "What language tasks did they experiment on?",
        "question_id": "594a6bf37eab64a16c6a05c365acc100e38fcff1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "sentiment analysis",
              "the disambiguation of demonstrative pronouns,"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluated RobBERT in several different settings on multiple downstream tasks. First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks. Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning."
            ],
            "highlighted_evidence": [
              "First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks. ",
              "Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/DelobelleWB20",
    "dblp_title": "RobBERT: a Dutch RoBERTa-based Language Model.",
    "year": "2020"
  },
  {
    "id": "1910.02789",
    "title": "Natural Language State Representation for Reinforcement Learning",
    "qas": [
      {
        "question": "What result from experiments suggest that natural language based agents are more robust?",
        "question_id": "d79d897f94e666d5a6fcda3b0c7e807c8fad109e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances",
            "evidence": [
              "Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods. This is contrary to the fact that it contains the same information as the semantic segmentation maps. More interestingly, comparing the vision-based and feature-based representations render inconsistent conclusions with respect to their relative performance. NLP representations remain robust to changes in the environment as well as task-nuisances in the state. As depicted in fig: nuisance scenarios, inflating the state space with task-nuisances impairs the performance of all representations. There, a large amount of unnecessary objects were spawned in the level, increasing the state's description length to over 250 words, whilst retaining the same amount of useful information. Nevertheless, the NLP representation outperformed the vision and feature based representations, with high robustness to the applied noise."
            ],
            "highlighted_evidence": [
              "Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods. ",
              "NLP representations remain robust to changes in the environment as well as task-nuisances in the state. "
            ]
          }
        ]
      },
      {
        "question": "How better is performance of natural language based agents in experiments?",
        "question_id": "599d9ca21bbe2dbe95b08cf44dfc7537bde06f98",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How much faster natural language agents converge in performed experiments?",
        "question_id": "827464c79f33e69959de619958ade2df6f65fdee",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What experiments authors perform?",
        "question_id": "8e857e44e4233193c7b2d538e520d37be3ae1552",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We tested the natural language representation against the visual-based and feature representations on several tasks, with varying difficulty. In these tasks, the agent could navigate, shoot, and collect items such as weapons and medipacks. Often, enemies of different types attacked the agent, and a positive reward was given when an enemy was killed. Occasionally, the agent also suffered from health degeneration. The tasks included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios was designed to challenge the agent."
            ],
            "highlighted_evidence": [
              "We tested the natural language representation against the visual-based and feature representations on several tasks, with varying difficulty.",
              "The tasks included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios was designed to challenge the agent."
            ]
          }
        ]
      },
      {
        "question": "How is state to learn and complete tasks represented via natural language?",
        "question_id": "084fb7c80a24b341093d4bf968120e3aff56f693",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " represent the state using natural language"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The term representation is used differently in different contexts. For the purpose of this paper we define a semantic representation of a state as one that reflects its meaning as it is understood by an expert. The semantic representation of a state should thus be paired with a reliable and computationally efficient method for extracting information from it. Previous success in RL has mainly focused on representing the state in its raw form (e.g., visual input in Atari-based games BIBREF2). This approach stems from the belief that neural networks (specifically convolutional networks) can extract meaningful features from complex inputs. In this work, we challenge current representation techniques and suggest to represent the state using natural language, similar to the way we, as humans, summarize and transfer information efficiently from one to the other BIBREF5."
            ],
            "highlighted_evidence": [
              ". In this work, we challenge current representation techniques and suggest to represent the state using natural language, similar to the way we, as humans, summarize and transfer information efficiently from one to the other BIBREF5."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2007-09774",
    "dblp_title": "An Overview of Natural Language State Representation for Reinforcement Learning.",
    "year": "2020"
  },
  {
    "id": "1902.00672",
    "title": "Query-oriented text summarization based on hypergraph transversals",
    "qas": [
      {
        "question": "How does the model compare with the MMR baseline?",
        "question_id": "babe72f0491e65beff0e5889380e8e32d7a81f78",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Various classes of NP-hard problems involving a submodular and non-decreasing function can be solved approximately by polynomial time algorithms with provable approximation factors. Algorithms \"Detection of hypergraph transversals for text summarization\" and \"Detection of hypergraph transversals for text summarization\" are our core methods for the detection of approximations of maximal budgeted hypergraph transversals and minimal soft hypergraph transversals, respectively. In each case, a transversal is found and the summary is formed by extracting and aggregating the associated sentences. Algorithm \"Detection of hypergraph transversals for text summarization\" is based on an adaptation of an algorithm presented in BIBREF30 for the maximization of submodular functions under a Knaspack constraint. It is our primary transversal-based summarization model, and we refer to it as the method of Transversal Summarization with Target Length (TL-TranSum algorithm). Algorithm \"Detection of hypergraph transversals for text summarization\" is an application of the algorithm presented in BIBREF20 for solving the submodular set covering problem. We refer to it as Transversal Summarization with Target Coverage (TC-TranSum algorithm). Both algorithms produce transversals by iteratively appending the node inducing the largest increase in the total weight of the covered hyperedges relative to the node weight. While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only. The main difference between algorithms \"Detection of hypergraph transversals for text summarization\" and \"Detection of hypergraph transversals for text summarization\" is the stopping criterion: in algorithm \"Detection of hypergraph transversals for text summarization\" , the approximate minimal soft transversal is obtained whenever the targeted hyperedge coverage is reached while algorithm \"Detection of hypergraph transversals for text summarization\" appends a given sentence to the approximate maximal budgeted transversal only if its addition does not make the summary length exceed the target length $L$ .",
              "FLOAT SELECTED: Table 2: Comparison with related graph- and hypergraph-based summarization systems."
            ],
            "highlighted_evidence": [
              "While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only.",
              "FLOAT SELECTED: Table 2: Comparison with related graph- and hypergraph-based summarization systems."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/ipm/LierdeC19",
    "dblp_title": "Query-oriented text summarization based on hypergraph transversals.",
    "year": "2019"
  },
  {
    "id": "2001.07209",
    "title": "Text-based inference of moral sentiment change",
    "qas": [
      {
        "question": "Does the paper discuss previous models which have been applied to the same task?",
        "question_id": "31ee92e521be110b6a5a8d08cc9e6f90a3a97aae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16."
            ],
            "highlighted_evidence": [
              "An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16."
            ]
          }
        ]
      },
      {
        "question": "Which datasets are used in the paper?",
        "question_id": "737397f66751624bcf4ef891a10b29cfc46b0520",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n",
            "evidence": [
              "To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text. We use the MFD for moral polarity classification by dividing seed words into positive and negative sets, and for fine-grained categorization by splitting them into the 10 MFT categories.",
              "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.",
              "We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:",
              "Google N-grams BIBREF31: a corpus of $8.5 \\times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.",
              "COHA BIBREF32: a smaller corpus of $4.1 \\times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009."
            ],
            "highlighted_evidence": [
              "To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text.",
              "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.",
              "We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:\n\nGoogle N-grams BIBREF31: a corpus of $8.5 \\times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.\n\nCOHA BIBREF32: a smaller corpus of $4.1 \\times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009."
            ]
          }
        ]
      },
      {
        "question": "How does the parameter-free model work?",
        "question_id": "87cb19e453cf7e248f24b5f7d1ff9f02d87fc261",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;",
              "A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF2 specifies the formulation of each model. Note that we adopt a parsimonious design principle in our modelling: both Centroid and Naïve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.",
              "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;",
              "A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"
            ],
            "highlighted_evidence": [
              " Note that we adopt a parsimonious design principle in our modelling: both Centroid and Naïve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.",
              "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;",
              "A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"
            ]
          }
        ]
      },
      {
        "question": "How do they quantify moral relevance?",
        "question_id": "5fb6a21d10adf4e81482bb5c1ec1787dc9de260d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence",
            "evidence": [
              "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words."
            ],
            "highlighted_evidence": [
              "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words."
            ]
          }
        ]
      },
      {
        "question": "Which fine-grained moral dimension examples do they showcase?",
        "question_id": "542a87f856cb2c934072bacaa495f3c2645f93be",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We draw from research in social psychology to inform our methodology, most prominently Moral Foundations Theory BIBREF26. MFT seeks to explain the structure and variation of human morality across cultures, and proposes five moral foundations: Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation. Each foundation is summarized by a positive and a negative pole, resulting in ten fine-grained moral categories."
            ],
            "highlighted_evidence": [
              "We draw from research in social psychology to inform our methodology, most prominently Moral Foundations Theory BIBREF26. MFT seeks to explain the structure and variation of human morality across cultures, and proposes five moral foundations: Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation. Each foundation is summarized by a positive and a negative pole, resulting in ten fine-grained moral categories."
            ]
          }
        ]
      },
      {
        "question": "Which dataset sources to they use to demonstrate moral sentiment through history?",
        "question_id": "4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2001-07209",
    "dblp_title": "Text-based inference of moral sentiment change.",
    "year": "2020"
  },
  {
    "id": "2001.10161",
    "title": "Bringing Stories Alive: Generating Interactive Fiction Worlds",
    "qas": [
      {
        "question": "How well did the system do?",
        "question_id": "c180f44667505ec03214d44f4970c0db487a8bae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the neural approach is generally preferred by a greater percentage of participants than the rules or random",
              "human-made game outperforms them all"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales. This is done in part to test the relative effectiveness of our approach across different genres with varying thematic commonsense knowledge. The dataset used was compiled via story summaries that were scraped from Wikipedia via a recursive crawling bot. The bot searched pages for both for plot sections as well as links to other potential stories. From the process, 695 fairy-tales and 536 mystery stories were compiled from two categories: novels and short stories. We note that the mysteries did not often contain many fantasy elements, i.e. they consisted of mysteries set in our world such as Sherlock Holmes, while the fairy-tales were much more removed from reality. Details regarding how each of the studies were conducted and the corresponding setup are presented below.",
              "Each participant was was asked to play the neural game and then another one from one of the three additional models within a genre. The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities. This provided the participant with multiple possible methods of finishing a particular game. On completion, the participant was asked to rank the two games according to overall perceived coherence, interestingness, and adherence to the genre. We additionally provided a required initial tutorial game which demonstrated all of these mechanics. The order in which participants played the games was also randomized as in the graph evaluation to remove potential correlations. We had 75 participants in total, 39 for mystery and 36 for fairy-tales. As each player played the neural model created game and one from each of the other approaches—this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales.",
              "FLOAT SELECTED: Table 4: Results of the full game evaluation participant study. *Indicates statistical significance (p < 0.05).",
              "In the mystery genre, the neural approach is generally preferred by a greater percentage of participants than the rules or random. The human-made game outperforms them all. A significant exception to is that participants thought that the rules-based game was more interesting than the neural game. The trends in the fairy-tale genre are in general similar with a few notable deviations. The first deviation is that the rules-based and random approaches perform significantly worse than neural in this genre. We see also that the neural game is as coherent as the human-made game."
            ],
            "highlighted_evidence": [
              "We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales.",
              "Each participant was was asked to play the neural game and then another one from one of the three additional models within a genre. The completion criteria for each game is collect half the total score possible in the game, i.e. explore half of all possible rooms and examine half of all possible entities. This provided the participant with multiple possible methods of finishing a particular game. On completion, the participant was asked to rank the two games according to overall perceived coherence, interestingness, and adherence to the genre. We additionally provided a required initial tutorial game which demonstrated all of these mechanics. The order in which participants played the games was also randomized as in the graph evaluation to remove potential correlations. We had 75 participants in total, 39 for mystery and 36 for fairy-tales. As each player played the neural model created game and one from each of the other approaches—this gave us 13 on average for the other approaches in the mystery genre and 12 for fairy-tales.",
              "FLOAT SELECTED: Table 4: Results of the full game evaluation participant study. *Indicates statistical significance (p < 0.05).",
              "In the mystery genre, the neural approach is generally preferred by a greater percentage of participants than the rules or random. The human-made game outperforms them all. A significant exception to is that participants thought that the rules-based game was more interesting than the neural game. The trends in the fairy-tale genre are in general similar with a few notable deviations. The first deviation is that the rules-based and random approaches perform significantly worse than neural in this genre. We see also that the neural game is as coherent as the human-made game."
            ]
          }
        ]
      },
      {
        "question": "How is the information extracted?",
        "question_id": "76d62e414a345fe955dc2d99562ef5772130bc7e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "neural question-answering technique to extract relations from a story text",
              "OpenIE5, a commonly used rule-based information extraction technique"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the “next to” and “has” edges respectively in Figure FIGREF4.",
              "While many neural models already exist that perform similar tasks such as named entity extraction and part of speech tagging, they often come at the cost of large amounts of specialized labeled data suited for that task. We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.",
              "The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.",
              "The next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation—or edge—at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions “What location can I visit from $x$?” and “Who/What is in $x$?”. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. The probability that vertices $x,u$ are related:",
              "We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.",
              "As in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples—identifying the set of characters and objects in the story.",
              "The graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph."
            ],
            "highlighted_evidence": [
              "The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the “next to” and “has” edges respectively in Figure FIGREF4.",
              "We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.",
              "The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.",
              "The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.\n\nThe next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation—or edge—at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions “What location can I visit from $x$?” and “Who/What is in $x$?”. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model.",
              "We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.\n\nAs in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples—identifying the set of characters and objects in the story.\n\nThe graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aiide/AmmanabroluCTBR20",
    "dblp_title": "Bringing Stories Alive: Generating Interactive Fiction Worlds.",
    "year": "2020"
  },
  {
    "id": "1909.00279",
    "title": "Generating Classical Chinese Poems from Vernacular Chinese",
    "qas": [
      {
        "question": "What are some guidelines in writing input vernacular so model can generate ",
        "question_id": "6b9310b577c6232e3614a1612cbbbb17067b3886",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score",
              "poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "1) In classical Chinese poems, poetic images UTF8gbsn(意象) were widely used to express emotions and to build artistic conception. A certain poetic image usually has some fixed implications. For example, autumn is usually used to imply sadness and loneliness. However, with the change of time, poetic images and their implications have also changed. According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score. As illustrated in Table TABREF12, both paragraph 2 and 3 are generated from pop song lyrics, paragraph 2 uses many poetic images from classical literature (e.g. pear flowers, makeup), while paragraph 3 uses modern poetic images (e.g. sparrows on the utility pole). Obviously, compared with poem 2, sentences in poem 3 seems more confusing, as the poetic images in modern times may not fit well into the language model of classical poems.",
              "2) We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs. For example, in Table TABREF12, both paragraph 4 (more descriptive) and paragraph 5 (more philosophical) were selected from famous modern prose. However, compared with poem 4, poem 5 seems semantically more confusing. We offer two explanations to the above phenomenon: i. Limited by the 28-character restriction, it is hard for quatrain poems to cover complex logical or philosophical explanation. ii. As vernacular paragraphs are more detailed and lengthy, some information in a vernacular paragraph may be lost when it is summarized into a classical poem. While losing some information may not change the general meaning of a descriptive paragraph, it could make a big difference in a logical or philosophical paragraph."
            ],
            "highlighted_evidence": [
              "According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score.",
              "We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs."
            ]
          }
        ]
      },
      {
        "question": "How much is proposed model better in perplexity and BLEU score than typical UMT models?",
        "question_id": "d484a71e23d128f146182dccc30001df35cdf93f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50.",
            "evidence": [
              "As illustrated in Table TABREF12 (ID 1). Given the vernacular translation of each gold poem in test set, we generate five poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.",
              "FLOAT SELECTED: Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set fluctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence."
            ],
            "highlighted_evidence": [
              "We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.",
              "FLOAT SELECTED: Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set fluctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence."
            ]
          }
        ]
      },
      {
        "question": "What dataset is used for training?",
        "question_id": "5787ac3e80840fe4cf7bfae7e8983fa6644d6220",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We collected a corpus of poems and a corpus of vernacular literature from online resources"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set."
            ],
            "highlighted_evidence": [
              "We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/YangCFLFCY19",
    "dblp_title": "Generating Classical Chinese Poems from Vernacular Chinese.",
    "year": "2019"
  },
  {
    "id": "1909.06762",
    "title": "Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever",
    "qas": [
      {
        "question": "What were the evaluation metrics?",
        "question_id": "ee31c8a94e07b3207ca28caef3fbaf9a38d94964",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BLEU",
              "Micro Entity F1",
              "quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30.",
              "We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response."
            ],
            "highlighted_evidence": [
              "Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. ",
              "We provide human evaluation on our framework and the compared models. ",
              "We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5."
            ]
          }
        ]
      },
      {
        "question": "What were the baseline systems?",
        "question_id": "66d743b735ba75589486e6af073e955b6bb9d2a4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Attn seq2seq",
              "Ptr-UNK",
              "KV Net",
              "Mem2Seq",
              "DSR"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our model with several baselines including:",
              "Attn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.",
              "Ptr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.",
              "KV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.",
              "Mem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.",
              "DSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding."
            ],
            "highlighted_evidence": [
              "We compare our model with several baselines including:\n\nAttn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.\n\nPtr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.\n\nKV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.\n\nMem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.\n\nDSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding."
            ]
          }
        ]
      },
      {
        "question": "Which dialog datasets did they experiment with?",
        "question_id": "b9f852256113ef468d60e95912800fab604966f6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Camrest",
              "InCar Assistant"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since dialogue dataset is not typically annotated with the retrieval results, training the KB-retriever is non-trivial. To make the training feasible, we propose two methods: 1) we use a set of heuristics to derive the training data and train the retriever in a distant supervised fashion; 2) we use Gumbel-Softmax BIBREF14 as an approximation of the non-differentiable selecting process and train the retriever along with the Seq2Seq dialogue generation model. Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever. Both the retrievers trained with distant-supervision and Gumbel-Softmax technique outperform the compared systems in the automatic and human evaluations. Analysis empirically verifies our assumption that more than 80% responses in the dataset can be supported by a single KB row and better retrieval results lead to better task-oriented dialogue generation performance."
            ],
            "highlighted_evidence": [
              "Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever."
            ]
          }
        ]
      },
      {
        "question": "What KB is used?",
        "question_id": "88f8ab2a417eae497338514142ac12c3cec20876",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/QinLCWLL19",
    "dblp_title": "Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever.",
    "year": "2019"
  },
  {
    "id": "1812.07023",
    "title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal Context",
    "qas": [
      {
        "question": "At which interval do they extract video and audio frames?",
        "question_id": "05e3b831e4c02bbd64a6e35f6c52f0922a41539a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they use pretrained word vectors for dialogue context embedding?",
        "question_id": "bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "The utterance is concatenated with a special symbol marking the end of the sequence. We initialize our word embeddings using 300-dimensional GloVe BIBREF30 and then fine-tune them during training."
            ]
          }
        ]
      },
      {
        "question": "Do they train a different training method except from scheduled sampling?",
        "question_id": "6472f9d0a385be81e0970be91795b1b97aa5a9cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Answer with content missing: (list missing) \nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\n\nYes.",
            "evidence": [
              "Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:"
            ],
            "highlighted_evidence": [
              "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1812-07023",
    "dblp_title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal Context.",
    "year": "2018"
  },
  {
    "id": "1610.04377",
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "qas": [
      {
        "question": "Is the web interface publicly accessible?",
        "question_id": "2173809eb117570d289cefada6971e946b902bd6",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Is the Android application publicly available?",
        "question_id": "293e9a0b30670f4f0a380e574a416665a8c55bc2",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What classifier is used for emergency categorization?",
        "question_id": "17de58c17580350c9da9c2f3612784b432154d11",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "multi-class Naive Bayes"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate. This multi-class classifier is trained on data manually labeled with classes. We tokenize the training data using “NgramTokenizer” and then, apply a filter to create word vectors of strings before training. We use “trigrams” as features to build a model which, later, classifies tweets into appropriate categories, in real time. We then perform cross validation using standard techniques to calculate the results, which are shown under the label “Stage 2”, in table TABREF20 ."
            ],
            "highlighted_evidence": [
              "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate."
            ]
          }
        ]
      },
      {
        "question": "What classifier is used for emergency detection?",
        "question_id": "ff27d6e6eb77e55b4d39d343870118d1a6debd5e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SVM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the results and choose SVM later for stage one classification model, owing to a better F-score. The training is performed on tweets labeled with classes , and based on unigrams as features. We create word vectors of strings in the tweet using a filter available in the WEKA API BIBREF9 , and perform cross validation using standard classification techniques."
            ]
          }
        ]
      },
      {
        "question": "Do the tweets come from any individual?",
        "question_id": "29772ba04886bee2d26b7320e1c6d9b156078891",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with <emergency>and <non-emergency>labels for classification as stage one. Our dataset contains 1313 tweet with positive label <emergency>and 1887 tweets with a negative label <non-emergency>. We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc."
            ],
            "highlighted_evidence": [
              "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. "
            ]
          }
        ]
      },
      {
        "question": "How many categories are there?",
        "question_id": "94dc437463f7a7d68b8f6b57f1e3606eacc4490a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What was the baseline?",
        "question_id": "9d9d84822a9c42eb0257feb7c18086d390dae3be",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Are the tweets specific to a region?",
        "question_id": "d27e3a099954e917b6491e81b2e907478d7f1233",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with and labels for classification as stage one. Our dataset contains 1313 tweet with positive label and 1887 tweets with a negative label . We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/KanojiaKR16",
    "dblp_title": "Civique: Using Social Media to Detect Urban Emergencies.",
    "year": "2016"
  },
  {
    "id": "1906.06448",
    "title": "Can neural networks understand monotonicity reasoning?",
    "qas": [
      {
        "question": "Do they release MED?",
        "question_id": "c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" ). Compared with manual or automatic construction, we can collect naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning."
            ],
            "highlighted_evidence": [
              "To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" )."
            ]
          }
        ]
      },
      {
        "question": "What NLI models do they analyze?",
        "question_id": "dfc393ba10ec4af5a17e5957fcbafdffdb1a6443",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BiMPM",
              "ESIM",
              "Decomposable Attention Model",
              "KIM",
              "BERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment."
            ],
            "highlighted_evidence": [
              "To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI."
            ]
          }
        ]
      },
      {
        "question": "How do they define upward and downward reasoning?",
        "question_id": "311a7fa62721e82265f4e0689b4adc05f6b74215",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",
            "evidence": [
              "A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where French dinner is replaced by a more general concept dinner. On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where workers is replaced by a more specific concept new workers. Interestingly, the direction of monotonicity can be reversed again by embedding yet another downward entailing context (e.g., not in ( \"Introduction\" )), as witness the fact that ( \"Introduction\" ) entails ( \"Introduction\" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure.",
              "All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner"
            ],
            "highlighted_evidence": [
              "A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where French dinner is replaced by a more general concept dinner. ",
              "On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where workers is replaced by a more specific concept new workers.",
              "All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner"
            ]
          }
        ]
      },
      {
        "question": "What is monotonicity reasoning?",
        "question_id": "82bcacad668351c0f81bd841becb2dbf115f000e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in ( \"Introduction\" ) and ( \"Introduction\" )."
            ],
            "highlighted_evidence": [
              "Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/blackboxnlp/YanakaMBISAB19",
    "dblp_title": "Can Neural Networks Understand Monotonicity Reasoning?",
    "year": "2019"
  },
  {
    "id": "1912.00819",
    "title": "Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.",
    "qas": [
      {
        "question": "What other relations were found in the datasets?",
        "question_id": "5937ebbf04f62d41b48cbc6b5c38fc309e5c2328",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration'",
              "Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset",
              "Acknowledgements (b) are mostly with positive or neutral",
              "Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP)",
              "Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral",
              "No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny).",
              "Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We can see emotional dialogue act co-occurrences with respect to emotion labels in Figure FIGREF12 for both datasets. There are sets of three bars per dialogue act in the figure, the first and second bar represent emotion labels of IEMOCAP (IE) and MELD (ME), and the third bar is for MELD sentiment (MS) labels. MELD emotion and sentiment statistics are interesting as they are strongly correlated to each other. The bars contain the normalized number of utterances for emotion labels with respect to the total number of utterances for that particular dialogue act category. The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts.",
              "Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'.",
              "FLOAT SELECTED: Figure 4: EDAs: Visualizing co-occurrence of utterances with respect to emotion states in the particular dialogue acts (only major and significant are shown here). IE: IEMOCAP, ME: MELD Emotion and MS: MELD Sentiment."
            ],
            "highlighted_evidence": [
              "The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts.\n\nQuotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'.\n\n",
              "FLOAT SELECTED: Figure 4: EDAs: Visualizing co-occurrence of utterances with respect to emotion states in the particular dialogue acts (only major and significant are shown here). IE: IEMOCAP, ME: MELD Emotion and MS: MELD Sentiment."
            ]
          }
        ]
      },
      {
        "question": "How does the ensemble annotator extract the final label?",
        "question_id": "dcd6f18922ac5c00c22cef33c53ff5ae08b42298",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "First preference is given to the labels that are perfectly matching in all the neural annotators.",
              "In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models.",
              "When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. ",
              "Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).",
              "Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). The statistics of the EDAs is reported in Table TABREF13 for both datasets. Total utterances in MELD includes training, validation and test datasets."
            ],
            "highlighted_evidence": [
              "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).\n\nFinally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM)."
            ]
          }
        ]
      },
      {
        "question": "How were dialogue act labels defined?",
        "question_id": "2965c86467d12b79abc16e1457d848cb6ca88973",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Dialogue Act Markup in Several Layers (DAMSL) tag set"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17."
            ],
            "highlighted_evidence": [
              "There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17."
            ]
          }
        ]
      },
      {
        "question": "How many models were used?",
        "question_id": "b99948ac4810a7fe3477f6591b8cf211d6398e67",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "five"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We have annotated two potential multi-modal conversation datasets for emotion recognition: IEMOCAP (Interactive Emotional dyadic MOtion CAPture database) BIBREF6 and MELD (Multimodal EmotionLines Dataset) BIBREF8. Figure FIGREF2, shows an example of dialogue acts with emotion and sentiment labels from the MELD dataset. We confirmed the reliability of annotations with inter-annotator metrics. We analysed the co-occurrences of the dialogue act and emotion labels and discovered a key relationship between them; certain dialogue acts of the utterances show significant and useful association with respective emotional states. For example, Accept/Agree dialogue act often occurs with the Joy emotion while Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc. The detailed analysis of the emotional dialogue acts (EDAs) and annotated datasets are being made available at the SECURE EU Project website.",
              "We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). From conversational analysis using dialogue acts in Bothe et al. bothe2018interspeech, we learned that the preceding two utterances contribute significantly to recognizing the dialogue act of the current utterance. Hence, we adapt this setting for the context model and create a pool of annotators using recurrent neural networks (RNNs). RNNs can model the contextual information in the sequence of words of an utterance and in the sequence of utterances of a dialogue. Each word in an utterance is represented with a word embedding vector of dimension 1024. We use the word embedding vectors from pre-trained ELMo (Embeddings from Language Models) embeddings BIBREF22. We have a pool of five neural annotators as shown in Figure FIGREF6. Our online tool called Discourse-Wizard is available to practice automated dialogue act labeling. In this tool we use the same neural architectures but model-trained embeddings (while, in this work we use pre-trained ELMo embeddings, as they are better performant but computationally and size-wise expensive to be hosted in the online tool). The annotators are:"
            ],
            "highlighted_evidence": [
              "n this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models.",
              "We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1912-00819",
    "dblp_title": "Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.",
    "year": "2019"
  },
  {
    "id": "1907.00758",
    "title": "Synchronising audio and ultrasound by learning cross-modal embeddings",
    "qas": [
      {
        "question": "Do they compare their neural network against any other model?",
        "question_id": "73d657d6faed0c11c65b1ab60e553db57f4971ca",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they annotate their own dataset or use an existing one?",
        "question_id": "9ef182b61461d0d8b6feb1d6174796ccde290a15",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Use an existing one",
            "evidence": [
              "For our experiments, we select a dataset whose utterances have been correctly synchronised at recording time. This allows us to control how the model is trained and verify its performance using ground truth synchronisation offsets. We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). See BIBREF15 for details."
            ],
            "highlighted_evidence": [
              "We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F). "
            ]
          }
        ]
      },
      {
        "question": "Does their neural network predict a single offset in a recording?",
        "question_id": "f6f8054f327a2c084a73faca16cf24a180c094ae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. The candidate set is independent of the model, and is chosen based on task knowledge (Section SECREF5 )."
            ],
            "highlighted_evidence": [
              "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance. "
            ]
          }
        ]
      },
      {
        "question": "What kind of neural network architecture do they use?",
        "question_id": "b8f711179a468fec9a0d8a961fb0f51894af4b31",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "CNN",
            "evidence": [
              "We adopt the approach in BIBREF4 , modifying it to synchronise audio with UTI data. Our model, UltraSync, consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise BIBREF21 , BIBREF22 . This model can be viewed as an extension of a siamese neural network BIBREF23 but with two asymmetrical streams and no shared parameters. Figure FIGREF1 illustrates the main architecture. The visual data INLINEFORM0 (ultrasound) and audio data INLINEFORM1 (MFCC), which have different shapes, are mapped to low dimensional embeddings INLINEFORM2 (visual) and INLINEFORM3 (audio) of the same size: DISPLAYFORM0",
              "FLOAT SELECTED: Figure 1: UltraSync maps high dimensional inputs to low dimensional vectors using a contrastive loss function, such that the Euclidean distance is small between vectors from positive pairs and large otherwise. Inputs span '200ms: 5 consecutive raw ultrasound frames on one stream and 20 frames of the corresponding MFCC features on the other."
            ],
            "highlighted_evidence": [
              "Figure FIGREF1 illustrates the main architecture. ",
              "FLOAT SELECTED: Figure 1: UltraSync maps high dimensional inputs to low dimensional vectors using a contrastive loss function, such that the Euclidean distance is small between vectors from positive pairs and large otherwise. Inputs span '200ms: 5 consecutive raw ultrasound frames on one stream and 20 frames of the corresponding MFCC features on the other."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/EshkyRRR19",
    "dblp_title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings.",
    "year": "2019"
  },
  {
    "id": "1710.06536",
    "title": "Basic tasks of sentiment analysis",
    "qas": [
      {
        "question": "How are aspects identified in aspect extraction?",
        "question_id": "3bf429633ecbbfec3d7ffbcfa61fa90440cc918b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "apply an ensemble of deep learning and linguistics t"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Most of the previous works in aspect term extraction have either used conditional random fields (CRFs) BIBREF9 , BIBREF10 or linguistic patterns BIBREF7 , BIBREF11 . Both of these approaches have their own limitations: CRF is a linear model, so it needs a large number of features to work well; linguistic patterns need to be crafted by hand, and they crucially depend on the grammatical accuracy of the sentences. In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection."
            ],
            "highlighted_evidence": [
              "In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection."
            ]
          }
        ]
      }
    ],
    "dblp_id": "reference/snam/ChaturvediPC18",
    "dblp_title": "Sentiment Analysis, Basic Tasks of.",
    "year": "2018"
  },
  {
    "id": "1701.02877",
    "title": "Generalisation in Named Entity Recognition: A Quantitative Analysis",
    "qas": [
      {
        "question": "What web and user-generated NER datasets are used for the analysis?",
        "question_id": "94e0cf44345800ef46a8c7d52902f074a1139e1a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",
            "evidence": [
              "Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details). These datasets were chosen such that they have been annotated with the same or very similar entity classes, in particular, names of people, locations, and organisations. Thus corpora including only domain-specific entities (e.g. biomedical corpora) were excluded. The choice of corpora was also motivated by their chronological age; we wanted to ensure a good temporal spread, in order to study possible effects of entity drift over time.",
              "FLOAT SELECTED: Table 1 Corpora genres and number of NEs of different classes."
            ],
            "highlighted_evidence": [
              "Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
              "FLOAT SELECTED: Table 1 Corpora genres and number of NEs of different classes."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/csl/AugensteinDB17",
    "dblp_title": "Generalisation in named entity recognition: A quantitative analysis.",
    "year": "2017"
  },
  {
    "id": "1904.05862",
    "title": "wav2vec: Unsupervised Pre-training for Speech Recognition",
    "qas": [
      {
        "question": "Which unlabeled data do they pretrain with?",
        "question_id": "ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "1000 hours of WSJ audio data",
            "evidence": [
              "We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ). For the pre-training experiments we feed the output of the context network to the acoustic model, instead of log-mel filterbank features.",
              "Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2. On the TIMIT task, pre-training enables us to match the best reported result in the literature. In a simulated low-resource setup with only eight hours of transcriped audio data, reduces WER by up to 32% compared to a baseline model that relies on labeled data only (§ SECREF3 & § SECREF4 )."
            ],
            "highlighted_evidence": [
              "We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ). ",
              "Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2. "
            ]
          }
        ]
      },
      {
        "question": "How many convolutional layers does their model have?",
        "question_id": "12eaaf3b6ebc51846448c6e1ad210dbef7d25a96",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "wav2vec has 12 convolutional layers",
            "evidence": [
              "Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 . Alternatively, one could use other architectures such as the trainable frontend of BIBREF24 amongst others. The encoder layers have kernel sizes INLINEFORM2 and strides INLINEFORM3 . The output of the encoder is a low frequency feature representation INLINEFORM4 which encodes about 30ms of 16KHz of audio and the striding results in representation INLINEFORM5 every 10ms.",
              "Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. The total receptive field of the context network is about 180ms."
            ],
            "highlighted_evidence": [
              "Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 .",
              "Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one. "
            ]
          }
        ]
      },
      {
        "question": "Do they explore how much traning data is needed for which magnitude of improvement for WER? ",
        "question_id": "828615a874512844ede9d7f7d92bdc48bb48b18d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance."
            ],
            "highlighted_evidence": [
              "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/SchneiderBCA19",
    "dblp_title": "wav2vec: Unsupervised Pre-Training for Speech Recognition.",
    "year": "2019"
  },
  {
    "id": "1708.09157",
    "title": "Cross-lingual, Character-Level Neural Morphological Tagging",
    "qas": [
      {
        "question": "How are character representations from various languages joint?",
        "question_id": "a43c400ae37a8705ff2effb4828f4b0b177a74c4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "shared character embeddings for taggers in both languages together through optimization of a joint loss function"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our formulation of transfer learning builds on work in multi-task learning BIBREF15 , BIBREF9 . We treat each individual language as a task and train a joint model for all the tasks. We first discuss the current state of the art in morphological tagging: a character-level recurrent neural network. After that, we explore three augmentations to the architecture that allow for the transfer learning scenario. All of our proposals force the embedding of the characters for both the source and the target language to share the same vector space, but involve different mechanisms, by which the model may learn language-specific features.",
              "Cross-lingual morphological tagging may be formulated as a multi-task learning problem. We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one. The first loss function we consider is the following:"
            ],
            "highlighted_evidence": [
              "We treat each individual language as a task and train a joint model for all the tasks.",
              "We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one."
            ]
          }
        ]
      },
      {
        "question": "On which dataset is the experiment conducted?",
        "question_id": "4056ee2fd7a0a0f444275e627bb881134a1c2a10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . We list the size of the training, development and test splits of the UD treebanks we used in tab:lang-size. Also, we list the number of unique morphological tags in each language in tab:num-tags, which serves as an approximate measure of the morphological complexity each language exhibits. Crucially, the data are annotated in a cross-linguistically consistent manner, such that words in the different languages that have the same syntacto-semantic function have the same bundle of tags (see sec:morpho-tagging for a discussion). Potentially, further gains would be possible by using a more universal scheme, e.g., the UniMorph scheme."
            ],
            "highlighted_evidence": [
              "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/CotterellH17",
    "dblp_title": "Cross-lingual Character-Level Neural Morphological Tagging.",
    "year": "2017"
  },
  {
    "id": "1911.00069",
    "title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping",
    "qas": [
      {
        "question": "Do they train their own RE model?",
        "question_id": "f6496b8d09911cdf3a9b72aec0b0be6232a6dba1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "First we compare our neural network English RE models with the state-of-the-art RE models on the ACE05 English data. The ACE05 English data can be divided to 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.",
              "We learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.",
              "We apply model ensemble to further improve the accuracy of the Bi-LSTM model. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. This Ensemble approach improves the single model by 0.6-1.9 $F_1$ points, except for Arabic."
            ],
            "highlighted_evidence": [
              "We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.\n\nWe learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping.",
              "We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models."
            ]
          }
        ]
      },
      {
        "question": "How big are the datasets?",
        "question_id": "5c90e1ed208911dbcae7e760a553e912f8c237a5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents",
            "evidence": [
              "FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.",
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.",
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical)."
            ]
          }
        ]
      },
      {
        "question": "What languages do they experiment on?",
        "question_id": "3c3b4797e2b21e2c31cf117ad9e52f327721790f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English, German, Spanish, Italian, Japanese and Portuguese",
              " English, Arabic and Chinese"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical)."
            ],
            "highlighted_evidence": [
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. ",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical)."
            ]
          }
        ]
      },
      {
        "question": "What datasets are used?",
        "question_id": "a7d72f308444616a0befc8db7ad388b3216e2143",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "in-house dataset",
              "ACE05 dataset "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).",
              "In this section, we evaluate the performance of the proposed cross-lingual RE approach on both in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11."
            ],
            "highlighted_evidence": [
              "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).",
              "The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).",
              "the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/NiF19",
    "dblp_title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping.",
    "year": "2019"
  },
  {
    "id": "1910.04887",
    "title": "Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities",
    "qas": [
      {
        "question": "How better does auto-completion perform when using both language and vision than only language?",
        "question_id": "dfb0351e8fa62ceb51ce77b0f607885523d1b8e8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How big is data provided by this research?",
        "question_id": "a130aa735de3b65c71f27018f20d3c068bafb826",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "16k images and 740k corresponding region descriptions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For training, we aggregated (query, image) pairs using the region descriptions from the VG dataset and referring expressions from the ReferIt dataset. Our VG training set consists of 85% of the data: 16k images and 740k corresponding region descriptions. The Referit training data consists of 9k images and 54k referring expressions."
            ],
            "highlighted_evidence": [
              "Our VG training set consists of 85% of the data: 16k images and 740k corresponding region descriptions."
            ]
          }
        ]
      },
      {
        "question": "How they complete a user query prefix conditioned upon an image?",
        "question_id": "0c1663a7f7750b399f40ef7b4bf19d5c598890ff",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we replace user embeddings with a low-dimensional image representation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation. We extract features from an input image using a CNN pretrained on ImageNet, retraining only the last two fully connected layers. The image feature vector is fed into the FactorCell through the adaptation matrix. We perform beam search over the sequence of predicted characters to chose the optimal completion for the given prefix."
            ],
            "highlighted_evidence": [
              "To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-04887",
    "dblp_title": "Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities.",
    "year": "2019"
  },
  {
    "id": "1810.00663",
    "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
    "qas": [
      {
        "question": "Did the collection process use a WoZ method?",
        "question_id": "aa800b424db77e634e82680f804894bfa37f2a34",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.",
              "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
            ],
            "highlighted_evidence": [
              "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.",
              "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
            ]
          }
        ]
      },
      {
        "question": "By how much did their model outperform the baseline?",
        "question_id": "fbd47705262bfa0a2ba1440a2589152def64cbbd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively",
              "over INLINEFORM0 increase in EM and GM between our model and the next best two models"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.",
              "Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models."
            ],
            "highlighted_evidence": [
              "First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.",
              "Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models."
            ]
          }
        ]
      },
      {
        "question": "What baselines did they compare their model with?",
        "question_id": "51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search",
            "evidence": [
              "The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path."
            ],
            "highlighted_evidence": [
              "The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path."
            ]
          }
        ]
      },
      {
        "question": "What was the performance of their model?",
        "question_id": "3aee5c856e0ee608a7664289ffdd11455d153234",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",
            "evidence": [
              "FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better."
            ]
          }
        ]
      },
      {
        "question": "What evaluation metrics are used?",
        "question_id": "f42d470384ca63a8e106c7caf1cb59c7b92dbc27",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "exact match, f1 score, edit distance and goal match",
            "evidence": [
              "We compare the performance of translation approaches based on four metrics:",
              "[align=left,leftmargin=0em,labelsep=0.4em,font=]",
              "As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.",
              "The harmonic average of the precision and recall over all the test set BIBREF26 .",
              "The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .",
              "GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0."
            ],
            "highlighted_evidence": [
              "We compare the performance of translation approaches based on four metrics:\n\n[align=left,leftmargin=0em,labelsep=0.4em,font=]\n\nAs in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.\n\nThe harmonic average of the precision and recall over all the test set BIBREF26 .\n\nThe minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .\n\nGM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0."
            ]
          }
        ]
      },
      {
        "question": "Did the authors use a crowdsourcing platform?",
        "question_id": "29bdd1fb20d013b23b3962a065de3a564b14f0fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans."
            ],
            "highlighted_evidence": [
              "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. "
            ]
          }
        ]
      },
      {
        "question": "How were the navigation instructions collected?",
        "question_id": "25b2ae2d86b74ea69b09c140a41593c00c47a82b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "using Amazon Mechanical Turk using simulated environments with topological maps",
            "evidence": [
              "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.",
              "We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.",
              "As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:",
              "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
            ],
            "highlighted_evidence": [
              "This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. ",
              "We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.\n\nAs shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:\n\nWhile the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.\n\n"
            ]
          }
        ]
      },
      {
        "question": "What language is the experiment done in?",
        "question_id": "fd7f13b63f6ba674f5d5447b6114a201fe3137cb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "english language",
            "evidence": [
              "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
            ],
            "highlighted_evidence": [
              "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/ZangPVCNSS18",
    "dblp_title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.",
    "year": "2018"
  },
  {
    "id": "1809.05752",
    "title": "Analysis of Risk Factor Domains in Psychosis Patient Health Records",
    "qas": [
      {
        "question": "What additional features are proposed for future work?",
        "question_id": "c82e945b43b2e61c8ea567727e239662309e9508",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort",
            "evidence": [
              "Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time."
            ],
            "highlighted_evidence": [
              "Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time."
            ]
          }
        ]
      },
      {
        "question": "What are their initial results on this task?",
        "question_id": "fbee81a9d90ff23603ee4f5986f9e8c0eb035b52",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",
            "evidence": [
              "FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs."
            ]
          }
        ]
      },
      {
        "question": "What datasets did the authors use?",
        "question_id": "39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital",
              "an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data.",
              "These patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort.",
              "We also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction."
            ],
            "highlighted_evidence": [
              "Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data.\n\nThese patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort.\n\nWe also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/biomedsem/HoldernessMCBMP19",
    "dblp_title": "Analysis of risk factor domains in psychosis patient health records.",
    "year": "2019"
  },
  {
    "id": "2001.01589",
    "title": "Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation",
    "qas": [
      {
        "question": "How many linguistic and semantic features are learned?",
        "question_id": "1f6180bba0bc657c773bd3e4269f87540a520ead",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is morphology knowledge implemented in the method?",
        "question_id": "57388bf2693d71eb966d42fa58ab66d7f595e55f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A BPE model is applied to the stem after morpheme segmentation.",
            "evidence": [
              "The problem with morpheme segmentation is that the vocabulary of stem units is still very large, which leads to many rare and unknown words at the training time. The problem with BPE is that it do not consider the morpheme boundaries inside words, which might cause a loss of morphological properties and semantic information. Hence, on the analyses of the above popular word segmentation methods, we propose the morphologically motivated segmentation strategy that combines the morpheme segmentation and BPE for further improving the translation performance of NMT.",
              "Compared with the sentence of word surface forms, the corresponding sentence of stem units only contains the structure information without considering morphological information, which can make better generalization over inflectional variants of the same word and reduce data sparseness BIBREF8. Therefore, we learn a BPE model on the stem units in the training corpus rather than the words, and then apply it on the stem unit of each word after morpheme segmentation."
            ],
            "highlighted_evidence": [
              "The problem with morpheme segmentation is that the vocabulary of stem units is still very large, which leads to many rare and unknown words at the training time. ",
              "Therefore, we learn a BPE model on the stem units in the training corpus rather than the words, and then apply it on the stem unit of each word after morpheme segmentation."
            ]
          }
        ]
      },
      {
        "question": "How does the word segmentation method work?",
        "question_id": "47796c7f0a7de76ccb97ccbd43dc851bb8a613d5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5",
              "Zemberek",
              "BIBREF12"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We will elaborate two popular word segmentation methods and our newly proposed segmentation strategies in this section. The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively. The sentence examples with different segmentation strategies for Turkish-English machine translation task are shown in Table 1.",
              "We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units. We employ the python toolkits of jieba for Chinese word segmentation. We apply BPE on the target-side words and we set the number of merge operations to 35K for Chinese and 30K for English and we set the maximum sentence length to 150 tokens. The training corpus statistics of Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 2 and Table 3 respectively."
            ],
            "highlighted_evidence": [
              "The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively. ",
              "We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units. "
            ]
          }
        ]
      },
      {
        "question": "Is the word segmentation method independently evaluated?",
        "question_id": "9d5153a7553b7113716420a6ddceb59f877eb617",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2001-01589",
    "dblp_title": "Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation.",
    "year": "2020"
  },
  {
    "id": "1910.10324",
    "title": "Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks",
    "qas": [
      {
        "question": "Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?",
        "question_id": "55c840a2f1f663ab2bff984ae71501b17429d0c0",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many layers do they use in their best performing network?",
        "question_id": "fa5357c56ea80a21a7ca88a80f21711c5431042c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "36"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF20 shows results for Librispeech with SpecAugment. We test both CTC and CE/hybrid systems. There are consistent gains first from iterated loss, and then from multiple feature presentation. We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results. This shows that our proposed methods can improve even very deep models."
            ],
            "highlighted_evidence": [
              "We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results. "
            ]
          }
        ]
      },
      {
        "question": "Do they just sum up all the loses the calculate to end up with one single loss?",
        "question_id": "35915166ab2fd3d39c0297c427d4ac00e8083066",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We have found it beneficial to apply the loss function at several intermediate layers of the network. Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as",
              "where $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. The coefficient $\\lambda $ scales the auxiliary loss and we set $\\lambda = 0.3$ based on our preliminary experiments. We illustrate the auxiliary prediction and loss in Figure FIGREF6."
            ],
            "highlighted_evidence": [
              "Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as\n\nwhere $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. T"
            ]
          }
        ]
      },
      {
        "question": "Does their model take more time to train than regular transformer models?",
        "question_id": "e6c872fea474ea96ca2553f7e9d5875df4ef55cd",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/icassp/TjandraLZZWS0Z20",
    "dblp_title": "DEJA-VU: Double Feature Presentation and Iterated Loss in Deep Transformer Networks.",
    "year": "2020"
  },
  {
    "id": "1910.05456",
    "title": "Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge",
    "qas": [
      {
        "question": "Are agglutinative languages used in the prediction of both prefixing and suffixing languages?",
        "question_id": "fc29bb14f251f18862c100e0d3cd1396e8f2c3a1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue).",
              "Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing.",
              "Analyzing next the errors concerning affixes, we find that models pretrained on HUN, ITA, DEU, and FRA (in that order) commit the fewest errors. This supports two of our previous hypotheses: First, given that ITA and FRA are both from the same language family as SPA, relatedness seems to be benficial for learning of the second language. Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well.",
              "In Table TABREF39, the errors for Zulu are shown, and Table TABREF19 reveals the relative performance for different source languages: TUR $>$ HUN $>$ DEU $>$ ITA $>$ FRA $>$ NAV $>$ EUS $>$ QVH. Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language."
            ],
            "highlighted_evidence": [
              "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue).",
              "We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing.",
              "Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well.",
              "Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language."
            ]
          }
        ]
      },
      {
        "question": "What is an example of a prefixing language?",
        "question_id": "f3e96c5487d87557a661a65395b0162033dc05b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Zulu"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Since English and Spanish are both Indo-European languages, and, thus, relatively similar, we further add a third, unrelated target language. We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing."
            ],
            "highlighted_evidence": [
              "We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing."
            ]
          }
        ]
      },
      {
        "question": "How is the performance on the task evaluated?",
        "question_id": "74db8301d42c7e7936eb09b2171cd857744c52eb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors",
            "evidence": [
              "Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection – the task of learning a mapping from lemmata to their inflected forms – in the last years BIBREF6. Thus, in this work, we experiment on such models, asking not what they learn, but, motivated by the respective research on human subjects, the related question of how what they learn depends on their prior knowledge. We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the \"native language\", in neural network models.",
              "For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations. We manually annotate the outputs for the first 75 development examples for each source–target language combination. All found errors are categorized as belonging to one of the following categories."
            ],
            "highlighted_evidence": [
              "We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages.",
              "For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison.",
              "We manually annotate the outputs for the first 75 development examples for each source–target language combination. All found errors are categorized as belonging to one of the following categories."
            ]
          }
        ]
      },
      {
        "question": "What are the tree target languages studied in the paper?",
        "question_id": "587885bc86543b8f8b134c20e2c62f6251195571",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English, Spanish and Zulu"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To this goal, we select a diverse set of eight source languages from different language families – Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua – and three target languages – English, Spanish and Zulu. We pretrain a neural sequence-to-sequence architecture on each of the source languages and then fine-tune the resulting models on small datasets in each of the target languages. Analyzing the errors made by the systems, we find that (i) source and target language being closely related simplifies the successful learning of inflection in the target language, (ii) the task is harder to learn in a prefixing language if the source language is suffixing – as well as the other way around, and (iii) a source language which exhibits an agglutinative morphology simplifies learning of a second language's inflectional morphology."
            ],
            "highlighted_evidence": [
              "To this goal, we select a diverse set of eight source languages from different language families – Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua – and three target languages – English, Spanish and Zulu. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-05456",
    "dblp_title": "Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge.",
    "year": "2019"
  },
  {
    "id": "1910.05154",
    "title": "How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages",
    "qas": [
      {
        "question": "Is the model evaluated against any baseline?",
        "question_id": "b72264a73eea36c828e7de778a8b4599a5d02b39",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
            ],
            "highlighted_evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
            ]
          }
        ]
      },
      {
        "question": "Does the paper report the accuracy of the model?",
        "question_id": "24cc1586e5411a7f8574796d3c576b7c677d6e21",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9."
            ],
            "highlighted_evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. "
            ]
          }
        ]
      },
      {
        "question": "How is the performance of the model evaluated?",
        "question_id": "db291d734524fa51fb314628b64ebe1bac7f7e1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8.",
              "For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). ",
              "Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9.",
              "For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). We observe that the performance improvement is smaller than the one observed in previous work BIBREF10, which we attribute to the fact that our dataset was artificially augmented. This could result in the available multilingual form of supervision not being as rich as in a manually generated dataset. Finally, the best boundary segmentation result is obtained by performing multilingual voting with all the languages and an agreement of 50%, which indicates that the information learned by different languages will provide additional complementary evidence.",
              "Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá). However, the models still differ for most of their alignments in the table. This hints that while a portion of the lexicon might be captured independently of the language used, other structures might be more dependent of the chosen language. On this note, BIBREF11 suggests the notion of word cannot always be meaningfully defined cross-linguistically."
            ],
            "highlighted_evidence": [
              "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. ",
              "For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). We observe that the performance improvement is smaller than the one observed in previous work BIBREF10, which we attribute to the fact that our dataset was artificially augmented.",
              "Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá)."
            ]
          }
        ]
      },
      {
        "question": "What are the different bilingual models employed?",
        "question_id": "85abd60094c92eb16f39f861c6de8c2064807d02",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the bilingual neural-based Unsupervised Word Segmentation (UWS) approach from BIBREF6 to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence). Due to the attention mechanism present in these networks BIBREF7, posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences. These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs.",
              "In this work we apply two simple methods for including multilingual information into the bilingual models from BIBREF6. The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold $T$ over the output boundaries. This threshold balances between accepting all boundaries from all the bilingual models (zero agreement) and accepting only input boundaries discovered by all these models (total agreement). The second method is ANE Selection. For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) BIBREF8 computed over these matrices for selecting the most confident one for segmenting each phoneme sequence. This exploits the idea that models trained on different language pairs will have language-related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence.",
              "Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá). However, the models still differ for most of their alignments in the table. This hints that while a portion of the lexicon might be captured independently of the language used, other structures might be more dependent of the chosen language. On this note, BIBREF11 suggests the notion of word cannot always be meaningfully defined cross-linguistically.",
              "FLOAT SELECTED: Table 3: Top 10 confident (discovered type, translation) pairs for the five bilingual models. The “+” mark means the discovered type is a concatenation of two existing true types."
            ],
            "highlighted_evidence": [
              "In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",
              "The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold $T$ over the output boundaries. ",
              " Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá). However, the models still differ for most of their alignments in the table. ",
              "FLOAT SELECTED: Table 3: Top 10 confident (discovered type, translation) pairs for the five bilingual models. The “+” mark means the discovered type is a concatenation of two existing true types."
            ]
          }
        ]
      },
      {
        "question": "How does the well-resourced language impact the quality of the output?",
        "question_id": "50f09a044f0c0795cc636c3e25a4f7c3231fb08d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved. This might be due to the different language-dependent structures that are captured by using more than one language. Lastly, we extend the bilingual Mboshi-French parallel corpus, creating a multilingual corpus for the endangered language Mboshi that we make available to the community."
            ],
            "highlighted_evidence": [
              "In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved. This might be due to the different language-dependent structures that are captured by using more than one language. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-05154",
    "dblp_title": "How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages.",
    "year": "2019"
  },
  {
    "id": "1806.00722",
    "title": "Dense Information Flow for Neural Machine Translation",
    "qas": [
      {
        "question": "what are the baselines?",
        "question_id": "26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default. As a comparison, we design a densely connected model with same number of layers, but the hidden size is set as 128 in order to keep the model size consistent. The models adopting DenseAtt-1, DenseAtt-2 are named as DenseNMT-4L-1 and DenseNMT-4L-2 respectively. In order to check the effect of dense connections on deeper models, we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96."
            ],
            "highlighted_evidence": [
              "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default."
            ]
          }
        ]
      },
      {
        "question": "did they outperform previous methods?",
        "question_id": "8c0621016e96d86a7063cb0c9ec20c76a2dba678",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well."
            ],
            "highlighted_evidence": [
              " In almost all genres, DenseNMT models are significantly better than the baselines."
            ]
          }
        ]
      },
      {
        "question": "what language pairs are explored?",
        "question_id": "f1214a05cc0e6d870c789aed24a8d4c768e1db2f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "German-English",
              "Turkish-English",
              "English-German"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
            ],
            "highlighted_evidence": [
              "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
            ]
          }
        ]
      },
      {
        "question": "what datasets were used?",
        "question_id": "41d3ab045ef8e52e4bbe5418096551a22c5e9c43",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",
            "evidence": [
              "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
            ],
            "highlighted_evidence": [
              "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/ShenTHQL18",
    "dblp_title": "Dense Information Flow for Neural Machine Translation.",
    "year": "2018"
  },
  {
    "id": "2003.03612",
    "title": "Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text",
    "qas": [
      {
        "question": "How is order of binomials tracked across time?",
        "question_id": "62736ad71c76a20aee8e003c462869bab4ab4b1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "draw our data from news publications, wine reviews, and Reddit",
              "develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time",
              " develop a null model to determine how much variation in binomial orderings we might expect across communities and across time"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The present work: Binomials in large-scale online text. In this work, we use data from large-scale Internet text corpora to study binomials at a massive scale, drawing on text created by millions of users. Our approach is more wholesale than prior work - we focus on all binomials of sufficient frequency, without first restricting to small samples of binomials that might be frozen. We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. Furthermore, the subject matter on Reddit leads to many lists about people and organizations that lets us study orderings of proper names — a key setting for word ordering which has been difficult to study by other means.",
              "We also address temporal and community structure in collections of binomials. While it has been recognized that the orderings of binomials may change over time or between communities BIBREF5, BIBREF10, BIBREF1, BIBREF13, BIBREF14, BIBREF15, there has been little analysis of this change. We develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time. Using subreddits as communities, these metrics reveal variations in orderings, some of which suggest cultural change influencing language. For example, in one community, we find that over a period of 10 years, the binomial `son and daughter' went from nearly frozen to appearing in that order only 64% of the time.",
              "While these changes do happen, they are generally quite rare. Most binomials — frozen or not — are ordered in one way about the same percentage of the time, regardless of community or the year. We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values. We find that there is less variation across time and communities in the data compared to this model, implying that binomial orderings are indeed remarkably stable."
            ],
            "highlighted_evidence": [
              " We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. ",
              "We develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time. Using subreddits as communities, these metrics reveal variations in orderings, some of which suggest cultural change influencing language.",
              "We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values. "
            ]
          }
        ]
      },
      {
        "question": "What types of various community texts have been investigated for exploring global structure of binomials?",
        "question_id": "aaf50a6a9f449389ef212d25d0fae59c10b0df92",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "news publications, wine reviews, and Reddit"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The present work: Binomials in large-scale online text. In this work, we use data from large-scale Internet text corpora to study binomials at a massive scale, drawing on text created by millions of users. Our approach is more wholesale than prior work - we focus on all binomials of sufficient frequency, without first restricting to small samples of binomials that might be frozen. We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. Furthermore, the subject matter on Reddit leads to many lists about people and organizations that lets us study orderings of proper names — a key setting for word ordering which has been difficult to study by other means."
            ],
            "highlighted_evidence": [
              "We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time"
            ]
          }
        ]
      },
      {
        "question": "Are there any new finding in analasys of trinomials that was not present binomials?",
        "question_id": "a1917232441890a89b9a268ad8f987184fa50f7a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Trinomials are likely to appear in exactly one order"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Finally, we expand our work to include multinomials, which are lists of more than two words. There already appears to be more structure in trinomials (lists of three) compared to binomials. Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances. For instance, in one section of our Reddit data, `Fraud, Waste, and Abuse' appears 34 times, and `Waste, Fraud, and Abuse' appears 20 times. This could point to, for example, recency principles being more important in lists of three than in lists of two. While multinomials were in principle part of the scope of past research in this area, they were difficult to study in smaller corpora, suggesting another benefit of working at our current scale."
            ],
            "highlighted_evidence": [
              "Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances. "
            ]
          }
        ]
      },
      {
        "question": "What new model is proposed for binomial lists?",
        "question_id": "574f17134e4dd041c357ebb75a7ef590da294d22",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "null model "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While these changes do happen, they are generally quite rare. Most binomials — frozen or not — are ordered in one way about the same percentage of the time, regardless of community or the year. We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values. We find that there is less variation across time and communities in the data compared to this model, implying that binomial orderings are indeed remarkably stable."
            ],
            "highlighted_evidence": [
              "We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values. "
            ]
          }
        ]
      },
      {
        "question": "How was performance of previously proposed rules at very large scale?",
        "question_id": "41fd359b8c1402b31b6f5efd660143d1414783a0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " close to random,"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Basic Features. We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials."
            ],
            "highlighted_evidence": [
              "We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials."
            ]
          }
        ]
      },
      {
        "question": "What previously proposed rules for predicting binoial ordering are used?",
        "question_id": "d216d715ec27ee2d4949f9e908895a18fb3238e2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "word length, number of phonemes, number of syllables, alphabetical order, and frequency"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Basic Features. We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials."
            ],
            "highlighted_evidence": [
              "We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. "
            ]
          }
        ]
      },
      {
        "question": "What online text resources are used to test binomial lists?",
        "question_id": "ba973b13f26cd5eb1da54663c0a72842681d5bf5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "news publications, wine reviews, and Reddit"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The present work: Binomials in large-scale online text. In this work, we use data from large-scale Internet text corpora to study binomials at a massive scale, drawing on text created by millions of users. Our approach is more wholesale than prior work - we focus on all binomials of sufficient frequency, without first restricting to small samples of binomials that might be frozen. We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. Furthermore, the subject matter on Reddit leads to many lists about people and organizations that lets us study orderings of proper names — a key setting for word ordering which has been difficult to study by other means."
            ],
            "highlighted_evidence": [
              " We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/www/KoeveringBK20",
    "dblp_title": "Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text.",
    "year": "2020"
  },
  {
    "id": "1904.08386",
    "title": "Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism",
    "qas": [
      {
        "question": "How do they model a city description using embeddings?",
        "question_id": "508580af51483b5fb0df2630e8ea726ff08d537b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While each of the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm."
            ],
            "highlighted_evidence": [
              "While each of the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm."
            ]
          }
        ]
      },
      {
        "question": "How do they obtain human judgements?",
        "question_id": "89d1687270654979c53d0d0e6a845cdc89414c67",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Using crowdsourcing ",
            "evidence": [
              "As the book is too small to train any models, we leverage recent advances in large-scale language model-based representations BIBREF5 , BIBREF6 to compute a representation of each city. We feed these representations into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects."
            ],
            "highlighted_evidence": [
              "We feed these representations into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects."
            ]
          }
        ]
      },
      {
        "question": "Which clustering method do they use to cluster city description embeddings?",
        "question_id": "fc6cfac99636adda28654e1e19931c7394d76c7c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Given 55 city representations, how do we group them into eleven clusters of five cities each? Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity BIBREF20 , but we found no simple way to constrain this method to produce a specific number of equally-sized clusters. The brute force approach of enumerating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4"
            ],
            "highlighted_evidence": [
              "Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity BIBREF20 , but we found no simple way to constrain this method to produce a specific number of equally-sized clusters. The brute force approach of enumerating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/WangI19",
    "dblp_title": "Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism.",
    "year": "2019"
  },
  {
    "id": "1909.00754",
    "title": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation",
    "qas": [
      {
        "question": "Does this approach perform better in the multi-domain or single-domain setting?",
        "question_id": "ed7a3e7fc1672f85a768613e7d1b419475950ab4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "single-domain setting",
            "evidence": [
              "FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018)."
            ]
          }
        ]
      },
      {
        "question": "What are the performance metrics used?",
        "question_id": "72ceeb58e783e3981055c70a3483ea706511fac3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "joint goal accuracy"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement."
            ],
            "highlighted_evidence": [
              "As a convention, the metric of joint goal accuracy is used to compare our model to previous work."
            ]
          }
        ]
      },
      {
        "question": "Which datasets are used to evaluate performance?",
        "question_id": "9bfa46ad55136f2a365e090ce585fc012495393c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the single domain dataset, WoZ2.0 ",
              "the multi-domain dataset, MultiWoZ"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. The statistics of these two datsets are shown in Table 2 ."
            ],
            "highlighted_evidence": [
              "We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/RenNM19",
    "dblp_title": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation.",
    "year": "2019"
  },
  {
    "id": "1906.00180",
    "title": "Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization",
    "qas": [
      {
        "question": "How does the automatic theorem prover infer the relation?",
        "question_id": "42812113ec720b560eb9463ff5e74df8764d1bff",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "If these model can learn the first-order logic on artificial language, why can't it lear for natural language?",
        "question_id": "4f4892f753b1d9c5e5e74c7c94d8c9b6ef523e7b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many samples did they generate for the artificial language?",
        "question_id": "f258ada8577bb71873581820a94695f4a2c223b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "70,000",
            "evidence": [
              "In a new experiment, we train the models on pairs of sentences with length 5, 7 or 8, and test on pairs of sentences with lengths 6 or 9. As before, the training and test sets contain some 30,000 and 5,000 sentence pairs, respectively. Results are shown in Table UID19 ."
            ],
            "highlighted_evidence": [
              "As before, the training and test sets contain some 30,000 and 5,000 sentence pairs, respectively"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1906-00180",
    "dblp_title": "Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization.",
    "year": "2019"
  },
  {
    "id": "1806.02847",
    "title": "A Simple Method for Commonsense Reasoning",
    "qas": [
      {
        "question": "Which of their training domains improves performance the most?",
        "question_id": "05bb75a1e1202850efa9191d6901de0a34744af0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "documents from the CommonCrawl dataset that has the most overlapping n-grams with the question"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$",
              "The top 0.1% of highest ranked documents is chosen as our new training corpus. Details of the ranking is shown in Figure 2 . This procedure resulted in nearly 1,000,000 documents, with the highest ranking document having a score of $8\\times 10^{-2}$ , still relatively small to a perfect score of $1.0$ . We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events.",
              "Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. We next rank the text corpora based on ensemble performance for more reliable results. Namely, we compare the previous ensemble of 10 models against the same set of models trained on each single text corpus. This time, the original ensemble trained on a diverse set of text corpora outperforms all other single-corpus ensembles including STORIES. This highlights the important role of diversity in training data for commonsense reasoning accuracy of the final system."
            ],
            "highlighted_evidence": [
              "In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions.",
              "We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events.",
              "Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing."
            ]
          }
        ]
      },
      {
        "question": "Do they fine-tune their model on the end task?",
        "question_id": "770aeff30846cd3d0d5963f527691f3685e8af02",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$"
            ],
            "highlighted_evidence": [
              "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1810-00521",
    "dblp_title": "A Simple Machine Learning Method for Commonsense Reasoning? A Short Commentary on Trinh &amp; Le (2018).",
    "year": "2018"
  },
  {
    "id": "1906.04571",
    "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    "qas": [
      {
        "question": "Why does not the approach from English work on other languages?",
        "question_id": "f7817b949605fb04b1e4fec9dd9ca8804fb92ae9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Because, unlike other languages, English does not mark grammatical genders",
            "evidence": [
              "To date, the NLP community has focused primarily on approaches for detecting and mitigating gender stereotypes in English BIBREF5 , BIBREF6 , BIBREF7 . Yet, gender stereotypes also exist in other languages because they are a function of society, not of grammar. Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . In these languages, the words in a sentence are marked with morphological endings that reflect the grammatical gender of the surrounding nouns. This means that if the gender of one word changes, the others have to be updated to match. As a result, simple heuristics, such as augmenting a corpus with additional sentences in which he and she have been swapped BIBREF9 , will yield ungrammatical sentences. Consider the Spanish phrase el ingeniero experto (the skilled engineer). Replacing ingeniero with ingeniera is insufficient—el must also be replaced with la and experto with experta."
            ],
            "highlighted_evidence": [
              "Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 ."
            ]
          }
        ]
      },
      {
        "question": "How do they measure grammaticality?",
        "question_id": "8255f74cae1352e5acb2144fb857758dda69be02",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "by calculating log ratio of grammatical phrase over ungrammatical phrase",
            "evidence": [
              "Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase):"
            ]
          }
        ]
      },
      {
        "question": "Which model do they use to convert between masculine-inflected and feminine-inflected sentences?",
        "question_id": "db62d5d83ec187063b57425affe73fef8733dd28",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Markov random field with an optional neural parameterization"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns. To the best of our knowledge, this task has not been studied previously. As a result, there is no existing annotated corpus of paired sentences that can be used as “ground truth.” Despite this limitation, we evaluated our approach both intrinsically and extrinsically, achieving promising results. For example, we demonstrated that our approach reduces gender stereotyping in neural language models. Finally, we also identified avenues for future work, such as the inclusion of co-reference information."
            ],
            "highlighted_evidence": [
              "We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/ZmigrodMWC19",
    "dblp_title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology.",
    "year": "2019"
  },
  {
    "id": "1909.04625",
    "title": "Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study",
    "qas": [
      {
        "question": "What is the performance achieved by the model described in the paper?",
        "question_id": "946676f1a836ea2d6fe98cb4cfc26b9f4f81984d",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the best performance achieved by supervised models?",
        "question_id": "3b090b416c4ad7d9b5b05df10c5e7770a4590f6a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the size of the datasets employed?",
        "question_id": "a1e07c7563ad038ee2a7de5093ea08efdd6077d4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "(about 4 million sentences, 138 million word tokens)",
              "one trained on the Billion Word benchmark"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.",
              "We also compare LSTM language models trained on large corpora. We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15. We set the size of the input embeddings and hidden layers to 400 for the LSTM (frWaC) model since it is trained on a large dataset."
            ],
            "highlighted_evidence": [
              "The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13.",
              "We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15."
            ]
          }
        ]
      },
      {
        "question": "What are the baseline models?",
        "question_id": "a1c4f9e8661d4d488b8684f055e0ee0e2275f767",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Recurrent Neural Network (RNN)",
              "ActionLSTM",
              "Generative Recurrent Neural Network Grammars (RNNG)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models",
              "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.",
              "Methods ::: Models Tested ::: ActionLSTM",
              "models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.",
              "Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)",
              "jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18."
            ],
            "highlighted_evidence": [
              " Recurrent Neural Network (RNN) Language Models\nare trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10.",
              "ActionLSTM\nmodels the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16.",
              "Generative Recurrent Neural Network Grammars (RNNG)\njointly model the word sequence as well as the underlying syntactic structure BIBREF18."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/AnQWL19",
    "dblp_title": "Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study.",
    "year": "2019"
  },
  {
    "id": "1809.07629",
    "title": "Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation",
    "qas": [
      {
        "question": "What evaluation metrics are used?",
        "question_id": "c5171daf82107fce0f285fa18f19e91fbd1215c5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the evaluation metrics include BLEU and ROUGE (1, 2, L) scores"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The probability of activating inter-layer and inner-layer teacher forcing is set to 0.5, the probability of teacher forcing is attenuated every epoch, and the decaying ratio is 0.9. The models are trained for 20 training epochs without early stop; when curriculum learning is applied, only the first layer is trained during first five epochs, the second decoder layer starts to be trained at the sixth epoch, and so on. To evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 ."
            ],
            "highlighted_evidence": [
              "o evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 ."
            ]
          }
        ]
      },
      {
        "question": "What datasets did they use?",
        "question_id": "baeb6785077931e842079e9d0c9c9040947ffa4e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The E2E NLG challenge dataset BIBREF21"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set. As shown in Figure FIGREF2 , the inputs are semantic frames containing specific slots and corresponding values, and the outputs are the associated natural language utterances with the given semantics. For example, a semantic frame with the slot-value pairs “name[Bibimbap House], food[English], priceRange[moderate], area [riverside], near [Clare Hall]” corresponds to the target sentence “Bibimbap House is a moderately priced restaurant who's main cuisine is English food. You will find this local gem near Clare Hall in the Riverside area.”."
            ],
            "highlighted_evidence": [
              "The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/slt/SuC18",
    "dblp_title": "Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation.",
    "year": "2018"
  },
  {
    "id": "1807.05154",
    "title": "Deep Enhanced Representation for Implicit Discourse Relation Recognition",
    "qas": [
      {
        "question": "Why does their model do better than prior models?",
        "question_id": "bb570d4a1b814f508a07e74baac735bf6ca0f040",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "better sentence pair representations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations."
            ],
            "highlighted_evidence": [
              "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/coling/BaiZ18",
    "dblp_title": "Deep Enhanced Representation for Implicit Discourse Relation Recognition.",
    "year": "2018"
  },
  {
    "id": "2002.11402",
    "title": "Detecting Potential Topics In News Using BERT, CRF and Wikipedia",
    "qas": [
      {
        "question": "What is the difference in recall score between the systems?",
        "question_id": "1771a55236823ed44d3ee537de2e85465bf03eaf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",
            "evidence": [
              "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
              "FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
              "FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference"
            ]
          }
        ]
      },
      {
        "question": "What is their f1 score and recall?",
        "question_id": "1d74fd1d38a5532d20ffae4abbadaeda225b6932",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",
            "evidence": [
              "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
              "FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
              "FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference"
            ]
          }
        ]
      },
      {
        "question": "How many layers does their system have?",
        "question_id": "da8bda963f179f5517a864943dc0ee71249ee1ce",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "4 layers",
            "evidence": [
              "FLOAT SELECTED: Figure 1. BERT + Bi-GRU + CRF, Final Architecture Chosen For Topic Detection Task."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Figure 1. BERT + Bi-GRU + CRF, Final Architecture Chosen For Topic Detection Task."
            ]
          }
        ]
      },
      {
        "question": "Which news corpus is used?",
        "question_id": "5c059a13d59947f30877bed7d0180cca20a83284",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "We have a dump of 15 million English news articles published in past 4 years."
            ]
          }
        ]
      },
      {
        "question": "How large is the dataset they used?",
        "question_id": "a1885f807753cff7a59f69b5cf6d0fdef8484057",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English wikipedia dataset has more than 18 million",
              "a dump of 15 million English news articles "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily. To remove such titles, we deployed simple rules as follows -",
              "After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles. Then selected minimum articles required to cover all possible 2-grams to 5-grams. This step is done to save some training time without loosing accuracy. Do note that, in future we are planning to use whole dataset and hope to see gains in F1 and Recall further. But as per manual inspection, our dataset contains enough variations of sentences with rich vocabulary which contains names of celebrities, politicians, local authorities, national/local organisations and almost all locations, India and International, mentioned in the news text, in last 4 years."
            ],
            "highlighted_evidence": [
              "We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily. ",
              "After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2002-11402",
    "dblp_title": "Detecting Potential Topics In News Using BERT, CRF and Wikipedia.",
    "year": "2020"
  },
  {
    "id": "1804.09301",
    "title": "Gender Bias in Coreference Resolution",
    "qas": [
      {
        "question": "Which coreference resolution systems are tested?",
        "question_id": "c2553166463b7b5ae4d9786f0446eb06a90af458",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.",
              "We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
            ],
            "highlighted_evidence": [
              "In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.",
              "We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/cogsci/LiorS23",
    "dblp_title": "Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender Bias Evaluation in Coreference Resolution.",
    "year": "2023"
  },
  {
    "id": "2002.00652",
    "title": "How Far are We from Effective Context Modeling ? An Exploratory Study on Semantic Parsing in Context",
    "qas": [
      {
        "question": "How big is improvement in performances of proposed model over state of the art?",
        "question_id": "cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We consider three models as our baselines. SyntaxSQL-con and CD-Seq2Seq are two strong baselines introduced in the SParC dataset paper BIBREF2. SyntaxSQL-con employs a BiLSTM model to encode dialogue history upon the SyntaxSQLNet model (analogous to our Turn) BIBREF23, while CD-Seq2Seq is adapted from BIBREF4 for cross-domain settings (analogous to our Turn+Tree Copy). EditSQL BIBREF5 is a STOA baseline which mainly makes use of SQL attention and token-level copy (analogous to our Turn+SQL Attn+Action Copy).",
              "Taking Concat as a representative, we compare the performance of our model with other models, as shown in Table TABREF34. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.",
              "FLOAT SELECTED: Table 1: We report the best performance observed in 5 runs on the development sets of both SPARC and COSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the results show the improvements of our model are significant with p < 0.005."
            ],
            "highlighted_evidence": [
              "EditSQL BIBREF5 is a STOA baseline which mainly makes use of SQL attention and token-level copy (analogous to our Turn+SQL Attn+Action Copy).",
              "Taking Concat as a representative, we compare the performance of our model with other models, as shown in Table TABREF34. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.",
              "FLOAT SELECTED: Table 1: We report the best performance observed in 5 runs on the development sets of both SPARC and COSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the results show the improvements of our model are significant with p < 0.005."
            ]
          }
        ]
      },
      {
        "question": "What two large datasets are used for evaluation?",
        "question_id": "69e678666d11731c9bfa99953e2cd5a5d11a4d4f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SParC BIBREF2 and CoSQL BIBREF6"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we try to fulfill the above insufficiency via an exploratory study on real-world semantic parsing in context. Concretely, we present a grammar-based decoding semantic parser and adapt typical context modeling methods on top of it. Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods. Our best model achieves state-of-the-art (SOTA) performances on both datasets with significant improvements. Furthermore, we summarize and generalize the most frequent contextual phenomena, with a fine-grained analysis on representative models. Through the analysis, we obtain some interesting findings, which may benefit the community on the potential research directions. We will open-source our code and materials to facilitate future work upon acceptance."
            ],
            "highlighted_evidence": [
              "Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods. "
            ]
          }
        ]
      },
      {
        "question": "What context modelling methods are evaluated?",
        "question_id": "471d624498ab48549ce492ada9e6129da05debac",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Concat\nTurn\nGate\nAction Copy\nTree Copy\nSQL Attn\nConcat + Action Copy\nConcat + Tree Copy\nConcat + SQL Attn\nTurn + Action Copy\nTurn + Tree Copy\nTurn + SQL Attn\nTurn + SQL Attn + Action Copy",
            "evidence": [
              "To conduct a thorough comparison, we evaluate 13 different context modeling methods upon the same parser, including 6 methods introduced in Section SECREF2 and 7 selective combinations of them (e.g., Concat+Action Copy). The experimental results are presented in Figure FIGREF37. Taken as a whole, it is very surprising to observe that none of these methods can be consistently superior to the others. The experimental results on BERT-based models show the same trend. Diving deep into the methods only using recent questions as context, we observe that Concat and Turn perform competitively, outperforming Gate by a large margin. With respect to the methods only using precedent SQL as context, Action Copy significantly surpasses Tree Copy and SQL Attn in all metrics. In addition, we observe that there is little difference in the performance of Action Copy and Concat, which implies that using precedent SQL as context gives almost the same effect with using recent questions. In terms of the combinations of different context modeling methods, they do not significantly improve the performance as we expected.",
              "FLOAT SELECTED: Figure 5: Question Match, Interaction Match and Turn i Match on SPARC and COSQL development sets. The numbers are averaged over 5 runs. The first column represents absolute values. The rest are improvements of different context modeling methods over CONCAT."
            ],
            "highlighted_evidence": [
              "To conduct a thorough comparison, we evaluate 13 different context modeling methods upon the same parser, including 6 methods introduced in Section SECREF2 and 7 selective combinations of them (e.g., Concat+Action Copy). The experimental results are presented in Figure FIGREF37. Taken as a whole, it is very surprising to observe that none of these methods can be consistently superior to the others. The experimental results on BERT-based models show the same trend. Diving deep into the methods only using recent questions as context, we observe that Concat and Turn perform competitively, outperforming Gate by a large margin. With respect to the methods only using precedent SQL as context, Action Copy significantly surpasses Tree Copy and SQL Attn in all metrics. In addition, we observe that there is little difference in the performance of Action Copy and Concat, which implies that using precedent SQL as context gives almost the same effect with using recent questions. In terms of the combinations of different context modeling methods, they do not significantly improve the performance as we expected.",
              "FLOAT SELECTED: Figure 5: Question Match, Interaction Match and Turn i Match on SPARC and COSQL development sets. The numbers are averaged over 5 runs. The first column represents absolute values. The rest are improvements of different context modeling methods over CONCAT."
            ]
          }
        ]
      },
      {
        "question": "What are two datasets models are tested on?",
        "question_id": "f858031ebe57b6139af46ee0f25c10870bb00c3c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SParC BIBREF2 and CoSQL BIBREF6"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we try to fulfill the above insufficiency via an exploratory study on real-world semantic parsing in context. Concretely, we present a grammar-based decoding semantic parser and adapt typical context modeling methods on top of it. Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods. Our best model achieves state-of-the-art (SOTA) performances on both datasets with significant improvements. Furthermore, we summarize and generalize the most frequent contextual phenomena, with a fine-grained analysis on representative models. Through the analysis, we obtain some interesting findings, which may benefit the community on the potential research directions. We will open-source our code and materials to facilitate future work upon acceptance."
            ],
            "highlighted_evidence": [
              "Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcai/LiuCGLZZ20",
    "dblp_title": "How Far are We from Effective Context Modeling? An Exploratory Study on Semantic Parsing in Context.",
    "year": "2020"
  },
  {
    "id": "1909.00324",
    "title": "A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis",
    "qas": [
      {
        "question": "How big is the improvement over the state-of-the-art results?",
        "question_id": "1763a029daca7cab10f18634aba02a6bd1b6faa7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset",
              "Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets",
              "In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task",
              "We present the overall performance of our model and baseline models in Table TABREF27. Results show that our AGDT outperforms all baseline models on both “restaurant-14” and “restaurant-large” datasets. ATAE-LSTM employs an aspect-weakly associative encoder to generate the aspect-specific sentence representation by simply concatenating the aspect, which is insufficient to exploit the given aspect. Although GCAE incorporates the gating mechanism to control the sentiment information flow according to the given aspect, the information flow is generated by an aspect-independent encoder. Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction. Our model benefits from the following aspects. First, our AGDT utilizes an aspect-guided encoder, which leverages the given aspect to guide the sentence encoding from scratch and generates the aspect-guided representation. Second, the AGDT guarantees that the aspect-specific information has been fully embedded in the sentence representation via reconstructing the given aspect. Third, the given aspect embedding is concatenated on the aspect-guided sentence representation for final predictions.",
              "The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets. This indicates that the given aspect information is very pivotal to the accurate sentiment prediction, especially when the sentence has different sentiment labels, which is consistent with existing work BIBREF2, BIBREF3, BIBREF4. Those results demonstrate the effectiveness of our model and suggest that our AGDT has better ability to distinguish the different sentiments of multiple aspects compared to GCAE.",
              "Experiments ::: Main Results and Analysis ::: Aspect-Term Sentiment Analysis Task",
              "In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets."
            ],
            "highlighted_evidence": [
              "Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task",
              "Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction.",
              "The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets.",
              "Experiments ::: Main Results and Analysis ::: Aspect-Term Sentiment Analysis Task",
              "In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets."
            ]
          }
        ]
      },
      {
        "question": "Is the model evaluated against other Aspect-Based models?",
        "question_id": "f9de9ddea0c70630b360167354004ab8cbfff041",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Experiments ::: Baselines",
              "To comprehensively evaluate our AGDT, we compare the AGDT with several competitive models."
            ],
            "highlighted_evidence": [
              "Experiments ::: Baselines\nTo comprehensively evaluate our AGDT, we compare the AGDT with several competitive models."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/LiangMZXCZ19",
    "dblp_title": "A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis.",
    "year": "2019"
  },
  {
    "id": "1905.06566",
    "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization",
    "qas": [
      {
        "question": "Is the baseline a non-heirarchical model like BERT?",
        "question_id": "fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "There were hierarchical and non-hierarchical baselines; BERT was one of those baselines",
            "evidence": [
              "FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).",
              "Our main results on the CNNDM dataset are shown in Table 1 , with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage BIBREF9 , Abstract-ML+RL BIBREF10 and DCA BIBREF42 are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite BIBREF26 and InconsisLoss BIBREF25 all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summarization (or compression). Bottom-Up BIBREF27 generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; BIBREF3 and NeuSum; BIBREF11 ). They have been extended with reinforcement learning (Refresh; BIBREF4 and BanditSum; BIBREF20 ), Maximal Marginal Relevance (NeuSum-MMR; BIBREF21 ), latent variable modeling (LatentSum; BIBREF5 ) and syntactic compression (JECS; BIBREF38 ). Lead3 is a baseline which simply selects the first three sentences. Our model $\\text{\\sc Hibert}_S$ (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences between them are all significant with a 0.95 confidence interval (estimated with the ROUGE script). Note that pre-training $\\text{\\sc Hibert}_S$ (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages ( $\\text{\\sc Hibert}_S$ ) or larger size ( $\\text{\\sc Hibert}_M$ ) perform even better and $\\text{\\sc Hibert}_M$ outperforms BERT by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section \"Pre-training\" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\\text{BERT}_{\\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\\text{BERT}_{\\text{large}}$ for the summarization task even with batch size of 1. The positional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model $\\text{\\sc Hibert}_S$1 outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters ( $\\text{\\sc Hibert}_S$2 54.6M v.s. BERT 110M). Results on the NYT50 dataset show the similar trends (see Table 2 ). EXTRACTION is a extractive model based hierarchical LSTM and we use the numbers reported by xu:2019:arxiv. The improvement of $\\text{\\sc Hibert}_S$3 over the baseline without pre-training (HeriTransformer) becomes 2.0 ROUGE. $\\text{\\sc Hibert}_S$4 (in-domain), $\\text{\\sc Hibert}_S$5 (in-domain), $\\text{\\sc Hibert}_S$6 and $\\text{\\sc Hibert}_S$7 all outperform BERT significantly according to the ROUGE script."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).",
              "We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in \"Extractive Summarization\" ) without pre-training."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/ZhangWZ19",
    "dblp_title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization.",
    "year": "2019"
  },
  {
    "id": "2003.04032",
    "title": "Shallow Discourse Annotation for Chinese TED Talks",
    "qas": [
      {
        "question": "Do they build a model to recognize discourse relations on their dataset?",
        "question_id": "58e65741184c81c9e7fe0ca15832df2d496beb6f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which inter-annotator metric do they use?",
        "question_id": "269b05b74d5215b09c16e95a91ae50caedd9e2aa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "agreement rates",
              "Kappa value"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We measured intra-annotator agreement between two annotators in three aspects: relations, senses, arguments. To be specific, the annotators’ consistency in annotating the type of a specific relation or sense and the position and scope of arguments are measured. To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value. And the final agreement study was carried out for the first 300 relations in our corpus. We obtained high agreement results and Kappa value for the discourse relation type and top-level senses ($\\ge {0.9} $ ). However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
            ],
            "highlighted_evidence": [
              "To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value."
            ]
          }
        ]
      },
      {
        "question": "How high is the inter-annotator agreement?",
        "question_id": "0d7f514f04150468b2d1de9174c12c28e52c5511",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "agreement of 0.85 and Kappa value of 0.83"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We measured intra-annotator agreement between two annotators in three aspects: relations, senses, arguments. To be specific, the annotators’ consistency in annotating the type of a specific relation or sense and the position and scope of arguments are measured. To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value. And the final agreement study was carried out for the first 300 relations in our corpus. We obtained high agreement results and Kappa value for the discourse relation type and top-level senses ($\\ge {0.9} $ ). However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
            ],
            "highlighted_evidence": [
              "However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
            ]
          }
        ]
      },
      {
        "question": "How are resources adapted to properties of Chinese text?",
        "question_id": "4d223225dbf84a80e2235448a4d7ba67bfb12490",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "removing AltLexC and adding Progression into our sense hierarchy"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we describe our scheme and process in annotating shallow discourse relations using PDTB-style. In view of the differences between English and Chinese, we made adaptations for the PDTB-3 scheme such as removing AltLexC and adding Progression into our sense hierarchy. To ensure the annotation quality, we formulated detailed annotation criteria and quality assurance strategies. After serious training, we annotated 3212 discourse relations, and we achieved a satisfactory consistency of labelling with a Kappa value of greater than 0.85 for most of the indicators. Finally, we display our annotation results in which the distribution of discourse relations and senses differ from that in other corpora which annotate news report or newspaper texts. Our corpus contains more Contingency, Temporal and Comparison relations instead of being governed by Expansion."
            ],
            "highlighted_evidence": [
              "In view of the differences between English and Chinese, we made adaptations for the PDTB-3 scheme such as removing AltLexC and adding Progression into our sense hierarchy."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/LongCRWX20",
    "dblp_title": "Shallow Discourse Annotation for Chinese TED Talks.",
    "year": "2020"
  },
  {
    "id": "2004.03034",
    "title": "The Role of Pragmatic and Discourse Context in Determining Argument Impact",
    "qas": [
      {
        "question": "How better are results compared to baseline models?",
        "question_id": "ca26cfcc755f9d0641db0e4d88b4109b903dbb26",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.",
            "evidence": [
              "We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\\%$) than distance from the thesis and linguistic features. Claims with higher impact parents are more likely to be have higher impact. Similarity with the parent and thesis is not significantly better than the majority baseline. Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline.",
              "We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\\%$)."
            ],
            "highlighted_evidence": [
              "We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\\%$) than distance from the thesis and linguistic features.",
              "Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline.",
              "We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\\%$)."
            ]
          }
        ]
      },
      {
        "question": "What models that rely only on claim-specific linguistic features are used as baselines?",
        "question_id": "6cdd61ebf84aa742155f4554456cc3233b6ae2bf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SVM with RBF kernel"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Similar to BIBREF9, we experiment with SVM with RBF kernel, with features that represent (1) the simple characteristics of the argument tree and (2) the linguistic characteristics of the claim."
            ],
            "highlighted_evidence": [
              "Similar to BIBREF9, we experiment with SVM with RBF kernel, with features that represent (1) the simple characteristics of the argument tree and (2) the linguistic characteristics of the claim."
            ]
          }
        ]
      },
      {
        "question": "How is pargmative and discourse context added to the dataset?",
        "question_id": "8e8097cada29d89ca07166641c725e0f8fed6676",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Claims and impact votes. We collected 47,219 claims from kialo.com for 741 controversial topics and their corresponding impact votes. Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented."
            ],
            "highlighted_evidence": [
              "Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented."
            ]
          }
        ]
      },
      {
        "question": "What annotations are available in the dataset?",
        "question_id": "951098f0b7169447695b47c142384f278f451a1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Claims and impact votes. We collected 47,219 claims from kialo.com for 741 controversial topics and their corresponding impact votes. Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented."
            ],
            "highlighted_evidence": [
              " Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2004-03034",
    "dblp_title": "The Role of Pragmatic and Discourse Context in Determining Argument Impact.",
    "year": "2020"
  },
  {
    "id": "1910.12618",
    "title": "Textual Data for Time Series Forecasting",
    "qas": [
      {
        "question": "How big is dataset used for training/testing?",
        "question_id": "07c59824f5e7c5399d15491da3543905cfa5f751",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "4,261  days for France and 4,748 for the UK",
            "evidence": [
              "Our work aims at predicting time series using exclusively text. Therefore for both countries the inputs of all our models consist only of written daily weather reports. Under their raw shape, those reports take the form of PDF documents giving a short summary of the country's overall weather, accompanied by pressure, temperature, wind, etc. maps. Note that those reports are written a posteriori, although they could be written in a predictive fashion as well. The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively. An excerpt for each language may be found in tables TABREF6 and TABREF7. The relevant text was extracted from the PDF documents using the Python library PyPDF2."
            ],
            "highlighted_evidence": [
              "The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively."
            ]
          }
        ]
      },
      {
        "question": "Is there any example where geometric property is visible for context similarity between words?",
        "question_id": "77f04cd553df691e8f4ecbe19da89bc32c7ac734",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance)."
            ],
            "highlighted_evidence": [
              "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. "
            ]
          }
        ]
      },
      {
        "question": "What geometric properties do embeddings display?",
        "question_id": "728a55c0f628f2133306b6bd88af00eb54017b12",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.",
            "evidence": [
              "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance)."
            ],
            "highlighted_evidence": [
              "For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days."
            ]
          }
        ]
      },
      {
        "question": "How accurate is model trained on text exclusively?",
        "question_id": "d5498d16e8350c9785782b57b1e5a82212dbdaad",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Relative error is less than 5%",
            "evidence": [
              "The main contribution of our paper is to suggest the use of a certain type of textual documents, namely daily weather report, to build forecasters of the daily national electricity load, average temperature and wind speed for both France and the United-Kingdom (UK). Consequently this work represents a significant break with traditional methods, and we do not intend to best state-of-the-art approaches. Textual information is naturally more fuzzy than numerical one, and as such the same accuracy is not expected from the presented approaches. With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets. Furthermore, the quality of our predictions of temperature and wind speed is satisfying enough to replace missing or unavailable data in traditional models. Two different approaches are considered to represent the text numerically, as well as multiple forecasting algorithms. Our empirical results are consistent across encoding, methods and language, thus proving the intrinsic value weather reports have for the prediction of the aforementioned time series. Moreover, a major distinction between previous works is our interpretation of the models. We quantify the impact of a word on the forecast and analyze the geometric properties of the word embedding we trained ourselves. Note that although multiple time series are discussed in our paper, the main focus of this paper remains electricity consumption. As such, emphasis is put on the predictive results on the load demand time series."
            ],
            "highlighted_evidence": [
              "With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iiaiaai/MouXZKHCTA24",
    "dblp_title": "Integrating Textual and Financial Time Series Data for Enhanced Forecasting.",
    "year": "2024"
  },
  {
    "id": "1911.12569",
    "title": "Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis",
    "qas": [
      {
        "question": "What was their result on Stance Sentiment Emotion Corpus?",
        "question_id": "3e839783d8a4f2fe50ece4a9b476546f0842b193",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "F1 score of 66.66%",
            "evidence": [
              "FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.",
              "We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.",
              "We implement our model in Python using Tensorflow on a single GPU. We experiment with six different BiLSTM based architectures. The three architectures correspond to BiLSTM based systems without primary attention i.e. only with secondary attention for sentiment analysis (S1), emotion analysis (E1) and the multi-task system (M1) for joint sentiment and emotion analysis. The remaining three architectures correspond to the systems for sentiment analysis (S2), emotion analysis (E2) and multi-task system (M2), with both primary and secondary attention. The weight matrices were initialized randomly using numbers form a truncated normal distribution. The batch size was 64 and the dropout BIBREF34 was 0.6 with the Adam optimizer BIBREF35. The hidden state vectors of both the forward and backward LSTM were 300-dimensional, whereas the context vector was 150-dimensional. Relu BIBREF36 was used as the activation for the hidden layers, whereas in the output layer we used sigmoid as the activation function. Sigmoid cross-entropy was used as the loss function. F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. Therefore, we report the F1-score for sentiment and precision, recall and F1-score for emotion analysis."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET.",
              "We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.",
              "F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. "
            ]
          }
        ]
      },
      {
        "question": "What performance did they obtain on the SemEval dataset?",
        "question_id": "2869d19e54fb554fcf1d6888e526135803bb7d75",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "F1 score of 82.10%",
            "evidence": [
              "We implement our model in Python using Tensorflow on a single GPU. We experiment with six different BiLSTM based architectures. The three architectures correspond to BiLSTM based systems without primary attention i.e. only with secondary attention for sentiment analysis (S1), emotion analysis (E1) and the multi-task system (M1) for joint sentiment and emotion analysis. The remaining three architectures correspond to the systems for sentiment analysis (S2), emotion analysis (E2) and multi-task system (M2), with both primary and secondary attention. The weight matrices were initialized randomly using numbers form a truncated normal distribution. The batch size was 64 and the dropout BIBREF34 was 0.6 with the Adam optimizer BIBREF35. The hidden state vectors of both the forward and backward LSTM were 300-dimensional, whereas the context vector was 150-dimensional. Relu BIBREF36 was used as the activation for the hidden layers, whereas in the output layer we used sigmoid as the activation function. Sigmoid cross-entropy was used as the loss function. F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. Therefore, we report the F1-score for sentiment and precision, recall and F1-score for emotion analysis.",
              "We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.",
              "FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET."
            ],
            "highlighted_evidence": [
              "F1-score was reported for the sentiment analysis BIBREF7 and precision, recall and F1-score were used as the evaluation metric for emotion analysis BIBREF15. ",
              "We compare the performance of our proposed system with the state-of-the-art systems of SemEval 2016 Task 6 and the systems of BIBREF15. Experimental results show that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis. We summarize the results of evaluation in Table TABREF18.",
              "FLOAT SELECTED: TABLE II F-SCORE OF VARIOUS MODELS ON SENTIMENT AND EMOTION TEST DATASET."
            ]
          }
        ]
      },
      {
        "question": "What are the state-of-the-art systems?",
        "question_id": "894c086a2cbfe64aa094c1edabbb1932a3d7c38a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",
            "evidence": [
              "FLOAT SELECTED: TABLE III COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS OF SEMEVAL 2016 TASK 6 ON SENTIMENT DATASET.",
              "FLOAT SELECTED: TABLE IV COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS PROPOSED BY [16] ON EMOTION DATASET. THE METRICS P, R AND F STAND FOR PRECISION, RECALL AND F1-SCORE.",
              "Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.",
              "We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: TABLE III COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS OF SEMEVAL 2016 TASK 6 ON SENTIMENT DATASET.",
              "FLOAT SELECTED: TABLE IV COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS PROPOSED BY [16] ON EMOTION DATASET. THE METRICS P, R AND F STAND FOR PRECISION, RECALL AND F1-SCORE.",
              "Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset.",
              "We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22"
            ]
          }
        ]
      },
      {
        "question": "How is multi-tasking performed?",
        "question_id": "722e9b6f55971b4c48a60f7a9fe37372f5bf3742",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks.",
              "Each of the shared representations is then fed to the primary attention mechanism"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We propose a novel two-layered multi-task attention based neural network for sentiment analysis where emotion analysis is utilized to improve its efficiency. Figure FIGREF1 illustrates the overall architecture of the proposed multi-task system. The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. The BiLSTM encodes the word representation of each word. This representation is shared between the subsystems of sentiment and emotion analysis. Each of the shared representations is then fed to the primary attention mechanism of both the subsystems. The primary attention mechanism finds the best representation for each word for each task. The secondary attention mechanism acts on top of the primary attention to extract the best sentence representation by focusing on the suitable context for each task. Finally, the representations of both the tasks are fed to two different feed-forward neural networks to produce two outputs - one for sentiment analysis and one for emotion analysis. Each component is explained in the subsequent subsections."
            ],
            "highlighted_evidence": [
              "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. The BiLSTM encodes the word representation of each word. This representation is shared between the subsystems of sentiment and emotion analysis. Each of the shared representations is then fed to the primary attention mechanism of both the subsystems. The primary attention mechanism finds the best representation for each word for each task. The secondary attention mechanism acts on top of the primary attention to extract the best sentence representation by focusing on the suitable context for each task. Finally, the representations of both the tasks are fed to two different feed-forward neural networks to produce two outputs - one for sentiment analysis and one for emotion analysis. Each component is explained in the subsequent subsections."
            ]
          }
        ]
      },
      {
        "question": "What are the datasets used for training?",
        "question_id": "9c2f306044b3d1b3b7fdd05d1c046e887796dd7a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SemEval 2016 Task 6 BIBREF7",
              "Stance Sentiment Emotion Corpus (SSEC) BIBREF15"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016 Task 6 corpus helps to bridge the gap between the unavailability of a corpus with sentiment and emotion labels. The SemEval 2016 corpus contains tweets which are classified into positive, negative or other. It contains 2,914 training and 1,956 test instances. The SSEC corpus is annotated with anger, anticipation, disgust, fear, joy, sadness, surprise and trust labels. Each tweet could belong to one or more emotion classes and one sentiment class. Table TABREF15 shows the data statistics of SemEval 2016 task 6 and SSEC which are used for sentiment and emotion analysis, respectively."
            ],
            "highlighted_evidence": [
              "We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15."
            ]
          }
        ]
      },
      {
        "question": "How many parameters does the model have?",
        "question_id": "3d99bc8ab2f36d4742e408f211bec154bc6696f7",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the previous state-of-the-art model?",
        "question_id": "9219eef636ddb020b9d394868959325562410f83",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF7",
              "BIBREF39",
              "BIBREF37",
              "LitisMind",
              "Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.",
              "We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise."
            ],
            "highlighted_evidence": [
              "Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features.",
              "We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15."
            ]
          }
        ]
      },
      {
        "question": "What is the previous state-of-the-art performance?",
        "question_id": "ff83eea2df9976c1a01482818340871b17ad4f8c",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcnn/KumarEKK19",
    "dblp_title": "Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis.",
    "year": "2019"
  },
  {
    "id": "1910.01363",
    "title": "Mapping (Dis-)Information Flow about the MH17 Plane Crash",
    "qas": [
      {
        "question": "How can the classifier facilitate the annotation task for human annotators?",
        "question_id": "0ee20a3a343e1e251b74a804e9aa1393d17b46d6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to get high precision predictions for unlabeled tweets, we choose the probability thresholds for predicting a pro-Russian or pro-Ukrainian tweet such that the classifier would achieve 80% precision on the test splits (recall at this precision level is 23%). Table TABREF38 shows the amount of polarized edges we can predict at this precision level. Upon manual inspection, we however find that the quality of predictions is lower than estimated. Hence, we manually re-annotate the pro-Russian and pro-Ukrainian predictions according to the official annotation guidelines used by BIBREF4. This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class)."
            ],
            "highlighted_evidence": [
              "This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class)."
            ]
          }
        ]
      },
      {
        "question": "What recommendations are made to improve the performance in future?",
        "question_id": "f0e8f045e2e33a2129e67fb32f356242db1dc280",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "applying reasoning BIBREF36 or irony detection methods BIBREF37"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "From the error analysis, we conclude that category I errors need further investigation, as here the model makes mistakes on seemingly easy instances. This might be due to the model not being able to correctly represent Twitter specific language or unknown words, such as Eukraine in example e). Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37."
            ],
            "highlighted_evidence": [
              "Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37."
            ]
          }
        ]
      },
      {
        "question": "What type of errors do the classifiers use?",
        "question_id": "b6c235d5986914b380c084d9535a7b01310c0278",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "correct class can be directly inferred from the text content easily, even without background knowledge",
              "correct class can be inferred from the text content, given that event-specific knowledge is provided",
              "orrect class can be inferred from the text content if the text is interpreted correctly"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to integrate automatically labeled examples into a network analysis that studies the flow of polarized information in the network, we need to produce high precision predictions for the pro-Russian and the pro-Ukrainian class. Polarized tweets that are incorrectly classified as neutral will hurt an analysis much less than neutral tweets that are erroneously classified as pro-Russian or pro-Ukrainian. However, the worst type of confusion is between the pro-Russian and pro-Ukrainian class. In order to gain insights into why these confusions happen, we manually inspect incorrectly predicted examples that are confused between the pro-Russian and pro-Ukrainian class. We analyse the misclassifications in the development set of all 10 runs, which results in 73 False Positives of pro-Ukrainian tweets being classified as pro-Russian (referred to as pro-Russian False Positives), and 88 False Positives of pro-Russian tweets being classified as pro-Ukrainian (referred to as pro-Ukrainian False Positives). We can identify three main cases for which the model produces an error:",
              "the correct class can be directly inferred from the text content easily, even without background knowledge",
              "the correct class can be inferred from the text content, given that event-specific knowledge is provided",
              "the correct class can be inferred from the text content if the text is interpreted correctly"
            ],
            "highlighted_evidence": [
              "We can identify three main cases for which the model produces an error:\n\nthe correct class can be directly inferred from the text content easily, even without background knowledge\n\nthe correct class can be inferred from the text content, given that event-specific knowledge is provided\n\nthe correct class can be inferred from the text content if the text is interpreted correctly"
            ]
          }
        ]
      },
      {
        "question": "What neural classifiers are used?",
        "question_id": "e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " convolutional neural network (CNN) BIBREF29"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27. The model performs 1d convolutions over a sequence of word embeddings. We use the same pre-trained fasttext embeddings as for the logistic regression model. We use a model with one convolutional layer and a relu activation function, and one max pooling layer. The number of filters is 100 and the filter size is set to 4."
            ],
            "highlighted_evidence": [
              "As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27."
            ]
          }
        ]
      },
      {
        "question": "What is the hashtags does the hashtag-based baseline use?",
        "question_id": "1e4450e23ec81fdd59821055f998fd9db0398b16",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "Hashtags are often used as a means to assess the content of a tweet BIBREF25, BIBREF26, BIBREF27. We identify hashtags indicative of a class in the annotated dataset using the pointwise mutual information (pmi) between a hashtag $hs$ and a class $c$, which is defined as\n\nWe then predict the class for unseen tweets as the class that has the highest pmi score for the hashtags contained in the tweet."
            ]
          }
        ]
      },
      {
        "question": "What languages are included in the dataset?",
        "question_id": "02ce4c288df14a90a210cb39973c6ac0fb4cec59",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.",
              "BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates."
            ],
            "highlighted_evidence": [
              "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\nBIBREF4 provide annotations for a subset of the English tweets contained in the dataset."
            ]
          }
        ]
      },
      {
        "question": "What dataset is used for this study?",
        "question_id": "60726d9792d301d5ff8e37fbb31d5104a520dea3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MH17 Twitter dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016."
            ],
            "highlighted_evidence": [
              "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter."
            ]
          }
        ]
      },
      {
        "question": "What proxies for data annotation were used in previous datasets?",
        "question_id": "e39d90b8d959697d9780eddce3a343e60543be65",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet",
              "Natural Language Processing (NLP) models can be used to automatically label text content"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Several studies analyse the framing of the crash and the spread of (dis)information about the event in terms of pro-Russian or pro-Ukrainian framing. These studies analyse information based on manually labeled content, such as television transcripts BIBREF2 or tweets BIBREF4, BIBREF5. Restricting the analysis to manually labeled content ensures a high quality of annotations, but prohibits analysis from being extended to the full amount of available data. Another widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet BIBREF6, BIBREF7, BIBREF8. Often, this approach treats content from uncredible sources as misleading (e.g. misinformation, disinformation or fake news). This methods enables researchers to scale up the number of observations without having to evaluate the fact value of each piece of content from low-quality sources. However, the approach fails to address an important issue: Not all content from uncredible sources is necessarily misleading or false and not all content from credible sources is true. As often emphasized in the propaganda literature, established media outlets too are vulnerable to state-driven disinformation campaigns, even if they are regarded as credible sources BIBREF9, BIBREF10, BIBREF11.",
              "In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. For example, several works developed classifiers for annotating text content with frame labels that can subsequently be used for large-scale content analysis BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19. Similarly, automatically labeling attitudes expressed in text BIBREF20, BIBREF21, BIBREF22, BIBREF23 can aid the analysis of disinformation and misinformation spread BIBREF24. In this work, we examine to which extent such classifiers can be used to detect pro-Russian framing related to the MH17 crash, and to which extent classifier predictions can be relied on for analysing information flow on Twitter."
            ],
            "highlighted_evidence": [
              "Another widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet BIBREF6, BIBREF7, BIBREF8.",
              "In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-01363",
    "dblp_title": "Mapping (Dis-)Information Flow about the MH17 Plane Crash.",
    "year": "2019"
  },
  {
    "id": "1901.04899",
    "title": "Conversational Intent Understanding for Passengers in Autonomous Vehicles",
    "qas": [
      {
        "question": "What are the supported natural commands?",
        "question_id": "c6e63e3b807474e29bfe32542321d015009e7148",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Set/Change Destination",
              "Set/Change Route",
              "Go Faster",
              "Go Slower",
              "Stop",
              "Park",
              "Pull Over",
              "Drop Off",
              "Open Door",
              "Other "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."
            ],
            "highlighted_evidence": [
              "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."
            ]
          }
        ]
      },
      {
        "question": "What is the size of their collected dataset?",
        "question_id": "4ef2fd79d598accc54c084f0cca8ad7c1b3f892a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "3347 unique utterances ",
            "evidence": [
              "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."
            ],
            "highlighted_evidence": [
              "We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."
            ]
          }
        ]
      },
      {
        "question": "Did they compare against other systems?",
        "question_id": "40e3639b79e2051bf6bce300d06548e7793daee0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",
              "FLOAT SELECTED: Table 3: Utterance-level Intent Recognition Results (10-fold CV)"
            ],
            "highlighted_evidence": [
              "The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",
              "FLOAT SELECTED: Table 3: Utterance-level Intent Recognition Results (10-fold CV)"
            ]
          }
        ]
      },
      {
        "question": "What intents does the paper explore?",
        "question_id": "8383e52b2adbbfb533fbe8179bc8dae11b3ed6da",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Set/Change Destination",
              "Set/Change Route",
              "Go Faster",
              "Go Slower",
              "Stop",
              "Park",
              "Pull Over",
              "Drop Off",
              "Open Door",
              "Other "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."
            ],
            "highlighted_evidence": [
              "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1901-04899",
    "dblp_title": "Conversational Intent Understanding for Passengers in Autonomous Vehicles.",
    "year": "2019"
  },
  {
    "id": "1606.05320",
    "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
    "qas": [
      {
        "question": "What kind of features are used by the HMM models, and how interpretable are those?",
        "question_id": "5f7850254b723adf891930c6faced1058b99bd57",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. ",
            "evidence": [
              "We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data).",
              "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.",
              "FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments."
            ],
            "highlighted_evidence": [
              "We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data).",
              "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components.",
              "FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments."
            ]
          }
        ]
      },
      {
        "question": "What kind of information do the HMMs learn that the LSTMs don't?",
        "question_id": "4d05a264b2353cff310edb480a917d686353b007",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The HMM can identify punctuation or pick up on vowels.",
            "evidence": [
              "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.",
              "FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments."
            ],
            "highlighted_evidence": [
              "We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data.",
              "FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments."
            ]
          }
        ]
      },
      {
        "question": "Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?",
        "question_id": "7cdce4222cea6955b656c1a3df1129bb8119e2d0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "decision trees to predict individual hidden state dimensions",
              "apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Several promising approaches to interpreting RNNs have been developed recently. BIBREF3 have approached this by using gradient boosting trees to predict LSTM output probabilities and explain which features played a part in the prediction. They do not model the internal structure of the LSTM, but instead approximate the entire architecture as a black box. BIBREF4 showed that in LSTM language models, around 10% of the memory state dimensions can be interpreted with the naked eye by color-coding the text data with the state values; some of them track quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ).",
              "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data."
            ],
            "highlighted_evidence": [
              "Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ).",
              "In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters."
            ]
          }
        ]
      },
      {
        "question": "How large is the gap in performance between the HMMs and the LSTMs?",
        "question_id": "6ea63327ffbab2fc734dd5c2414e59d3acc56ea5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.",
            "evidence": [
              "FLOAT SELECTED: Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/KrakovnaD16",
    "dblp_title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models.",
    "year": "2016"
  },
  {
    "id": "1809.10644",
    "title": "Predictive Embeddings for Hate Speech Detection on Twitter",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "50690b72dc61748e0159739a9a0243814d37f360",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we use three data sets from the literature to train and evaluate our own classifier. Although all address the category of hateful speech, they used different strategies of labeling the collected data. Table TABREF5 shows the characteristics of the datasets.",
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .",
              "Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. Such examples may be innocuous in isolation but could potentially be sexist or racist in context. While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.",
              "Debra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two.",
              "Along these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:",
              "@LoveAndLonging ...how is that example \"sexism\"?",
              "@amberhasalamb ...in what way?"
            ],
            "highlighted_evidence": [
              "In this paper, we use three data sets from the literature to train and evaluate our own classifier.",
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 .",
              "Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general. ",
              "While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.\n\nDebra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two.\n\nAlong these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:\n\n@LoveAndLonging ...how is that example \"sexism\"?\n\n@amberhasalamb ...in what way?"
            ]
          }
        ]
      },
      {
        "question": "Which publicly available datasets are used?",
        "question_id": "8266642303fbc6a1138b4e23ee1d859a6f584fbb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF3",
              "BIBREF4",
              "BIBREF9"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we use three data sets from the literature to train and evaluate our own classifier. Although all address the category of hateful speech, they used different strategies of labeling the collected data. Table TABREF5 shows the characteristics of the datasets.",
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 ."
            ],
            "highlighted_evidence": [
              "In this paper, we use three data sets from the literature to train and evaluate our own classifier.",
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 ."
            ]
          }
        ]
      },
      {
        "question": "What embedding algorithm and dimension size are used?",
        "question_id": "3685bf2409b23c47bfd681989fb4a763bcab6be2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "300 Dimensional Glove"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We tokenize the data using Spacy BIBREF10 . We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task. We experimented extensively with pre-processing variants and our results showed better performance without lemmatization and lower-casing (see supplement for details). We pad each input to 50 words. We train using RMSprop with a learning rate of .001 and a batch size of 512. We add dropout with a drop rate of 0.1 in the final layer to reduce overfitting BIBREF12 , batch size, and input length empirically through random hyperparameter search."
            ],
            "highlighted_evidence": [
              "We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task"
            ]
          }
        ]
      },
      {
        "question": "What data are the embeddings trained on?",
        "question_id": "19225e460fff2ac3aebc7fe31fcb4648eda813fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Common Crawl "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We tokenize the data using Spacy BIBREF10 . We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task. We experimented extensively with pre-processing variants and our results showed better performance without lemmatization and lower-casing (see supplement for details). We pad each input to 50 words. We train using RMSprop with a learning rate of .001 and a batch size of 512. We add dropout with a drop rate of 0.1 in the final layer to reduce overfitting BIBREF12 , batch size, and input length empirically through random hyperparameter search."
            ],
            "highlighted_evidence": [
              "We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task."
            ]
          }
        ]
      },
      {
        "question": "how much was the parameter difference between their model and previous methods?",
        "question_id": "f37026f518ab56c859f6b80b646d7f19a7b684fa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "our model requires 100k parameters , while BIBREF8 requires 250k parameters"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential added value of metadata, we believe a tweet-only classifier has merits because retrieving features from the social graph is not always tractable in production settings. Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters."
            ],
            "highlighted_evidence": [
              "Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters."
            ]
          }
        ]
      },
      {
        "question": "how many parameters did their model use?",
        "question_id": "1231934db6adda87c1b15e571468b8e9d225d6fe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Excluding the embedding weights, our model requires 100k parameters"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "On the SR dataset, we outperform BIBREF8 's text based model by 3 F1 points, while just falling short of the Text + Metadata Interleaved Training model. While we appreciate the potential added value of metadata, we believe a tweet-only classifier has merits because retrieving features from the social graph is not always tractable in production settings. Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters."
            ],
            "highlighted_evidence": [
              "Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters."
            ]
          }
        ]
      },
      {
        "question": "which datasets were used?",
        "question_id": "81303f605da57ddd836b7c121490b0ebb47c60e7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Sexist/Racist (SR) data set",
              "HATE dataset",
              "HAR"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 ."
            ],
            "highlighted_evidence": [
              "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets."
            ]
          }
        ]
      },
      {
        "question": "what was their system's f1 performance?",
        "question_id": "a3f108f60143d13fe38d911b1cc3b17bdffde3bd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",
            "evidence": [
              "FLOAT SELECTED: Table 2: F1 Results3",
              "The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points. Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: F1 Results3",
              "Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1."
            ]
          }
        ]
      },
      {
        "question": "what was the baseline?",
        "question_id": "118ff1d7000ea0d12289d46430154cc15601fd8e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "logistic regression"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "All of our results are produced from 10-fold cross validation to allow comparison with previous results. We trained a logistic regression baseline model (line 1 in Table TABREF10 ) using character ngrams and word unigrams using TF*IDF weighting BIBREF13 , to provide a baseline since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model's results on their respective datasets."
            ],
            "highlighted_evidence": [
              "We trained a logistic regression baseline model (line 1 in Table TABREF10 ) using character ngrams and word unigrams using TF*IDF weighting BIBREF13 , to provide a baseline since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model's results on their respective datasets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl-alw/KshirsagarCMM18",
    "dblp_title": "Predictive Embeddings for Hate Speech Detection on Twitter.",
    "year": "2018"
  },
  {
    "id": "1606.02006",
    "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation",
    "qas": [
      {
        "question": "What datasets were used?",
        "question_id": "102a0439739428aac80ac11795e73ce751b93ea1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "KFTT BIBREF12 and BTEC BIBREF13"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Dataset: We perform experiments on two widely-used tasks for the English-to-Japanese language pair: KFTT BIBREF12 and BTEC BIBREF13 . KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is an easier translation task than KFTT, because KFTT covers a broader domain, has a larger vocabulary of rare words, and has relatively long sentences. The details of each corpus are depicted in Table TABREF19 ."
            ],
            "highlighted_evidence": [
              "Dataset: We perform experiments on two widely-used tasks for the English-to-Japanese language pair: KFTT BIBREF12 and BTEC BIBREF13 . KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is an easier translation task than KFTT, because KFTT covers a broader domain, has a larger vocabulary of rare words, and has relatively long sentences. "
            ]
          }
        ]
      },
      {
        "question": "What language pairs did they experiment with?",
        "question_id": "d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English-Japanese"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We perform experiments (§ SECREF5 ) on two English-Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training."
            ],
            "highlighted_evidence": [
              "We perform experiments (§ SECREF5 ) on two English-Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/ArthurNN16",
    "dblp_title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation.",
    "year": "2016"
  },
  {
    "id": "1911.03243",
    "title": "Crowdsourcing a High-Quality Gold Standard for QA-SRL",
    "qas": [
      {
        "question": "How much more coverage is in the new dataset?",
        "question_id": "04f72eddb1fc73dd11135a80ca1cf31e9db75578",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "278 more annotations",
            "evidence": [
              "The measured precision with respect to PropBank is low for adjuncts due to the fact that our annotators were capturing many correct arguments not covered in PropBank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts. Overall, the comparison to PropBank reinforces the quality of our gold dataset and shows its better coverage relative to the 2015 dataset."
            ],
            "highlighted_evidence": [
              "Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts. "
            ]
          }
        ]
      },
      {
        "question": "How was coverage measured?",
        "question_id": "f74eaee72cbd727a6dffa1600cdf1208672d713e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "QA pairs per predicate"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The original 2015 QA-SRL dataset BIBREF4 was annotated by non-expert workers after completing a brief training procedure. They annotated 7.8K verbs, reporting an average of 2.4 QA pairs per predicate. Even though multiple annotators were shown to produce greater coverage, their released dataset was produced using only a single annotator per verb. In subsequent work, BIBREF5 constructed a large-scale corpus and used it to train a parser. They crowdsourced 133K verbs with 2.0 QA pairs per verb on average. Since crowd-workers had no prior training, quality was established using an additional validation step, where workers had to ascertain the validity of the question, but not of its answers. Instead, the validator provided additional answers, independent of the other annotators. Each verb in the corpus was annotated by a single QA-generating worker and validated by two others."
            ],
            "highlighted_evidence": [
              "They annotated 7.8K verbs, reporting an average of 2.4 QA pairs per predicate. Even though multiple annotators were shown to produce greater coverage, their released dataset was produced using only a single annotator per verb."
            ]
          }
        ]
      },
      {
        "question": "How was quality measured?",
        "question_id": "068dbcc117c93fa84c002d3424bafb071575f431",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",
            "evidence": [
              "Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)",
              "To estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate. Individual worker-vs-worker agreement yields 79.8 F1 over 10 experiments with 150 predicates, indicating high consistency across our annotators, inline with results by other structured semantic annotations (e.g. BIBREF6). Overall consistency of the dataset is assessed by measuring agreement between different consolidated annotations, obtained by disjoint triplets of workers, which achieves F1 of 84.1 over 4 experiments, each with 35 distinct predicates. Notably, consolidation boosts agreement, suggesting it is a necessity for semantic annotation consistency.",
              "Dataset Quality Analysis ::: Dataset Assessment and Comparison",
              "We assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates. To construct the expert set, we blindly merged the Dense set with our worker annotations and manually corrected them. We further corrected the evaluation decisions, accounting for some automatic evaluation mistakes introduced by the span-matching and question paraphrasing criteria. As seen in Table TABREF19, our gold set yields comparable precision with significantly higher recall, which is in line with our 25% higher yield.",
              "Dataset Quality Analysis ::: Agreement with PropBank Data",
              "It is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7. In Table TABREF22, we replicate the experiments in BIBREF4 for both our gold set and theirs, over a sample of 200 sentences from Wall Street Journal (agreement evaluation is automatic and the metric is somewhat similar to our UA). We report macro-averaged (over predicates) precision and recall for all roles, including core and adjuncts, while considering the PropBank data as the reference set. Our recall of the PropBank roles is notably high, reconfirming the coverage obtained by our annotation protocol."
            ],
            "highlighted_evidence": [
              "Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)\nTo estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate.",
              "Dataset Quality Analysis ::: Dataset Assessment and Comparison\nWe assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates. ",
              "Dataset Quality Analysis ::: Agreement with PropBank Data\nIt is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7. "
            ]
          }
        ]
      },
      {
        "question": "How was the corpus obtained?",
        "question_id": "96526a14820b7debfd6f7c5beeade0a854b93d1a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " trained annotators BIBREF4",
              "crowdsourcing BIBREF5 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL. As BIBREF5 acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s."
            ],
            "highlighted_evidence": [
              "Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability."
            ]
          }
        ]
      },
      {
        "question": "How are workers trained?",
        "question_id": "32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "extensive personal feedback"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our pool of annotators is selected after several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback. 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations."
            ],
            "highlighted_evidence": [
              "Our pool of annotators is selected after several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback."
            ]
          }
        ]
      },
      {
        "question": "What is different in the improved annotation protocol?",
        "question_id": "78c010db6413202b4063dc3fb6e3cc59ec16e7e3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "a trained worker consolidates existing annotations ",
            "evidence": [
              "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen “47 people”, while another chose “the councillor”; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix."
            ],
            "highlighted_evidence": [
              "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. "
            ]
          }
        ]
      },
      {
        "question": "How was the previous dataset annotated?",
        "question_id": "a69af5937cab861977989efd72ad1677484b5c8c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the annotation machinery of BIBREF5"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen “47 people”, while another chose “the councillor”; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix."
            ],
            "highlighted_evidence": [
              "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. "
            ]
          }
        ]
      },
      {
        "question": "How big is the dataset?",
        "question_id": "8847f2c676193189a0f9c0fe3b86b05b5657b76a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1593 annotations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The measured precision with respect to PropBank is low for adjuncts due to the fact that our annotators were capturing many correct arguments not covered in PropBank. To examine this, we analyzed 100 false positive arguments. Only 32 of those were due to wrong or incomplete QA annotations in our gold, while most others were outside of PropBank's scope, capturing either implied arguments or roles not covered in PropBank. Extrapolating from this manual analysis estimates our true precision (on all roles) to be about 91%, which is consistent with the 88% precision figure in Table TABREF19. Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts. Overall, the comparison to PropBank reinforces the quality of our gold dataset and shows its better coverage relative to the 2015 dataset."
            ],
            "highlighted_evidence": [
              "Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-03243",
    "dblp_title": "Crowdsourcing a High-Quality Gold Standard for QA-SRL.",
    "year": "2019"
  },
  {
    "id": "1809.04686",
    "title": "Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation",
    "qas": [
      {
        "question": "Do the other multilingual baselines make use of the same amount of training data?",
        "question_id": "05196588320dfb0b9d9be7d64864c43968d329bc",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How big is the impact of training data size on the performance of the multilingual encoder?",
        "question_id": "e930f153c89dfe9cff75b7b15e45cd3d700f4c72",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What data were they used to train the multilingual encoder?",
        "question_id": "545ff2f76913866304bfacdb4cc10d31dbbd2f37",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "WMT 2014 En-Fr parallel corpus",
            "evidence": [
              "For the MT task, we use the WMT 2014 En $\\leftrightarrow $ Fr parallel corpus. The dataset contains 36 million En $\\rightarrow $ Fr sentence pairs. We swapped the source and target sentences to obtain parallel data for the Fr $\\rightarrow $ En translation task. We use these two datasets (72 million sentence pairs) to train a single multilingual NMT model to learn both these translation directions simultaneously. We generated a shared sub-word vocabulary BIBREF37 , BIBREF38 of 32K units from all source and target training data. We use this sub-word vocabulary for all of our experiments below."
            ],
            "highlighted_evidence": [
              "For the MT task, we use the WMT 2014 En $\\leftrightarrow $ Fr parallel corpus. The dataset contains 36 million En $\\rightarrow $ Fr sentence pairs. We swapped the source and target sentences to obtain parallel data for the Fr $\\rightarrow $ En translation task. We use these two datasets (72 million sentence pairs) to train a single multilingual NMT model to learn both these translation directions simultaneously. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1809-04686",
    "dblp_title": "Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation.",
    "year": "2018"
  },
  {
    "id": "1703.09684",
    "title": "An Analysis of Visual Question Answering Algorithms",
    "qas": [
      {
        "question": "From when are many VQA datasets collected?",
        "question_id": "cf93a209c8001ffb4ef505d306b6ced5936c6b63",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "late 2014",
            "evidence": [
              "VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used."
            ],
            "highlighted_evidence": [
              "VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iccv/KafleK17",
    "dblp_title": "An Analysis of Visual Question Answering Algorithms.",
    "year": "2017"
  },
  {
    "id": "1911.11744",
    "title": "Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration",
    "qas": [
      {
        "question": "What is task success rate achieved? ",
        "question_id": "fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "96-97.6% using the objects color or shape and 79% using shape alone",
            "evidence": [
              "To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls. A task is considered to be successfully completed when the cube is withing the boundaries of the targeted bowl. Bowls have a bounding box of 12.5 and 17.5cm edge length for the small and large variant, respectively. Our experiments showed that using the objects color or shape to uniquely identify an object allows the robot successfully complete the binning task in 97.6% and 96.0% of the cases. However, using the shape alone as a unique identifier, the task could only be completed in 79.0% of the cases. We suspect that the loss of accuracy is due to the low image resolution of the input image, preventing the network from reliably distinguishing the object shapes. In general, our approach is able to actuate the robot with an target error well below 5cm, given the target was correctly identified."
            ],
            "highlighted_evidence": [
              "Our experiments showed that using the objects color or shape to uniquely identify an object allows the robot successfully complete the binning task in 97.6% and 96.0% of the cases. However, using the shape alone as a unique identifier, the task could only be completed in 79.0% of the cases."
            ]
          }
        ]
      },
      {
        "question": "What simulations are performed by the authors to validate their approach?",
        "question_id": "1e2ffa065b640e912d6ed299ff713a12195e12c4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate our model in a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command. Each environment contains between three and five objects differentiated by their size (small, large), shape (round, square) and color (red, green, blue, yellow, pink), totalling in 20 different objects. Depending on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity."
            ],
            "highlighted_evidence": [
              "We evaluate our model in a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command. Each environment contains between three and five objects differentiated by their size (small, large), shape (round, square) and color (red, green, blue, yellow, pink), totalling in 20 different objects. Depending on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity."
            ]
          }
        ]
      },
      {
        "question": "Does proposed end-to-end approach learn in reinforcement or supervised learning manner?",
        "question_id": "28b2a20779a78a34fb228333dc4b93fd572fda15",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "supervised learning",
            "evidence": [
              "To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario.",
              "To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls. A task is considered to be successfully completed when the cube is withing the boundaries of the targeted bowl. Bowls have a bounding box of 12.5 and 17.5cm edge length for the small and large variant, respectively. Our experiments showed that using the objects color or shape to uniquely identify an object allows the robot successfully complete the binning task in 97.6% and 96.0% of the cases. However, using the shape alone as a unique identifier, the task could only be completed in 79.0% of the cases. We suspect that the loss of accuracy is due to the low image resolution of the input image, preventing the network from reliably distinguishing the object shapes. In general, our approach is able to actuate the robot with an target error well below 5cm, given the target was correctly identified."
            ],
            "highlighted_evidence": [
              "To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario.",
              "To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-11744",
    "dblp_title": "Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration.",
    "year": "2019"
  },
  {
    "id": "1910.03467",
    "title": "Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation",
    "qas": [
      {
        "question": "Are synonymous relation taken into account in the Japanese-Vietnamese task?",
        "question_id": "b367b823c5db4543ac421d0057b02f62ea16bf9f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Due to the fact that Vietnamese WordNet is not available, we only exploit WordNet to tackle unknown words of Japanese texts in our Japanese$\\rightarrow $Vietnamese translation system. After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. We choose 1-best synonym for each OOV word. Table TABREF18 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3). We observed an improvement of +0.7 BLEU points between (3) and (5) without using data augmentation described in BIBREF21."
            ],
            "highlighted_evidence": [
              "After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. "
            ]
          }
        ]
      },
      {
        "question": "Is the supervised morphological learner tested on Japanese?",
        "question_id": "84737d871bde8058d8033e496179f7daec31c2d3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.",
              "FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems"
            ],
            "highlighted_evidence": [
              "We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.",
              "FLOAT SELECTED: Table 1: Results of Japanese-Vietnamese NMT systems"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aclwat/NgoHNN19",
    "dblp_title": "Overcoming the Rare Word Problem for low-resource language pairs in Neural Machine Translation.",
    "year": "2019"
  },
  {
    "id": "1908.09156",
    "title": "A framework for anomaly detection using language modeling, and its applications to finance",
    "qas": [
      {
        "question": "What is the dataset that is used in the paper?",
        "question_id": "7b3d207ed47ae58286029b62fd0c160a0145e73d",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the performance of the models discussed in the paper?",
        "question_id": "d58c264068d8ca04bb98038b4894560b571bab3e",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Does the paper consider the use of perplexity in order to identify text anomalies?",
        "question_id": "f80d89fb905b3e7e17af1fe179b6f441405ad79b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              "PERPLEXITY"
            ]
          }
        ]
      },
      {
        "question": "Does the paper report a baseline for the task?",
        "question_id": "5f6fac08c97c85d5f4f4d56d8b0691292696f8e6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-09156",
    "dblp_title": "A framework for anomaly detection using language modeling, and its applications to finance.",
    "year": "2019"
  },
  {
    "id": "1911.00523",
    "title": "What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments",
    "qas": [
      {
        "question": "What non-contextual properties do they refer to?",
        "question_id": "6adec34d86095643e6b89cda5c7cd94f64381acc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "These features are derived directly from the word and capture the general tendency of a word being echoed in explanations."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations."
            ],
            "highlighted_evidence": [
              "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations."
            ]
          }
        ]
      },
      {
        "question": "What is the baseline?",
        "question_id": "62ba1fefc1eb826fe0cbac092d37a3e2098967e9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "random method ",
              "LSTM "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).",
              "To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features. We scale all features to $[0, 1]$ before fitting the models. As introduced in Section SECREF3, we split our tuples of (OP, PC, explanation) into training, validation, and test sets, and use the validation set for hyperparameter tuning. Refer to the supplementary material for additional details in the experiment."
            ],
            "highlighted_evidence": [
              " To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).",
              "To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features."
            ]
          }
        ]
      },
      {
        "question": "What are their proposed features?",
        "question_id": "93ac147765ee2573923f68aa47741d4bcbf88fa8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Non-contextual properties of a word",
              "Word usage in an OP or PC (two groups)",
              "How a word connects an OP and PC.",
              "General OP/PC properties"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):",
              "[itemsep=0pt,leftmargin=*,topsep=0pt]",
              "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.",
              "Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.",
              "How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.",
              "General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.",
              "Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:"
            ],
            "highlighted_evidence": [
              "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]"
            ]
          }
        ]
      },
      {
        "question": "What are overall baseline results on new this new task?",
        "question_id": "14c0328e8ec6360a913b8ecb3e50cb27650ff768",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "all of our models outperform the random baseline by a wide margin",
              "he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).",
              "Overall performance (Figure FIGREF28). Although our word-level task is heavily imbalanced, all of our models outperform the random baseline by a wide margin. As expected, content words are much more difficult to predict than stopwords, but the best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116). Notably, although we strongly improve on our random baseline, even our best F1 scores are relatively low, and this holds true regardless of the model used. Despite involving more tokens than standard tagging tasks (e.g., BIBREF41 and BIBREF42), predicting whether a word is going to be echoed in explanations remains a challenging problem."
            ],
            "highlighted_evidence": [
              "To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).",
              "Overall performance (Figure FIGREF28). Although our word-level task is heavily imbalanced, all of our models outperform the random baseline by a wide margin. As expected, content words are much more difficult to predict than stopwords, but the best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116). Notably, although we strongly improve on our random baseline, even our best F1 scores are relatively low, and this holds true regardless of the model used. Despite involving more tokens than standard tagging tasks (e.g., BIBREF41 and BIBREF42), predicting whether a word is going to be echoed in explanations remains a challenging problem."
            ]
          }
        ]
      },
      {
        "question": "What metrics are used in evaluation of this task?",
        "question_id": "6073fa9050da76eeecd8aa3ccc7ecb16a238d83f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "F1 score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances)."
            ],
            "highlighted_evidence": [
              "Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances)."
            ]
          }
        ]
      },
      {
        "question": "Do authors provide any explanation for intriguing patterns of word being echoed?",
        "question_id": "eacd7e540cc34cb45770fcba463f4bf968681d59",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations."
            ],
            "highlighted_evidence": [
              "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations."
            ]
          }
        ]
      },
      {
        "question": "What features are proposed?",
        "question_id": "1124804c3702499b78cf0678bab5867e81284b6c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Non-contextual properties of a word",
              "Word usage in an OP or PC (two groups)",
              "How a word connects an OP and PC",
              "General OP/PC properties"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):",
              "[itemsep=0pt,leftmargin=*,topsep=0pt]",
              "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.",
              "Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.",
              "How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.",
              "General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.",
              "Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:",
              "Although we expect more complicated words (#characters) to be echoed more often, this is not the case on average. We also observe an interesting example of Simpson's paradox in the results for Wordnet depth BIBREF38: shallower words are more likely to be echoed across all words, but deeper words are more likely to be echoed in content words and stopwords."
            ],
            "highlighted_evidence": [
              "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nAlthough we expect more complicated words (#characters) to be echoed more often, this is not the case on average. We also observe an interesting example of Simpson's paradox in the results for Wordnet depth BIBREF38: shallower words are more likely to be echoed across all words, but deeper words are more likely to be echoed in content words and stopwords."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/AtkinsonST19",
    "dblp_title": "What Gets Echoed? Understanding the &quot;Pointers&quot; in Explanations of Persuasive Arguments.",
    "year": "2019"
  },
  {
    "id": "1803.03664",
    "title": "Automating Reading Comprehension by Generating Question and Answer Pairs",
    "qas": [
      {
        "question": "Which datasets are used to train this model?",
        "question_id": "2b78052314cb730824836ea69bc968df7964b4e4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SQUAD"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate performance of our models on the SQUAD BIBREF16 dataset (denoted $\\mathcal {S}$ ). We use the same split as that of BIBREF4 , where a random subset of 70,484 instances from $\\mathcal {S}\\ $ are used for training ( ${\\mathcal {S}}^{tr}$ ), 10,570 instances for validation ( ${\\mathcal {S}}^{val}$ ), and 11,877 instances for testing ( ${\\mathcal {S}}^{te}$ )."
            ],
            "highlighted_evidence": [
              "We evaluate performance of our models on the SQUAD BIBREF16 dataset (denoted $\\mathcal {S}$ ). We use the same split as that of BIBREF4 , where a random subset of 70,484 instances from $\\mathcal {S}\\ $ are used for training ( ${\\mathcal {S}}^{tr}$ ), 10,570 instances for validation ( ${\\mathcal {S}}^{val}$ ), and 11,877 instances for testing ( ${\\mathcal {S}}^{te}$ )."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/pakdd/KumarBMRL18",
    "dblp_title": "Automating Reading Comprehension by Generating Question and Answer Pairs.",
    "year": "2018"
  },
  {
    "id": "1910.11949",
    "title": "Automatic Reminiscence Therapy for Dementia.",
    "qas": [
      {
        "question": "How is performance of this system measured?",
        "question_id": "11d2f0d913d6e5f5695f8febe2b03c6c125b667c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "using the BLEU score as a quantitative metric and human evaluation for quality",
            "evidence": [
              "We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.",
              "Our chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation."
            ],
            "highlighted_evidence": [
              "We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.\n\nOur chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation."
            ]
          }
        ]
      },
      {
        "question": "How many questions per image on average are available in dataset?",
        "question_id": "1c85a25ec9d0c4f6622539f48346e23ff666cd5f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "5 questions per image"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual."
            ],
            "highlighted_evidence": [
              "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions."
            ]
          }
        ]
      },
      {
        "question": "Is machine learning system underneath similar to image caption ML systems?",
        "question_id": "37d829cd42db9ae3d56ab30953a7cf9eda050841",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning. In our case, we focus on generating questions from pictures. Our second architecture is inspired by Neural Conversational Model from BIBREF14 where the author presents an end-to-end approach to generate simple conversations. Building an open-domain conversational agent is a challenging problem. As addressed in BIBREF15 and BIBREF16, the lack of a consistent personality and lack of long-term memory which produces some meaningless responses in these models are still unresolved problems."
            ],
            "highlighted_evidence": [
              "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning."
            ]
          }
        ]
      },
      {
        "question": "How big dataset is used for training this system?",
        "question_id": "4b41f399b193d259fd6e24f3c6e95dc5cae926dd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",
            "evidence": [
              "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual.",
              "We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters."
            ],
            "highlighted_evidence": [
              "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions.",
              "We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/mir/CarosGRG20",
    "dblp_title": "Automatic Reminiscence Therapy for Dementia.",
    "year": "2020"
  },
  {
    "id": "1902.09087",
    "title": "Lattice CNNs for Matching Based Chinese Question Answering",
    "qas": [
      {
        "question": "How do they obtain word lattices from words?",
        "question_id": "76377e5bb7d0a374b0aefc54697ac9cd89d2eba8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "By considering words as vertices and generating directed edges between neighboring words within a sentence",
            "evidence": [
              "Word Lattice",
              "As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 ."
            ],
            "highlighted_evidence": [
              "Word Lattice\nAs shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 ."
            ]
          }
        ]
      },
      {
        "question": "Which metrics do they use to evaluate matching?",
        "question_id": "85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Precision@1",
              "Mean Average Precision",
              "Mean Reciprocal Rank"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For both datasets, we follow the evaluation metrics used in the original evaluation tasks BIBREF13 . For DBQA, P@1 (Precision@1), MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are adopted. For KBRE, since only one golden candidate is labeled for each question, only P@1 and MRR are used."
            ],
            "highlighted_evidence": [
              "For both datasets, we follow the evaluation metrics used in the original evaluation tasks BIBREF13 . For DBQA, P@1 (Precision@1), MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are adopted. For KBRE, since only one golden candidate is labeled for each question, only P@1 and MRR are used."
            ]
          }
        ]
      },
      {
        "question": "Which dataset(s) do they evaluate on?",
        "question_id": "4b128f9e94d242a8e926bdcb240ece279d725729",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DBQA",
              "KBRE"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Datasets",
              "We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .",
              "DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.",
              "KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve."
            ],
            "highlighted_evidence": [
              "Datasets\nWe conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .",
              "DBQA is a document based question answering dataset. ",
              "KBRE is a knowledge based relation extraction dataset."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/LaiFYWXZ19",
    "dblp_title": "Lattice CNNs for Matching Based Chinese Question Answering.",
    "year": "2019"
  },
  {
    "id": "2003.04748",
    "title": "On the coexistence of competing languages",
    "qas": [
      {
        "question": "What languages do they look at?",
        "question_id": "f8f13576115992b0abb897ced185a4f9d35c5de9",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2003-04748",
    "dblp_title": "On the coexistence of competing languages.",
    "year": "2020"
  },
  {
    "id": "1907.01413",
    "title": "Speaker-independent classification of phonetic segments from raw ultrasound in child speech",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "1fdcc650c65c11908f6bde67d5052087245f3dde",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they propose any further additions that could be made to improve generalisation to unseen speakers?",
        "question_id": "abad9beb7295d809d7e5e1407cbf673c9ffffd19",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. Recurrent architectures are natural candidates for such systems. Additionally, if using these techniques for speech therapy, the audio signal will be available. An extension of these analyses should not be limited to the ultrasound signal, but instead evaluate whether audio and ultrasound can be complementary. Further work should aim to extend the four classes to more a fine-grained place of articulation, possibly based on phonological processes. Similarly, investigating which classes lead to classification errors might help explain some of the observed results. Although we have looked at variables such as age, gender, or amount of data to explain speaker variation, there may be additional factors involved, such as the general quality of the ultrasound image. Image quality could be affected by probe placement, dry mouths, or other factors. Automatically identifying or measuring such cases could be beneficial for speech therapy, for example, by signalling the therapist that the data being collected is sub-optimal."
            ],
            "highlighted_evidence": [
              "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame."
            ]
          }
        ]
      },
      {
        "question": "What are the characteristics of the dataset?",
        "question_id": "265c9b733e4dfffb76acfbade4c0c9b14d3ccde1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male)",
              "data was aligned at the phone-level",
              "121fps with a 135 field of view",
              "single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances."
            ],
            "highlighted_evidence": [
              "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 .",
              "The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)."
            ]
          }
        ]
      },
      {
        "question": "What type of models are used for classification?",
        "question_id": "0f928732f226185c76ad5960402e9342c0619310",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "feedforward neural networks (DNNs)",
              "convolutional neural networks (CNNs)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept."
            ],
            "highlighted_evidence": [
              "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function.",
              "As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes."
            ]
          }
        ]
      },
      {
        "question": "Do they compare to previous work?",
        "question_id": "11c5b12e675cfd8d1113724f019d8476275bd700",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many instances does their dataset have?",
        "question_id": "d24acc567ebaec1efee52826b7eaadddc0a89e8b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "10700"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."
            ],
            "highlighted_evidence": [
              "This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples."
            ]
          }
        ]
      },
      {
        "question": "What model do they use to classify phonetic segments? ",
        "question_id": "2d62a75af409835e4c123a615b06235a352a67fe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "feedforward neural networks",
              "convolutional neural networks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept."
            ],
            "highlighted_evidence": [
              "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. ",
              "As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. "
            ]
          }
        ]
      },
      {
        "question": "How many speakers do they have in the dataset?",
        "question_id": "fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "58"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances."
            ],
            "highlighted_evidence": [
              "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icassp/RibeiroERR19",
    "dblp_title": "Speaker-independent Classification of Phonetic Segments from Raw Ultrasound in Child Speech.",
    "year": "2019"
  },
  {
    "id": "1908.07816",
    "title": "A Multi-Turn Emotionally Engaging Dialog Model",
    "qas": [
      {
        "question": "How better is proposed method than baselines perpexity wise?",
        "question_id": "c034f38a570d40360c3551a6469486044585c63c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.",
            "evidence": [
              "Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$).",
              "FLOAT SELECTED: Table 2: Perplexity scores achieved by the models. Validation set 1 comes from the Cornell dataset, while validation set 2 comes from the DailyDialog dataset."
            ],
            "highlighted_evidence": [
              "Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets.",
              "FLOAT SELECTED: Table 2: Perplexity scores achieved by the models. Validation set 1 comes from the Cornell dataset, while validation set 2 comes from the DailyDialog dataset."
            ]
          }
        ]
      },
      {
        "question": "How does the multi-turn dialog system learns?",
        "question_id": "9cbea686732b5b85f77868ca47d2f93cf34516ed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Usually the probability distribution $p(\\mathbf {y}\\,|\\,\\mathbf {X})$ can be modeled by an RNN language model conditioned on $\\mathbf {X}$. When generating the word $y_t$ at time step $t$, the context $\\mathbf {X}$ is encoded into a fixed-sized dialog context vector $\\mathbf {c}_t$ by following the hierarchical attention structure in HRAN BIBREF13. Additionally, we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution. The overall architecture of the model is depicted in Figure FIGREF4. We are going to elaborate on how to obtain $\\mathbf {c}_t$ and $\\mathbf {e}$, and how they are combined in the decoding part."
            ],
            "highlighted_evidence": [
              "When generating the word $y_t$ at time step $t$, the context $\\mathbf {X}$ is encoded into a fixed-sized dialog context vector $\\mathbf {c}_t$ by following the hierarchical attention structure in HRAN BIBREF13. Additionally, we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution."
            ]
          }
        ]
      },
      {
        "question": "How is human evaluation performed?",
        "question_id": "6aee16c4f319a190c2a451c1c099b66162299a28",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "(1) grammatical correctness",
              "(2) contextual coherence",
              "(3) emotional appropriateness"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For human evaluation of the models, we recruited another four English-speaking students from our university without any relationship to the authors' lab to rate the responses generated by the models. Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared and let them generate the responses. According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria: (1) grammatical correctness—whether or not the response is fluent and free of grammatical mistakes; (2) contextual coherence—whether or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness—whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral."
            ],
            "highlighted_evidence": [
              "According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria: (1) grammatical correctness—whether or not the response is fluent and free of grammatical mistakes; (2) contextual coherence—whether or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness—whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral."
            ]
          }
        ]
      },
      {
        "question": "Is some other metrics other then perplexity measured?",
        "question_id": "4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The evaluation of chatbots remains an open problem in the field. Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work."
            ],
            "highlighted_evidence": [
              "The evaluation of chatbots remains an open problem in the field. Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work."
            ]
          }
        ]
      },
      {
        "question": "What two baseline models are used?",
        "question_id": "180047e1ccfc7c98f093b8d1e1d0479a4cca99cc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " sequence-to-sequence model (denoted as S2S)",
              "HRAN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compared our multi-turn emotionally engaging dialog model (denoted as MEED) with two baselines—the vanilla sequence-to-sequence model (denoted as S2S) and HRAN. We chose S2S and HRAN as baselines because we would like to evaluate our model's capability to keep track of the multi-turn context and to produce emotionally more appropriate responses, respectively. In order to adapt S2S to the multi-turn setting, we concatenate all the history utterances in the context into one."
            ],
            "highlighted_evidence": [
              "We compared our multi-turn emotionally engaging dialog model (denoted as MEED) with two baselines—the vanilla sequence-to-sequence model (denoted as S2S) and HRAN."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iui/XieSP20",
    "dblp_title": "A Multi-Turn Emotionally Engaging Dialog Model.",
    "year": "2020"
  },
  {
    "id": "1703.03097",
    "title": "Information Extraction in Illicit Domains",
    "qas": [
      {
        "question": "Do they evaluate on relation extraction?",
        "question_id": "fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/www/KejriwalS17",
    "dblp_title": "Information Extraction in Illicit Web Domains.",
    "year": "2017"
  },
  {
    "id": "1808.09409",
    "title": "Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data",
    "qas": [
      {
        "question": "What is the baseline model for the agreement-based mode?",
        "question_id": "b5d6357d3a9e3d5fdf9b344ae96cddd11a407875",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "PCFGLA-based parser, viz. Berkeley parser BIBREF5",
              "minimal span-based neural parser BIBREF6"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our second concern is to mimic the human's robust semantic processing ability by computer programs. The feasibility of reusing the annotation specification for L1 implies that we can reuse standard CPB data to train an SRL system to process learner texts. To test the robustness of the state-of-the-art SRL algorithms, we evaluate two types of SRL frameworks. The first one is a traditional SRL system that leverages a syntactic parser and heavy feature engineering to obtain explicit information of semantic roles BIBREF4 . Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 . The other SRL system uses a stacked BiLSTM to implicitly capture local and non-local information BIBREF7 . and we call it the neural syntax-agnostic system. All systems can achieve state-of-the-art performance on L1 texts but show a significant degradation on L2 texts. This highlights the weakness of applying an L1-sentence-trained system to process learner texts."
            ],
            "highlighted_evidence": [
              "Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 ."
            ]
          }
        ]
      },
      {
        "question": "Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?",
        "question_id": "f33a21c6a9c75f0479ffdbb006c40e0739134716",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "syntax-based system may generate correct syntactic analyses for partial grammatical fragments"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While the neural syntax-agnostic system obtains superior performance on the L1 data, the two syntax-based systems both produce better analyses on the L2 data. Furthermore, as illustrated in the comparison between different parsers, the better the parsing results we get, the better the performance on L2 we achieve. This shows that syntactic parsing is important in semantic construction for learner Chinese. The main reason, according to our analysis, is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, which provides crucial information for SRL. Therefore, syntactic parsing helps build more generalizable SRL models that transfer better to new languages, and enhancing syntactic parsing can improve SRL to some extent."
            ],
            "highlighted_evidence": [
              "While the neural syntax-agnostic system obtains superior performance on the L1 data, the two syntax-based systems both produce better analyses on the L2 data. Furthermore, as illustrated in the comparison between different parsers, the better the parsing results we get, the better the performance on L2 we achieve. This shows that syntactic parsing is important in semantic construction for learner Chinese. The main reason, according to our analysis, is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, which provides crucial information for SRL."
            ]
          }
        ]
      },
      {
        "question": "Who manually annotated the semantic roles for the set of learner texts?",
        "question_id": "8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Authors",
            "evidence": [
              "In this paper, we manually annotate the predicate–argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese. It is from the above corpus that we carefully select 600 pairs of L2-L1 parallel sentences. We would choose the most appropriate one among multiple versions of corrections and recorrect the L1s if necessary. Because word structure is very fundamental for various NLP tasks, our annotation also contains gold word segmentation for both L2 and L1 sentences. Note that there are no natural word boundaries in Chinese text. We first employ a state-of-the-art word segmentation system to produce initial segmentation results and then manually fix segmentation errors."
            ],
            "highlighted_evidence": [
              "In this paper, we manually annotate the predicate–argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/LinDZS018",
    "dblp_title": "Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.",
    "year": "2018"
  },
  {
    "id": "1808.00265",
    "title": "Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining",
    "qas": [
      {
        "question": "By how much do they outperform existing state-of-the-art VQA models?",
        "question_id": "17f5f4a5d943c91d46552fb75940b67a72144697",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding."
            ],
            "highlighted_evidence": [
              "Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding."
            ]
          }
        ]
      },
      {
        "question": "How do they measure the correlation between manual groundings and model generated ones?",
        "question_id": "83f22814aaed9b5f882168e22a3eac8f5fda3882",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "rank-correlation BIBREF25"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate the performance of our proposed method using two criteria: i) rank-correlation BIBREF25 to evaluate visual grounding and ii) accuracy to evaluate question answering. Intuitively, rank-correlation measures the similarity between human and model attention maps under a rank-based metric. A high rank-correlation means that the model is `looking at' image areas that agree to the visual information used by a human to answer the same question. In terms of accuracy of a predicted answer INLINEFORM0 is evaluated by: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "We evaluate the performance of our proposed method using two criteria: i) rank-correlation BIBREF25 to evaluate visual grounding and ii) accuracy to evaluate question answering. Intuitively, rank-correlation measures the similarity between human and model attention maps under a rank-based metric. A high rank-correlation means that the model is `looking at' image areas that agree to the visual information used by a human to answer the same question. In terms of accuracy of a predicted answer INLINEFORM0 is evaluated by: DISPLAYFORM0\n\n"
            ]
          }
        ]
      },
      {
        "question": "How do they obtain region descriptions and object annotations?",
        "question_id": "ed11b4ff7ca72dd80a792a6028e16ba20fccff66",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "they are available in the Visual Genome dataset",
            "evidence": [
              "In this work, we introduce a methodology that provides VQA algorithms with the ability to generate human interpretable attention maps which effectively ground the answer to the relevant image regions. We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels. Our framework achieves competitive state-of-the-art VQA performance, while generating visual groundings that outperform other algorithms that use human annotated attention during training."
            ],
            "highlighted_evidence": [
              "In this work, we introduce a methodology that provides VQA algorithms with the ability to generate human interpretable attention maps which effectively ground the answer to the relevant image regions. We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/wacv/ZhangNS19",
    "dblp_title": "Interpretable Visual Question Answering by Visual Grounding From Attention Supervision Mining.",
    "year": "2019"
  },
  {
    "id": "1810.09774",
    "title": "Testing the Generalization Power of Neural Network Models Across NLI Benchmarks",
    "qas": [
      {
        "question": "Which training dataset allowed for the best generalization to benchmark sets?",
        "question_id": "a48c6d968707bd79469527493a72bfb4ef217007",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "MultiNLI",
            "evidence": [
              "FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined."
            ]
          }
        ]
      },
      {
        "question": "Which model generalized the best?",
        "question_id": "b69897deb5fb80bf2adb44f9cbf6280d747271b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Also including a pretrained ELMo language model did not improve the results significantly. The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points. Our baselines for SNLI (90.4%) and SNLI + MultiNLI (90.6%) outperform the previous state-of-the-art accuracy for SNLI (90.1%) by BIBREF24 ."
            ],
            "highlighted_evidence": [
              " The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points."
            ]
          }
        ]
      },
      {
        "question": "Which models were compared?",
        "question_id": "ad1f230f10235413d1fe501e414358245b415476",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",
            "evidence": [
              "For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 . KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by BIBREF1 . The success of pre-trained language models in multiple NLP tasks make ESIM + ELMo and BERT interesting additions to this experiment. Table 3 lists the different models used in the experiments."
            ],
            "highlighted_evidence": [
              "For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 ."
            ]
          }
        ]
      },
      {
        "question": "Which datasets were used?",
        "question_id": "0a521541b9e2b5c6d64fb08eb318778eba8ac9f7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SNLI, MultiNLI and SICK"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We chose three different datasets for the experiments: SNLI, MultiNLI and SICK. All of them have been designed for NLI involving three-way classification with the labels entailment, neutral and contradiction. We did not include any datasets with two-way classification, e.g. SciTail BIBREF14 . As SICK is a relatively small dataset with approximately only 10k sentence pairs, we did not use it as training data in any experiment. We also trained the models with a combined SNLI + MultiNLI training set.",
              "The Stanford Natural Language Inference (SNLI) corpus BIBREF4 is a dataset of 570k human-written sentence pairs manually labeled with the labels entailment, contradiction, and neutral. The source for the premise sentences in SNLI were image captions taken from the Flickr30k corpus BIBREF15 .",
              "The Multi-Genre Natural Language Inference (MultiNLI) corpus BIBREF5 consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral. MultiNLI contains sentence pairs from ten distinct genres of both written and spoken English. Only five genres are included in the training set. The development and test sets have been divided into matched and mismatched, where the former includes only sentences from the same genres as the training data, and the latter includes sentences from the remaining genres not present in the training data.",
              "SICK BIBREF6 is a dataset that was originally constructed to test compositional distributional semantics (DS) models. The dataset contains 9,840 examples pertaining to logical inference (negation, conjunction, disjunction, apposition, relative clauses, etc.). The dataset was automatically constructed taking pairs of sentences from a random subset of the 8K ImageFlickr data set BIBREF15 and the SemEval 2012 STS MSRVideo Description dataset BIBREF16 ."
            ],
            "highlighted_evidence": [
              "We chose three different datasets for the experiments: SNLI, MultiNLI and SICK.",
              "The Stanford Natural Language Inference (SNLI) corpus BIBREF4 is a dataset of 570k human-written sentence pairs manually labeled with the labels entailment, contradiction, and neutral. ",
              "The Multi-Genre Natural Language Inference (MultiNLI) corpus BIBREF5 consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral.",
              "SICK BIBREF6 is a dataset that was originally constructed to test compositional distributional semantics (DS) models. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/blackboxnlp/TalmanC19",
    "dblp_title": "Testing the Generalization Power of Neural Network Models across NLI Benchmarks.",
    "year": "2019"
  },
  {
    "id": "1910.05608",
    "title": "VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination",
    "qas": [
      {
        "question": "What was the baseline?",
        "question_id": "11e376f98df42f487298ec747c32d485c845b5cd",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Is the data all in Vietnamese?",
        "question_id": "284ea817fd79bc10b7a82c88d353e8f8a9d7e93c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs. HSD is required to build a multi-class classification model that is capable of classifying an item to one of 3 classes (hate, offensive, clean). Hate speech (hate): an item is identified as hate speech if it (1) targets individuals or groups on the basis of their characteristics; (2) demonstrates a clear intention to incite harm, or to promote hatred; (3) may or may not use offensive or profane words. Offensive but not hate speech (offensive): an item (posts/comments) may contain offensive words but it does not target individuals or groups on the basis of their characteristics. Neither offensive nor hate speech (clean): normal item, it does not contain offensive language or hate speech.",
              "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16."
            ],
            "highlighted_evidence": [
              "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs.",
              "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type."
            ]
          }
        ]
      },
      {
        "question": "What classifier do they use?",
        "question_id": "c0122190119027dc3eb51f0d4b4483d2dbedc696",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Stacking method",
              "LSTMCNN",
              "SARNN",
              "simple LSTM bidirectional model",
              "TextCNN"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.",
              "The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other.",
              "The second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks. The addition in this model is its residual connection.",
              "The third model is a simple LSTM bidirectional model (figure FIGREF15). It contains multiple LSTM bidirectional blocks stacked to each other.",
              "The fourth model is LSTMCNN (figure FIGREF24). Before going through CNN blocks, series of word embedding will be transformed by LSTM bidirectional block.",
              "The final model is the system named SARNN (figure FIGREF25). It adds an attention block between LTSM blocks.",
              "Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. Have the main three types of ensemble methods including Bagging, Boosting and Stacking. In this system, we use the Stacking method. In this method, the output of each model is not only class id but also the probability of each class in the set of three classes. This probability will become a feature for the ensemble model. The stacking ensemble model here is a simple full-connection model with input is all of probability that output from sub-model. The output is the probability of each class."
            ],
            "highlighted_evidence": [
              " After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13.",
              "The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other.\n\nThe second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks. The addition in this model is its residual connection.\n\nThe third model is a simple LSTM bidirectional model (figure FIGREF15). It contains multiple LSTM bidirectional blocks stacked to each other.\n\nThe fourth model is LSTMCNN (figure FIGREF24). Before going through CNN blocks, series of word embedding will be transformed by LSTM bidirectional block.",
              "The final model is the system named SARNN (figure FIGREF25). It adds an attention block between LTSM blocks.",
              "In this system, we use the Stacking method. In this method, the output of each model is not only class id but also the probability of each class in the set of three classes. This probability will become a feature for the ensemble model. The stacking ensemble model here is a simple full-connection model with input is all of probability that output from sub-model. The output is the probability of each class."
            ]
          }
        ]
      },
      {
        "question": "What is private dashboard?",
        "question_id": "1ed6acb88954f31b78d2821bb230b722374792ed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).",
            "evidence": [
              "For each model having the best fit on the dev set, we export the probability distribution of classes for each sample in the dev set. In this case, we only use the result of model that has f1_macro score that larger than 0.67. The probability distribution of classes is then used as feature to input into a dense model with only one hidden layer (size 128). The training process of the ensemble model is done on samples of the dev set. The best fit result is 0.7356. The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
            ],
            "highlighted_evidence": [
              "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
            ]
          }
        ]
      },
      {
        "question": "What is public dashboard?",
        "question_id": "5a33ec23b4341584a8079db459d89a4e23420494",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",
            "evidence": [
              "For each model having the best fit on the dev set, we export the probability distribution of classes for each sample in the dev set. In this case, we only use the result of model that has f1_macro score that larger than 0.67. The probability distribution of classes is then used as feature to input into a dense model with only one hidden layer (size 128). The training process of the ensemble model is done on samples of the dev set. The best fit result is 0.7356. The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
            ],
            "highlighted_evidence": [
              "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use?",
        "question_id": "1b9119813ea637974d21862a8ace83bc1acbab8e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).",
            "evidence": [
              "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.",
              "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."
            ],
            "highlighted_evidence": [
              "Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7",
              "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-05608",
    "dblp_title": "VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination.",
    "year": "2019"
  },
  {
    "id": "1906.07668",
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "qas": [
      {
        "question": "Do the authors report results only on English data?",
        "question_id": "8abb96b2450ebccfcc5c98772cec3d86cd0f53e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 1: Topics and top-10 keywords of the corresponding topic",
              "FLOAT SELECTED: Figure 5: Visualization using pyLDAVis. Best viewed in electronic format (zoomed in)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Topics and top-10 keywords of the corresponding topic",
              "FLOAT SELECTED: Figure 5: Visualization using pyLDAVis. Best viewed in electronic format (zoomed in)."
            ]
          }
        ]
      },
      {
        "question": "What other interesting correlations are observed?",
        "question_id": "f52ec4d68de91dba66668f0affc198706949ff90",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Women-Yoga"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'."
            ],
            "highlighted_evidence": [
              "We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'.",
              "Women-Yoga"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1906-07668",
    "dblp_title": "Yoga-Veganism: Correlation Mining of Twitter Health Data.",
    "year": "2019"
  },
  {
    "id": "1605.04655",
    "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment",
    "qas": [
      {
        "question": "what were the baselines?",
        "question_id": "225a567eeb2698a9d3f1024a8b270313a6d15f82",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "RNN model",
              "CNN model ",
              "RNN-CNN model",
              "attn1511 model",
              "Deep Averaging Network model",
              "avg mean of word embeddings in the sentence with projection matrix"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We refer the reader to BIBREF6 and its references for detailed model descriptions. We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 ."
            ],
            "highlighted_evidence": [
              "We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 ."
            ]
          }
        ]
      },
      {
        "question": "what is the state of the art for ranking mc test answers?",
        "question_id": "35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "ensemble of hand-crafted syntactic and frame-semantic features BIBREF16"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the MCTest dataset, Fig. FIGREF30 compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features BIBREF16 , as well as past neural models from the literature, all using attention mechanisms — the Attentive Reader of BIBREF26 , Neural Reasoner of BIBREF27 and the HABCNN model family of BIBREF17 . We see that averaging-based models are surprisingly effective on this task, and in particular on the MC-500 dataset it can beat even the best so far reported model of HABCNN-TE. Our proposed transfer model is statistically equivalent to the best model on both datasets (furthermore, previous work did not include confidence intervals, even though their models should also be stochastically initialized)."
            ],
            "highlighted_evidence": [
              "For the MCTest dataset, Fig. FIGREF30 compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features BIBREF16 , as well as past neural models from the literature, all using attention mechanisms — the Attentive Reader of BIBREF26 , Neural Reasoner of BIBREF27 and the HABCNN model family of BIBREF17 . "
            ]
          }
        ]
      },
      {
        "question": "what is the size of the introduced dataset?",
        "question_id": "f5eac66c08ebec507c582a2445e99317a83e9ebe",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "what datasets did they use?",
        "question_id": "62613aca3d7c7d534c9f6d8cb91ff55626bb8695",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Argus Dataset",
              "AI2-8grade/CK12 Dataset",
              "MCTest Dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Argus Dataset",
              "AI2-8grade/CK12 Dataset",
              "We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0). However, we released it to the public and still included it in the comparison as these qualities reflect many realistic datasets of unknown qualities, so we find relative performances of models on such datasets instructive.",
              "MCTest Dataset",
              "The Machine Comprehension Test BIBREF8 dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehension, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation. The dataset consists of a set of 660 stories spanning multiple sentences, written in simple and clean language (but with less restricted vocabulary than e.g. the bAbI dataset BIBREF9 ). Each story is accompanied by four questions and each of these lists four possible answers; the questions are tagged as based on just one in-story sentence, or requiring multiple sentence inference. We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers."
            ],
            "highlighted_evidence": [
              "Argus Dataset",
              "AI2-8grade/CK12 Dataset",
              "We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0). ",
              "MCTest Dataset",
              "We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/rep4nlp/BaudisSS16",
    "dblp_title": "Joint Learning of Sentence Embeddings for Relevance and Entailment.",
    "year": "2016"
  },
  {
    "id": "1911.09483",
    "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
    "qas": [
      {
        "question": "What evaluation metric is used?",
        "question_id": "6e4505609a280acc45b0a821755afb1b3b518ffd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The BLEU metric "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of BIBREF14. We do not tune beam width and length penalty but use the setting reported in BIBREF0. The BLEU metric is adopted to evaluate the model performance during evaluation."
            ],
            "highlighted_evidence": [
              "The BLEU metric is adopted to evaluate the model performance during evaluation."
            ]
          }
        ]
      },
      {
        "question": "What datasets are used?",
        "question_id": "9bd938859a8b063903314a79f09409af8801c973",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WMT14 En-Fr and En-De datasets",
              "IWSLT De-En and En-Vi datasets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. We use the standard split of development set and test set. We use newstest2014 as the test set and use newstest2012 +newstest2013 as the development set. Following BIBREF11, we also adopt a joint source and target BPE factorization with the vocabulary size of $40K$. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. The test and validation datasets we used are the same as BIBREF0.",
              "IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs. For the En-Vi task, we build a dictionary including all source and target tokens. The vocabulary size for English is $17.2K$, and the vocabulary size for the Vietnamese is $6.8K$."
            ],
            "highlighted_evidence": [
              "WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model.",
              "For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$.",
              "IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs."
            ]
          }
        ]
      },
      {
        "question": "What are three main machine translation tasks?",
        "question_id": "68ba5bf18f351e8c83fae7b444cc50bef7437f13",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "De-En, En-Fr and En-Vi translation tasks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of BIBREF14. We do not tune beam width and length penalty but use the setting reported in BIBREF0. The BLEU metric is adopted to evaluate the model performance during evaluation."
            ],
            "highlighted_evidence": [
              "During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks."
            ]
          }
        ]
      },
      {
        "question": "How big is improvement in performance over Transformers?",
        "question_id": "f6a1125c5621a2f32c9bcdd188dff14efa096083",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2.2 BLEU gains"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.",
              "Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation."
            ],
            "highlighted_evidence": [
              "As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.\n\nCompared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-09483",
    "dblp_title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning.",
    "year": "2019"
  },
  {
    "id": "1805.00760",
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "qas": [
      {
        "question": "How do they determine the opinion summary?",
        "question_id": "282aa4e160abfa7569de7d99b8d45cabee486ba4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the weighted sum of the new opinion representations, according to their associations with the current aspect representation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step."
            ],
            "highlighted_evidence": [
              "As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step."
            ]
          }
        ]
      },
      {
        "question": "Do they explore how useful is the detection history and opinion summary?",
        "question_id": "ecfb2e75eb9a8eba8f640a039484874fa0d2fceb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Ablation Study",
              "To further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed."
            ],
            "highlighted_evidence": [
              "Ablation Study\nTo further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed."
            ]
          }
        ]
      },
      {
        "question": "Which dataset(s) do they use to train the model?",
        "question_id": "a6950c22c7919f86b16384facc97f2cf66e5941d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To evaluate the effectiveness of the proposed framework for the ATE task, we conduct experiments over four benchmark datasets from the SemEval ABSA challenge BIBREF1 , BIBREF18 , BIBREF12 . Table TABREF24 shows their statistics. INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain. In these datasets, aspect terms have been labeled by the task organizer."
            ],
            "highlighted_evidence": [
              "To evaluate the effectiveness of the proposed framework for the ATE task, we conduct experiments over four benchmark datasets from the SemEval ABSA challenge BIBREF1 , BIBREF18 , BIBREF12 . Table TABREF24 shows their statistics. INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain. In these datasets, aspect terms have been labeled by the task organizer."
            ]
          }
        ]
      },
      {
        "question": "By how much do they outperform state-of-the-art methods?",
        "question_id": "54be3541cfff6574dba067f1e581444537a417db",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.",
              "Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours). We find that INLINEFORM2 and INLINEFORM3 contain many informal reviews, thus RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts."
            ],
            "highlighted_evidence": [
              "As shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.\n\nOur framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours). We find that INLINEFORM2 and INLINEFORM3 contain many informal reviews, thus RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcai/LiBLLY18",
    "dblp_title": "Aspect Term Extraction with History Attention and Selective Transformation.",
    "year": "2018"
  },
  {
    "id": "1909.05358",
    "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
    "qas": [
      {
        "question": "What is the average number of turns per dialog?",
        "question_id": "221e9189a9d2431902d8ea833f486a38a76cbd8e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The average number of utterances per dialog is about 23 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors."
            ],
            "highlighted_evidence": [
              "Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors."
            ]
          }
        ]
      },
      {
        "question": "What baseline models are offered?",
        "question_id": "a276d5931b989e0a33f2a0bc581456cca25658d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "3-gram and 4-gram conditional language model",
              "Convolution",
              "LSTM models BIBREF27 with and without attention BIBREF28",
              "Transformer",
              "GPT-2"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model.",
              "Convolution: We use the fconv architecture BIBREF24 and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2.",
              "LSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors.",
              "Transformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2.",
              "GPT-2: Apart from supervised seq2seq models, we also include results from pre-trained GPT-2 BIBREF30 containing 117M parameters."
            ],
            "highlighted_evidence": [
              "n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model.\n\nConvolution: We use the fconv architecture BIBREF24 and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2.\n\nLSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors.\n\nTransformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2.\n\nGPT-2: Apart from supervised seq2seq models, we also include results from pre-trained GPT-2 BIBREF30 containing 117M parameters."
            ]
          }
        ]
      },
      {
        "question": "Which six domains are covered in the dataset?",
        "question_id": "c21d26130b521c9596a1edd7b9ef3fe80a499f1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. For the spoken dialogs, we created a “Wizard of Oz” (WOz) system BIBREF12 to collect two-person, spoken conversations. Crowdsourced workers playing the “user\" interacted with human operators playing the “digital assistant” using a web-based interface. In this way, users were led to believe they were interacting with an automated system while it was in fact a human, allowing them to express their turns in natural ways but in the context of an automated interface. We refer to this spoken dialog type as “two-person dialogs\". For the written dialogs, we engaged crowdsourced workers to write the full conversation themselves based on scenarios outlined for each task, thereby playing roles of both the user and assistant. We refer to this written dialog type as “self-dialogs\". In a departure from traditional annotation techniques BIBREF10, BIBREF8, BIBREF13, dialogs are labeled with simple API calls and arguments. This technique is much easier for annotators to learn and simpler to apply. As such it is more cost effective and, in addition, the same model can be used for multiple service providers."
            ],
            "highlighted_evidence": [
              "To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/ByrneKSNGDYDKC19",
    "dblp_title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset.",
    "year": "2019"
  },
  {
    "id": "2003.06279",
    "title": "Using word embeddings to improve the discriminability of co-occurrence text networks",
    "qas": [
      {
        "question": "What other natural processing tasks authors think could be studied by using word embeddings?",
        "question_id": "ec8043290356fcb871c2f5d752a9fe93a94c2f71",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "general classification tasks",
              "use of the methodology in other networked systems",
              "a network could be enriched with embeddings obtained from graph embeddings techniques"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our findings paves the way for research in several new directions. While we probed the effectiveness of virtual edges in a specific text classification task, we could extend this approach for general classification tasks. A systematic comparison of embeddings techniques could also be performed to include other recent techniques BIBREF54, BIBREF55. We could also identify other relevant techniques to create virtual edges, allowing thus the use of the methodology in other networked systems other than texts. For example, a network could be enriched with embeddings obtained from graph embeddings techniques. A simpler approach could also consider link prediction BIBREF56 to create virtual edges. Finally, other interesting family of studies concerns the discrimination between co-occurrence and virtual edges, possibly by creating novel network measurements considering heterogeneous links."
            ],
            "highlighted_evidence": [
              "Our findings paves the way for research in several new directions. While we probed the effectiveness of virtual edges in a specific text classification task, we could extend this approach for general classification tasks. A systematic comparison of embeddings techniques could also be performed to include other recent techniques BIBREF54, BIBREF55. We could also identify other relevant techniques to create virtual edges, allowing thus the use of the methodology in other networked systems other than texts. For example, a network could be enriched with embeddings obtained from graph embeddings techniques. A simpler approach could also consider link prediction BIBREF56 to create virtual edges. Finally, other interesting family of studies concerns the discrimination between co-occurrence and virtual edges, possibly by creating novel network measurements considering heterogeneous links."
            ]
          }
        ]
      },
      {
        "question": "What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?",
        "question_id": "728c2fb445173fe117154a2a5482079caa42fe24",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In a more practical scenario, text networks have been used in text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.",
              "While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. In addition, semantically similar words not sharing the same lemma are mapped into distinct nodes. In order to address these issues, here we introduce a modification of the traditional network representation by establishing additional edges, referred to as “virtual” edges. In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar. While this approach still does not merge similar nodes into the same concept, similar nodes are explicitly linked via virtual edges."
            ],
            "highlighted_evidence": [
              "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.\n\nWhile the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12."
            ]
          }
        ]
      },
      {
        "question": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
        "question_id": "23d32666dfc29ed124f3aa4109e2527efa225fbc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They use it as addition to previous model - they add new edge between words if word embeddings are similar.",
            "evidence": [
              "While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. In addition, semantically similar words not sharing the same lemma are mapped into distinct nodes. In order to address these issues, here we introduce a modification of the traditional network representation by establishing additional edges, referred to as “virtual” edges. In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar. While this approach still does not merge similar nodes into the same concept, similar nodes are explicitly linked via virtual edges."
            ],
            "highlighted_evidence": [
              "In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar."
            ]
          }
        ]
      },
      {
        "question": "On what model architectures are previous co-occurence networks based?",
        "question_id": "076928bebde4dffcb404be216846d9d680310622",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window",
              "connects only adjacent words in the so called word adjacency networks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In a more practical scenario, text networks have been used in text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks."
            ],
            "highlighted_evidence": [
              "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2003-06279",
    "dblp_title": "Using word embeddings to improve the discriminability of co-occurrence text networks.",
    "year": "2020"
  },
  {
    "id": "2004.03744",
    "title": "e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations",
    "qas": [
      {
        "question": "Is model explanation output evaluated, what metric was used?",
        "question_id": "f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "balanced accuracy, i.e., the average of the three accuracies on each class"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Finally, we note that only about 62% of the originally neutral pairs remain neutral, while 21% become contradiction and 17% entailment pairs. Therefore, we are now facing an imbalance between the neutral, entailment, and contradiction instances in the validation and testing sets of SNLI-VE-2.0. The neutral class becomes underrepresented and the label distributions in the corrected validation and testing sets both become E / N / C: 39% / 20% / 41%. To account for this, we compute the balanced accuracy, i.e., the average of the three accuracies on each class."
            ],
            "highlighted_evidence": [
              "To account for this, we compute the balanced accuracy, i.e., the average of the three accuracies on each class."
            ]
          }
        ]
      },
      {
        "question": "How many annotators are used to write natural language explanations to SNLI-VE-2.0?",
        "question_id": "66bf0d61ffc321f15e7347aaed191223f4ce4b4a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2,060 workers"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used Amazon Mechanical Turk (MTurk) to collect new labels and explanations for SNLI-VE. 2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. We required the workers to have a previous approval rate above 90%. No restriction was put on the workers' location."
            ],
            "highlighted_evidence": [
              "We used Amazon Mechanical Turk (MTurk) to collect new labels and explanations for SNLI-VE. 2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54."
            ]
          }
        ]
      },
      {
        "question": "How many natural language explanations are human-written?",
        "question_id": "5dfa59c116e0ceb428efd99bab19731aa3df4bbd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Totally 6980 validation and test image-sentence pairs have been corrected.",
            "evidence": [
              "e-SNLI-VE-2.0 is the combination of SNLI-VE-2.0 with explanations from either e-SNLI or our crowdsourced annotations where applicable. The statistics of e-SNLI-VE-2.0 are shown in Table TABREF40.",
              "FLOAT SELECTED: Table 3. Summary of e-SNLI-VE-2.0 (= SNLI-VE-2.0 + explanations). Image-sentence pairs labelled as neutral in the training set have not been corrected."
            ],
            "highlighted_evidence": [
              "The statistics of e-SNLI-VE-2.0 are shown in Table TABREF40.",
              "FLOAT SELECTED: Table 3. Summary of e-SNLI-VE-2.0 (= SNLI-VE-2.0 + explanations). Image-sentence pairs labelled as neutral in the training set have not been corrected."
            ]
          }
        ]
      },
      {
        "question": "How much is performance difference of existing model between original and corrected corpus?",
        "question_id": "0c557b408183630d1c6c325b5fb9ff1573661290",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The same BUTD model that achieves 73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set from SNLI-VE-2.0. Hence, for this model, we do not notice a significant difference in performance. This could be due to randomness. Finally, when we run the training loop again, this time doing the model selection on the corrected validation set from SNLI-VE-2.0, we obtain a slightly worse performance of 72.52%, although the difference is not clearly significant."
            ],
            "highlighted_evidence": [
              "The same BUTD model that achieves 73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set from SNLI-VE-2.0. Hence, for this model, we do not notice a significant difference in performance. This could be due to randomness."
            ]
          }
        ]
      },
      {
        "question": "What is the class with highest error rate in SNLI-VE?",
        "question_id": "a08b5018943d4428f067c08077bfff1af3de9703",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "neutral class"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Xie also propose the SNLI-VE dataset as the first dataset for VTE. SNLI-VE is built from the textual entailment SNLI dataset BIBREF0 by replacing textual premises with the Flickr30k images that they originally described BIBREF2. However, images contain more information than their descriptions, which may entail or contradict the textual hypotheses (see Figure FIGREF3). As a result, the neutral class in SNLI-VE has substantial labelling errors. Vu BIBREF3 estimated ${\\sim }31\\%$ errors in this class, and ${\\sim }1\\%$ for the contradiction and entailment classes."
            ],
            "highlighted_evidence": [
              "As a result, the neutral class in SNLI-VE has substantial labelling errors. Vu BIBREF3 estimated ${\\sim }31\\%$ errors in this class, and ${\\sim }1\\%$ for the contradiction and entailment classes."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2004-03744",
    "dblp_title": "e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations.",
    "year": "2020"
  },
  {
    "id": "2001.09332",
    "title": "An Analysis of Word2Vec for the Italian Language",
    "qas": [
      {
        "question": "What is the dataset used as input to the Word2Vec algorithm?",
        "question_id": "9447ec36e397853c04dcb8f67492ca9f944dbd4b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words",
            "evidence": [
              "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences.",
              "The text was previously preprocessed by removing the words whose absolute frequency was less than 5 and eliminating all special characters. Since it is impossible to represent every imaginable numerical value, but not wanting to eliminate the concept of “numerical representation\" linked to certain words, it was also decided to replace every number present in the text with the particular $\\langle NUM \\rangle $ token; which probably also assumes a better representation in the embedding space (not separating into the various possible values). All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words."
            ],
            "highlighted_evidence": [
              "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences.",
              "All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words."
            ]
          }
        ]
      },
      {
        "question": "Are the word embeddings tested on a NLP task?",
        "question_id": "12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To analyse the results we chose to use the test provided by BIBREF10, which consists of $19\\,791$ analogies divided into 19 different categories: 6 related to the “semantic\" macro-area (8915 analogies) and 13 to the “syntactic\" one (10876 analogies). All the analogies are composed by two pairs of words that share a relation, schematized with the equation: $a:a^{*}=b:b^{*}$ (e.g. “man : woman = king : queen\"); where $b^{*}$ is the word to be guessed (“queen\"), $b$ is the word coupled to it (“king\"), $a$ is the word for the components to be eliminated (“man\"), and $a^{*}$ is the word for the components to be added (“woman\")."
            ],
            "highlighted_evidence": [
              "To analyse the results we chose to use the test provided by BIBREF10, which consists of $19\\,791$ analogies divided into 19 different categories: 6 related to the “semantic\" macro-area (8915 analogies) and 13 to the “syntactic\" one (10876 analogies). All the analogies are composed by two pairs of words that share a relation, schematized with the equation: $a:a^{*}=b:b^{*}$ (e.g. “man : woman = king : queen\"); where $b^{*}$ is the word to be guessed (“queen\"), $b$ is the word coupled to it (“king\"), $a$ is the word for the components to be eliminated (“man\"), and $a^{*}$ is the word for the components to be added (“woman\")."
            ]
          }
        ]
      },
      {
        "question": "Are the word embeddings evaluated?",
        "question_id": "32c149574edf07b1a96d7f6bc49b95081de1abd2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).",
              "As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs."
            ],
            "highlighted_evidence": [
              "Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).\n\nAs it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas."
            ]
          }
        ]
      },
      {
        "question": "How big is dataset used to train Word2Vec for the Italian Language?",
        "question_id": "3de27c81af3030eb2d9de1df5ec9bfacdef281a9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences."
            ],
            "highlighted_evidence": [
              "The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences."
            ]
          }
        ]
      },
      {
        "question": "How does different parameter settings impact the performance and semantic capacity of resulting model?",
        "question_id": "cc680cb8f45aeece10823a3f8778cf215ccc8af0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work we have analysed the Word2Vec model for Italian Language obtaining a substantial increase in performance respect to other two models in the literature (and despite the fixed size of the embedding). These results, in addition to the number of learning epochs, are probably also due to the different phase of data pre-processing, very carefully excuted in performing a complete cleaning of the text and above all in substituting the numerical values with a single particular token. We have observed that the number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others."
            ],
            "highlighted_evidence": [
              "We have observed that the number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others."
            ]
          }
        ]
      },
      {
        "question": "Are the semantic analysis findings for Italian language similar to English language version?",
        "question_id": "fab4ec639a0ea1e07c547cdef1837c774ee1adb8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What dataset is used for training Word2Vec in Italian language?",
        "question_id": "9190c56006ba84bf41246a32a3981d38adaf422c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences."
            ],
            "highlighted_evidence": [
              "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2001-09332",
    "dblp_title": "An Analysis of Word2Vec for the Italian Language.",
    "year": "2020"
  },
  {
    "id": "1804.06506",
    "title": "Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation",
    "qas": [
      {
        "question": "How are the auxiliary signals from the morphology table incorporated in the decoder?",
        "question_id": "7aab78e90ba1336950a2b0534cc0cb214b96b4fd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "an additional morphology table including target-side affixes.",
              "We inject the decoder with morphological properties of the target language."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models."
            ],
            "highlighted_evidence": [
              "In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. "
            ]
          }
        ]
      },
      {
        "question": "What type of morphological information is contained in the \"morphology table\"?",
        "question_id": "b7fe91e71da8f4dc11e799b3bd408d253230e8c6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "target-side affixes"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models."
            ],
            "highlighted_evidence": [
              "In the first scenario we equip the decoder with an additional morphology table including target-side affixes."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/PassbanLW18",
    "dblp_title": "Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation.",
    "year": "2018"
  },
  {
    "id": "1904.07342",
    "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "16fa6896cf4597154363a6c9a98deb49fffef15f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We henceforth refer to a tweet affirming climate change as a “positive\" sample (labeled as 1 in the data), and a tweet denying climate change as a “negative\" sample (labeled as -1 in the data). All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). We refer to the first data batch as “influential\" tweets, and the second data batch as “event-related\" tweets."
            ],
            "highlighted_evidence": [
              "All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.). "
            ]
          }
        ]
      },
      {
        "question": "Do the authors mention any confounds to their study?",
        "question_id": "0f60864503ecfd5b048258e21d548ab5e5e81772",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters."
            ],
            "highlighted_evidence": [
              "There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters."
            ]
          }
        ]
      },
      {
        "question": "Which machine learning models are used?",
        "question_id": "fe578842021ccfc295209a28cf2275ca18f8d155",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "RNNs",
              "CNNs",
              "Naive Bayes with Laplace Smoothing",
              "k-clustering",
              "SVM with linear kernel"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our first goal is to train a sentiment analysis model (on training and validation datasets) in order to perform classification inference on event-based tweets. We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 ."
            ],
            "highlighted_evidence": [
              " Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). "
            ]
          }
        ]
      },
      {
        "question": "What methodology is used to compensate for limited labelled data?",
        "question_id": "00ef9cc1d1d60f875969094bb246be529373cb1d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.",
            "evidence": [
              "The first data batch consists of tweets relevant to blizzards, hurricanes, and wildfires, under the constraint that they are tweeted by “influential\" tweeters, who we define as individuals certain to have a classifiable sentiment regarding the topic at hand. For example, we assume that any tweet composed by Al Gore regarding climate change is a positive sample, whereas any tweet from conspiracy account @ClimateHiJinx is a negative sample. The assumption we make in ensuing methods (confirmed as reasonable in Section SECREF2 ) is that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets. Here, we enforce binary labels for all tweets composed by each of the 133 influential tweeters that we identified on Twitter (87 of whom accept climate change), yielding a total of 16,360 influential tweets."
            ],
            "highlighted_evidence": [
              "The first data batch consists of tweets relevant to blizzards, hurricanes, and wildfires, under the constraint that they are tweeted by “influential\" tweeters, who we define as individuals certain to have a classifiable sentiment regarding the topic at hand. For example, we assume that any tweet composed by Al Gore regarding climate change is a positive sample, whereas any tweet from conspiracy account @ClimateHiJinx is a negative sample. The assumption we make in ensuing methods (confirmed as reasonable in Section SECREF2 ) is that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets. "
            ]
          }
        ]
      },
      {
        "question": "Which five natural disasters were examined?",
        "question_id": "279b633b90fa2fd69e84726090fadb42ebdf4c02",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the East Coast Bomb Cyclone",
              " the Mendocino, California wildfires",
              "Hurricane Florence",
              "Hurricane Michael",
              "the California Camp Fires"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). For each disaster, we scraped tweets starting from two weeks prior to the beginning of the event, and continuing through two weeks after the end of the event. Summary statistics on the downloaded event-specific tweets are provided in Table TABREF1 . Note that the number of tweets occurring prior to the two 2018 sets of California fires are relatively small. This is because the magnitudes of these wildfires were relatively unpredictable, whereas blizzards and hurricanes are often forecast weeks in advance alongside public warnings. The first (influential tweet data) and second (event-related tweet data) batches are de-duplicated to be mutually exclusive. In Section SECREF2 , we perform geographic analysis on the event-related tweets from which we can scrape self-reported user city from Twitter user profile header cards; overall this includes 840 pre-event and 5,984 post-event tweets."
            ],
            "highlighted_evidence": [
              "The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icwsm/KoeneckeF20",
    "dblp_title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data.",
    "year": "2020"
  },
  {
    "id": "2001.06888",
    "title": "A multimodal deep learning approach for named entity recognition from social media",
    "qas": [
      {
        "question": "Which social media platform is explored?",
        "question_id": "0106bd9d54e2f343cc5f30bb09a5dbdd171e964b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "twitter "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets."
            ],
            "highlighted_evidence": [
              "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets.\n\n"
            ]
          }
        ]
      },
      {
        "question": "What datasets did they use?",
        "question_id": "e015d033d4ee1c83fe6f192d3310fb820354a553",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF8 a refined collection of tweets gathered from twitter"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets."
            ],
            "highlighted_evidence": [
              "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets.\n\n"
            ]
          }
        ]
      },
      {
        "question": "What are the baseline state of the art models?",
        "question_id": "8a871b136ccef78391922377f89491c923a77730",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",
            "evidence": [
              "FLOAT SELECTED: Table 3: Evaluation results of different approaches compared to ours"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Evaluation results of different approaches compared to ours"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/nca/Asgari-Chenaghlu22",
    "dblp_title": "CWI: A multimodal deep learning approach for named entity recognition from social media using character, word and image features.",
    "year": "2022"
  },
  {
    "id": "1911.00547",
    "title": "Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization",
    "qas": [
      {
        "question": "What is the size of the dataset?",
        "question_id": "acd05f31e25856b9986daa1651843b8dc92c2d99",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " 9,892 stories of sexual harassment incidents"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser\", “time\", “location\", “trigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below."
            ],
            "highlighted_evidence": [
              "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser\", “time\", “location\", “trigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below."
            ]
          }
        ]
      },
      {
        "question": "What model did they use?",
        "question_id": "8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "2. We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6."
            ],
            "highlighted_evidence": [
              "We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6."
            ]
          }
        ]
      },
      {
        "question": "What patterns were discovered from the stories?",
        "question_id": "af60462881b2d723adeb4acb5fbc07ea27b6bde2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we demonstrate that harassment occurred more frequently during the night time than the day time",
              "it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives",
              "we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) ",
              "We also found that the majority of young perpetrators engaged in harassment behaviors on the streets",
              "we found that adult perpetrators of sexual harassment are more likely to act alone",
              "we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location ",
              "commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.",
              "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.",
              "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."
            ],
            "highlighted_evidence": [
              "We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.",
              "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). ",
              "We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. ",
              "In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.",
              "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators.",
              "In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."
            ]
          }
        ]
      },
      {
        "question": "Did they use a crowdsourcing platform?",
        "question_id": "879bec20c0fdfda952444018e9435f91e34d8788",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": [
              " foogreen!0.11444976553320885 foothe foogreen!0.013002995729038958 foomicro foogreen!0.016201976904994808 foobus foogreen!0.14046543219592422 fooand foogreen!0.12413455988280475 foothere foogreen!0.18423641449771821 foowas foogreen!0.3394613158889115 fooa foogreen!1.0372470133006573 foogirl foogreen!0.20553644571918994 fooopposite foogreen!0.2821453963406384 footo foogreen!0.5574009846895933 foome foogreen!0.2709480468183756 foojust foogreen!0.2582515007816255 foothen foogreen!0.9223996312357485 fooa",
              " foogreen!0.11444976553320885 foothe foogreen!0.013002995729038958 foomicro foogreen!0.016201976904994808 foobus foogreen!0.14046543219592422 fooand foogreen!0.12413455988280475 foothere foogreen!0.18423641449771821 foowas foogreen!0.3394613158889115 fooa foogreen!1.0372470133006573 foogirl foogreen!0.20553644571918994 fooopposite foogreen!0.2821453963406384 footo foogreen!0.5574009846895933 foome foogreen!0.2709480468183756 foojust foogreen!0.2582515007816255 foothen foogreen!0.9223996312357485 fooa"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/LiuLCLZS19",
    "dblp_title": "Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization.",
    "year": "2019"
  },
  {
    "id": "1604.00117",
    "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding",
    "qas": [
      {
        "question": "Does the performance increase using their method?",
        "question_id": "3c378074111a6cc7319c0db0aced5752c30bfffb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The multi-task model outperforms the single-task model at all data sizes",
              "but none have an overall benefit from the open vocabulary system"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In Figure 1 we show the single-task vs. multi-task model performance for each of three different applications. The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%. Because the performance of the multi-task model decays much more slowly as the amount of training data is reduced, the multi-task model can deliver the same performance with a considerable reduction in the amount of labeled data.",
              "Table 4 reports F1 scores on the test set for both the closed and open vocabulary systems. The results differ between the tasks, but none have an overall benefit from the open vocabulary system. Looking at the subset of sentences that contain an OOV token, the open vocabulary system delivers increased performance on the Airbnb and Greyhound tasks. These two are the most difficult apps out of the four and therefore had the most room for improvement. The United app is also all lower case and casing is an important clue for detecting proper nouns that the open vocabulary model takes advantage of."
            ],
            "highlighted_evidence": [
              "The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%.",
              "The results differ between the tasks, but none have an overall benefit from the open vocabulary system."
            ]
          }
        ]
      },
      {
        "question": "What tasks are they experimenting with in this paper?",
        "question_id": "b464bc48f176a5945e54051e3ffaea9a6ad886d7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Slot filling",
              "we consider the actions that a user might perform via apps on their phone",
              "The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action. For example, dates, departure cities and destinations represent slots to fill in a flight booking task. This information is extracted from natural language queries leveraging typical context associated with each slot type. Researchers have been exploring data-driven approaches to learning models for automatic identification of slot information since the 90's, and significant advances have been made BIBREF0 . Our paper builds on recent work on slot-filling using recurrent neural networks (RNNs) with a focus on the problem of training from minimal annotated data, taking an approach of sharing data from multiple tasks to reduce the amount of data for developing a new task.",
              "As candidate tasks, we consider the actions that a user might perform via apps on their phone. Typically, a separate slot-filling model would be trained for each app. For example, one model understands queries about classified ads for cars BIBREF1 and another model handles queries about the weather BIBREF2 . As the number of apps increases, this approach becomes impractical due to the burden of collecting and labeling the training data for each model. In addition, using independent models for each task has high storage costs for mobile devices.",
              "Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant. In order to elicit natural language, crowd workers were instructed to simulate a conversation with a friend planning an activity as opposed to giving a command to the computer. Workers were prompted with a slot type/value pair and asked to form a reply to their friend using that information. The instructions were to not include any other potential slots in the sentence but this instruction was not always followed by the workers."
            ],
            "highlighted_evidence": [
              "Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action",
              "As candidate tasks, we consider the actions that a user might perform via apps on their phone.",
              "Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant."
            ]
          }
        ]
      },
      {
        "question": "What is the size of the open vocabulary?",
        "question_id": "3b40799f25dbd98bba5b526e0a1d0d0bb51173e0",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/JaechHO16",
    "dblp_title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding.",
    "year": "2016"
  },
  {
    "id": "1908.06725",
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "qas": [
      {
        "question": "How do they select answer candidates for their QA task?",
        "question_id": "3c16d4cf5dc23223980d9c0f924cb9e4e6943f13",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "AMS method."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Each training sample is generated by three steps: align, mask and select, which we call as AMS method. Each sample in the dataset consists of a question and several candidate answers, which has the same form as the CommonsenseQA dataset. An example of constructing one training sample by masking concept $_2$ is shown in Table 2 ."
            ],
            "highlighted_evidence": [
              "Each training sample is generated by three steps: align, mask and select, which we call as AMS method. Each sample in the dataset consists of a question and several candidate answers, which has the same form as the CommonsenseQA dataset."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-06725",
    "dblp_title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models.",
    "year": "2019"
  },
  {
    "id": "1604.05781",
    "title": "What we write about when we write about causality: Features of causal statements across large-scale social discourse",
    "qas": [
      {
        "question": "How do they extract causality from text?",
        "question_id": "4c822bbb06141433d04bbc472f08c48bc8378865",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",
            "evidence": [
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively."
            ],
            "highlighted_evidence": [
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'."
            ]
          }
        ]
      },
      {
        "question": "What is the source of the \"control\" corpus?",
        "question_id": "1baf87437b70cc0375b8b7dc2cfc2830279bc8b5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Randomly selected from a Twitter dump, temporally matched to causal documents",
            "evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work.",
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively."
            ],
            "highlighted_evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.",
              "Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present."
            ]
          }
        ]
      },
      {
        "question": "What are the selection criteria for \"causal statements\"?",
        "question_id": "0b31eb5bb111770a3aaf8a3931d8613e578e07a8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Presence of only the exact unigrams 'caused', 'causing', or 'causes'",
            "evidence": [
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively."
            ],
            "highlighted_evidence": [
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'."
            ]
          }
        ]
      },
      {
        "question": "Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",
        "question_id": "7348e781b2c3755b33df33f4f0cab4b94fcbeb9b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Only automatic methods",
            "evidence": [
              "The rest of this paper is organized as follows: In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. \"Results\" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. \"Discussion\" ."
            ],
            "highlighted_evidence": [
              "In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with."
            ]
          }
        ]
      },
      {
        "question": "how do they collect the comparable corpus?",
        "question_id": "f68bd65b5251f86e1ed89f0c858a8bb2a02b233a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Randomly from a Twitter dump",
            "evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work.",
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively."
            ],
            "highlighted_evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.",
              "Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present."
            ]
          }
        ]
      },
      {
        "question": "How do they collect the control corpus?",
        "question_id": "e111925a82bad50f8e83da274988b9bea8b90005",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Randomly from Twitter",
            "evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work.",
              "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively."
            ],
            "highlighted_evidence": [
              "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.",
              "Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/asunam/McAndrewBDDHB16",
    "dblp_title": "What we write about when we write about causality: Features of causal statements across large-scale social discourse.",
    "year": "2016"
  },
  {
    "id": "1607.06275",
    "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering",
    "qas": [
      {
        "question": "What languages do they experiment with?",
        "question_id": "ba48c095c496d01c7717eaa271470c3406bf2d7c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Chinese"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to train and evaluate open-domain factoid QA system for real-world questions, we build a new Chinese QA dataset named as WebQA. The dataset consists of tuples of (question, evidences, answer), which is similar to example in Figure FIGREF3 . All the questions, evidences and answers are collected from web. Table TABREF20 shows some statistics of the dataset."
            ],
            "highlighted_evidence": [
              "In order to train and evaluate open-domain factoid QA system for real-world questions, we build a new Chinese QA dataset named as WebQA."
            ]
          }
        ]
      },
      {
        "question": "What are the baselines?",
        "question_id": "42a61773aa494f7b12838f71a949034c12084de1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MemN2N BIBREF12",
              "Attentive and Impatient Readers BIBREF6"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare our model with two sets of baselines:",
              "MemN2N BIBREF12 is an end-to-end trainable version of memory networks BIBREF9 . It encodes question and evidence with a bag-of-word method and stores the representations of evidences in an external memory. A recurrent attention model is used to retrieve relevant information from the memory to answer the question.",
              "Attentive and Impatient Readers BIBREF6 use bidirectional LSTMs to encode question and evidence, and do classification over a large vocabulary based on these two encodings. The simpler Attentive Reader uses a similar way as our work to compute attention for the evidence. And the more complex Impatient Reader computes attention after processing each question word."
            ],
            "highlighted_evidence": [
              "We compare our model with two sets of baselines:\n\nMemN2N BIBREF12 is an end-to-end trainable version of memory networks BIBREF9 .",
              "Attentive and Impatient Readers BIBREF6 use bidirectional LSTMs to encode question and evidence, and do classification over a large vocabulary based on these two encodings."
            ]
          }
        ]
      },
      {
        "question": "What was the inter-annotator agreement?",
        "question_id": "48c3e61b2ed7b3f97706e2a522172bf9b51ec467",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "correctness of all the question answer pairs are verified by at least two annotators"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The questions and answers are mainly collected from a large community QA website Baidu Zhidao and a small portion are from hand collected web documents. Therefore, all these questions are indeed asked by real-world users in daily life instead of under controlled conditions. All the questions are of single-entity factoid type, which means (1) each question is a factoid question and (2) its answer involves only one entity (but may have multiple words). The question in Figure FIGREF3 is a positive example, while the question “Who are the children of Albert Enistein?” is a counter example because the answer involves three persons. The type and correctness of all the question answer pairs are verified by at least two annotators."
            ],
            "highlighted_evidence": [
              "The type and correctness of all the question answer pairs are verified by at least two annotators."
            ]
          }
        ]
      },
      {
        "question": "Did they use a crowdsourcing platform?",
        "question_id": "61fba3ab10f7b6906e27b028fb1d42ec601c3fb8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/LiLHWCZX16",
    "dblp_title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering.",
    "year": "2016"
  },
  {
    "id": "1603.04553",
    "title": "Unsupervised Ranking Model for Entity Coreference Resolution",
    "qas": [
      {
        "question": "Are resolution mode variables hand crafted?",
        "question_id": "80de3baf97a55ea33e0fe0cafa6f6221ba347d0a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:",
              "$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .",
              "$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.",
              "$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
            ],
            "highlighted_evidence": [
              "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
            ]
          }
        ]
      },
      {
        "question": "What are resolution model variables?",
        "question_id": "f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",
            "evidence": [
              "According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:",
              "$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .",
              "$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.",
              "$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
            ],
            "highlighted_evidence": [
              "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
            ]
          }
        ]
      },
      {
        "question": "Is the model presented in the paper state of the art?",
        "question_id": "e76139c63da0f861c097466983fbe0c94d1d9810",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "No, supervised models perform better for this task.",
            "evidence": [
              "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems — IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 . Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 ."
            ],
            "highlighted_evidence": [
              "Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/MaLH16",
    "dblp_title": "Unsupervised Ranking Model for Entity Coreference Resolution.",
    "year": "2016"
  },
  {
    "id": "1709.10217",
    "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
    "qas": [
      {
        "question": "What problems are found with the evaluation scheme?",
        "question_id": "b8b588ca1e876b3094ae561a875dd949c8965b2e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "From Figure FIGREF6 , we can see that it is quite different between the open domain chit-chat system and the task-oriented dialogue system. For the open domain chit-chat system, as it has no exact goal in a conversation, given an input message, the responses can be various. For example, for the input message “How is it going today?”, the responses can be “I'm fine!”, “Not bad.”, “I feel so depressed!”, “What a bad day!”, etc. There may be infinite number of responses for an open domain messages. Hence, it is difficult to construct a gold standard (usually a reference set) to evaluate a response which is generated by an open domain chit-chat system. For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue."
            ],
            "highlighted_evidence": [
              "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue."
            ]
          }
        ]
      },
      {
        "question": "How is the data annotated?",
        "question_id": "2ec640e6b4f1ebc158d13ee6589778b4c08a04c8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What collection steps do they mention?",
        "question_id": "ab0bb4d0a9796416d3d7ceba0ba9ab50c964e9d6",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many intents were classified?",
        "question_id": "0460019eb2186aef835f7852fc445b037bd43bb7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "two"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue. The task-oriented dialogue also includes 30 sub categories. In this evaluation, we only consider to classify the user intent in single utterance."
            ],
            "highlighted_evidence": [
              "In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue."
            ]
          }
        ]
      },
      {
        "question": "What was the result of the highest performing system?",
        "question_id": "96c09ece36a992762860cde4c110f1653c110d96",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",
            "evidence": [
              "There are 74 participants who are signing up the evaluation. The final number of participants is 28 and the number of submitted systems is 43. Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively. Due to the space limitation, we only present the top 5 results of task 1. We will add the complete lists of the evaluation results in the version of full paper.",
              "Note that for task 2, there are 7 submitted systems. However, only 4 systems can provide correct results or be connected in a right way at the test phase. Therefore, Table TABREF16 shows the complete results of the task 2.",
              "FLOAT SELECTED: Table 4: Top 5 results of the closed test of the task 1.",
              "FLOAT SELECTED: Table 5: Top 5 results of the open test of the task 1.",
              "FLOAT SELECTED: Table 6: The results of the task 2. Ratio, Satisfaction, Fluency, Turns and Guide indicate the task completion ratio, user satisfaction degree, response fluency, number of dialogue turns and guidance ability for out of scope input respectively."
            ],
            "highlighted_evidence": [
              "Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively.",
              "Therefore, Table TABREF16 shows the complete results of the task 2.",
              "FLOAT SELECTED: Table 4: Top 5 results of the closed test of the task 1.",
              "FLOAT SELECTED: Table 5: Top 5 results of the open test of the task 1.",
              "FLOAT SELECTED: Table 6: The results of the task 2. Ratio, Satisfaction, Fluency, Turns and Guide indicate the task completion ratio, user satisfaction degree, response fluency, number of dialogue turns and guidance ability for out of scope input respectively."
            ]
          }
        ]
      },
      {
        "question": "What metrics are used in the evaluation?",
        "question_id": "a9cc4b17063711c8606b8fc1c5eaf057b317a0c9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "For task 1, we use F1-score",
              "Task completion ratio",
              "User satisfaction degree",
              "Response fluency",
              "Number of dialogue turns",
              "Guidance ability for out of scope input"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "It is worth noting that besides the released data for training and developing, we also allow to collect external data for training and developing. To considering that, the task 1 is indeed includes two sub tasks. One is a closed evaluation, in which only the released data can be used for training and developing. The other is an open evaluation that allow to collect external data for training and developing. For task 1, we use F1-score as evaluation metric.",
              "We use manual evaluation for task 2. For each system and each complete user intent, the initial sentence, which is used to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following.",
              "Task completion ratio: The number of completed tasks divided by the number of total tasks.",
              "User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively.",
              "Response fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency.",
              "Number of dialogue turns: The number of utterances in a task-completed dialogue.",
              "Guidance ability for out of scope input: There are two scores 0, 1, which represent able to guide or unable to guide."
            ],
            "highlighted_evidence": [
              "For task 1, we use F1-score as evaluation metric.",
              "We use manual evaluation for task 2.",
              "There are five evaluation metrics for task 2 as following.\n\nTask completion ratio: The number of completed tasks divided by the number of total tasks.\n\nUser satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively.\n\nResponse fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency.\n\nNumber of dialogue turns: The number of utterances in a task-completed dialogue.\n\nGuidance ability for out of scope input: There are two scores 0, 1, which represent able to guide or unable to guide."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1709-10217",
    "dblp_title": "The First Evaluation of Chinese Human-Computer Dialogue Technology.",
    "year": "2017"
  },
  {
    "id": "1901.02262",
    "title": "Multi-style Generative Reading Comprehension",
    "qas": [
      {
        "question": "How do they measure the quality of summaries?",
        "question_id": "6ead576ee5813164684a8cdda36e6a8c180455d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Rouge-L",
              "Bleu-1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state-of-the-art extractive approaches BIBREF6 , BIBREF7 ."
            ],
            "highlighted_evidence": [
              "In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1."
            ]
          }
        ]
      },
      {
        "question": "Does their model also take the expected answer style as input?",
        "question_id": "0117aa1266a37b0d2ef429f1b0653b9dde3677fe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture. Note that introducing the tokens on the decoder side prevents the passage ranker and answer possibility classifier from depending on the answer style."
            ],
            "highlighted_evidence": [
              "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles."
            ]
          }
        ]
      },
      {
        "question": "What do they mean by answer styles?",
        "question_id": "5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "well-formed sentences vs concise answers",
            "evidence": [
              "We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL."
            ],
            "highlighted_evidence": [
              "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question."
            ]
          }
        ]
      },
      {
        "question": "Is there exactly one \"answer style\" per dataset?",
        "question_id": "6c80bc3ed6df228c8ca6e02c0a8a1c2889498688",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL."
            ],
            "highlighted_evidence": [
              "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question."
            ]
          }
        ]
      },
      {
        "question": "What are the baselines that Masque is compared against?",
        "question_id": "2d274c93901c193cf7ad227ab28b1436c5f410af",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",
            "evidence": [
              "FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.",
              "FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.",
              "FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set."
            ]
          }
        ]
      },
      {
        "question": "What is the performance achieved on NarrativeQA?",
        "question_id": "e63bde5c7b154fbe990c3185e2626d13a1bad171",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",
            "evidence": [
              "FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set."
            ]
          }
        ]
      },
      {
        "question": "What is an \"answer style\"?",
        "question_id": "cb8a6f5c29715619a137e21b54b29e9dd48dad7d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "well-formed sentences vs concise answers",
            "evidence": [
              "We conducted experiments on the two tasks of MS MARCO 2.1 BIBREF5 . The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. For instance, for the question “tablespoon in cup”, the answer in the Q&A task will be “16”, and the answer in the NLG task will be “There are 16 tablespoons in a cup.” In addition to the ALL dataset, we prepared two subsets (Table 1 ). The ANS set consists of answerable questions, and the WFA set consists of the answerable questions and well-formed answers, where WFA $\\subset $ ANS $\\subset $ ALL."
            ],
            "highlighted_evidence": [
              "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/Yang0Z21",
    "dblp_title": "Multi-span Style Extraction for Generative Reading Comprehension.",
    "year": "2021"
  },
  {
    "id": "1908.04917",
    "title": "A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading",
    "qas": [
      {
        "question": "What was the previous state of the art model for this task?",
        "question_id": "8a7bd9579d2783bfa81e055a7a6ebc3935da9d20",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WAS",
              "LipCH-Net-seq",
              "CSSMCM-w/o video"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.",
              "LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.",
              "CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character."
            ],
            "highlighted_evidence": [
              "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.",
              "LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.",
              "CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character."
            ]
          }
        ]
      },
      {
        "question": "What syntactic structure is used to model tones?",
        "question_id": "27b01883ed947b457d3bab0c66de26c0736e4f90",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "syllables"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Based on the above considerations, in this paper, we present CSSMCM, a sentence-level Chinese Mandarin lip reading network, which contains three sub-networks. Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation. Compared with pinyin characters, syllables are a longer linguistic unit, and can reduce the difficulty of syllable choices in the decoder by sequence-to-sequence attention-based models BIBREF6 . Chen et al. BIBREF7 find that there might be a relationship between the production of lexical tones and the visible movements of the neck, head, and mouth. Motivated by this observation, in the second sub-network, both video and pinyin sequence is used as input to predict tone. Then in the third sub-network, video, pinyin, and tone sequence work together to predict the Chinese character sequence. At last, three sub-networks are jointly finetuned to improve overall performance."
            ],
            "highlighted_evidence": [
              "Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation."
            ]
          }
        ]
      },
      {
        "question": "What visual information characterizes tones?",
        "question_id": "9714cb7203c18a0c53805f6c889f2e20b4cab5dd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "video sequence is first fed into the VGG model BIBREF9 to extract visual feature"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence. This problem is modeled as a sequence-to-sequence learning problem too. The corresponding model architecture is shown in Figure FIGREF8 .",
              "In order to take both video and pinyin information into consideration when producing tone, a dual attention mechanism BIBREF3 is employed. Two independent attention mechanisms are used for video and pinyin sequence. Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.",
              "The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0",
              "The pinyin prediction sub-network transforms video sequence into pinyin sequence, which corresponds to INLINEFORM0 in Equation ( EQREF6 ). This sub-network is based on the sequence-to-sequence architecture with attention mechanism BIBREF8 . We name the encoder and decoder the video encoder and pinyin decoder, for the encoder process video sequence, and the decoder predicts pinyin sequence. The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder. The video encoder can be denoted as: DISPLAYFORM0"
            ],
            "highlighted_evidence": [
              "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence.",
              "Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.",
              "The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0",
              "The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/mmasia/ZhaoXS19",
    "dblp_title": "A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading.",
    "year": "2019"
  },
  {
    "id": "1906.03338",
    "title": "Dissecting Content and Context in Argumentative Relation Analysis",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "a22b900fcd76c3d36b5679691982dc6e9a3d34bf",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How do they demonstrate the robustness of their results?",
        "question_id": "fb2593de1f5cc632724e39d92e4dd82477f06ea1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "performances of a purely content-based model naturally stays stable"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The results are displayed in Figure FIGREF43 . In the standard setting (Figure FIGREF43 ), the models that have access to the context besides the content ( INLINEFORM0 ) and the models that are only allowed to access the context ( INLINEFORM1 ), always perform better than the content-based models ( INLINEFORM2 ) (bars above zero). However, when we randomly flip contexts of the test instances (Figure FIGREF43 ), or suppress them entirely (Figure FIGREF43 ), the opposite picture emerges: the content-based models always outperform the other models. For some classes (support, INLINEFORM3 ) the difference can exceed 50 F1 percentage points. These two studies, where testing examples are varied regarding their context (randomized-context or no-context) simulates what can be expected if we apply our systems for relation class assignment to EAUs stemming from heterogeneous sources. While the performances of a purely content-based model naturally stays stable, the performance of the other systems decrease notably – they perform worse than the content-based model."
            ],
            "highlighted_evidence": [
              "While the performances of a purely content-based model naturally stays stable, the performance of the other systems decrease notably – they perform worse than the content-based model."
            ]
          }
        ]
      },
      {
        "question": "What baseline and classification systems are used in experiments?",
        "question_id": "476d0b5579deb9199423bb843e584e606d606bc7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF13",
              "majority baseline"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features.",
              "The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1. Intriguingly, the content-ignorant models ( INLINEFORM6 ) always perform significantly better than the models which only have access to the EAUs' content ( INLINEFORM7 , INLINEFORM8 ). In the most general task formulation ( INLINEFORM9 ), we observe that INLINEFORM10 even significantly outperforms the model which has maximum access (seeing both EAU spans and surrounding contexts: INLINEFORM11 )."
            ],
            "highlighted_evidence": [
              "The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features.",
              "The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1."
            ]
          }
        ]
      },
      {
        "question": "How are the EAU text spans annotated?",
        "question_id": "eddabb24bc6de6451bcdaa7940f708e925010912",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.",
            "evidence": [
              "Tree-based sentiment annotations are sentiment scores assigned to nodes in constituency parse trees BIBREF26 . We represent these scores by a one-hot vector of dimension 5 (5 is very positive, 1 is very negative). We determine the contextual ( INLINEFORM0 ) sentiment by looking at the highest possible node of the context which does not contain the EAU (ADVP in Figure FIGREF26 ). The sentiment for an EAU span ( INLINEFORM1 ) is assigned to the highest possible node covering the EAU span which does not contain the context sub-tree (S in Figure FIGREF26 ). The full-access ( INLINEFORM2 ) score is assigned to the lowest possible node which covers both the EAU span and its surrounding context (S' in Figure FIGREF26 ). Next to the sentiment scores for the selected tree nodes and analogously to the word embeddings, we also calculate the element-wise subtraction of the one-hot sentiment source vectors from the one-hot sentiment target vectors. This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors.",
              "Results"
            ],
            "highlighted_evidence": [
              "This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors.\n\nResults"
            ]
          }
        ]
      },
      {
        "question": "How are elementary argumentative units defined?",
        "question_id": "f0946fb9df9839977f4d16c43476e4c2724ff772",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/argmining/OpitzF19",
    "dblp_title": "Dissecting Content and Context in Argumentative Relation Analysis.",
    "year": "2019"
  },
  {
    "id": "1602.08741",
    "title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?",
    "qas": [
      {
        "question": "Which Twitter corpus was used to train the word vectors?",
        "question_id": "e51d0c2c336f255e342b5f6c3cf2a13231789fed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They collected tweets in Russian language using a heuristic query specific to Russian",
            "evidence": [
              "Twitter provides well-documented API, which allows to request any information about Tweets, users and their profiles, with respect to rate limits. There is special type of API, called Streaming API, that provides a real-time stream of tweets. The key difference with regular API is that connection is kept alive as long as possible, and Tweets are sent in real-time to the client. There are three endpoints of Streaming API of our interest: “sample”, “filter” and “firehose”. The first one provides a sample (random subset) of the full Tweet stream. The second one allows to receive Tweets matching some search criteria: matching to one or more search keywords, produced by subset of users, or coming from certain geo location. The last one provides the full set of Tweets, although it is not available by default. In order to get Twitter “firehose” one can contact Twitter, or buy this stream from third-parties.",
              "In our case the simplest approach would be to use “sample” endpoint, but it provides Tweets in all possible languages from all over the World, while we are concerned only about one language (Russian). In order to use this endpoint we implemented filtering based on language. The filter is simple: if Tweet does not contain a substring of 3 or more cyrillic symbols, it is considered non-Russian. Although this approach keeps Tweets in Mongolian, Ukrainian and other slavic languages (because they use cyrillic alphabet), the total amount of false-positives in this case is negligible. To demonstrate this we conducted simple experiment: on a random sample of 200 tweets only 5 were in a language different from Russian. In order not to rely on Twitter language detection, we chose to proceed with this method of language-based filtering.",
              "However, the amount of Tweets received through “sample” endpoint was not satisfying. This is probably because “sample” endpoint always streams the same content to all its clients, and small portion of it comes in Russian language. In order to force mining of Tweets in Russian language, we chose \"filter\" endpoint, which requires some search query. We constructed heuristic query, containing some auxiliary words, specific to Russian language: conjunctions, pronouns, prepositions. The full list is as follows:",
              "russian я, у, к, в, по, на, ты, мы, до, на, она, он, и, да."
            ],
            "highlighted_evidence": [
              "Twitter provides well-documented API, which allows to request any information about Tweets, users and their profiles, with respect to rate limits. There is special type of API, called Streaming API, that provides a real-time stream of tweets. The key difference with regular API is that connection is kept alive as long as possible, and Tweets are sent in real-time to the client. There are three endpoints of Streaming API of our interest: “sample”, “filter” and “firehose”. The first one provides a sample (random subset) of the full Tweet stream. The second one allows to receive Tweets matching some search criteria: matching to one or more search keywords, produced by subset of users, or coming from certain geo location. The last one provides the full set of Tweets, although it is not available by default. In order to get Twitter “firehose” one can contact Twitter, or buy this stream from third-parties.\n\nIn our case the simplest approach would be to use “sample” endpoint, but it provides Tweets in all possible languages from all over the World, while we are concerned only about one language (Russian). In order to use this endpoint we implemented filtering based on language. The filter is simple: if Tweet does not contain a substring of 3 or more cyrillic symbols, it is considered non-Russian. Although this approach keeps Tweets in Mongolian, Ukrainian and other slavic languages (because they use cyrillic alphabet), the total amount of false-positives in this case is negligible. To demonstrate this we conducted simple experiment: on a random sample of 200 tweets only 5 were in a language different from Russian. In order not to rely on Twitter language detection, we chose to proceed with this method of language-based filtering.\n\nHowever, the amount of Tweets received through “sample” endpoint was not satisfying. This is probably because “sample” endpoint always streams the same content to all its clients, and small portion of it comes in Russian language. In order to force mining of Tweets in Russian language, we chose \"filter\" endpoint, which requires some search query. We constructed heuristic query, containing some auxiliary words, specific to Russian language: conjunctions, pronouns, prepositions. The full list is as follows:\n\nrussian я, у, к, в, по, на, ты, мы, до, на, она, он, и, да."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/Vasiliev16",
    "dblp_title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?",
    "year": "2016"
  },
  {
    "id": "1911.12579",
    "title": "A New Corpus for Low-Resourced Sindhi Language with Word Embeddings",
    "qas": [
      {
        "question": "How does proposed word embeddings compare to Sindhi fastText word representations?",
        "question_id": "5b6aec1b88c9832075cd343f59158078a91f3597",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",
            "evidence": [
              "Generally, closer words are considered more important to a word’s meaning. The word embeddings models have the ability to capture the lexical relations between words. Identifying such relationship that connects words is important in NLP applications. We measure that semantic relationship by calculating the dot product of two vectors using Eq. DISPLAY_FORM48. The high cosine similarity score denotes the closer words in the embedding matrix, while less cosine similarity score means the higher distance between word pairs. We present the cosine similarity score of different semantically or syntactically related word pairs taken from the vocabulary in Table TABREF77 along with English translation, which shows the average similarity of 0.632, 0.650, 0.591 yields by CBoW, SG and GloVe respectively. The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. This shows that along with performance, the vocabulary in SdfastText is also limited as compared to our proposed word embeddings.",
              "Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The first query word China-Beijing is not available the vocabulary of SdfastText. However, the similarity score between Afghanistan-Kabul is lower in our proposed CBoW, SG, GloVe models because the word Kabul is the name of the capital of Afghanistan as well as it frequently appears as an adjective in Sindhi text which means able."
            ],
            "highlighted_evidence": [
              "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText.",
              "Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391."
            ]
          }
        ]
      },
      {
        "question": "Are trained word embeddings used for any other NLP task?",
        "question_id": "a6717e334c53ebbb87e5ef878a77ef46866e3aed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a flow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not sufficient to design a language independent or machine learning algorithms. The present work is a first comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
            ],
            "highlighted_evidence": [
              "In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition.",
              "Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
            ]
          }
        ]
      },
      {
        "question": "How many uniue words are in the dataset?",
        "question_id": "a1064307a19cd7add32163a70b6623278a557946",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "908456 unique words are available in collected corpus.",
            "evidence": [
              "The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens.",
              "FLOAT SELECTED: Table 2: Complete statistics of collected corpus from multiple resources."
            ],
            "highlighted_evidence": [
              "The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens.",
              "FLOAT SELECTED: Table 2: Complete statistics of collected corpus from multiple resources."
            ]
          }
        ]
      },
      {
        "question": "How is the data collected, which web resources were used?",
        "question_id": "8cb9006bcbd2f390aadc6b70d54ee98c674e45cc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "daily Kawish and Awami Awaz Sindhi newspapers",
              "Wikipedia dumps",
              "short stories and sports news from Wichaar social blog",
              "news from Focus Word press blog",
              "historical writings, novels, stories, books from Sindh Salamat literary website",
              "novels, history and religious books from Sindhi Adabi Board",
              " tweets regarding news and sports are collected from twitter"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter."
            ],
            "highlighted_evidence": [
              "In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-12579",
    "dblp_title": "A New Corpus for Low-Resourced Sindhi Language with Word Embeddings.",
    "year": "2019"
  },
  {
    "id": "1908.10275",
    "title": "The Wiki Music dataset: A tool for computational analysis of popular music",
    "qas": [
      {
        "question": "What trends are found in musical preferences?",
        "question_id": "75043c17a2cddfce6578c3c0e18d4b7cf2f18933",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "How the music taste of the audience of popular music changed in the last century? The trend lines of the MUSIC model features, reported in figure FIGREF12, reveal that audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious. In other words, the audiences of popular music are getting more demanding as the quality and variety of the music products increases."
            ],
            "highlighted_evidence": [
              "How the music taste of the audience of popular music changed in the last century? The trend lines of the MUSIC model features, reported in figure FIGREF12, reveal that audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious. In other words, the audiences of popular music are getting more demanding as the quality and variety of the music products increases."
            ]
          }
        ]
      },
      {
        "question": "Which decades did they look at?",
        "question_id": "95bb3ea4ebc3f2174846e8d422abc076e1407d6a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "between 1900s and 2010s"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "time: decade (classes between 1900s and 2010s) and year representative of the time when the genre became meainstream"
            ],
            "highlighted_evidence": [
              "time: decade (classes between 1900s and 2010s) and year representative of the time when the genre became meainstream"
            ]
          }
        ]
      },
      {
        "question": "How many genres did they collect from?",
        "question_id": "3ebdc15480250f130cf8f5ab82b0595e4d870e2f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "77 genres"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "From the description of music genres provided above emerges that there is a limited number of super-genres and derivation lines BIBREF19, BIBREF20, as shown in figure FIGREF1.",
              "From a computational perspective, genres are classes and, although can be treated by machine learning algorithms, they do not include information about the relations between them. In order to formalize the relations between genres for computing purposes, we define a continuous genre scale from the most experimental and introverted super-genre to the most euphoric and inclusive one. We selected from Wikipedia the 77 genres that we mentioned in bold in the previous paragraph and asked to two independent raters to read the Wikipedia pages of the genres, listen to samples or artists of the genres (if they did not know already) and then annotate the following dimensions:"
            ],
            "highlighted_evidence": [
              "From the description of music genres provided above emerges that there is a limited number of super-genres and derivation lines BIBREF19, BIBREF20, as shown in figure FIGREF1.\n\nFrom a computational perspective, genres are classes and, although can be treated by machine learning algorithms, they do not include information about the relations between them. In order to formalize the relations between genres for computing purposes, we define a continuous genre scale from the most experimental and introverted super-genre to the most euphoric and inclusive one. We selected from Wikipedia the 77 genres that we mentioned in bold in the previous paragraph and asked to two independent raters to read the Wikipedia pages of the genres, listen to samples or artists of the genres (if they did not know already) and then annotate the following dimensions:"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-10275",
    "dblp_title": "The Wiki Music dataset: A tool for computational analysis of popular music.",
    "year": "2019"
  },
  {
    "id": "2004.02929",
    "title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines",
    "qas": [
      {
        "question": "Does the paper mention other works proposing methods to detect anglicisms in Spanish?",
        "question_id": "bbc58b193c08ccb2a1e8235a36273785a3b375fb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "In terms of automatic detection of anglicisms, previous approaches in different languages have mostly depended on resource lookup (lexicon or corpus frequencies), character n-grams and pattern matching. alex-2008-comparing combined lexicon lookup and a search engine module that used the web as a corpus to detect English inclusions in a corpus of German texts and compared her results with a maxent Markov model. furiassi2007retrieval explored corpora lookup and character n-grams to extract false anglicisms from a corpus of Italian newspapers. andersen2012semi used dictionary lookup, regular expressions and lexicon-derived frequencies of character n-grams to detect anglicism candidates in the Norwegian Newspaper Corpus (NNC) BIBREF21, while losnegaard2012data explored a Machine Learning approach to anglicism detection in Norwegian by using TiMBL (Tilburg Memory-Based Learner, an implementation of a k-nearest neighbor classifier) with character trigrams as features. garley-hockenmaier-2012-beefmoves trained a maxent classifier with character n-gram and morphological features to identify anglicisms in German online communities. In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish."
            ],
            "highlighted_evidence": [
              "In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish."
            ]
          }
        ]
      },
      {
        "question": "What is the performance of the CRF model on the task described?",
        "question_id": "3c34187a248d179856b766e9534075da1aa5d1cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Results on all sets show an important difference between precision and recall, precision being significantly higher than recall. There is also a significant difference between the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49). The time difference between the supplemental test set and the development and test set (the headlines from the the supplemental test set being from a different time period to the training set) can probably explain these differences."
            ],
            "highlighted_evidence": [
              "Results on all sets show an important difference between precision and recall, precision being significantly higher than recall. There is also a significant difference between the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49). The time difference between the supplemental test set and the development and test set (the headlines from the the supplemental test set being from a different time period to the training set) can probably explain these differences."
            ]
          }
        ]
      },
      {
        "question": "Does the paper motivate the use of CRF as the baseline model?",
        "question_id": "8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A baseline model for automatic extraction of anglicisms was created using the annotated corpus we just presented as training material. As mentioned in Section 3, the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data BIBREF23, BIBREF24."
            ],
            "highlighted_evidence": [
              "A baseline model for automatic extraction of anglicisms was created using the annotated corpus we just presented as training material. As mentioned in Section 3, the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data BIBREF23, BIBREF24."
            ]
          }
        ]
      },
      {
        "question": "What are the handcrafted features used?",
        "question_id": "97757a69d9fc28b260e68284fd300726fbe358d0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Bias feature",
              "Token feature",
              "Uppercase feature (y/n)",
              "Titlecase feature (y/n)",
              "Character trigram feature",
              "Quotation feature (y/n)",
              "Word suffix feature (last three characters)",
              "POS tag (provided by spaCy utilities)",
              "Word shape (provided by spaCy utilities)",
              "Word embedding (see Table TABREF26)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The following handcrafted features were used for the model:",
              "Bias feature",
              "Token feature",
              "Uppercase feature (y/n)",
              "Titlecase feature (y/n)",
              "Character trigram feature",
              "Quotation feature (y/n)",
              "Word suffix feature (last three characters)",
              "POS tag (provided by spaCy utilities)",
              "Word shape (provided by spaCy utilities)",
              "Word embedding (see Table TABREF26)"
            ],
            "highlighted_evidence": [
              "The following handcrafted features were used for the model:\n\nBias feature\n\nToken feature\n\nUppercase feature (y/n)\n\nTitlecase feature (y/n)\n\nCharacter trigram feature\n\nQuotation feature (y/n)\n\nWord suffix feature (last three characters)\n\nPOS tag (provided by spaCy utilities)\n\nWord shape (provided by spaCy utilities)\n\nWord embedding (see Table TABREF26)"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl-codeswitch/Alvarez-Mellado20",
    "dblp_title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines.",
    "year": "2020"
  },
  {
    "id": "1908.06809",
    "title": "Style Transfer for Texts: to Err is Human, but Error Margins Matter",
    "qas": [
      {
        "question": "What is state of the art method?",
        "question_id": "41830ebb8369a24d490e504b7cdeeeaa9b09fd9c",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "By how much do proposed architectures autperform state-of-the-art?",
        "question_id": "4904ef32a8f84cf2f53b1532ccf7aa77273b3d19",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are three new proposed architectures?",
        "question_id": "45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information",
              "shifted autoencoder or SAE",
              "combination of both approaches"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Let us propose two further extensions of this baseline architecture. To improve reproducibility of the research the code of the studied models is open. Both extensions aim to improve the quality of information decomposition within the latent representation. In the first one, shown in Figure FIGREF12, a special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information. The loss of this discriminator is defined as",
              "The second extension of the baseline architecture does not use an adversarial component $D_z$ that is trying to eradicate information on $c$ from component $z$. Instead, the system, shown in Figure FIGREF16 feeds the \"soft\" generated sentence $\\tilde{G}$ into encoder $E$ and checks how close is the representation $E(\\tilde{G} )$ to the original representation $z = E(x)$ in terms of the cosine distance. We further refer to it as shifted autoencoder or SAE. Ideally, both $E(\\tilde{G} (E(x), c))$ and $E(\\tilde{G} (E(x), \\bar{c}))$, where $\\bar{c}$ denotes an inverse style code, should be both equal to $E(x)$. The loss of the shifted autoencoder is",
              "We also study a combination of both approaches described above, shown on Figure FIGREF17."
            ],
            "highlighted_evidence": [
              "In the first one, shown in Figure FIGREF12, a special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information.",
              "The second extension of the baseline architecture does not use an adversarial component $D_z$ that is trying to eradicate information on $c$ from component $z$. Instead, the system, shown in Figure FIGREF16 feeds the \"soft\" generated sentence $\\tilde{G}$ into encoder $E$ and checks how close is the representation $E(\\tilde{G} )$ to the original representation $z = E(x)$ in terms of the cosine distance. We further refer to it as shifted autoencoder or SAE.",
              "We also study a combination of both approaches described above, shown on Figure FIGREF17."
            ]
          }
        ]
      },
      {
        "question": "How much does the standard metrics for style accuracy vary on different re-runs?",
        "question_id": "d6a27c41c81f12028529e97e255789ec2ba39eaa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "On Figure FIGREF1 one can see that the outcomes for every single rerun differ significantly. Namely, accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points. This variance can be partially explained with the stochasticity incurred due to sampling from the latent variables. However, we show that results for state of the art models sometimes end up within error margins from one another, so one has to report the margins to compare the results rigorously. More importantly, one can see that there is an inherent trade-off between these two performance metrics. This trade-off is not only visible across models but is also present for the same retrained architecture. Therefore, improving one of the two metrics is not enough to confidently state that one system solves the style-transfer problem better than the other. One has to report error margins after several consecutive retrains and instead of comparing one of the two metrics has to talk about Pareto-like optimization that would show confident improvement of both."
            ],
            "highlighted_evidence": [
              "On Figure FIGREF1 one can see that the outcomes for every single rerun differ significantly. Namely, accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-06809",
    "dblp_title": "Style Transfer for Texts: to Err is Human, but Error Margins Matter.",
    "year": "2019"
  },
  {
    "id": "1707.00110",
    "title": "Efficient Attention using a Fixed-Size Memory Representation",
    "qas": [
      {
        "question": "Which baseline methods are used?",
        "question_id": "2d3bf170c1647c5a95abae50ee3ef3b404230ce4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "standard parametrized attention and a non-attention baseline",
            "evidence": [
              "Table 1 shows the BLEU scores of our model on different sequence lengths while varying $K$ . This is a study of the trade-off between computational time and representational power. A large $K$ allows us to compute complex source representations, while a $K$ of 1 limits the source representation to a single vector. We can see that performance consistently increases with $K$ up to a point that depends on the data length, with longer sequences requiring more complex representations. The results with and without position encodings are almost identical on the toy data. Our technique learns to fit the data as well as the standard attention mechanism despite having less representational power. Both beat the non-attention baseline by a significant margin.",
              "All models are implemented using TensorFlow based on the seq2seq implementation of BIBREF15 and trained on a single machine with a Nvidia K40m GPU. We use a 2-layer 256-unit, a bidirectional LSTM BIBREF16 encoder, a 2-layer 256-unit LSTM decoder, and 256-dimensional embeddings. For the attention baseline, we use the standard parametrized attention BIBREF2 . Dropout of 0.2 (0.8 keep probability) is applied to the input of each cell and we optimize using Adam BIBREF17 at a learning rate of 0.0001 and batch size 128. We train for at most 200,000 steps (see Figure 3 for sample learning curves). BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moses. We decode using beam search with a beam"
            ],
            "highlighted_evidence": [
              "Both beat the non-attention baseline by a significant margin.",
              "For the attention baseline, we use the standard parametrized attention BIBREF2 ."
            ]
          }
        ]
      },
      {
        "question": "How much is the BLEU score?",
        "question_id": "6e8c587b6562fafb43a7823637b84cd01487059a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Ranges from 44.22 to 100.00 depending on K and the sequence length.",
            "evidence": [
              "FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention."
            ]
          }
        ]
      },
      {
        "question": "Which datasets are used in experiments?",
        "question_id": "ab9453fa2b927c97b60b06aeda944ac5c1bfef1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Sequence Copy Task and WMT'17",
            "evidence": [
              "Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.",
              "Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016."
            ],
            "highlighted_evidence": [
              "To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 .",
              "For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/BritzGL17",
    "dblp_title": "Efficient Attention using a Fixed-Size Memory Representation.",
    "year": "2017"
  },
  {
    "id": "1909.01013",
    "title": "Duality Regularization for Unsupervised Bilingual Lexicon Induction",
    "qas": [
      {
        "question": "What regularizers were used to encourage consistency in back translation cycles?",
        "question_id": "3a8d65eb8e1dbb995981a0e02d86ebf3feab107a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "an adversarial loss ($\\ell _{adv}$) for each model as in the baseline",
              "a cycle consistency loss ($\\ell _{cycle}$) on each side"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We train $\\mathcal {F}$ and $\\mathcal {G}$ jointly and introduce two regularizers. Formally, we hope that $\\mathcal {G}(\\mathcal {F}(X))$ is similar to $X$ and $\\mathcal {F}(\\mathcal {G}(Y))$ is similar to $Y$. We implement this constraint as a cycle consistency loss. As a result, the proposed model has two learning objectives: i) an adversarial loss ($\\ell _{adv}$) for each model as in the baseline. ii) a cycle consistency loss ($\\ell _{cycle}$) on each side to avoid $\\mathcal {F}$ and $\\mathcal {G}$ from contradicting each other. The overall architecture of our model is illustrated in Figure FIGREF4."
            ],
            "highlighted_evidence": [
              "We train $\\mathcal {F}$ and $\\mathcal {G}$ jointly and introduce two regularizers. Formally, we hope that $\\mathcal {G}(\\mathcal {F}(X))$ is similar to $X$ and $\\mathcal {F}(\\mathcal {G}(Y))$ is similar to $Y$. We implement this constraint as a cycle consistency loss. As a result, the proposed model has two learning objectives: i) an adversarial loss ($\\ell _{adv}$) for each model as in the baseline. ii) a cycle consistency loss ($\\ell _{cycle}$) on each side to avoid $\\mathcal {F}$ and $\\mathcal {G}$ from contradicting each other."
            ]
          }
        ]
      },
      {
        "question": "What are new best results on standard benchmark?",
        "question_id": "d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "New best results of accuracy (P@1) on Vecmap:\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",
            "evidence": [
              "Table TABREF15 shows the final results on Vecmap. We first compare our model with the state-of-the-art unsupervised methods. Our model based on procrustes (Ours-Procrustes) outperforms Sinkhorn-BT on all test language pairs, and shows better performance than Adv-C-Procrustes on most language pairs. Adv-C-Procrustes gives very low precision on DE-EN, FI-EN and ES-EN, while Ours-Procrustes obtains reasonable results consistently. A possible explanation is that dual learning is helpful for providing good initiations, so that the procrustes solution is not likely to fall in poor local optima. The reason why Unsup-SL gives strong results on all language pairs is that it uses a robust self-learning framework, which contains several techniques to avoid poor local optima.",
              "FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs."
            ],
            "highlighted_evidence": [
              "Table TABREF15 shows the final results on Vecmap.",
              "FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs."
            ]
          }
        ]
      },
      {
        "question": "How better is performance compared to competitive baselines?",
        "question_id": "54c7fc08598b8b91a8c0399f6ab018c45e259f79",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed method vs best baseline result on Vecmap (Accuracy P@1):\nEN-IT: 50 vs 50\nIT-EN: 42.67 vs 42.67\nEN-DE: 51.6 vs 51.47\nDE-EN: 47.22 vs 46.96\nEN-FI: 35.88 vs 36.24\nFI-EN: 39.62 vs 39.57\nEN-ES: 39.47 vs 39.30\nES-EN: 36.43 vs 36.06",
            "evidence": [
              "FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.",
              "Table TABREF15 shows the final results on Vecmap. We first compare our model with the state-of-the-art unsupervised methods. Our model based on procrustes (Ours-Procrustes) outperforms Sinkhorn-BT on all test language pairs, and shows better performance than Adv-C-Procrustes on most language pairs. Adv-C-Procrustes gives very low precision on DE-EN, FI-EN and ES-EN, while Ours-Procrustes obtains reasonable results consistently. A possible explanation is that dual learning is helpful for providing good initiations, so that the procrustes solution is not likely to fall in poor local optima. The reason why Unsup-SL gives strong results on all language pairs is that it uses a robust self-learning framework, which contains several techniques to avoid poor local optima."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.",
              "Table TABREF15 shows the final results on Vecmap."
            ]
          }
        ]
      },
      {
        "question": "How big is data used in experiments?",
        "question_id": "5112bbf13c7cf644bf401daecb5e3265889a4bfc",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What 6 language pairs is experimented on?",
        "question_id": "03ce42ff53aa3f1775bc57e50012f6eb1998c480",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI",
            "evidence": [
              "Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization.",
              "FLOAT SELECTED: Table 1: Accuracy on MUSE and Vecmap."
            ],
            "highlighted_evidence": [
              "Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12.",
              "FLOAT SELECTED: Table 1: Accuracy on MUSE and Vecmap."
            ]
          }
        ]
      },
      {
        "question": "What are current state-of-the-art methods that consider the two tasks independently?",
        "question_id": "ebeedbb8eecdf118d543fdb5224ae610eef212c8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Procrustes",
              "GPA",
              "GeoMM",
              "GeoMM$_{semi}$",
              "Adv-C-Procrustes",
              "Unsup-SL",
              "Sinkhorn-BT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes BIBREF11, which learns a linear mapping through Procrustes Analysis BIBREF36. (2) GPA BIBREF37, an extension of Procrustes Analysis. (3) GeoMM BIBREF38, a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM$_{semi}$, iterative GeoMM with weak supervision. (5) Adv-C-Procrustes BIBREF11, which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL BIBREF13, which integrates a weak unsupervised mapping with a robust self-learning. (7) Sinkhorn-BT BIBREF28, which combines sinkhorn distance BIBREF29 and back-translation. For fair comparison, we integrate our model with two iterative refinement methods (Procrustes and GeoMM$_{semi}$)."
            ],
            "highlighted_evidence": [
              "In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes BIBREF11, which learns a linear mapping through Procrustes Analysis BIBREF36. (2) GPA BIBREF37, an extension of Procrustes Analysis. (3) GeoMM BIBREF38, a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM$_{semi}$, iterative GeoMM with weak supervision. (5) Adv-C-Procrustes BIBREF11, which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL BIBREF13, which integrates a weak unsupervised mapping with a robust self-learning. (7) Sinkhorn-BT BIBREF28, which combines sinkhorn distance BIBREF29 and back-translation."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-01013",
    "dblp_title": "Duality Regularization for Unsupervised Bilingual Lexicon Induction.",
    "year": "2019"
  },
  {
    "id": "1901.02534",
    "title": "Team Papelo: Transformer Networks at FEVER",
    "qas": [
      {
        "question": "How big is their training set?",
        "question_id": "9efd025cfa69c6ff2777528bd158f79ead9353d1",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What baseline do they compare to?",
        "question_id": "559c1307610a15427caeb8aff4d2c01ae5c9de20",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 ."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods."
            ],
            "highlighted_evidence": [
              "Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods."
            ]
          }
        ]
      },
      {
        "question": "Which pre-trained transformer do they use?",
        "question_id": "4ecb6674bcb4162bf71aea8d8b82759255875df3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF5"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods."
            ],
            "highlighted_evidence": [
              "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . "
            ]
          }
        ]
      },
      {
        "question": "What is the FEVER task?",
        "question_id": "eacc1eb65daad055df934e0e878f417b73b2ecc1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted.",
              "As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher.",
              "The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence."
            ],
            "highlighted_evidence": [
              "The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted.",
              "As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher.",
              "The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/Malon18",
    "dblp_title": "Team Papelo: Transformer Networks at FEVER.",
    "year": "2018"
  },
  {
    "id": "2004.04435",
    "title": "Automatic Differentiation in ROOT",
    "qas": [
      {
        "question": "How is correctness of automatic derivation proved?",
        "question_id": "d353a6bbdc66be9298494d0c853e0d8d752dec4b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND."
            ],
            "highlighted_evidence": [
              "In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND."
            ]
          }
        ]
      },
      {
        "question": "Is this AD implementation used in any deep learning framework?",
        "question_id": "e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2004-04435",
    "dblp_title": "Automatic Differentiation in ROOT.",
    "year": "2020"
  },
  {
    "id": "1910.10408",
    "title": "Controlling the Output Length of Neural Machine Translation",
    "qas": [
      {
        "question": "Do they conduct any human evaluation?",
        "question_id": "22c36082b00f677e054f0f0395ed685808965a02",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."
            ],
            "highlighted_evidence": [
              "We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use for experiments?",
        "question_id": "85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English$\\rightarrow $Italian/German portions of the MuST-C corpus",
              "As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."
            ],
            "highlighted_evidence": [
              "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)."
            ]
          }
        ]
      },
      {
        "question": "How do they enrich the positional embedding with length information",
        "question_id": "90bc60320584ebba11af980ed92a309f0c1b5507",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).",
            "evidence": [
              "Methods ::: Length Encoding Method",
              "Inspired by BIBREF11, we use length encoding to provide the network with information about the remaining sentence length during decoding. We propose two types of length encoding: absolute and relative. Let pos and len be, respectively, a token position and the end of the sequence, both expressed in terms of number characters. Then, the absolute approach encodes the remaining length:",
              "where $i=1,\\ldots ,d/2$.",
              "Similarly, the relative difference encodes the relative position to the end. This representation is made consistent with the absolute encoding by quantizing the space of the relative positions into a finite set of $N$ integers:",
              "where $q_N: [0, 1] \\rightarrow \\lbrace 0, 1, .., N\\rbrace $ is simply defined as $q_N(x) = \\lfloor {x \\times N}\\rfloor $. As we are interested in the character length of the target sequence, len and pos are given in terms of characters, but we represent the sequence as a sequence of BPE-segmented subwords BIBREF17. To solve the ambiguity, len is the character length of the sequence, while pos is the character count of all the preceding tokens. We prefer a representation based on BPE, unlike BIBREF11, as it leads to better translations with less training time BIBREF18, BIBREF19. During training, len is the observed length of the target sentence, while at inference time it is the length of the source sentence, as it is the length that we aim to match. The process is exemplified in Figure FIGREF9."
            ],
            "highlighted_evidence": [
              "Methods ::: Length Encoding Method\nInspired by BIBREF11, we use length encoding to provide the network with information about the remaining sentence length during decoding. We propose two types of length encoding: absolute and relative. Let pos and len be, respectively, a token position and the end of the sequence, both expressed in terms of number characters. Then, the absolute approach encodes the remaining length:\n\nwhere $i=1,\\ldots ,d/2$.\n\nSimilarly, the relative difference encodes the relative position to the end. This representation is made consistent with the absolute encoding by quantizing the space of the relative positions into a finite set of $N$ integers:\n\nwhere $q_N: [0, 1] \\rightarrow \\lbrace 0, 1, .., N\\rbrace $ is simply defined as $q_N(x) = \\lfloor {x \\times N}\\rfloor $. As we are interested in the character length of the target sequence, len and pos are given in terms of characters, but we represent the sequence as a sequence of BPE-segmented subwords BIBREF17. To solve the ambiguity, len is the character length of the sequence, while pos is the character count of all the preceding tokens."
            ]
          }
        ]
      },
      {
        "question": "How do they condition the output to a given target-source class?",
        "question_id": "f52b2ca49d98a37a6949288ec5f281a3217e5ae8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",
            "evidence": [
              "Methods ::: Length Token Method",
              "Our first approach to control the length is inspired by target forcing in multilingual NMT BIBREF15, BIBREF16. We first split the training sentence pairs into three groups according to the target/source length ratio (in terms of characters). Ideally, we want a group where the target is shorter than the source (short), one where they are equally-sized (normal) and a last group where the target is longer than the source (long). In practice, we select two thresholds $t_\\text{min}$ and $t_\\text{max}$ according to the length ratio distribution. All the sentence pairs with length ratio between $t_\\text{min}$ and $t_\\text{max}$ are in the normal group, the ones with ratio below $t_\\text{min}$ in short and the remaining in long. At training time we prepend a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$), in order to let a single network to discriminate between the groups (see Figure FIGREF2). At inference time, the length token is used to bias the network to generate a translation that belongs to the desired length group."
            ],
            "highlighted_evidence": [
              "Methods ::: Length Token Method\nOur first approach to control the length is inspired by target forcing in multilingual NMT BIBREF15, BIBREF16. We first split the training sentence pairs into three groups according to the target/source length ratio (in terms of characters). Ideally, we want a group where the target is shorter than the source (short), one where they are equally-sized (normal) and a last group where the target is longer than the source (long). In practice, we select two thresholds $t_\\text{min}$ and $t_\\text{max}$ according to the length ratio distribution. All the sentence pairs with length ratio between $t_\\text{min}$ and $t_\\text{max}$ are in the normal group, the ones with ratio below $t_\\text{min}$ in short and the remaining in long. At training time we prepend a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$), in order to let a single network to discriminate between the groups (see Figure FIGREF2). At inference time, the length token is used to bias the network to generate a translation that belongs to the desired length group."
            ]
          }
        ]
      },
      {
        "question": "Which languages do they focus on?",
        "question_id": "228425783a4830e576fb98696f76f4c7c0a1b906",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "two translation directions (En-It and En-De)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."
            ],
            "highlighted_evidence": [
              "We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source.",
              "En-It, En-De in both directions"
            ]
          }
        ]
      },
      {
        "question": "What dataset do they use?",
        "question_id": "9d1135303212356f3420ed010dcbe58203cc7db4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English$\\rightarrow $Italian/German portions of the MuST-C corpus",
              "As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."
            ],
            "highlighted_evidence": [
              "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)."
            ]
          }
        ]
      },
      {
        "question": "Do they experiment with combining both methods?",
        "question_id": "d8bf4a29c7af213a9a176eb1503ec97d01cc8f51",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Methods ::: Combining the two methods",
              "We further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length."
            ],
            "highlighted_evidence": [
              "Methods ::: Combining the two methods\nWe further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iwslt/LakewGF19",
    "dblp_title": "Controlling the Output Length of Neural Machine Translation.",
    "year": "2019"
  },
  {
    "id": "1606.05286",
    "title": "Spectral decomposition method of dialog state tracking via collective matrix factorization",
    "qas": [
      {
        "question": "What state-of-the-art models are compared against?",
        "question_id": "73abb173a3cc973ab229511cf53b426865a2738b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a deep neural network (DNN) architecture proposed in BIBREF24 ",
              "maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As a comparison to the state of the art methods, Table 1 presents accuracy results of the best Collective Matrix Factorization model, with a latent space dimension of 350, which has been determined by cross-validation on a development set, where the value of each slot is instantiated as the most probable w.r.t the inference procedure presented in Section \"Spectral decomposition model for state tracking in slot-filling dialogs\" . In our experiments, the variance is estimated using standard dataset reshuffling. The same results are obtained for several state of the art methods of generative and discriminative state tracking on this dataset using the publicly available results as reported in BIBREF22 . More precisely, as provided by the state-of-the-art approaches, the accuracy scores computes $p(s^*_{t+1}|s_t,z_t)$ commonly name the joint goal. Our proposition is compared to the 4 baseline trackers provided by the DSTC organisers. They are the baseline tracker (Baseline), the focus tracker (Focus), the HWU tracker (HWU) and the HWU tracker with “original” flag set to (HWU+) respectively. Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented."
            ],
            "highlighted_evidence": [
              "Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented.\n\n"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/dad/Perez16",
    "dblp_title": "Spectral decomposition method of dialog state tracking via collective matrix factorization.",
    "year": "2016"
  },
  {
    "id": "2002.00876",
    "title": "Torch-Struct: Deep Structured Prediction Library",
    "qas": [
      {
        "question": "Does API provide ability to connect to models written in some other deep learning framework?",
        "question_id": "1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can request samples $z \\sim \\textsc {CRF}(\\ell )$, probabilities $\\textsc {CRF}(z;\\ell )$, modes $\\arg \\max _z \\textsc {CRF}(\\ell )$, or other distributional properties such as $\\mathbb {H}(\\textsc {CRF}(\\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning."
            ],
            "highlighted_evidence": [
              "The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29."
            ]
          }
        ]
      },
      {
        "question": "Is this library implemented into Torch or is framework agnostic?",
        "question_id": "093039f974805952636c19c12af3549aa422ec43",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "It uses deep learning framework (pytorch)",
            "evidence": [
              "With this challenge in mind, we introduce Torch-Struct with three specific contributions:",
              "Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.",
              "Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python.",
              "Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization."
            ],
            "highlighted_evidence": [
              "With this challenge in mind, we introduce Torch-Struct with three specific contributions:\n\nModularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.\n\nCompleteness: a broad array of classical algorithms are implemented and new models can easily be added in Python.\n\nEfficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization."
            ]
          }
        ]
      },
      {
        "question": "What baselines are used in experiments?",
        "question_id": "8df89988adff57279db10992846728ec4f500eaa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Typical implementations of dynamic programming algorithms are serial in the length of the sequence",
              "Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized",
              "Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Optimizations ::: a) Parallel Scan Inference",
              "The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \\bigoplus _c \\ell _{t, \\cdot , c} \\otimes \\ell _{t^{\\prime }, c, \\cdot }$. Under this approach, we only need $O(\\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models.",
              "Optimizations ::: b) Vectorized Parsing",
              "Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$,",
              "Optimizations ::: c) Semiring Matrix Operations",
              "The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing,"
            ],
            "highlighted_evidence": [
              "Parallel Scan Inference\nThe commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence.",
              "Vectorized Parsing\nComputational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized.",
              "Semiring Matrix Operations\nThe two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost."
            ]
          }
        ]
      },
      {
        "question": "What general-purpose optimizations are included?",
        "question_id": "94edac71eea1e78add678fb5ed2d08526b51016b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Parallel Scan Inference",
              "Vectorized Parsing",
              "Semiring Matrix Operations"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Optimizations ::: a) Parallel Scan Inference",
              "Optimizations ::: b) Vectorized Parsing",
              "Optimizations ::: c) Semiring Matrix Operations",
              "Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms."
            ],
            "highlighted_evidence": [
              "a) Parallel Scan Inference",
              "b) Vectorized Parsing",
              "c) Semiring Matrix Operations",
              "Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/Rush20",
    "dblp_title": "Torch-Struct: Deep Structured Prediction Library.",
    "year": "2020"
  },
  {
    "id": "1906.10519",
    "title": "Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study",
    "qas": [
      {
        "question": "what baseline do they compare to?",
        "question_id": "9c4ed8ca59ba6d240f031393b01f634a9dc3615d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "VecMap",
              "Muse",
              "Barista"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section \"Previous Work\" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources. For all models (Mono, MT, VecMap, Muse, Barista), we take the average of the word embeddings in the source-language training examples and train a linear SVM. We report this instead of using the same feed-forward network as in Blse as it is the stronger upper bound. We choose the parameter $c$ on the target language development set and evaluate on the target language test set."
            ],
            "highlighted_evidence": [
              "We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section \"Previous Work\" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1906-10519",
    "dblp_title": "Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study.",
    "year": "2019"
  },
  {
    "id": "1905.13413",
    "title": "Improving Open Information Extraction via Iterative Rank-Aware Learning",
    "qas": [
      {
        "question": "How does this compare to traditional calibration methods like Platt Scaling?",
        "question_id": "ca7e71131219252d1fab69865804b8f89a2c0a8f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.",
            "evidence": [
              "Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. Ideally, the resulting model after one-round of training becomes better not only at confidence modeling, but also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\\mathcal {D}$ , initial model $\\theta ^{(0)}$ model after convergence $\\theta $ $t \\leftarrow 0$ # iteration",
              "A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 . However, we observe that this often yields sub-optimal ranking results, with incorrect extractions of one sentence having higher likelihood than correct extractions of another sentence. We hypothesize this is due to the issue of a disconnect between training and test-time objectives. Specifically, the system is trained solely to raise likelihood of gold-standard extractions, and during training the model is not aware of its test-time behavior of ranking a set of system-generated assertions across sentences that potentially include incorrect extractions.",
              "We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score. An extraction is judged as correct if the predicate and arguments include the syntactic head of the gold standard counterparts."
            ],
            "highlighted_evidence": [
              "Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision.",
              "For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 ",
              "We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score."
            ]
          }
        ]
      },
      {
        "question": "What's the input representation of OpenIE tuples into the model?",
        "question_id": "d77c9ede2727c28e0b5a240b2521fd49a19442e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "word embeddings",
            "evidence": [
              "Our training method in sec:ours could potentially be used with any probabilistic open IE model, since we make no assumptions about the model and only the likelihood of the extraction is required for iterative rank-aware learning. As a concrete instantiation in our experiments, we use RnnOIE BIBREF3 , BIBREF9 , a stacked BiLSTM with highway connections BIBREF10 , BIBREF11 and recurrent dropout BIBREF12 . Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $"
            ],
            "highlighted_evidence": [
              "Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/JiangYN19",
    "dblp_title": "Improving Open Information Extraction via Iterative Rank-Aware Learning.",
    "year": "2019"
  },
  {
    "id": "1909.07863",
    "title": "Character-Centric Storytelling",
    "qas": [
      {
        "question": "What statistics on the VIST dataset are reported?",
        "question_id": "a9610cbcca813f4376fbfbf21cc14689c7fbd677",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We used the Visual storytelling (VIST) dataset comprising of image sequences obtained from Flickr albums and respective annotated descriptions collected through Amazon Mechanical Turk BIBREF1. Each sequence has 5 images with corresponding descriptions that together make up for a story. Furthermore, for each Flickr album there are 5 permutations of a selected set of its images. In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
            ],
            "highlighted_evidence": [
              "We used the Visual storytelling (VIST) dataset comprising of image sequences obtained from Flickr albums and respective annotated descriptions collected through Amazon Mechanical Turk BIBREF1. Each sequence has 5 images with corresponding descriptions that together make up for a story. Furthermore, for each Flickr album there are 5 permutations of a selected set of its images. In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-07863",
    "dblp_title": "Character-Centric Storytelling.",
    "year": "2019"
  },
  {
    "id": "1906.07234",
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "qas": [
      {
        "question": "What is the performance difference in performance in unsupervised feature learning between adverserial training and FHVAE-based disentangled speech represenation learning?",
        "question_id": "64ab2b92e986e0b5058bf4f1758e849f6a41168b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/FengLP19",
    "dblp_title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling.",
    "year": "2019"
  },
  {
    "id": "1806.04535",
    "title": "Automatic Target Recovery for Hindi-English Code Mixed Puns",
    "qas": [
      {
        "question": "What are puns?",
        "question_id": "bcd6befa65cab3ffa6334c8ecedd065a4161028b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect BIBREF3 . Puns where the two meanings share the same pronunciation are known as homophonic or perfect puns, while those relying on similar but non-identical sounding words are known as heterophonic BIBREF4 or imperfect puns BIBREF5 . In this paper, we study automatic target recoverability of English-Hindi code mixed puns - which are more commonly imperfect puns, but may also be perfect puns in some cases."
            ],
            "highlighted_evidence": [
              "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect BIBREF3 ."
            ]
          }
        ]
      },
      {
        "question": "What are the categories of code-mixed puns?",
        "question_id": "479fc9e6d6d80e69f425d9e82e618e6b7cd12764",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "intra-sequential and intra-word"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "With India being a diverse linguistic region, there is an ever increasing usage of code-mixed Hindi-English language (along with various others) because bilingualism and even multilingualism are quite common. Consequently, we have also seen an increase in the usage of code-mixed language in online forums, advertisements etc. Code-mixed humour, especially puns have become increasingly popular because being able to use the same punning techniques but with two languages in play has opened up numerous avenues for new and interesting wordplays. With the increasing popularity and acceptance for the usage of code-mixed language, it has become important that computers are also able to process it and even decipher complex phenomena like humour. Traditional Word Sense Disambiguation (WSD) based methods cannot be used in target recovery of code-mixed puns, because they are no longer about multiple senses of a single word but about two words from two different languages. Code-switching comes with no markers, and the punning word may not even be a word in either of the languages being used. Sometimes words from the two languages can be combined to form a word which only a bilingual speaker would understand. Hence, this task on such data calls for a different set of strategies altogether. We approach this problem in two parts. First, we analyze the types of structures in code-mixed puns and classify them into two categories namely intra-sequential and intra-word. Second, we develop a four stage pipeline to achieve our goal - Language Identification, Pun Candidate Identification, Context Lookup and Phonetic Distance Minimization. We then test our approach on a small dataset and note that our method is successfully able to recover targets for a majority of the puns."
            ],
            "highlighted_evidence": [
              " First, we analyze the types of structures in code-mixed puns and classify them into two categories namely intra-sequential and intra-word. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1806-04535",
    "dblp_title": "Automatic Target Recovery for Hindi-English Code Mixed Puns.",
    "year": "2018"
  },
  {
    "id": "2003.05995",
    "title": "CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues",
    "qas": [
      {
        "question": "How is dialogue guided to avoid interactions that breach procedures and processes only known to experts?",
        "question_id": "bc26eee4ef1c8eff2ab8114a319901695d044edb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction. We then perform a data collection and compare our dataset to a similar dataset collected in a more controlled lab setting with a single Wizard BIBREF4 and discuss the advantages/disadvantages of both approaches. Finally, we present future work. Our contributions are as follows:",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions."
            ],
            "highlighted_evidence": [
              "In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions."
            ]
          }
        ]
      },
      {
        "question": "What is meant by semiguided dialogue, what part of dialogue is guided?",
        "question_id": "9c94ff8c99d3e51c256f2db78c34b2361f26b9c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The solution we propose in this paper tries to minimise these costs by increasing the pool of Wizards to anyone wanting to collaborate in the data collection, by providing them the necessary guidance to generate the desired dialogue behaviour. This is a valuable solution for collecting dialogues in domains where specific expertise is required and the cost of training capable Wizards is high. We required fine-grained control over the Wizard interface so as to be able to generate more directed dialogues for specialised domains, such as emergency response for offshore facilities. By providing the Wizard with several dialogue options (aside from free text), we guided the conversation and could introduce actions that change an internal system state. This proposes several advantages:",
              "A guided dialogue allows for set procedures to be learned and reduces the amount of data needed for a machine learning model for dialogue management to converge.",
              "Providing several dialogue options to the Wizard increases the pace of the interaction and allows them to understand and navigate more complex scenarios.",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions.",
              "The dialogue structure for the Emergency Assistant (the Wizard) followed a dialogue flow previously used for the original lab-based Wizard-of-Oz study BIBREF4 but which was slightly modified and simplified for this crowdsourced data collection. In addition to the transitions that the FSM provides, there are other fixed dialogue options always available such as “Hold on, 2 seconds”, “Okay” or “Sorry, can you repeat that?” as a shortcut for commonly used dialogue acts, as well as the option to type a message freely.",
              "The dialogue has several paths to reach the same states with varying levels of Operator control or engagement that enriched the heterogeneity of conversations. The Emergency Assistant dialogue options show various speaking styles, with a more assertive tone (“I am sending Husky 1 to east tower”) or others with more collaborative connotations (“Which robot do you want to send?” or “Husky 1 is available to send to east tower”). Refer to BIBREF4 for more details. Furthermore, neither participants were restricted in the number of messages that they could send and we did not require a balanced number of turns between them. However, there were several dialogue transitions that required an answer or authorisation from the Operator, so the FSM would lock the dialogue state until the condition was met. As mentioned earlier, the commands to control the robots are also transitions of the FSM, so they were not always available."
            ],
            "highlighted_evidence": [
              "By providing the Wizard with several dialogue options (aside from free text), we guided the conversation and could introduce actions that change an internal system state. This proposes several advantages:\n\nA guided dialogue allows for set procedures to be learned and reduces the amount of data needed for a machine learning model for dialogue management to converge.\n\nProviding several dialogue options to the Wizard increases the pace of the interaction and allows them to understand and navigate more complex scenarios.",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions.",
              "In addition to the transitions that the FSM provides, there are other fixed dialogue options always available such as “Hold on, 2 seconds”, “Okay” or “Sorry, can you repeat that?” as a shortcut for commonly used dialogue acts, as well as the option to type a message freely.",
              "The dialogue has several paths to reach the same states with varying levels of Operator control or engagement that enriched the heterogeneity of conversations."
            ]
          }
        ]
      },
      {
        "question": "Is CRWIZ already used for data collection, what are the results?",
        "question_id": "8e9de181fa7d96df9686d0eb2a5c43841e6400fa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",
            "evidence": [
              "For the intitial data collection using the CRWIZ platform, 145 unique dialogues were collected (each dialogue consists of a conversation between two participants). All the dialogues were manually checked by one of the authors and those where the workers were clearly not partaking in the task or collaborating were removed from the dataset. The average time per assignment was 10 minutes 47 seconds, very close to our initial estimate of 10 minutes, and the task was available for 5 days in AMT. Out of the 145 dialogues, 14 (9.66%) obtained the bonus of $0.2 for resolving the emergency. We predicted that only a small portion of the participants would be able to resolve the emergency in less than 6 minutes, thus it was framed as a bonus challenge rather than a requirement to get paid. The fastest time recorded to resolve the emergency was 4 minutes 13 seconds with a mean of 5 minutes 8 seconds. Table TABREF28 shows several interaction statistics for the data collected compared to the single lab-based WoZ study BIBREF4.",
              "Data Analysis ::: Subjective Data",
              "Table TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest.",
              "Mann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues at the 95% confidence level (Q1: $U = 1654.5$, $p < 0.0001$; Q2: $U = 2195$, $p = 0.009$, both $p < 0.05$). This indicates that effective collaboration and information ease are key to task completion in this setting.",
              "Regarding the qualitative data, one of the objectives of the Wizard-of-Oz technique was to make the participant believe that they are interacting with an automated agent and the qualitative feedback seemed to reflect this: “The AI in the game was not helpful at all [...]” or “I was talking to Fred a bot assistant, I had no other partner in the game“.",
              "Data Analysis ::: Single vs Multiple Wizards",
              "In Table TABREF28, we compare various metrics from the dialogues collected with crowdsourcing with the dialogues previously collected in a lab environment for a similar task. Most figures are comparable, except the number of emergency assistant turns (and consequently the total number of turns). To further understand these differences, we have first grouped the dialogue acts in four different broader types: Updates, Actions, Interactions and Requests, and computed the relative frequency of each of these types in both data collections. In addition, Figures FIGREF29 and FIGREF30 show the distribution of the most frequent dialogue acts in the different settings. It is visible that in the lab setting where the interaction was face-to-face with a robot, the Wizard used more Interaction dialogue acts (Table TABREF32). These were often used in context where the Wizard needed to hold the turn while looking for the appropriate prompt or waiting for the robot to arrive at the specified goal in the environment. On the other hand, in the crowdsourced data collection utterances, the situation updates were a more common choice while the assistant was waiting for the robot to travel to the specified goal in the environment.",
              "Perhaps not surprisingly, the data shows a medium strong positive correlation between task success and the number of Action type dialogue acts the Wizard performs, triggering events in the world leading to success ($R=0.475$). There is also a positive correlation between task success and the number of Request dialogue acts requesting confirmation before actions ($R=0.421$), e.g., “Which robot do you want to send?”. As Table 3 shows, these are relatively rare but perhaps reflect a level of collaboration needed to further the task to completion. Table TABREF40 shows one of the dialogues collected where the Emergency Assistant continuously engaged with the Operator through these types of dialogue acts.",
              "The task success rate was also very different between the two set-ups. In experiments reported in BIBREF4, 96% of the dialogues led to the extinction of the fire whereas in the crowdsourcing setting only 9.66% achieved the same goal. In the crowdsourced setting, the robots were slower moving at realistic speeds unlike the lab setting. A higher bonus and more time for the task might lead to a higher task success rate.",
              "Data Analysis ::: Limitations",
              "It is important to consider the number of available participants ready and willing to perform the task at any one time. This type of crowdsourcing requires two participants to connect within a few minutes of each other to be partnered together. As mentioned above, there were some issues with participants not collaborating and these dialogues had to be discarded as they were not of use."
            ],
            "highlighted_evidence": [
              "For the intitial data collection using the CRWIZ platform, 145 unique dialogues were collected (each dialogue consists of a conversation between two participants). ",
              "The average time per assignment was 10 minutes 47 seconds, very close to our initial estimate of 10 minutes, and the task was available for 5 days in AMT. Out of the 145 dialogues, 14 (9.66%) obtained the bonus of $0.2 for resolving the emergency. We predicted that only a small portion of the participants would be able to resolve the emergency in less than 6 minutes, thus it was framed as a bonus challenge rather than a requirement to get paid. The fastest time recorded to resolve the emergency was 4 minutes 13 seconds with a mean of 5 minutes 8 seconds. Table TABREF28 shows several interaction statistics for the data collected compared to the single lab-based WoZ study BIBREF4.\n\nData Analysis ::: Subjective Data\nTable TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest.\n\nMann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues at the 95% confidence level (Q1: $U = 1654.5$, $p < 0.0001$; Q2: $U = 2195$, $p = 0.009$, both $p < 0.05$). This indicates that effective collaboration and information ease are key to task completion in this setting.\n\nRegarding the qualitative data, one of the objectives of the Wizard-of-Oz technique was to make the participant believe that they are interacting with an automated agent and the qualitative feedback seemed to reflect this: “The AI in the game was not helpful at all [...]” or “I was talking to Fred a bot assistant, I had no other partner in the game“.\n\nData Analysis ::: Single vs Multiple Wizards\nIn Table TABREF28, we compare various metrics from the dialogues collected with crowdsourcing with the dialogues previously collected in a lab environment for a similar task. Most figures are comparable, except the number of emergency assistant turns (and consequently the total number of turns). To further understand these differences, we have first grouped the dialogue acts in four different broader types: Updates, Actions, Interactions and Requests, and computed the relative frequency of each of these types in both data collections. In addition, Figures FIGREF29 and FIGREF30 show the distribution of the most frequent dialogue acts in the different settings. It is visible that in the lab setting where the interaction was face-to-face with a robot, the Wizard used more Interaction dialogue acts (Table TABREF32). These were often used in context where the Wizard needed to hold the turn while looking for the appropriate prompt or waiting for the robot to arrive at the specified goal in the environment. On the other hand, in the crowdsourced data collection utterances, the situation updates were a more common choice while the assistant was waiting for the robot to travel to the specified goal in the environment.\n\nPerhaps not surprisingly, the data shows a medium strong positive correlation between task success and the number of Action type dialogue acts the Wizard performs, triggering events in the world leading to success ($R=0.475$). There is also a positive correlation between task success and the number of Request dialogue acts requesting confirmation before actions ($R=0.421$), e.g., “Which robot do you want to send?”. As Table 3 shows, these are relatively rare but perhaps reflect a level of collaboration needed to further the task to completion. Table TABREF40 shows one of the dialogues collected where the Emergency Assistant continuously engaged with the Operator through these types of dialogue acts.\n\nThe task success rate was also very different between the two set-ups. In experiments reported in BIBREF4, 96% of the dialogues led to the extinction of the fire whereas in the crowdsourcing setting only 9.66% achieved the same goal. In the crowdsourced setting, the robots were slower moving at realistic speeds unlike the lab setting. A higher bonus and more time for the task might lead to a higher task success rate.\n\nData Analysis ::: Limitations\nIt is important to consider the number of available participants ready and willing to perform the task at any one time. This type of crowdsourcing requires two participants to connect within a few minutes of each other to be partnered together. As mentioned above, there were some issues with participants not collaborating and these dialogues had to be discarded as they were not of use."
            ]
          }
        ]
      },
      {
        "question": "How does framework made sure that dialogue will not breach procedures?",
        "question_id": "ff1595a388769c6429423a75b6e1734ef88d3e46",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Wizard interface: the interface shown to participants with the Wizard role provides possible actions on the right-hand side of the browser window. These actions could be verbal, such as sending a message, or non-verbal, such as switching on/off a button to activate a robot. Figure FIGREF11 shows this interface with several actions available to be used in our data collection.",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions.",
              "System-changing actions: actions trigger transitions between the states in the FSM. We differentiate two types of actions:",
              "Verbal actions, such as the dialogue options available at that moment. The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.",
              "Non-verbal actions, such as commands to trigger events. These can take any form, but we used buttons to control robots in our data collection.",
              "Submitting an action would change the dialogue state in the FSM, altering the set of actions available in the subsequent turn visible to the Wizard. Some dialogue options are only possible at certain states, in a similar way as to how non-verbal actions are enabled or disabled depending on the state. This is reflected in the Wizard interface."
            ],
            "highlighted_evidence": [
              "Wizard interface: the interface shown to participants with the Wizard role provides possible actions on the right-hand side of the browser window. These actions could be verbal, such as sending a message, or non-verbal, such as switching on/off a button to activate a robot. ",
              "Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions.",
              "System-changing actions: actions trigger transitions between the states in the FSM. We differentiate two types of actions:\n\nVerbal actions, such as the dialogue options available at that moment. The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.\n\nNon-verbal actions, such as commands to trigger events. These can take any form, but we used buttons to control robots in our data collection.\n\nSubmitting an action would change the dialogue state in the FSM, altering the set of actions available in the subsequent turn visible to the Wizard. Some dialogue options are only possible at certain states, in a similar way as to how non-verbal actions are enabled or disabled depending on the state. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/GarciaLLH20",
    "dblp_title": "CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues.",
    "year": "2020"
  },
  {
    "id": "1710.07395",
    "title": "Detecting Online Hate Speech Using Context Aware Models",
    "qas": [
      {
        "question": "How do they combine the models?",
        "question_id": "dd2046f5481f11b7639a230e8ca92904da75feed",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "maximum of two scores assigned by the two separate models",
              "average score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF24 shows performance of ensemble models by combining prediction results of the best context-aware logistic regression model and the best context-aware neural network model. We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions."
            ],
            "highlighted_evidence": [
              "We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions."
            ]
          }
        ]
      },
      {
        "question": "What is their baseline?",
        "question_id": "47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Logistic regression model with character-level n-gram features"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For logistic regression model implementation, we use l2 loss. We adopt the balanced class weight as described in Scikit learn. Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective. BIBREF0 , BIBREF9"
            ],
            "highlighted_evidence": [
              " Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective."
            ]
          }
        ]
      },
      {
        "question": "What context do they use?",
        "question_id": "569ad21441e99ae782d325d5f5e1ac19e08d5e76",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "title of the news article",
              "screen name of the user"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In logistic regression models, we extract four types of features, word-level and character-level n-gram features as well as two types of lexicon derived features. We extract these four types of features from the target comment first. Then we extract these features from two sources of context texts, specifically the title of the news article that the comment was posted for and the screen name of the user who posted the comment."
            ],
            "highlighted_evidence": [
              "Then we extract these features from two sources of context texts, specifically the title of the news article that the comment was posted for and the screen name of the user who posted the comment."
            ]
          }
        ]
      },
      {
        "question": "What is their definition of hate speech?",
        "question_id": "90741b227b25c42e0b81a08c279b94598a25119d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful."
            ],
            "highlighted_evidence": [
              "We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation."
            ]
          }
        ]
      },
      {
        "question": "What architecture has the neural network?",
        "question_id": "1d739bb8e5d887fdfd1f4b6e39c57695c042fa25",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "three parallel LSTM BIBREF21 layers"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our neural network model mainly consists of three parallel LSTM BIBREF21 layers. It has three different inputs, including the target comment, its news title and its username. Comment and news title are encoded into a sequence of word embeddings. We use pre-trained word embeddings in word2vec. Username is encoded into a sequence of characters. We use one-hot encoding of characters."
            ],
            "highlighted_evidence": [
              "Our neural network model mainly consists of three parallel LSTM BIBREF21 layers."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ranlp/GaoH17",
    "dblp_title": "Detecting Online Hate Speech Using Context Aware Models.",
    "year": "2017"
  },
  {
    "id": "1904.02357",
    "title": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",
    "qas": [
      {
        "question": "How is human interaction consumed by the model?",
        "question_id": "5c70fdd3d6b67031768d3e28336942e49bf9a500",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "displays three different versions of a story written by three distinct models for a human to compare",
              "human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "gordon2009sayanything use an information retrieval based system to write by alternating turns between a human and their system. clark2018mil use a similar turn-taking approach to interactivity, but employ a neural model for generation and allow the user to edit the generated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. roemmele2017eval experiment with a user-driven setup, where the machine doesn't generate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful generation. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user's ability to knit a coherent overall story together out of locally related sentences. They also do not allow a user to control the novelty or “unexpectedness” of the generations, which clark2018mil find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sentences and have the system update later generations. We develop a system that allows a user to interact in all of these ways that were limitations in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged. We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories. The full range of interactions available to a user is: select a model, provide a topic, change diversity of content, collaborate on the planning for the story, and collaborate on the story sentences. It is entirely user-driven, as the users control how much is their own work and how much is the machine's at every stage. It supports revision; a user can modify an earlier part of a written story or of the story plan at any point, and observe how this affects later generations."
            ],
            "highlighted_evidence": [
              "We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories."
            ]
          }
        ]
      },
      {
        "question": "How do they evaluate generated stories?",
        "question_id": "f27502c3ece9ade265389d5ace90ca9ca42b46f3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "separate set of Turkers to rate the stories for overall quality and the three improvement areas"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis."
            ],
            "highlighted_evidence": [
              "We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis."
            ]
          }
        ]
      },
      {
        "question": "Do they evaluate in other language appart from English?",
        "question_id": "ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are the baselines?",
        "question_id": "aa4b38f601cc87bf93849245d5f65124da3dc112",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Title-to-Story system"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Title-to-Story system is a baseline, which generates directly from topic."
            ],
            "highlighted_evidence": [
              "The Title-to-Story system is a baseline, which generates directly from topic."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/Goldfarb-Tarrant19",
    "dblp_title": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation.",
    "year": "2019"
  },
  {
    "id": "1907.02636",
    "title": "Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling",
    "qas": [
      {
        "question": "What is used a baseline?",
        "question_id": "08b87a90139968095433f27fc88f571d939cd433",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As shown in TABLE TABREF24 , we report the micro average of precision, recall and F1-score for all 11 types of labels for a baseline as well as the proposed model. As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 . As presented in TABLE TABREF24 , the score obtained by the proposed model is clearly higher than the baseline. Here, as described in Section SECREF14 , the sizes of window and lower bounds of frequency for selecting contextual keywords are tuned as 4 and 7 throughout the evaluation of English dataset, and tuned as 3 and 4 throughout the evaluation of Chinese dataset. The number of extracted contextual keywords from the English dataset is 1,328, and from the Chinese dataset is 331."
            ],
            "highlighted_evidence": [
              "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 ."
            ]
          }
        ]
      },
      {
        "question": "What contextual features are used?",
        "question_id": "ef872807cb0c9974d18bbb886a7836e793727c3d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.",
            "evidence": [
              "IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word “ntdll.exe” is the name of a malicious file on the basis of the words “download” and “compromised” from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords."
            ],
            "highlighted_evidence": [
              " In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords."
            ]
          }
        ]
      },
      {
        "question": "Where are the cybersecurity articles used in the model sourced from?",
        "question_id": "4db3c2ca6ddc87209c31b20763b7a3c1c33387bc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018. All of these cybersecurity articles are used to train the English word embedding. Afterwards, we randomly select 370 articles, and manually annotate the IOCs contained in the articles. Among the selected articles, we randomly select 70 articles as the validation set and 70 articles as the test set; the remaining articles are used for training."
            ],
            "highlighted_evidence": [
              "For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018. "
            ]
          }
        ]
      },
      {
        "question": "What type of hand-crafted features are used in state of the art IOC detection systems?",
        "question_id": "63337fd803f6fdd060ebd0f53f9de79d451810cd",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcnn/LongTZHL19",
    "dblp_title": "Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling.",
    "year": "2019"
  },
  {
    "id": "1605.08675",
    "title": "Boosting Question Answering by Deep Entity Recognition",
    "qas": [
      {
        "question": "Do they compare DeepER against other approaches?",
        "question_id": "63496705fff20c55d4b3d8cdf4786f93e742dd3d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid)."
            ]
          }
        ]
      },
      {
        "question": "How is the data in RAFAEL labelled?",
        "question_id": "7b44bee49b7cb39cb7d5eec79af5773178c27d4d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",
            "evidence": [
              "Secondly, texts go through a cascade of annotation tools, enriching it with the following information:",
              "Morphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,",
              "Tagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,",
              "Syntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,",
              "Named entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 ."
            ],
            "highlighted_evidence": [
              "Secondly, texts go through a cascade of annotation tools, enriching it with the following information:\n\nMorphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,\n\nTagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,\n\nSyntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,\n\nNamed entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 .\n\n"
            ]
          }
        ]
      },
      {
        "question": "How do they handle polysemous words in their entity library?",
        "question_id": "6d54bad91b6ccd1108d1ddbff1d217c6806e0842",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "only the first word sense (usually the most common) is taken into account"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF54 shows an exemplary process of converting the first paragraph of a Polish Wikipedia entry, describing former Polish president Lech Wałęsa, into a list of WordNet synsets. First, we omit all unessential parts of the paragraph (1). This includes text in brackets or quotes, but also introductory expressions like jeden z (one of) or typ (type of). Then, an entity name is detached from the text by matching one of definition patterns (2). In the example we can see the most common one, a dash (–). Next, all occurrences of separators (full stops, commas and semicolons) are used to divide the text into separate chunks (3). The following step employs shallow parsing annotation – only nominal groups that appear at the beginning of the chunks are passed on (4). The first chunk that does not fulfil this requirement and all its successors get excluded from further analysis (4.1). Finally, we split the coordination groups and check, whether their lemmas correspond to any lexemes in WordNet (5). If not, the process repeats with the group replaced by its semantic head. In case of polysemous words, only the first word sense (usually the most common) is taken into account."
            ],
            "highlighted_evidence": [
              "In case of polysemous words, only the first word sense (usually the most common) is taken into account."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/Przybyla16",
    "dblp_title": "Boosting Question Answering by Deep Entity Recognition.",
    "year": "2016"
  },
  {
    "id": "1709.08858",
    "title": "Polysemy Detection in Distributed Representation of Word Sense",
    "qas": [
      {
        "question": "How is the fluctuation in the sense of the word and its neighbors measured?",
        "question_id": "238ec3c1e1093ce2f5122ee60209b969f7669fae",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.\n4) Computing the mean m and the sample variance σ for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",
            "evidence": [
              "Distributed representation of word sense provides us with the ability to perform several operations on the word. One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. When a word has several senses, it is called a polysemic word. When a word has only one sense, it is called a monosemic word. We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word. We can explain this fact as follows. Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation.",
              "To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor. We have found that there is a difference in the surrounding uniformity between a monosemic word and a polysemic word. This paper describes how to compute surrounding uniformity for a given word, and discuss the relationship between surrounding uniformity and polysemy.",
              "We choose the uniformity of vectors, which can be regarded as general case of triangle inequality. The uniformity of a set of vectors is a ratio, i.e., the size of the vector of the vector addition of the vectors divided by the scalar sum of the sizes of the vectors. If and only if all directions of the vectors are the same, the uniformity becomes 1.0. We compute this uniformity for the neighbors, including the word itself. Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$",
              "where $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$"
            ],
            "highlighted_evidence": [
              "One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word.",
              "We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word.",
              " Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation.",
              "To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity.",
              "The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor",
              "Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$\n\nwhere $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/kst/OomotoOY0OU17",
    "dblp_title": "Polysemy detection in distributed representation of word sense.",
    "year": "2017"
  },
  {
    "id": "1706.03610",
    "title": "Neural Domain Adaptation for Biomedical Question Answering",
    "qas": [
      {
        "question": "Among various transfer learning techniques, which technique yields to the best performance?",
        "question_id": "f704d182c9e01a2002381b76bf21e4bb3c0d3efc",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/conll/WieseWN17",
    "dblp_title": "Neural Domain Adaptation for Biomedical Question Answering.",
    "year": "2017"
  },
  {
    "id": "1908.11425",
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "qas": [
      {
        "question": "What is the architecture of the model?",
        "question_id": "da544015511e535503dee2eaf4912a5e36c806cd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF5 to train neural sequence-to-sequence",
              "NMF topic model with scikit-learn BIBREF14"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the method of BIBREF5 to train neural sequence-to-sequence Spanish-English ST models. As in that study, before training ST, we pre-train the models using English ASR data from the Switchboard Telephone speech corpus BIBREF7, which consists of around 300 hours of English speech and transcripts. This was reported to substantially improve translation quality when the training set for ST was only tens of hours.",
              "Obtaining gold topic labels for our data would require substantial manual annotation, so we instead use the human translations from the 1K (train20h) training set utterances to train the NMF topic model with scikit-learn BIBREF14, and then use this model to infer topics on the evaluation set. These silver topics act as an oracle: they tell us what a topic model would infer if it had perfect translations. NMF and model hyperparameters are described in Appendix SECREF7."
            ],
            "highlighted_evidence": [
              "We use the method of BIBREF5 to train neural sequence-to-sequence Spanish-English ST models.",
              "Obtaining gold topic labels for our data would require substantial manual annotation, so we instead use the human translations from the 1K (train20h) training set utterances to train the NMF topic model with scikit-learn BIBREF14"
            ]
          }
        ]
      },
      {
        "question": "What language do they look at?",
        "question_id": "7bc993b32484d6ae3c86d0b351a68e59fd2757a5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Spanish"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use the Fisher Spanish speech corpus BIBREF11, which consists of 819 phone calls, with an average duration of 12 minutes, amounting to a total of 160 hours of data. We discard the associated transcripts and pair the speech with English translations BIBREF12, BIBREF13. To simulate a low-resource scenario, we sampled 90 calls (20h) of data (train20h) to train both ST and topic models, reserving 450 calls (100h) to evaluate topic models (eval100h). Our experiments required ST models of varying quality, so we also trained models with decreasing amounts of data: ST-10h, ST-5h, and ST-2.5h are trained on 10, 5, and 2.5 hours of data respectively, sampled from train20h. To evaluate ST only, we use the designated Fisher test set, as in previous work."
            ],
            "highlighted_evidence": [
              "We use the Fisher Spanish speech corpus BIBREF11, which consists of 819 phone calls, with an average duration of 12 minutes, amounting to a total of 160 hours of data."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-11425",
    "dblp_title": "Classifying topics in speech when all you have is crummy translations.",
    "year": "2019"
  },
  {
    "id": "1711.04457",
    "title": "Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT",
    "qas": [
      {
        "question": "Where does the vocabulary come from?",
        "question_id": "da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LDC corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. Table 1 shows the detailed statistics of our training data. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
            ],
            "highlighted_evidence": [
              "Our training data consists of 2.09M sentence pairs extracted from LDC corpus."
            ]
          }
        ]
      },
      {
        "question": "What is the worst performing translation granularity?",
        "question_id": "e44a5514d7464993997212341606c2c0f3a72eb4",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What dataset did they use?",
        "question_id": "310e61b9dd4d75bc1bebbcb1dae578f55807cd04",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LDC corpus",
              "NIST 2003(MT03)",
              "NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06)",
              "NIST 2008(MT08)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. Table 1 shows the detailed statistics of our training data. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
            ],
            "highlighted_evidence": [
              "Our training data consists of 2.09M sentence pairs extracted from LDC corpus.",
              "To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/cwmt/WangZZZ17",
    "dblp_title": "Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT.",
    "year": "2017"
  },
  {
    "id": "1907.08501",
    "title": "A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data",
    "qas": [
      {
        "question": "How do they measure performance?",
        "question_id": "bdc6664cec2b94b0b3769bc70a60914795f39574",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online."
            ],
            "highlighted_evidence": [
              "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question."
            ]
          }
        ]
      },
      {
        "question": "Do they measure the performance of a combined approach?",
        "question_id": "e40df8c685a28b98006c47808f506def68f30e26",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which four QA systems do they use?",
        "question_id": "9653c89a93ac5c717a0a26cf80e9aa98a5ccf910",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The same 36 questions were answered using four QALD tools: WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8 ."
            ],
            "highlighted_evidence": [
              "The same 36 questions were answered using four QALD tools: WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8 ."
            ]
          }
        ]
      },
      {
        "question": "How many iterations of visual search are done on average until an answer is found?",
        "question_id": "b921a1771ed0ba9dbeff9da000336ecf2bb38322",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Do they test performance of their approaches using human judgements?",
        "question_id": "412aff0b2113b7d61c914edf90b90f2994390088",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online.",
              "To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given. INLINEFORM1 is the faction of correct answer (parts) given divided by all correct ones in the gold answer, and INLINEFORM2 is the harmonic mean of INLINEFORM3 and INLINEFORM4 . As an example, if the question is “Where was Albert Einstein born?” (gold answer: “Ulm”), and the system gives two answers “Ulm” and “Bern”, then INLINEFORM5 , INLINEFORM6 and INLINEFORM7 ."
            ],
            "highlighted_evidence": [
              "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online.",
              "To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ic3k/WohlgenanntMPEM19",
    "dblp_title": "A Comparative Evaluation of Visual and Natural Language Question Answering over Linked Data.",
    "year": "2019"
  },
  {
    "id": "2001.02943",
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "qas": [
      {
        "question": "What are the sizes of both datasets?",
        "question_id": "010e3793eb1342225857d3f95e147d8f8467192a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The Dutch section consists of 2,333,816 sentences and 53,487,257 words.",
              "The SONAR500 corpus consists of more than 500 million words obtained from different domains."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The datasets used for training, validation and testing contain sentences extracted from the Europarl corpus BIBREF1 and SoNaR corpus BIBREF2. The Europarl corpus is an open-source parallel corpus containing proceedings of the European Parliament. The Dutch section consists of 2,333,816 sentences and 53,487,257 words. The SoNaR corpus comprises two corpora: SONAR500 and SONAR1. The SONAR500 corpus consists of more than 500 million words obtained from different domains. Examples of text types are newsletters, newspaper articles, legal texts, subtitles and blog posts. All texts except for texts from social media have been automatically tokenized, POS tagged and lemmatized. It contains significantly more data and more varied data than the Europarl corpus. Due to the high amount of data in the corpus, only three subparts are used: Wikipedia texts, reports and newspaper articles. These subparts are chosen because the number of wrongly used die and dat is expected to be low."
            ],
            "highlighted_evidence": [
              "The datasets used for training, validation and testing contain sentences extracted from the Europarl corpus BIBREF1 and SoNaR corpus BIBREF2. The Europarl corpus is an open-source parallel corpus containing proceedings of the European Parliament. The Dutch section consists of 2,333,816 sentences and 53,487,257 words. The SoNaR corpus comprises two corpora: SONAR500 and SONAR1. The SONAR500 corpus consists of more than 500 million words obtained from different domains."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2001-02943",
    "dblp_title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction.",
    "year": "2020"
  },
  {
    "id": "1911.04952",
    "title": "'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs",
    "qas": [
      {
        "question": "Why are the scores for predicting perceived musical hardness and darkness extracted only for subsample of 503 songs?",
        "question_id": "c20bb0847ced490a793657fbaf6afb5ef54dad81",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How long is the model trained?",
        "question_id": "ff8557d93704120b65d9b597a4fab40b49d24b6d",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are lyrical topics present in the metal genre?",
        "question_id": "447eb98e602616c01187960c9c3011c62afd7c27",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Table TABREF10 displays the twenty resulting topics"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF10 displays the twenty resulting topics found within the text corpus using LDA. The topics are numbered in descending order according to their prevalence (weight) in the text corpus. For each topic, a qualitative interpretation is given along with the 10 most salient terms.",
              "FLOAT SELECTED: Table 1: Overview of the resulting topics found within the corpus of metal lyrics (n = 124,288) and their correlation to the dimensions hardness and darkness obtained from the audio signal (see section 3.2)"
            ],
            "highlighted_evidence": [
              "Table TABREF10 displays the twenty resulting topics found within the text corpus using LDA.",
              "FLOAT SELECTED: Table 1: Overview of the resulting topics found within the corpus of metal lyrics (n = 124,288) and their correlation to the dimensions hardness and darkness obtained from the audio signal (see section 3.2)"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-04952",
    "dblp_title": "&apos;Warriors of the Word&apos; - Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100, 000 Metal Songs.",
    "year": "2019"
  },
  {
    "id": "1910.00825",
    "title": "Abstractive Dialog Summarization with Semantic Scaffolds",
    "qas": [
      {
        "question": "By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?",
        "question_id": "f398587b9a0008628278a5ea858e01d3f5559f65",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "SPNet vs best baseline:\nROUGE-1: 90.97 vs 90.68\nCIC: 70.45 vs 70.25",
            "evidence": [
              "We show all the models' results in Table TABREF24. We observe that SPNet reaches the highest score in both ROUGE and CIC. Both Pointer-Generator and Transformer achieve high ROUGE scores, but a relative low CIC scores. It suggests that the baselines have more room for improvement on preserving critical slot information. All the scaffolds we propose can be applied to different neural network models. In this work we select Pointer-Generator as our base model in SPNet because we observe that Transformer only has a small improvement over Pointer-Generator but is having a higher cost on training time and computing resources. We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds. Semantic slot contributes the most to SPNet's increased performance, bringing the largest increase on all automatic evaluation metrics.",
              "FLOAT SELECTED: Table 1: Automatic evaluation results on MultiWOZ. We use Pointer-Generator as the base model and gradually add different semantic scaffolds."
            ],
            "highlighted_evidence": [
              "We show all the models' results in Table TABREF24",
              "FLOAT SELECTED: Table 1: Automatic evaluation results on MultiWOZ. We use Pointer-Generator as the base model and gradually add different semantic scaffolds."
            ]
          }
        ]
      },
      {
        "question": "What automatic and human evaluation metrics are used to compare SPNet to its counterparts?",
        "question_id": "d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "ROUGE and CIC",
              "relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We also perform human evaluation to verify if our method's increased performance on automatic evaluation metrics entails better human perceived quality. We randomly select 100 test samples from MultiWOZ test set for evaluation. We recruit 150 crowd workers from Amazon Mechanical Turk. For each sample, we show the conversation, reference summary, as well as summaries generated by Pointer-Generator and SPNet to three different participants. The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair (tie allowed).",
              "We show all the models' results in Table TABREF24. We observe that SPNet reaches the highest score in both ROUGE and CIC. Both Pointer-Generator and Transformer achieve high ROUGE scores, but a relative low CIC scores. It suggests that the baselines have more room for improvement on preserving critical slot information. All the scaffolds we propose can be applied to different neural network models. In this work we select Pointer-Generator as our base model in SPNet because we observe that Transformer only has a small improvement over Pointer-Generator but is having a higher cost on training time and computing resources. We observe that SPNet outperforms other methods in all the automatic evaluation metrics with a big margin, as it incorporates all the three semantic scaffolds. Semantic slot contributes the most to SPNet's increased performance, bringing the largest increase on all automatic evaluation metrics."
            ],
            "highlighted_evidence": [
              "The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair (tie allowed).",
              "We observe that SPNet reaches the highest score in both ROUGE and CIC"
            ]
          }
        ]
      },
      {
        "question": "Is proposed abstractive dialog summarization dataset open source?",
        "question_id": "58f3bfbd01ba9768172be45a819faaa0de2ddfa4",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",
        "question_id": "73633afbefa191b36cca594977204c6511f9dad4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Not at the moment, but summaries can be additionaly extended with this annotations.",
            "evidence": [
              "Moreover, we can easily extend SPNet to other summarization tasks. We plan to apply semantic slot scaffold to news summarization. Specifically, we can annotate the critical entities such as person names or location names to ensure that they are captured correctly in the generated summary. We also plan to collect a human-human dialog dataset with more diverse human-written summaries."
            ],
            "highlighted_evidence": [
              "We plan to apply semantic slot scaffold to news summarization. Specifically, we can annotate the critical entities such as person names or location names to ensure that they are captured correctly in the generated summary. We also plan to collect a human-human dialog dataset with more diverse human-written summaries."
            ]
          }
        ]
      },
      {
        "question": "How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?",
        "question_id": "db39a71080e323ba2ddf958f93778e2b875dcd24",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Our encoder-decoder framework employs separate encoding for different speakers in the dialog.",
              "We integrate semantic slot scaffold by performing delexicalization on original dialogs.",
              "We integrate dialog domain scaffold through a multi-task framework."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our encoder-decoder framework employs separate encoding for different speakers in the dialog. User utterances $x_t^{usr}$ and system utterances $x_t^{sys}$ are fed into a user encoder and a system encoder separately to obtain encoder hidden states $h_{i}^{usr}$ and $h_{i}^{sys}$ . The attention distributions and context vectors are calculated as described in section SECREF1. In order to merge these two encoders in our framework, the decoder's hidden state $s_0$ is initialized as:",
              "We integrate semantic slot scaffold by performing delexicalization on original dialogs. Delexicalization is a common pre-processing step in dialog modeling. Specifically, delexicalization replaces the slot values with its semantic slot name(e.g. replace 18:00 with [time]). It is easier for the language modeling to process delexicalized texts, as they have a reduced vocabulary size. But these generated sentences lack the semantic information due to the delexicalization. Some previous dialog system research ignored this issue BIBREF30 or completed single delexicalized utterance BIBREF31 as generated response. We propose to perform delexicalization in dialog summary, since delexicalized utterances can simplify dialog modeling. We fill the generated templates with slots with the copy and pointing mechanism.",
              "We integrate dialog domain scaffold through a multi-task framework. Dialog domain indicates different conversation task content, for example, booking hotel, restaurant and taxi in MultiWOZ dataset. Generally, the content in different domains varies so multi-domain task summarization is more difficult than single-domain. We include domain classification as the auxiliary task to incorporate the prior that different domains have different content. Feedback from the domain classification task provides domain specific information for the encoder to learn better representations. For domain classification, we feed the concatenated encoder hidden state through a binary classifier with two linear layers, producing domain probability $d$. The $i^{th}$ element $d_i$ in $d$ represents the probability of the $i^{th}$ domain:"
            ],
            "highlighted_evidence": [
              "Our encoder-decoder framework employs separate encoding for different speakers in the dialog. User utterances $x_t^{usr}$ and system utterances $x_t^{sys}$ are fed into a user encoder and a system encoder separately to obtain encoder hidden states $h_{i}^{usr}$ and $h_{i}^{sys}$ .",
              "We integrate semantic slot scaffold by performing delexicalization on original dialogs. Delexicalization is a common pre-processing step in dialog modeling.",
              "We integrate dialog domain scaffold through a multi-task framework. Dialog domain indicates different conversation task content, for example, booking hotel, restaurant and taxi in MultiWOZ dataset."
            ]
          }
        ]
      },
      {
        "question": "What are previous state-of-the-art document summarization methods used?",
        "question_id": "6da2cb3187d3f28b75ac0a61f6562a8adf716109",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Pointer-Generator",
              "Transformer"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To demonstrate SPNet's effectiveness, we compare it with two state-of-the-art methods, Pointer-Generator BIBREF5 and Transformer BIBREF6. Pointer-Generator is the state-of-the-art method in abstractive document summarization. In inference, we use length penalty and coverage penalty mentioned in BIBREF36. The hyperparameters in the original implementation BIBREF5 were used. Transformer uses attention mechanisms to replace recurrence for sequence transduction. Transformer generalizes well to many sequence-to-sequence problems, so we adapt it to our task, following the implementation in the official OpenNMT-py documentation."
            ],
            "highlighted_evidence": [
              "To demonstrate SPNet's effectiveness, we compare it with two state-of-the-art methods, Pointer-Generator BIBREF5 and Transformer BIBREF6."
            ]
          }
        ]
      },
      {
        "question": "How does new evaluation metric considers critical informative entities?",
        "question_id": "c47e87efab11f661993a14cf2d7506be641375e4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities",
            "evidence": [
              "In this case, the summary has a high ROUGE score, as it has a considerable proportion of word overlap with the reference summary. However, it still has poor relevance and readability, for leaving out one of the most critical information: [time]. ROUGE treats each word equally in computing n-gram overlap while the informativeness actually varies: common words or phrases (e.g. “You are going to\") significantly contribute to the ROUGE score and readability, but they are almost irrelevant to essential contents. The semantic slot values (e.g. [restaurant_name], [time]) are more essential compared to other words in the summary. However, ROUGE did not take this into consideration. To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows:",
              "where $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-occurring in the candidate summary and reference summary, and $m$ is the number of values in set $V$. In our experiments, CIC is computed as the arithmetic mean over all the dialog domains to retain the overall performance.",
              "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. For example, in news summarization the proper nouns are the critical information to retain."
            ],
            "highlighted_evidence": [
              "To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows:\n\nwhere $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-occurring in the candidate summary and reference summary, and $m$ is the number of values in set $V$. In our experiments, CIC is computed as the arithmetic mean over all the dialog domains to retain the overall performance.\n\nCIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities."
            ]
          }
        ]
      },
      {
        "question": "Is new evaluation metric extension of ROGUE?",
        "question_id": "14684ad200915ff1e3fc2a89cb614e472a1a2854",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. For example, in news summarization the proper nouns are the critical information to retain."
            ],
            "highlighted_evidence": [
              "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1910-00825",
    "dblp_title": "Abstractive Dialog Summarization with Semantic Scaffolds.",
    "year": "2019"
  },
  {
    "id": "1911.03705",
    "title": "CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning",
    "qas": [
      {
        "question": "What measures were used for human evaluation?",
        "question_id": "8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself)."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself). That is, if a model has the same reasoning ability with average performance of our crowd workers, its results should exceed this “human bound”."
            ],
            "highlighted_evidence": [
              "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself). That is, if a model has the same reasoning ability with average performance of our crowd workers, its results should exceed this “human bound”."
            ]
          }
        ]
      },
      {
        "question": "What automatic metrics are used for this task?",
        "question_id": "5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BLEU-3/4",
              "ROUGE-2/L",
              "CIDEr",
              "SPICE",
              "BERTScore"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For automatically evaluating our methods, we propose to use widely used metric for image/video captioning. This is because the proposed CommonGen task can be regarded as also a caption task where the context are incomplete scenes with given concept-sets. Therefore, we choose BLEU-3/4 BIBREF28, ROUGE-2/L BIBREF29, CIDEr BIBREF30, and SPICE BIBREF31 as the main metrics. Apart from these classic metrics, we also include a novel embedding-based metric named BERTScore BIBREF32. To make the comparisons more clear, we show the delta of BERTScore results by subtracting the score of merely using input concept-sets as target sentences, named $\\triangle $BERTS."
            ],
            "highlighted_evidence": [
              "For automatically evaluating our methods, we propose to use widely used metric for image/video captioning. This is because the proposed CommonGen task can be regarded as also a caption task where the context are incomplete scenes with given concept-sets. Therefore, we choose BLEU-3/4 BIBREF28, ROUGE-2/L BIBREF29, CIDEr BIBREF30, and SPICE BIBREF31 as the main metrics. Apart from these classic metrics, we also include a novel embedding-based metric named BERTScore BIBREF32. To make the comparisons more clear, we show the delta of BERTScore results by subtracting the score of merely using input concept-sets as target sentences, named $\\triangle $BERTS."
            ]
          }
        ]
      },
      {
        "question": "Are the models required to also generate rationales?",
        "question_id": "c34a15f1d113083da431e4157aceb11266e9a1b2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We explore how to utilize additional commonsense knowledge (i.e. rationales) as the input to the task. Like we mentioned in Section SECREF6, we search relevant sentences from the OMCS corpus as the additional distant rationales, and ground truth rationale sentences for dev/test data. The inputs are no longer the concept-sets themselves, but in a form of “[rationales$|$concept-set]” (i.e. concatenating the rationale sentences and original concept-set strings)."
            ],
            "highlighted_evidence": [
              "We explore how to utilize additional commonsense knowledge (i.e. rationales) as the input to the task. Like we mentioned in Section SECREF6, we search relevant sentences from the OMCS corpus as the additional distant rationales, and ground truth rationale sentences for dev/test data. The inputs are no longer the concept-sets themselves, but in a form of “[rationales$|$concept-set]” (i.e. concatenating the rationale sentences and original concept-set strings)."
            ]
          }
        ]
      },
      {
        "question": "Are the rationales generated after the sentences were written?",
        "question_id": "061682beb3dbd7c76cfa26f7ae650e548503d977",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. The crowd-sourced sentences correlate well with the associated captions, meaning that it is reasonable to use caption sentences as training data although they can be partly noisy. Additionally, we utilize a search engine over the OMCS corpus BIBREF16 for retrieving relevant propositions as distant rationales in training data."
            ],
            "highlighted_evidence": [
              "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes."
            ]
          }
        ]
      },
      {
        "question": "Are the sentences in the dataset written by humans who were shown the concept-sets?",
        "question_id": "3518d8eb84f6228407cfabaf509fd63d60351203",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "It is true that the above-mentioned associated caption sentences for each concept-set are human-written and do describe scenes that cover all given concepts. However, they are created under specific contexts (i.e. an image or a video) and thus might be less representative for common sense. To better measure the quality and interpretability of generative reasoners, we need to evaluate them with scenes and rationales created by using concept-sets only as the signals for annotators.",
              "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. The crowd-sourced sentences correlate well with the associated captions, meaning that it is reasonable to use caption sentences as training data although they can be partly noisy. Additionally, we utilize a search engine over the OMCS corpus BIBREF16 for retrieving relevant propositions as distant rationales in training data."
            ],
            "highlighted_evidence": [
              "It is true that the above-mentioned associated caption sentences for each concept-set are human-written and do describe scenes that cover all given concepts. However, they are created under specific contexts (i.e. an image or a video) and thus might be less representative for common sense. To better measure the quality and interpretability of generative reasoners, we need to evaluate them with scenes and rationales created by using concept-sets only as the signals for annotators.\n\nWe collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes."
            ]
          }
        ]
      },
      {
        "question": "Where do the concept sets come from?",
        "question_id": "617c77a600be5529b3391ab0c21504cd288cc7c7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "These concept-sets are sampled from several large corpora of image/video captions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Towards empowering machines with the generative commonsense reasoning ability, we create a large-scale dataset, named CommonGen, for the constrained text generation task. We collect $37,263$ concept-sets as the inputs, each of which contains three to five common concepts. These concept-sets are sampled from several large corpora of image/video captions, such that the concepts inside them are more likely to co-occur in natural scenes. Through crowd-sourcing via Amazon Mechanical Turk (AMT), we finally obtain $89,028$ human-written sentences as expected outputs. We investigate the performance of sophisticated sequence generation methods for the proposed task with both automatic metrics and human evaluation. The experiments show that all methods are far from human performance in generative commonsense reasoning. Our main contributions are as follows: 1) We introduce the first large-scale constrained text generation dataset targeting at generative commonsense reasoning; 2) We systematically compare methods for this (lexically) constrained text generation with extensive experiments and evaluation. 3) Our code and data are publicly available (w/ the URL in the abstract), so future research in this direction can be directly developed in a unified framework.",
              "Following the general definition in the largest commonsense knowledge graph, ConceptNet BIBREF11, we understand a concept as a common noun or verb. We aim to test the ability of generating natural scenes with a given set of concepts. The expected concept-sets in our task are supposed to be likely co-occur in natural, daily-life scenes . The concepts in images/videos captions, which usually describe scenes in our daily life, thus possess the desired property. We therefore collect a large amount of caption sentences from a variety of datasets, including VATEX BIBREF4, LSMDC BIBREF12, ActivityNet BIBREF13, and SNLI BIBREF15, forming 1,040,330 sentences in total."
            ],
            "highlighted_evidence": [
              "We collect $37,263$ concept-sets as the inputs, each of which contains three to five common concepts. These concept-sets are sampled from several large corpora of image/video captions, such that the concepts inside them are more likely to co-occur in natural scenes.",
              "The expected concept-sets in our task are supposed to be likely co-occur in natural, daily-life scenes . The concepts in images/videos captions, which usually describe scenes in our daily life, thus possess the desired property. We therefore collect a large amount of caption sentences from a variety of datasets, including VATEX BIBREF4, LSMDC BIBREF12, ActivityNet BIBREF13, and SNLI BIBREF15, forming 1,040,330 sentences in total."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-03705",
    "dblp_title": "CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning.",
    "year": "2019"
  },
  {
    "id": "1910.00458",
    "title": "MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension",
    "qas": [
      {
        "question": "How big are improvements of MMM over state of the art?",
        "question_id": "53d6cbee3606dd106494e2e98aa93fdd95920375",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "test accuracy of 88.9%, which exceeds the previous best by 16.9%"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We first evaluate our method on the DREAM dataset. The results are summarized in Table TABREF16. In the table, we first report the accuracy of the SOTA models in the leaderboard. We then report the performance of our re-implementation of fine-tuned models as another set of strong baselines, among which the RoBERTa-Large model has already surpassed the previous SOTA. For these baselines, the top-level classifier is a two-layer FCNN for BERT-based models and a one-layer FCNN for the RoBERTa-Large model. Lastly, we report model performances that use all our proposed method, MMM (MAN classifier + speaker normalization + two stage learning strategies). As direct comparisons, we also list the accuracy increment between MMM and the baseline with the same sentence encoder marked by the parentheses, from which we can see that the performance augmentation is over 9% for BERT-Base and BERT-Large. Although the RoBERTa-Large baseline has already outperformed the BERT-Large baseline by around 18%, MMM gives us another $\\sim $4% improvement, pushing the accuracy closer to the human performance. Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%."
            ],
            "highlighted_evidence": [
              "Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%."
            ]
          }
        ]
      },
      {
        "question": "What out of domain datasets authors used for coarse-tuning stage?",
        "question_id": "9dc844f82f520daf986e83466de0c84d93953754",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MultiNLI BIBREF15 and SNLI BIBREF16 "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use four MCQA datasets as the target datasets: DREAM BIBREF6, MCTest BIBREF9, TOEFL BIBREF5, and SemEval-2018 Task 11 BIBREF14, which are summarized in Table TABREF11. For the first coarse-tuning stage with NLI tasks, we use MultiNLI BIBREF15 and SNLI BIBREF16 as the out-of-domain source datasets. For the second stage, we use the current largest MCQA dataset, i.e., RACE BIBREF7 as in-domain source dataset. For all datasets, we use the official train/dev/test splits."
            ],
            "highlighted_evidence": [
              "For the first coarse-tuning stage with NLI tasks, we use MultiNLI BIBREF15 and SNLI BIBREF16 as the out-of-domain source datasets. "
            ]
          }
        ]
      },
      {
        "question": "What are state of the art methods MMM is compared to?",
        "question_id": "9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "FTLM++, BERT-large, XLNet",
            "evidence": [
              "FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines."
            ]
          }
        ]
      },
      {
        "question": "What four representative datasets are used for bechmark?",
        "question_id": "36d892460eb863220cd0881d5823d73bbfda172c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Recently large and powerful pre-trained language models such as BERT BIBREF8 have been achieving the state-of-the-art (SOTA) results on various tasks, however, its potency on MCQA datasets has been severely limited by the data insufficiency. For example, the MCTest dataset has two variants: MC160 and MC500, which are curated in a similar way, and MC160 is considered easier than MC500 BIBREF9. However, BERT-based models perform much worse on MC160 compared with MC500 (8–10% gap) since the data size of the former is about three times smaller. To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11."
            ],
            "highlighted_evidence": [
              "To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/JinGKCH20",
    "dblp_title": "MMM: Multi-Stage Multi-Task Learning for Multi-Choice Reading Comprehension.",
    "year": "2020"
  },
  {
    "id": "2001.11268",
    "title": "Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks",
    "qas": [
      {
        "question": "What baselines did they consider?",
        "question_id": "4cbc56d0d53c4c03e459ac43e3c374b75fd48efe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LSTM",
              "SCIBERT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this work we investigate state-of-the-art methods for language modelling and sentence classification. Our contributions are centred around developing transformer-based fine-tuning approaches tailored to SR tasks. We compare our sentence classification with the LSTM baseline and evaluate the biggest set of PICO sentence data available at this point BIBREF13. We demonstrate that models based on the BERT architecture solve problems related to ambiguous sentence labels by learning to predict multiple labels reliably. Further, we show that the improved feature representation and contextualization of embeddings lead to improved performance in biomedical data extraction tasks. These fine-tuned models show promising results while providing a level of flexibility to suit reviewing tasks, such as the screening of studies for inclusion in reviews. By predicting on multilingual and full text contexts we showed that the model's capabilities for transfer learning can be useful when dealing with diverse, real-world data.",
              "Figure FIGREF23 shows the same set of sentences, represented by concatenations of SCIBERT outputs. SCIBERT was chosen as an additional baseline model for fine-tuning because it provided the best representation of embedded PICO sentences. When clustered, its embeddings yielded an adjusted rand score of 0.57 for a concatenation of the two layers, compared with 0.25 for BERT-base."
            ],
            "highlighted_evidence": [
              "We compare our sentence classification with the LSTM baseline and evaluate the biggest set of PICO sentence data available at this point BIBREF13.",
              "SCIBERT was chosen as an additional baseline model for fine-tuning because it provided the best representation of embedded PICO sentences. "
            ]
          }
        ]
      },
      {
        "question": "What are the problems related to ambiguity in PICO sentence prediction tasks?",
        "question_id": "e5a965e7a109ae17a42dd22eddbf167be47fca75",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Some sentences are associated to ambiguous dimensions in the hidden state output",
            "evidence": [
              "Sentences 1 and 2 are labelled incorrectly, and clearly appear far away from the population class centroid. Sentence 3 is an example of an ambiguous case. It appears very close to the population centroid, but neither its label nor its position reflect the intervention content. This supports a need for multiple tags per sentence, and the fine-tuning of weights within the network.",
              "FLOAT SELECTED: Figure 2: Visualization of training sentences using BERTbase. The x and y-axis represent the two most dominant dimensions in the hidden state output, as selected by the t-SNE algorithm. This visualization uses the sixth layer from the top, and shows three examples of labelled P sentences and their embedded positions."
            ],
            "highlighted_evidence": [
              "Sentence 3 is an example of an ambiguous case. It appears very close to the population centroid, but neither its label nor its position reflect the intervention content. ",
              "FLOAT SELECTED: Figure 2: Visualization of training sentences using BERTbase. The x and y-axis represent the two most dominant dimensions in the hidden state output, as selected by the t-SNE algorithm. This visualization uses the sixth layer from the top, and shows three examples of labelled P sentences and their embedded positions."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/biostec/SchmidtWH20",
    "dblp_title": "Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.",
    "year": "2020"
  },
  {
    "id": "1706.07179",
    "title": "RelNet: End-to-End Modeling of Entities & Relations",
    "qas": [
      {
        "question": "How is knowledge retrieved in the memory?",
        "question_id": "082c88e132b4f1bf68abdc3a21ac4af180de1113",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Recently, BIBREF17 proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, this model lacks any module for relational reasoning. In response, we propose RelNet, which extends memory-augmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. The only supervision signal for our method comes from answering questions on the text."
            ],
            "highlighted_evidence": [
              "Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector."
            ]
          }
        ]
      },
      {
        "question": "How is knowledge stored in the memory?",
        "question_id": "74091e10f596428135b0ab06008608e09c051565",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "entity memory and relational memory.",
            "evidence": [
              "There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with $T$ sentences, where each sentence consists of a sequence of words represented with $K$ -dimensional word embeddings $\\lbrace e_1, \\ldots , e_N\\rbrace $ , a question on the document represented as another sequence of words and an answer to the question."
            ],
            "highlighted_evidence": [
              "There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with $T$ sentences, where each sentence consists of a sequence of words represented with $K$ -dimensional word embeddings $\\lbrace e_1, \\ldots , e_N\\rbrace $ , a question on the document represented as another sequence of words and an answer to the question."
            ]
          }
        ]
      },
      {
        "question": "What are the relative improvements observed over existing methods?",
        "question_id": "43b4f7eade7a9bcfaf9cc0edba921a41d6036e9c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ],
            "highlighted_evidence": [
              " The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks.",
              "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ]
          }
        ]
      },
      {
        "question": "What is the architecture of the neural network?",
        "question_id": "a75861e6dd72d69fdf77ebd81c78d26c6f7d0864",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "extends memory-augmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. ",
              "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We describe the RelNet model in this section. Figure 1 provides a high-level view of the model. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory."
            ],
            "highlighted_evidence": [
              "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory."
            ]
          }
        ]
      },
      {
        "question": "What methods is RelNet compared to?",
        "question_id": "60fd7ef7986a5752b31d3bd12bbc7da6843547a4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We evaluate the model's performance on the bAbI tasks BIBREF18 , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks.",
              "The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ],
            "highlighted_evidence": [
              "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 .",
              " The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/akbc/BansalNM17",
    "dblp_title": "RelNet: End-to-end Modeling of Entities &amp; Relations.",
    "year": "2017"
  },
  {
    "id": "1909.08824",
    "title": "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder",
    "qas": [
      {
        "question": "How do they measure the diversity of inferences?",
        "question_id": "7d59374d9301a0c09ea5d023a22ceb6ce07fb490",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "by number of distinct n-grams",
            "evidence": [
              "We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens."
            ],
            "highlighted_evidence": [
              "Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. "
            ]
          }
        ]
      },
      {
        "question": "By how much do they improve the accuracy of inferences over state-of-the-art methods?",
        "question_id": "8e2b125426d1220691cceaeaf1875f76a6049cbd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",
            "evidence": [
              "We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens.",
              "FLOAT SELECTED: Table 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind. The the best result for each dimension is emboldened.",
              "FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened."
            ],
            "highlighted_evidence": [
              "Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. ",
              "FLOAT SELECTED: Table 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind. The the best result for each dimension is emboldened.",
              "FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened."
            ]
          }
        ]
      },
      {
        "question": "Which models do they use as baselines on the Atomic dataset?",
        "question_id": "42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "RNN-based Seq2Seq",
              "Variational Seq2Seq",
              "VRNMT ",
              "CWVAE-Unpretrained"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compared our proposed model with the following four baseline methods:",
              "RNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.",
              "Variational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.",
              "VRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.",
              "CWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.",
              "Note that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.",
              "FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened."
            ],
            "highlighted_evidence": [
              "We compared our proposed model with the following four baseline methods:\n\nRNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.\n\nVariational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.\n\nVRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.\n\nCWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.\n\nNote that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.",
              "FLOAT SELECTED: Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference dimension of Atomic. The the best result for each dimension is emboldened."
            ]
          }
        ]
      },
      {
        "question": "How does the context-aware variational autoencoder learn event background information?",
        "question_id": "fb76e994e2e3fa129f1e94f1b043b274af8fb84c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": " CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",
            "evidence": [
              "In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.)."
            ],
            "highlighted_evidence": [
              "In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.)."
            ]
          }
        ]
      },
      {
        "question": "What is the size of the Atomic dataset?",
        "question_id": "99ef97336c0112d9f60df108f58c8b04b519a854",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/DuDLL19",
    "dblp_title": "Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder.",
    "year": "2019"
  },
  {
    "id": "1708.08615",
    "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
    "qas": [
      {
        "question": "what standard speech transcription pipeline was used?",
        "question_id": "95d8368b1055d97250df38d1e8c4a2b283d2b57e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "pipeline that is used at Microsoft for production data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The goal of reaching “human parity” in automatic CTS transcription raises the question of what should be considered human accuracy on this task. We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol. Using this methodology, and incorporating state-of-the-art convolutional and recurrent network architectures for both acoustic modeling BIBREF9 , BIBREF10 , BIBREF7 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 and language modeling BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 with extensive use of model combination, we obtained a machine error rate that was very slightly below that of the human transcription process (5.8% versus 5.9% on Switchboard data, and 11.0% versus 11.3% on CallHome English data) BIBREF19 . Since then, Saon et al. have reported even better results, along with a separate transcription experiment that puts the human error rate, on the same test data, at a lower point than measured by us (5.1% for Switchboard, 6.8% for CallHome) BIBREF20 ."
            ],
            "highlighted_evidence": [
              "We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/interspeech/StolckeD17",
    "dblp_title": "Comparing Human and Machine Errors in Conversational Speech Transcription.",
    "year": "2017"
  },
  {
    "id": "1701.03214",
    "title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation",
    "qas": [
      {
        "question": "How much improvement does their method get over the fine tuning baseline?",
        "question_id": "a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",
            "evidence": [
              "FLOAT SELECTED: Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE."
            ]
          }
        ]
      },
      {
        "question": "What kinds of neural networks did they use in this paper?",
        "question_id": "46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LSTMs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For NMT, we used the KyotoNMT system BIBREF16 . The NMT training settings are the same as those of the best systems that participated in WAT 2016. The sizes of the source and target vocabularies, the source and target side embeddings, the hidden states, the attention mechanism hidden states, and the deep softmax output with a 2-maxout layer were set to 32,000, 620, 1000, 1000, and 500, respectively. We used 2-layer LSTMs for both the source and target sides. ADAM was used as the learning algorithm, with a dropout rate of 20% for the inter-layer dropout, and L2 regularization with a weight decay coefficient of 1e-6. The mini batch size was 64, and sentences longer than 80 tokens were discarded. We early stopped the training process when we observed that the BLEU score of the development set converges. For testing, we self ensembled the three parameters of the best development loss, the best development BLEU, and the final parameters. Beam size was set to 100."
            ],
            "highlighted_evidence": [
              "For NMT, we used the KyotoNMT system BIBREF16 . The NMT training settings are the same as those of the best systems that participated in WAT 2016. The sizes of the source and target vocabularies, the source and target side embeddings, the hidden states, the attention mechanism hidden states, and the deep softmax output with a 2-maxout layer were set to 32,000, 620, 1000, 1000, and 500, respectively. We used 2-layer LSTMs for both the source and target sides. ADAM was used as the learning algorithm, with a dropout rate of 20% for the inter-layer dropout, and L2 regularization with a weight decay coefficient of 1e-6. The mini batch size was 64, and sentences longer than 80 tokens were discarded."
            ]
          }
        ]
      },
      {
        "question": "How did they use the domain tags?",
        "question_id": "4f12b41bd3bb2610abf7d7835291496aa69fb78c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Appending the domain tag “<2domain>\" to the source sentences of the respective corpora"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The multi domain method is originally motivated by BIBREF14 , which uses tags to control the politeness of NMT translations. The overview of this method is shown in the dotted section in Figure 2 . In this method, we simply concatenate the corpora of multiple domains with two small modifications: a. Appending the domain tag “<2domain>\" to the source sentences of the respective corpora. This primes the NMT decoder to generate sentences for the specific domain. b. Oversampling the smaller corpus so that the training procedure pays equal attention to each domain."
            ],
            "highlighted_evidence": [
              "In this method, we simply concatenate the corpora of multiple domains with two small modifications: a. Appending the domain tag “<2domain>\" to the source sentences of the respective corpora. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/ChuDK17",
    "dblp_title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation.",
    "year": "2017"
  },
  {
    "id": "1709.05411",
    "title": "Combining Search with Structured Data to Create a More Engaging User Experience in Open Domain Dialogue",
    "qas": [
      {
        "question": "Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?",
        "question_id": "65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "do not follow a particular plan or pursue a particular fixed information need",
              " integrating content found via search with content from structured data",
              "at each system turn, there are a large number of conversational moves that are possible",
              "most other domains do not have such high quality structured data available",
              "live search may not be able to achieve the required speed and efficiency"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Alexa Prize funded 12 international teams to compete to create a conversational agent that can discuss any topic for at least 20 minutes. UCSC's Slugbot was one of these funded teams. The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need. This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation. SlugBot's conversations over the semi-finals user evaluation averaged 8:17 minutes.",
              "More challenging is that at each system turn, there are a large number of conversational moves that are possible. Making good decisions about what to say next requires balancing a dialogue policy as to what dialogue acts might be good in this context, with real-time information as to what types of content might be possible to use in this context. Slugbot could offer an opinion as in turn S3, ask a follow-on question as in S3, take the initiative to provide unasked for information, as in S5, or decide, e.g. in the case of the user's request for plot information, to use search to retrieve some relevant content. Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary. Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence."
            ],
            "highlighted_evidence": [
              "The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need. ",
              "This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation",
              "More challenging is that at each system turn, there are a large number of conversational moves that are possible.",
              " Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence.",
              " Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1709-05411",
    "dblp_title": "Combining Search with Structured Data to Create a More Engaging User Experience in Open Domain Dialogue.",
    "year": "2017"
  },
  {
    "id": "1805.12032",
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "qas": [
      {
        "question": "How is speed measured?",
        "question_id": "b54fc86dc2cc6994e10c1819b6405de08c496c7b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The first metric we report is the reaction type. Recent studies have found that 59% of bitly-URLs on Twitter are shared without ever being read BIBREF11 , and 73% of Reddit posts were voted on without reading the linked article BIBREF12 . Instead, users tend to rely on the commentary added to retweets or the comments section of Reddit-posts for information on the content and its credibility. Faced with this reality, we ask: what kind of reactions do users find when they browse sources of varying credibility? Discourse acts, or speech acts, can be used to identify the use of language within a conversation, e.g., agreement, question, or answer. Recent work by Zhang et al. zhang2017characterizing classified Reddit comments by their primary discourse act (e.g., question, agreement, humor), and further analyzed patterns from these discussions.",
              "The second metric we report is reaction speed. A study by Jin et al. jin2013epidemiological found that trusted news stories spread faster than misinformation or rumor; Zeng et al. zeng2016rumors found that tweets which deny rumors had shorter delays than tweets of support. Our second goal is to determine if these trends are maintained for various types of news sources on Twitter and Reddit.",
              "To examine whether users react to content from trusted sources differently than from deceptive sources, we measure the reaction delay, which we define as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred. We report the cumulative distribution functions (CDFs) for each source type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility."
            ],
            "highlighted_evidence": [
              "The first metric we report is the reaction type.",
              "The second metric we report is reaction speed. A study by Jin et al. jin2013epidemiological found that trusted news stories spread faster than misinformation or rumor; Zeng et al. zeng2016rumors found that tweets which deny rumors had shorter delays than tweets of support. Our second goal is to determine if these trends are maintained for various types of news sources on Twitter and Reddit.",
              "To examine whether users react to content from trusted sources differently than from deceptive sources, we measure the reaction delay, which we define as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred. We report the cumulative distribution functions (CDFs) for each source type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility."
            ]
          }
        ]
      },
      {
        "question": "What is the architecture of their model?",
        "question_id": "b43a8a0f4b8496b23c89730f0070172cd5dca06a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.",
              "We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
            ],
            "highlighted_evidence": [
              "Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.",
              "We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
            ]
          }
        ]
      },
      {
        "question": "What are the nine types?",
        "question_id": "b161febf86cdd58bd247a934120410068b24b7d1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "agreement",
              "answer",
              "appreciation",
              "disagreement",
              "elaboration",
              "humor",
              "negative reaction",
              "question",
              "other"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models."
            ],
            "highlighted_evidence": [
              "\n",
              "In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/GlenskiWV18",
    "dblp_title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources.",
    "year": "2018"
  },
  {
    "id": "1611.02550",
    "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
    "qas": [
      {
        "question": "How do they represent input features of their model to train embeddings?",
        "question_id": "d40662236eed26f17dd2a3a9052a4cee1482d7d6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a vector of frame-level acoustic features"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 . The basic embedding model structure we use is shown in Fig. FIGREF1 . The model consists of a deep RNN with some number INLINEFORM3 of stacked layers, whose final hidden state vector is passed as input to a set of INLINEFORM4 of fully connected layers; the output of the final fully connected layer is the embedding INLINEFORM5 ."
            ],
            "highlighted_evidence": [
              "An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 ."
            ]
          }
        ]
      },
      {
        "question": "Which dimensionality do they use for their embeddings?",
        "question_id": "1d791713d1aa77358f11501f05c108045f53c8aa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1061"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our classifier-based embeddings use LSTM or GRU networks with 2–4 stacked layers and 1–3 fully connected layers. The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. The recurrent hidden state dimensionality is fixed at 512 and dropout BIBREF32 between stacked recurrent layers is used with probability INLINEFORM0 . The fully connected hidden layer dimensionality is fixed at 1024. Rectified linear unit (ReLU) non-linearities and dropout with INLINEFORM1 are used between fully-connected layers. However, between the final recurrent hidden state output and the first fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set."
            ],
            "highlighted_evidence": [
              "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061."
            ]
          }
        ]
      },
      {
        "question": "Which dataset do they use?",
        "question_id": "6b6360fab2edc836901195c0aba973eae4891975",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Switchboard conversational English corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models INLINEFORM0 ) are 39-dimensional MFCCs+ INLINEFORM1 + INLINEFORM2 . We use the same train, development, and test partitions as in prior work BIBREF13 , BIBREF11 , and the same acoustic features as in BIBREF13 , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in BIBREF13 , when training the classification-based embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments."
            ],
            "highlighted_evidence": [
              "The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 ."
            ]
          }
        ]
      },
      {
        "question": "By how much do they outpeform previous results on the word discrimination task?",
        "question_id": "b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Their best average precision tops previous best result by 0.202",
            "evidence": [
              "FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/SettleL16",
    "dblp_title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches.",
    "year": "2016"
  },
  {
    "id": "2003.05522",
    "title": "Semantic Holism and Word Representations in Artificial Neural Networks",
    "qas": [
      {
        "question": "How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis?",
        "question_id": "86a93a2d1c19cd0cd21ad1608f2a336240725700",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "interpretation of Frege's work are examples of holistic approaches to meaning"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Both the distributional hypothesis itself and Tugendhat's interpretation of Frege's work are examples of holistic approaches to meaning, where the meaning of the whole determines the meaning of parts. As we demonstrated on the opposition between Skip-gram and CBOW models, the distinction between semantic holism and atomism may play an essential role in semantic properties of neural language representations models."
            ],
            "highlighted_evidence": [
              "Both the distributional hypothesis itself and Tugendhat's interpretation of Frege's work are examples of holistic approaches to meaning, where the meaning of the whole determines the meaning of parts."
            ]
          }
        ]
      },
      {
        "question": "What does Frege's holistic and functional approach to meaning states?",
        "question_id": "6090d3187c41829613abe785f0f3665d9ecd90d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Only in the context of a sentence does a word have a meaning."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Frege promoted what we could call sentence holism: “Only in the context of a sentence does a word have a meaning.” BIBREF10 We will later use its modern reformulation to show an analogy with certain neural language models and therefore their holistic character."
            ],
            "highlighted_evidence": [
              "Frege promoted what we could call sentence holism: “Only in the context of a sentence does a word have a meaning.” BIBREF10"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2003-05522",
    "dblp_title": "Semantic Holism and Word Representations in Artificial Neural Networks.",
    "year": "2020"
  },
  {
    "id": "1601.06068",
    "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
    "qas": [
      {
        "question": "Do they evaluate the quality of the paraphrasing model?",
        "question_id": "117aa7811ed60e84d40cd8f9cb3ca78781935a98",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many paraphrases are generated per question?",
        "question_id": "c359ab8ebef6f60c5a38f5244e8c18d85e92761d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",
            "evidence": [
              "For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.? $\\mid $ NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging BIBREF38 . For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases. We generate ungrounded graphs for these paraphrases and treat the final entity disambiguation and paraphrase selection as part of the semantic parsing problem."
            ],
            "highlighted_evidence": [
              "For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases. "
            ]
          }
        ]
      },
      {
        "question": "What latent variables are modeled in the PCFG?",
        "question_id": "ad362365656b0b218ba324ae60701eb25fe664c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "syntactic information",
              "semantic and topical information"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In their traditional use, the latent states in L-PCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions."
            ],
            "highlighted_evidence": [
              "We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions."
            ]
          }
        ]
      },
      {
        "question": "What are the baselines?",
        "question_id": "423bb905e404e88a168e7e807950e24ca166306c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "GraphParser without paraphrases",
              "monolingual machine translation based model for paraphrase generation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases.",
              "We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions."
            ],
            "highlighted_evidence": [
              "We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases",
              "We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/inlg/NarayanRC16",
    "dblp_title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing.",
    "year": "2016"
  },
  {
    "id": "1709.07916",
    "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter",
    "qas": [
      {
        "question": "Do they evaluate only on English data?",
        "question_id": "e5ae8ac51946db7475bb20b96e0a22083b366a6d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "This phase collected tweets using Twitter's Application Programming Interfaces (API) BIBREF43 . Within the Twitter API, diabetes, diet, exercise, and obesity were selected as the related words BIBREF4 and the related health areas BIBREF19 . Twitter's APIs provides both historic and real-time data collections. The latter method randomly collects 1% of publicly available tweets. This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. We used the queries to collect approximately 4.5 million related tweets between 06/01/2016 and 06/30/2016. The data will be available in the first author's website. Figure FIGREF3 shows a sample of collected tweets in this research."
            ],
            "highlighted_evidence": [
              "This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame. "
            ]
          }
        ]
      },
      {
        "question": "How strong was the correlation between exercise and diabetes?",
        "question_id": "18288c7b0f8bd7839ae92f9c293e7fb85c7e146a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "weak correlation with p-value of 0.08",
            "evidence": [
              "The main DDEO topics showed some level of interrelationship by appearing as subtopics of other DDEO topics. The words with italic and underline styles in Table 2 demonstrate the relation among the four DDEO areas. Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ). The strongest correlation among the topics was determined to be between exercise and obesity ( INLINEFORM0 ). Other notable correlations were: diabetes and obesity ( INLINEFORM1 ), and diet and obesity ( INLINEFORM2 ).",
              "FLOAT SELECTED: Figure 2: DDEO Correlation P-Value"
            ],
            "highlighted_evidence": [
              "The main DDEO topics showed some level of interrelationship by appearing as subtopics of other DDEO topics.",
              "Our results show users' interest about posting their opinions, sharing information, and conversing about exercise & diabetes, exercise & diet, diabetes & diet, diabetes & obesity, and diet & obesity (Figure FIGREF9 ).",
              "FLOAT SELECTED: Figure 2: DDEO Correlation P-Value"
            ]
          }
        ]
      },
      {
        "question": "How were topics of interest about DDEO identified?",
        "question_id": "b5e883b15e63029eb07d6ff42df703a64613a18a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "using topic modeling model Latent Dirichlet Allocation (LDA)",
            "evidence": [
              "To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning “diabetes\", “cancer\", and “influenza\" into a topic that has an overall “disease\" theme BIBREF44 , BIBREF45 . Topic modeling has a wide range of applications in health and medical domains such as predicting protein-protein relationships based on the literature knowledge BIBREF46 , discovering relevant clinical concepts and structures in patients' health records BIBREF47 , and identifying patterns of clinical events in a cohort of brain cancer patients BIBREF48 .",
              "Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . LDA assumes that a corpus contains topics such that each word in each document can be assigned to the topics with different degrees of membership BIBREF53 , BIBREF54 , BIBREF55 .",
              "We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets. Before identifying the opinions, two pre-processing steps were implemented: (1) using a standard list for removing stop words, that do not have semantic value for analysis (such as “the\"); and, (2) finding the optimum number of topics. To determine a proper number of topics, log-likelihood estimation with 80% of tweets for training and 20% of tweets for testing was used to find the highest log-likelihood, as it is the optimum number of topics BIBREF57 . The highest log-likelihood was determined 425 topics."
            ],
            "highlighted_evidence": [
              "To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning “diabetes\", “cancer\", and “influenza\" into a topic that has an overall “disease\" theme BIBREF44 , BIBREF45 .",
              "Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 .",
              "We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1709-07916",
    "dblp_title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter.",
    "year": "2017"
  },
  {
    "id": "1909.00154",
    "title": "Rethinking travel behavior modeling representations through embeddings",
    "qas": [
      {
        "question": "What datasets are used for evaluation?",
        "question_id": "c45a160d31ca8eddbfea79907ec8e59f543aab86",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Swissmetro dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Swissmetro dataset consists of survey data collected on the trains between St. Gallen and Geneva, Switzerland, during March 1998. According to its description BIBREF0, the respondents provided information in order to analyze the impact of the modal innovation in transportation, represented by the Swissmetro, a revolutionary mag-lev underground system, against the usual transport modes represented by car and train. After discarding respondents for which some variables were not available (e.g. age, purpose), a total of 10469 responses from 1188 individuals were used for the experiments."
            ],
            "highlighted_evidence": [
              "The Swissmetro dataset consists of survey data collected on the trains between St. Gallen and Geneva, Switzerland, during March 1998. "
            ]
          }
        ]
      },
      {
        "question": "How do their train their embeddings?",
        "question_id": "7358a1ce2eae380af423d4feeaa67d2bd23ae9dd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The embeddings are learned several times using the training set, then the average is taken.",
            "evidence": [
              "For each variable in encoding set, learn the new embeddings using the embeddings train set . This should be done simultaneously (all variable embeddings estimated at once, as explained in the next section).",
              "Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). Whenever we want to analyse a particular model (e.g. to check the coefficients of a choice model), we select the one with the highest likelihood at the development set (i.e. in practice, its out-of-sample generalization performance), and report its performance on the test set."
            ],
            "highlighted_evidence": [
              "For each variable in encoding set, learn the new embeddings using the embeddings train set .",
              "Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics)."
            ]
          }
        ]
      },
      {
        "question": "How do they model travel behavior?",
        "question_id": "1165fb0b400ec1c521c1aef7a4e590f76fee1279",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The data from collected travel surveys is used to model travel behavior.",
            "evidence": [
              "Differently to textual data, our goal in this paper is to explore the large amount of categorical data that is often collected in travel surveys. This includes trip purpose, education level, or family type. We also consider other variables that are not necessarily of categorical nature, but typically end up as dummy encoding, due to segmentation, such as age, income, or even origin/destination pair."
            ],
            "highlighted_evidence": [
              "Differently to textual data, our goal in this paper is to explore the large amount of categorical data that is often collected in travel surveys. This includes trip purpose, education level, or family type. We also consider other variables that are not necessarily of categorical nature, but typically end up as dummy encoding, due to segmentation, such as age, income, or even origin/destination pair."
            ]
          }
        ]
      },
      {
        "question": "How do their interpret the coefficients?",
        "question_id": "f2c5da398e601e53f9f545947f61de5f40ede1ee",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The coefficients are projected back to the dummy variable space.",
            "evidence": [
              "We will apply the methodology to the well-known “Swissmetro\" dataset. We will compare it with a dummy variables and PCA baselines. We will follow the 3-way experimental design mentioned before: split the dataset into train, development and test sets, so that the embeddings, PCA eigenvectors and the choice model are estimated from the same train and development sets, and validate it out-of-sample. For the sake of interpretability, we will also project back coefficients from the embeddings as well as PCA models into the dummy variable space."
            ],
            "highlighted_evidence": [
              "For the sake of interpretability, we will also project back coefficients from the embeddings as well as PCA models into the dummy variable space."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-00154",
    "dblp_title": "Rethinking travel behavior modeling representations through embeddings.",
    "year": "2019"
  },
  {
    "id": "1908.05434",
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "qas": [
      {
        "question": "By how much do they outperform previous state-of-the-art models?",
        "question_id": "2d4d0735c50749aa8087d1502ab7499faa2f0dd8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",
            "evidence": [
              "All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent.",
              "We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss.",
              "FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted.",
              "FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted."
            ],
            "highlighted_evidence": [
              "We report the mean metrics from the CV in Table TABREF14 .",
              "We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models.",
              "FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted.",
              "FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted."
            ]
          }
        ]
      },
      {
        "question": "Do they use pretrained word embeddings?",
        "question_id": "43761478c26ad65bec4f0fd511ec3181a100681c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."
            ],
            "highlighted_evidence": [
              "As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."
            ]
          }
        ]
      },
      {
        "question": "How is the lexicon of trafficking flags expanded?",
        "question_id": "01866fe392d9196dda1d0b472290edbd48a99f66",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos."
            ],
            "highlighted_evidence": [
              "If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1908-05434",
    "dblp_title": "Sex Trafficking Detection with Ordinal Regression Neural Networks.",
    "year": "2019"
  },
  {
    "id": "1612.05310",
    "title": "Modeling Trolling in Social Media Conversations",
    "qas": [
      {
        "question": "Do they experiment with the dataset?",
        "question_id": "394cf73c0aac8ccb45ce1b133f4e765e8e175403",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549. This result is what makes this dataset interesting: there is still lots of room for research on this task. Again, the primary goal of this experiment is to help identify the difficult-to-classify instances for analysis in the next section."
            ],
            "highlighted_evidence": [
              "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549. "
            ]
          }
        ]
      },
      {
        "question": "Do they use a crowdsourcing platform for annotation?",
        "question_id": "2c4003f25e8d95a3768204f52a7a5f5e17cb2102",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word “troll” with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.",
              "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”."
            ],
            "highlighted_evidence": [
              "Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. ",
              "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. "
            ]
          }
        ]
      },
      {
        "question": "What is an example of a difficult-to-classify case?",
        "question_id": "65e32f73357bb26a29a58596e1ac314f7e9c6c91",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The lack of background",
              "Non-cursing aggressions and insults",
              "the presence of controversial topic words ",
              " shallow meaning representation",
              "directly ask the suspected troll if he/she is trolling or not",
              "a blurry line between “Frustrate” and “Neutralize”",
              "distinction between the classes “Troll” and “Engage”"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In order to provide directions for future work, we analyze the errors made by the classifier trained on the extended features on the four prediction tasks.",
              "Errors on Intention (I) prediction: The lack of background is a major problem when identifying trolling comments. For example, “your comments fit well in Stormfront” seems inoffensive on the surface. However, people who know that Stormfront is a white supremacist website will realize that the author of this comment had an annoying or malicious intention. But our system had no knowledge about it and simply predicted it as non-trolling. These kind of errors reduces recall on the prediction of trolling comments. A solution would be to include additional knowledge from anthologies along with a sentiment or polarity. One could modify NELL BIBREF12 to broaden the understanding of entities in the comments.",
              "Non-cursing aggressions and insults This is a challenging problem, since the majority of abusive and insulting comments rely on profanity and swearing. The problem arises with subtler aggressions and insults that are equally or even more annoying, such as “Troll? How cute.” and “settle down drama queen”. The classifier has a more difficult task of determining that these are indeed aggressions or insults. This error also decreases the recall of trolling intention. A solution would be to exploit all the comments made by the suspected troll in the entire conversation in order to increase the chances of finding curse words or other cues that lead the classifier to correctly classify the comment as trolling.",
              "Another source of error is the presence of controversial topic words such as “black”,“feminism”, “killing”, “racism”, “brown”, etc. that are commonly used by trolls. The classifier seems too confident to classify a comment as trolling in the presence of these words, but in many cases they do not. In order to ameliorate this problem, one could create ad-hoc word embeddings by training glove or other type of distributed representation on a large corpus for the specific social media platform in consideration. From these vectors one could expect a better representation of controversial topics and their interactions with other words so they might help to reduce these errors.",
              "Errors on Disclosure (D) prediction: A major source of error that affects disclosure is the shallow meaning representation obtained from the BOW model even when augmented with the distributional features given by the glove vectors. For example, the suspected troll's comment “how to deal with refugees? How about a bullet to the head” is clearly mean-spirited and is an example of disclosed trolling. However, to reach that conclusion the reader need to infer the meaning of “bullet to the head” and that this action is desirable for a vulnerable group like migrants or refugees. This problem produces low recall for the disclosed prediction task. A solution for this problem may be the use of deeper semantics, where we represent the comments and sentences in their logical form and infer from them the intended meaning.",
              "Errors on Interpretation (R) prediction: it is a common practice from many users to directly ask the suspected troll if he/she is trolling or not. There are several variations of this question, such as “Are you a troll?” and “not sure if trolling or not”. While the presence of a question like these seems to give us a hint of the responder's interpretation, we cannot be sure of his interpretation without also considering the context. One way to improve interpretation is to exploit the response strategy, but the response strategy in our model is predicted independently of interpretation. So one solution could be similar to the one proposed above for the disclosure task problem: jointly learning classifiers that predict both variables simultaneously. Another possibility is to use the temporal sequence of response comments and make use of older response interpretation as input features for later comments. This could be useful since commenters seem to influence each other as they read through the conversation.",
              "Errors on Response Strategy (B) prediction: In some cases there is a blurry line between “Frustrate” and “Neutralize”. The key distinction between them is that there exists some criticism in the Frustrate responses towards the suspected troll's comment, while “Neutralizing” comments acknowledge that the suspected troll has trolling intentions, but gives no importance to them. For example, response comments such as “oh, you are a troll” and “you are just a lame troll” are examples of this subtle difference. The first is a case of “neutralize” while the second is indeed criticizing the suspected troll's comment and therefore a “frustrate” response strategy. This kind of error affects both precision and recall for these two classes. A possible solution could be to train a specialized classifier to disambiguate between “frustrate” and “neutralize” only.",
              "Another challenging problem is the distinction between the classes “Troll” and “Engage”. This is true when the direct responder is intensely flared up with the suspected comment to the point that his own comment becomes a trolling attempt. A useful indicator for distinguishing these cases are the presence of insults, and to detect them we look for swear words, but as we noted before, there is no guarantee that swear words are used for insulting. This kind of error affects the precision and recall for the “troll” and “engage” classes. A solution to this problem may be the inclusion of longer parts of the conversation. It is typical in a troll-engaged comment scheme to observe longer than usual exchanges between two users, and the comments evolve in very agitated remarks. One may then use this information to disambiguate between the two classes."
            ],
            "highlighted_evidence": [
              "In order to provide directions for future work, we analyze the errors made by the classifier trained on the extended features on the four prediction tasks.\n\nErrors on Intention (I) prediction: The lack of background is a major problem when identifying trolling comments.",
              "Non-cursing aggressions and insults This is a challenging problem, since the majority of abusive and insulting comments rely on profanity and swearing. ",
              "Another source of error is the presence of controversial topic words such as “black”,“feminism”, “killing”, “racism”, “brown”, etc. that are commonly used by trolls.",
              "Errors on Disclosure (D) prediction: A major source of error that affects disclosure is the shallow meaning representation obtained from the BOW model even when augmented with the distributional features given by the glove vectors.",
              "Errors on Interpretation (R) prediction: it is a common practice from many users to directly ask the suspected troll if he/she is trolling or not. ",
              "Errors on Response Strategy (B) prediction: In some cases there is a blurry line between “Frustrate” and “Neutralize”. ",
              "Another challenging problem is the distinction between the classes “Troll” and “Engage”. "
            ]
          }
        ]
      },
      {
        "question": "What potential solutions are suggested?",
        "question_id": "46f175e1322d648ab2c0258a9609fe6f43d3b44e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " inclusion of longer parts of the conversation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Another challenging problem is the distinction between the classes “Troll” and “Engage”. This is true when the direct responder is intensely flared up with the suspected comment to the point that his own comment becomes a trolling attempt. A useful indicator for distinguishing these cases are the presence of insults, and to detect them we look for swear words, but as we noted before, there is no guarantee that swear words are used for insulting. This kind of error affects the precision and recall for the “troll” and “engage” classes. A solution to this problem may be the inclusion of longer parts of the conversation. It is typical in a troll-engaged comment scheme to observe longer than usual exchanges between two users, and the comments evolve in very agitated remarks. One may then use this information to disambiguate between the two classes."
            ],
            "highlighted_evidence": [
              "This kind of error affects the precision and recall for the “troll” and “engage” classes. A solution to this problem may be the inclusion of longer parts of the conversation. "
            ]
          }
        ]
      },
      {
        "question": "What is the size of the dataset?",
        "question_id": "7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1000 conversations composed of 6833 sentences and 88047 tokens"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column “Size”."
            ],
            "highlighted_evidence": [
              "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. "
            ]
          }
        ]
      },
      {
        "question": "What Reddit communities do they look at?",
        "question_id": "3fa638e6167e1c7a931c8ee5c0e2e397ec1b6cda",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/VegaN18",
    "dblp_title": "Modeling Trolling in Social Media Conversations.",
    "year": "2018"
  },
  {
    "id": "1912.09713",
    "title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data",
    "qas": [
      {
        "question": "How strong is negative correlation between compound divergence and accuracy in performed experiment?",
        "question_id": "d2b3f2178a177183b1aeb88784e48ff7e3e5070c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " between 0.81 and 0.88"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Note that the experiment based on output-length exhibits a worse accuracy than what we would expect based on its compositional divergence. One explanation for this is that the test distribution varies from the training distribution in other ways than compound divergence (namely in output length and a slightly higher atom divergence), which seems to make this split particularly difficult for the baseline architectures. To analyze the influence of the length ratio further, we compute the correlation between length ratios and accuracy of the baseline systems and compare it to the correlation between compound divergence and accuracy. We observe $R^2$ correlation coefficients between 0.11 and 0.22 for the input and output length ratios and between 0.81 and 0.88 for the compound divergence. This shows that despite the known phenomenon that the baseline systems struggle to generalize to longer lengths, the compound divergence seems to be a stronger explanation for the accuracy on different splits than the lengths ratios."
            ],
            "highlighted_evidence": [
              "We observe $R^2$ correlation coefficients between 0.11 and 0.22 for the input and output length ratios and between 0.81 and 0.88 for the compound divergence. "
            ]
          }
        ]
      },
      {
        "question": "What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?",
        "question_id": "d5ff8fc4d3996db2c96cb8af5a6d215484991e62",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments. The reason for this is that, instead of focusing on only one intuitive but rather arbitrary aspect of compositional generalization, the MCD splits aim to optimize divergence across all compounds directly."
            ],
            "highlighted_evidence": [
              "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments. "
            ]
          }
        ]
      },
      {
        "question": "How authors justify that question answering dataset presented is realistic?",
        "question_id": "d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "CFQ contains 239,357 English question-answer pairs that are answerable using the public Freebase data. (The data URL is not yet provided for anonymous review.) We include a list of MIDs such that their English names map unambiguously to a MID. Table TABREF17(a) summarizes the overall statistics of CFQ. Table TABREF17(b) uses numbers from BIBREF8 and from an analysis of WebQuestionsSP BIBREF17 and ComplexWebQuestions BIBREF18 to compare three key statistics of CFQ to other semantic parsing datasets (none of which provide annotations of their compositional structure). CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets. Note that it would be easy to boost the raw number of questions in CFQ almost arbitrarily by repeating the same question pattern with varying entities, but we use at most one entity substitution per question pattern. Appendix SECREF10 contains more detailed analyses of the data distribution."
            ],
            "highlighted_evidence": [
              "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets. "
            ]
          }
        ]
      },
      {
        "question": "What three machine architectures are analyzed?",
        "question_id": "0427ca83d6bf4ec113bc6fec484b2578714ae8ec",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "LSTM+attention",
              "Transformer ",
              "Universal Transformer"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use three encoder-decoder neural architectures as baselines: (1) LSTM+attention as an LSTM BIBREF19 with attention mechanism BIBREF20; (2) Transformer BIBREF21 and (3) Universal Transformer BIBREF22."
            ],
            "highlighted_evidence": [
              "We use three encoder-decoder neural architectures as baselines: (1) LSTM+attention as an LSTM BIBREF19 with attention mechanism BIBREF20; (2) Transformer BIBREF21 and (3) Universal Transformer BIBREF22."
            ]
          }
        ]
      },
      {
        "question": "How big is new question answering dataset?",
        "question_id": "f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "239,357 English question-answer pairs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "CFQ contains 239,357 English question-answer pairs that are answerable using the public Freebase data. (The data URL is not yet provided for anonymous review.) We include a list of MIDs such that their English names map unambiguously to a MID. Table TABREF17(a) summarizes the overall statistics of CFQ. Table TABREF17(b) uses numbers from BIBREF8 and from an analysis of WebQuestionsSP BIBREF17 and ComplexWebQuestions BIBREF18 to compare three key statistics of CFQ to other semantic parsing datasets (none of which provide annotations of their compositional structure). CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets. Note that it would be easy to boost the raw number of questions in CFQ almost arbitrarily by repeating the same question pattern with varying entities, but we use at most one entity substitution per question pattern. Appendix SECREF10 contains more detailed analyses of the data distribution."
            ],
            "highlighted_evidence": [
              "CFQ contains 239,357 English question-answer pairs that are answerable using the public Freebase data."
            ]
          }
        ]
      },
      {
        "question": "What are other approaches into creating compositional generalization benchmarks?",
        "question_id": "8db45a8217f6be30c31f9b9a3146bf267de68389",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "random ",
              "Output length",
              "Input length",
              "Output pattern",
              "Input pattern"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the goal of measuring compositional generalization as accurately as possible, it is particularly interesting to construct maximum compound divergence (MCD) splits, which aim for a maximum compound divergence at a low atom divergence (we use $\\mathcal {D}_A \\le 0.02$). Table TABREF18 compares the compound divergence $\\mathcal {D}_C$ and atom divergence $\\mathcal {D}_A$ of three MCD splits to a random split baseline as well as to several previously suggested compositionality experiments for both CFQ and the existing scan dataset (cf. Section SECREF30). The split methods (beyond random split) are the following:",
              "Output length: Variation of the setup described by BIBREF2 where the train set consists of examples with output (sparql query or action sequence) length $\\le \\hspace{-2.5pt} N$, while the test set consists of examples with output length $> \\hspace{-2.5pt} N$. For CFQ, we use $N = 7$ constraints. For scan, we use $N = 22$ actions.",
              "Input length: Variation of the above setup, in which the train set consists of examples with input (question or command) length $\\le N$, while test set consists of examples with input length $> N$. For CFQ, we use $N=19$ grammar leaves. For SCAN, we use $N=8$ tokens.",
              "Output pattern: Variation of setup described by BIBREF8, in which the split is based on randomly assigning clusters of examples sharing the same output (query or action sequence) pattern. Query patterns are determined by anonymizing entities and properties; action sequence patterns collapse primitive actions and directions.",
              "Input pattern: Variation of the previous setup in which the split is based on randomly assigning clusters of examples sharing the same input (question or command) pattern. Question patterns are determined by anonymizing entity and property names ; command patterns collapse verbs and the interchangeable pairs left/right, around/opposite, twice/thrice."
            ],
            "highlighted_evidence": [
              "The split methods (beyond random split) are the following:\n\nOutput length: Variation of the setup described by BIBREF2 where the train set consists of examples with output (sparql query or action sequence) length $\\le \\hspace{-2.5pt} N$, while the test set consists of examples with output length $> \\hspace{-2.5pt} N$. For CFQ, we use $N = 7$ constraints. For scan, we use $N = 22$ actions.\n\nInput length: Variation of the above setup, in which the train set consists of examples with input (question or command) length $\\le N$, while test set consists of examples with input length $> N$. For CFQ, we use $N=19$ grammar leaves. For SCAN, we use $N=8$ tokens.\n\nOutput pattern: Variation of setup described by BIBREF8, in which the split is based on randomly assigning clusters of examples sharing the same output (query or action sequence) pattern. Query patterns are determined by anonymizing entities and properties; action sequence patterns collapse primitive actions and directions.\n\nInput pattern: Variation of the previous setup in which the split is based on randomly assigning clusters of examples sharing the same input (question or command) pattern. Question patterns are determined by anonymizing entity and property names ; command patterns collapse verbs and the interchangeable pairs left/right, around/opposite, twice/thrice."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iclr/KeysersSSBFKMSS20",
    "dblp_title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data.",
    "year": "2020"
  },
  {
    "id": "1901.03860",
    "title": "Prototypical Metric Transfer Learning for Continuous Speech Keyword Spotting With Limited Training Data",
    "qas": [
      {
        "question": "What problem do they apply transfer learning to?",
        "question_id": "4e379d6d5f87554fabf6f7f7b6ed92d2025e7280",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CSKS task"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We took inspiration from these works to design our experiments to solve the CSKS task."
            ],
            "highlighted_evidence": [
              "We took inspiration from these works to design our experiments to solve the CSKS task."
            ]
          }
        ]
      },
      {
        "question": "What are the baselines?",
        "question_id": "518d0847b02b4f23a8f441faa38b935c9b892e1e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Honk",
              "DeepSpeech-finetune"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our baselines, Honk( UID9 ), DeepSpeech-finetune( UID10 ), had comparatively both lower recall and precision. We noticed an improvement when fine tuning DeepSpeech model with prototypical loss (DeepSpeech-finetune-prototypical ( UID11 )). While analysing the false positives of this model, it was observed that the model gets confused between the keywords and it also wrongly classifies background noise as a keyword. To improve this, we combined prototypical loss with a metric loss to reject background (DeepSpeech-finetune-prototypical+metric( UID14 )). This model gave us the best results."
            ],
            "highlighted_evidence": [
              "Our baselines, Honk( UID9 ), DeepSpeech-finetune( UID10 ), had comparatively both lower recall and precision."
            ]
          }
        ]
      },
      {
        "question": "What languages are considered?",
        "question_id": "8112d18681e266426cf7432ac4928b87f5ce8311",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English",
              "Hindi"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our learning data, which was created in-house, has 20 keywords to be spotted about television models of a consumer electronics brand. It was collected by making 40 participants utter each keyword 3 times. Each participant recorded in normal ambient noise conditions. As a result, after collection of learning data we have 120 (3 x 40) instances of each of the 20 keywords. We split the learning data 80:20 into train and validation sets. Train/Validation split was done on speaker level, so as to make sure that all occurrences of a particular speaker is present only on either of two sets. For testing, we used 10 different 5 minutes long simulated conversational recordings of television salesmen and customers from a shopping mall in India. These recordings contain background noise (as is expected in a mall) and have different languages (Indians speak a mixture of English and Hindi). The CSKS algorithm trained on instances of keywords in learning data is supposed to detect keywords embedded in conversations of test set."
            ],
            "highlighted_evidence": [
              "These recordings contain background noise (as is expected in a mall) and have different languages (Indians speak a mixture of English and Hindi)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/softcomp/SethKS19",
    "dblp_title": "Prototypical Metric Transfer Learning for Continuous Speech Keyword Spotting with Limited Training Data.",
    "year": "2019"
  },
  {
    "id": "1909.02480",
    "title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow",
    "qas": [
      {
        "question": "Does this model train faster than state of the art models?",
        "question_id": "b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What is the performance difference between proposed method and state-of-the-arts on these datasets?",
        "question_id": "ba6422e22297c7eb0baa381225a2f146b9621791",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Difference is around 1 BLEU score lower on average than state of the art methods.",
            "evidence": [
              "Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring. The first block in Table TABREF40 includes the baseline results from autoregressive Transformer. For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model BIBREF11 to obtain high-quality samples. We vary the temperature within $\\lbrace 0.1, 0.2, 0.3, 0.4, 0.5, 1.0\\rbrace $ and select the best temperature based on the performance on development sets. The analysis of the impact of sampling temperature and other hyper-parameters on samples is in § SECREF50. For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind auto-regressive Transformer on model data distributions. Comparing with CMLM BIBREF8 with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality. Leveraging iterative refinement to further improve the performance of FlowSeq has been left to future work.",
              "FLOAT SELECTED: Table 2: BLEU scores on two WMT datasets of models using advanced decoding methods. The first block are Transformer-base (Vaswani et al., 2017). The second and the third block are results of models trained w/w.o. knowledge distillation, respectively. n = l × r is the total number of candidates for rescoring."
            ],
            "highlighted_evidence": [
              "Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring.",
              "FLOAT SELECTED: Table 2: BLEU scores on two WMT datasets of models using advanced decoding methods. The first block are Transformer-base (Vaswani et al., 2017). The second and the third block are results of models trained w/w.o. knowledge distillation, respectively. n = l × r is the total number of candidates for rescoring."
            ]
          }
        ]
      },
      {
        "question": "What non autoregressive NMT models are used for comparison?",
        "question_id": "65e72ad72a9cbfc379f126b10b0ce80cfe44579b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "NAT w/ Fertility",
              "NAT-IR",
              "NAT-REG",
              "LV NAR",
              "CTC Loss",
              "CMLM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility BIBREF6, NAT-IR BIBREF7, NAT-REG BIBREF25, LV NAR BIBREF26, CTC Loss BIBREF27, and CMLM BIBREF8."
            ],
            "highlighted_evidence": [
              "We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility BIBREF6, NAT-IR BIBREF7, NAT-REG BIBREF25, LV NAR BIBREF26, CTC Loss BIBREF27, and CMLM BIBREF8."
            ]
          }
        ]
      },
      {
        "question": "What are three neural machine translation (NMT) benchmark datasets used for evaluation?",
        "question_id": "cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WMT2014, WMT2016 and IWSLT-2014"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FlowSeq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation – WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear."
            ],
            "highlighted_evidence": [
              " Experimentally, on three benchmark datasets for machine translation – WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/MaZLNH19",
    "dblp_title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow.",
    "year": "2019"
  },
  {
    "id": "1910.02754",
    "title": "On Leveraging the Visual Modality for Neural Machine Translation",
    "qas": [
      {
        "question": "What is result of their attention distribution analysis?",
        "question_id": "04aff4add28e6343634d342db92b3ac36aa8c255",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "visual attention is very sparse",
              " visual component of the attention hasn't learnt any variation over the source encodings"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this section, we analyze the visual and text based attention mechanisms. We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation. Thus, in practice, we find that a small weight ($\\gamma =0.1$) is necessary to prevent degradation due to this sparse visual attention component. Figure FIGREF18 & FIGREF19 shows the comparison of visual and text based attention for two sentences, one long source sentence of length 21 and one short source sentence of length 7. In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths."
            ],
            "highlighted_evidence": [
              "We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation.",
              "In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths."
            ]
          }
        ]
      },
      {
        "question": "What is result of their Principal Component Analysis?",
        "question_id": "a8e4522ce2ce7336e731286654d6ad0931927a4e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF14 (Top) shows the variance explained by the Top 100 principal components, obtained by applying PCA on the How2 and Multi30k training set visual features. The original feature dimensions are 2048 in both the cases. It is clear from the Figure FIGREF14 that most of the energy of the visual feature space resides in a low-dimensional subspace BIBREF14. In other words, there exist a few directions in the embedding space which disproportionately explain the variance. These \"common\" directions affect all of the embeddings in the same way, rendering them less discriminative. Figure FIGREF14 also shows the cumulative variance explained by Top 10, 20, 50 and 100 principal components respectively. It is clear that the visual features in the case of How2 dataset are much more dominated by the \"common\" dimensions, when compared to the Multi30k dataset. Further, this analysis is still at the sentence level, i.e. the visual features are much less discriminative among individual sentences, further aggravating the problem at the token level. This suggests that the existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT, since they won't provide discriminativeness among the vocabulary elements at the token level during prediction. Further, this also indicates that under subword vocabulary such as BPE BIBREF15 or Sentence-Piece BIBREF16, the utility of such visual embeddings will only aggravate."
            ],
            "highlighted_evidence": [
              "In other words, there exist a few directions in the embedding space which disproportionately explain the variance.",
              "It is clear that the visual features in the case of How2 dataset are much more dominated by the \"common\" dimensions, when compared to the Multi30k dataset. Further, this analysis is still at the sentence level, i.e. the visual features are much less discriminative among individual sentences, further aggravating the problem at the token level. This suggests that the existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT, since they won't provide discriminativeness among the vocabulary elements at the token level during prediction."
            ]
          }
        ]
      },
      {
        "question": "What are 3 novel fusion techniques that are proposed?",
        "question_id": "f6202100cfb83286dc51f57c68cffdbf5cf50a3f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Step-Wise Decoder Fusion",
              "Multimodal Attention Modulation",
              "Visual-Semantic (VS) Regularizer"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Proposed Fusion Techniques ::: Step-Wise Decoder Fusion",
              "Our first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. This differs from the usual practice of passing the visual feature only at the beginning of the decoding process BIBREF5.",
              "Proposed Fusion Techniques ::: Multimodal Attention Modulation",
              "Similar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Then, the true attention distribution $a_{t}(s)$ is computed as an interpolation between the visual and text based attention scores. The score function is a content based scoring mechanism as usual.",
              "Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer",
              "In terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction. However, to our knowledge, visual-semantic supervision hasn't been much explored for multimodal translation in terms of loss functions."
            ],
            "highlighted_evidence": [
              "Proposed Fusion Techniques ::: Step-Wise Decoder Fusion\nOur first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process.",
              "Proposed Fusion Techniques ::: Multimodal Attention Modulation\nSimilar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well.",
              "Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer\nIn terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/inlg/RaunakCLXM19",
    "dblp_title": "On Leveraging the Visual Modality for Neural Machine Translation.",
    "year": "2019"
  },
  {
    "id": "2004.02393",
    "title": "Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games",
    "qas": [
      {
        "question": "What are two models' architectures in proposed solution?",
        "question_id": "bd7039f81a5417474efa36f703ebddcf51835254",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Reasoner model, also implemented with the MatchLSTM architecture",
              "Ranker model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Method ::: Passage Ranking Model",
              "The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages.",
              "Method ::: Cooperative Reasoner",
              "To alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:"
            ],
            "highlighted_evidence": [
              "Method ::: Passage Ranking Model\nThe key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages.",
              "Method ::: Cooperative Reasoner\nTo alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages."
            ]
          }
        ]
      },
      {
        "question": "How do two models cooperate to select the most confident chains?",
        "question_id": "022e5c996a72aeab890401a7fdb925ecd0570529",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:"
            ],
            "highlighted_evidence": [
              "Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards."
            ]
          }
        ]
      },
      {
        "question": "How many hand-labeled reasoning chains have been created?",
        "question_id": "2a950ede24b26a45613169348d5db9176fda4f82",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What benchmarks are created?",
        "question_id": "34af2c512ec38483754e94e1ea814aa76552d60a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples",
            "evidence": [
              "In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).",
              "In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.",
              "The accuracy is defined as the ratio:"
            ],
            "highlighted_evidence": [
              "In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).\n\nIn MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.\n\nThe accuracy is defined as the ratio:",
              "The accuracy is defined as the ratio:"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ai/FengYXGHCCGZ21",
    "dblp_title": "extending PySyft with N.-IID Federated Learning Benchmarkby Houda Bouraqqadi, Ayoub Berrag, Mohamed Mhaouach, Afaf Bouhoute, Khalid Fardousse, and Ismail Berrada: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games.",
    "year": "2021"
  },
  {
    "id": "2004.01694",
    "title": "A Set of Recommendations for Assessing Human-Machine Parity in Language Translation",
    "qas": [
      {
        "question": "What empricial investigations do they reference?",
        "question_id": "c1429f7fed5a4dda11ac7d9643f97af87a83508b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human–machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human–machine parity in particular. All of our data are made publicly available for external validation and further analysis.",
              "In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human–machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.",
              "We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3–SECREF5. Based on our findings, we then distil a set of recommendations for human evaluation of strong MT systems, with a focus on assessing human–machine parity (Section SECREF6)."
            ],
            "highlighted_evidence": [
              "We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation.",
              "In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human–machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.",
              "We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3–SECREF5. "
            ]
          }
        ]
      },
      {
        "question": "What languages do they investigate for machine translation?",
        "question_id": "a93d4aa89ac3abbd08d725f3765c4f1bed35c889",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English ",
              "Chinese "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article:"
            ],
            "highlighted_evidence": [
              "We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article:"
            ]
          }
        ]
      },
      {
        "question": "What recommendations do they offer?",
        "question_id": "bc473c5bd0e1a8be9b2037aa7006fd68217c3f47",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " Choose professional translators as raters",
              " Evaluate documents, not sentences",
              "Evaluate fluency in addition to adequacy",
              "Do not heavily edit reference translations for fluency",
              "Use original source texts"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our experiments in Sections SECREF3–SECREF5 show that machine translation quality has not yet reached the level of professional human translation, and that human evaluation methods which are currently considered best practice fail to reveal errors in the output of strong NMT systems. In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human–machine parity, and will strengthen the human evaluation of MT in general.",
              "Recommendations ::: (R1) Choose professional translators as raters.",
              "In our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.",
              "Recommendations ::: (R2) Evaluate documents, not sentences.",
              "When evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).",
              "Recommendations ::: (R3) Evaluate fluency in addition to adequacy.",
              "Raters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.",
              "Recommendations ::: (R4) Do not heavily edit reference translations for fluency.",
              "In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).",
              "Recommendations ::: (R5) Use original source texts.",
              "Raters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT."
            ],
            "highlighted_evidence": [
              " In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human–machine parity, and will strengthen the human evaluation of MT in general.\n\nRecommendations ::: (R1) Choose professional translators as raters.\nIn our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.\n\nRecommendations ::: (R2) Evaluate documents, not sentences.\nWhen evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).\n\nRecommendations ::: (R3) Evaluate fluency in addition to adequacy.\nRaters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.\n\nRecommendations ::: (R4) Do not heavily edit reference translations for fluency.\nIn professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).\n\nRecommendations ::: (R5) Use original source texts.\nRaters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT."
            ]
          }
        ]
      },
      {
        "question": "What percentage fewer errors did professional translations make?",
        "question_id": "cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "36%",
            "evidence": [
              "FLOAT SELECTED: Table 5: Classification of errors in machine translation MT1 and two professional human translation outputs HA and HB. Errors represent the number of sentences (out of N = 150) that contain at least one error of the respective type. We also report the number of sentences that contain at least one error of any category (Any), and the total number of error categories present in all sentences (Total). Statistical significance is assessed with Fisher’s exact test (two-tailed) for each pair of translation outputs.",
              "To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 5: Classification of errors in machine translation MT1 and two professional human translation outputs HA and HB. Errors represent the number of sentences (out of N = 150) that contain at least one error of the respective type. We also report the number of sentences that contain at least one error of any category (Any), and the total number of error categories present in all sentences (Total). Statistical significance is assessed with Fisher’s exact test (two-tailed) for each pair of translation outputs.",
              "To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3",
              " The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32."
            ]
          }
        ]
      },
      {
        "question": "What was the weakness in Hassan et al's evaluation design?",
        "question_id": "9299fe72f19c1974564ea60278e03a423eb335dc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set\n",
            "evidence": [
              "The human evaluation of MT output in research scenarios is typically conducted by crowd workers in order to minimise costs. BIBREF13 shows that aggregated assessments of bilingual crowd workers are very similar to those of MT developers, and BIBREF14, based on experiments with data from WMT 2012, similarly conclude that with proper quality control, MT systems can be evaluated by crowd workers. BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. We hypothesise that expert translators will provide more nuanced ratings than non-experts, and that their ratings will show a higher difference between MT outputs and human translations.",
              "MT has been evaluated almost exclusively at the sentence level, owing to the fact that most MT systems do not yet take context across sentence boundaries into account. However, when machine translations are compared to those of professional translators, the omission of linguistic context—e. g., by random ordering of the sentences to be evaluated—does not do justice to humans who, in contrast to most MT systems, can and do take inter-sentential context into account BIBREF15, BIBREF16. We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.",
              "The human reference translations with which machine translations are compared within the scope of a human–machine parity assessment play an important role. BIBREF3 used all source texts of the WMT 2017 Chinese–English test set for their experiments, of which only half were originally written in Chinese; the other half were translated from English into Chinese. Since translated texts are usually simpler than their original counterparts BIBREF17, they should be easier to translate for MT systems. Moreover, different human translations of the same source text sometimes show considerable differences in quality, and a comparison with an MT system only makes sense if the human reference translations are of high quality. BIBREF3, for example, had the WMT source texts re-translated as they were not convinced of the quality of the human translations in the test set. At WMT 2018, the organisers themselves noted that the manual evaluation included several reports of ill-formed reference translations BIBREF5. We hypothesise that the quality of the human translations has a significant effect on findings of human–machine parity, which would indicate that it is necessary to ensure that human translations used to assess parity claims need to be carefully vetted for their quality."
            ],
            "highlighted_evidence": [
              " BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. ",
              "We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.",
              "BIBREF3 used all source texts of the WMT 2017 Chinese–English test set for their experiments, of which only half were originally written in Chinese; "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/jair/LaubliCNSST20",
    "dblp_title": "A Set of Recommendations for Assessing Human-Machine Parity in Language Translation.",
    "year": "2020"
  },
  {
    "id": "2003.00576",
    "title": "StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization",
    "qas": [
      {
        "question": "By how much they improve over the previous state-of-the-art?",
        "question_id": "2ed02be0c183fca7031ccb8be3fd7bc109f3694b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1.08 points in ROUGE-L over our base pointer-generator model ",
              "0.6 points in ROUGE-1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Finally, our combined model which uses both Latent and Explicit structure performs the best with a strong improvement of 1.08 points in ROUGE-L over our base pointer-generator model and 0.6 points in ROUGE-1. It shows that the latent and explicit information are complementary and a model can jointly leverage them to produce better summaries."
            ],
            "highlighted_evidence": [
              "Finally, our combined model which uses both Latent and Explicit structure performs the best with a strong improvement of 1.08 points in ROUGE-L over our base pointer-generator model and 0.6 points in ROUGE-1. "
            ]
          }
        ]
      },
      {
        "question": "Is there any evidence that encoders with latent structures work well on other tasks?",
        "question_id": "be73a88d5b695200e2ead4c2c24e2a977692970e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. Building on this motivation, our latent structure attention module builds upon BIBREF12 to model the dependencies between sentences in a document. It uses a variant of Kirchhoff’s matrix-tree theorem BIBREF14 to model such dependencies as non-projective tree structures(§SECREF3). The explicit attention module is linguistically-motivated and aims to incorporate sentence-level structures from externally annotated document structures. We incorporate a coreference based sentence dependency graph, which is then combined with the output of the latent structure attention module to produce a hybrid structure-aware sentence representation (§SECREF5)."
            ],
            "highlighted_evidence": [
              "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-2003-00576",
    "dblp_title": "StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization.",
    "year": "2020"
  },
  {
    "id": "1909.02635",
    "title": "Effective Use of Transformer Networks for Entity Tracking",
    "qas": [
      {
        "question": "Do they report results only on English?",
        "question_id": "0e45aae0e97a6895543e88705e153f084ce9c136",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What evidence do they present that the model attends to shallow context clues?",
        "question_id": "c515269b37cc186f6f82ab9ada5d9ca176335ded",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues",
            "evidence": [
              "One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics."
            ],
            "highlighted_evidence": [
              "One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics."
            ]
          }
        ]
      },
      {
        "question": "In what way is the input restructured?",
        "question_id": "43f86cd8aafe930ebb35ca919ada33b74b36c7dd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "In four entity-centric ways - entity-first, entity-last, document-level and sentence-level",
            "evidence": [
              "Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. The input sequence begins with a [START] token, then the entity under consideration, then a [SEP] token. After each sentence, a [CLS] token is used to anchor the prediction for that sentence. In this model, the transformer can always observe the entity it should be primarily “attending to” from the standpoint of building representations. We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. These variants are naturally more computationally-intensive than post-conditioned models, as we need to rerun the transformer for each distinct entity we want to make a prediction for.",
              "As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). In a sentence level model, we formulate each pair of entity $e$ and process step $t$ as a separate instance for our classification task. Thus, for a process with $T$ steps and $m$ entities we get $T \\times m$ input sequences for fine tuning our classification task."
            ],
            "highlighted_evidence": [
              "Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. ",
              "We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. ",
              "As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/GuptaD19",
    "dblp_title": "Effective Use of Transformer Networks for Entity Tracking.",
    "year": "2019"
  },
  {
    "id": "1904.00648",
    "title": "Recognizing Musical Entities in User-generated Content",
    "qas": [
      {
        "question": "What are their results on the entity recognition task?",
        "question_id": "aa60b0a6c1601e09209626fd8c8bdc463624b0b3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "With both test sets performances decrease, varying between 94-97%"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The performances of the NER experiments are reported separately for three different parts of the system proposed.",
              "Table 6 presents the comparison of the various methods while performing NER on the bot-generated corpora and the user-generated corpora. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. It can be considered a natural consequence of the complex nature of the users' informal language in comparison to the structured message created by the bot."
            ],
            "highlighted_evidence": [
              "The performances of the NER experiments are reported separately for three different parts of the system proposed.",
              "Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease."
            ]
          }
        ]
      },
      {
        "question": "What task-specific features are used?",
        "question_id": "3837ae1e91a4feb27f11ac3b14963e9a12f0c05e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "6)Contributor first names",
              "7)Contributor last names",
              "8)Contributor types (\"soprano\", \"violinist\", etc.)",
              "9)Classical work types (\"symphony\", \"overture\", etc.)",
              "10)Musical instruments",
              "11)Opus forms (\"op\", \"opus\")",
              "12)Work number forms (\"no\", \"number\")",
              "13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\")",
              "14)Work Modes (\"major\", \"minor\", \"m\")"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In total, we define 26 features for describing each token: 1)POS tag; 2)Chunk tag; 3)Position of the token within the text, normalized between 0 and 1; 4)If the token starts with a capital letter; 5)If the token is a digit. Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (\"soprano\", \"violinist\", etc.); 9)Classical work types (\"symphony\", \"overture\", etc.); 10)Musical instruments; 11)Opus forms (\"op\", \"opus\"); 12)Work number forms (\"no\", \"number\"); 13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\"); 14)Work Modes (\"major\", \"minor\", \"m\"). Finally, we complete the tokens' description including as token's features the surface form, the POS and the chunk tag of the previous and the following two tokens (12 features)."
            ],
            "highlighted_evidence": [
              "Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (\"soprano\", \"violinist\", etc.); 9)Classical work types (\"symphony\", \"overture\", etc.); 10)Musical instruments; 11)Opus forms (\"op\", \"opus\"); 12)Work number forms (\"no\", \"number\"); 13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\"); 14)Work Modes (\"major\", \"minor\", \"m\")."
            ]
          }
        ]
      },
      {
        "question": "What kind of corpus-based features are taken into account?",
        "question_id": "ef4d6c9416e45301ea1a4d550b7c381f377cacd9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "standard linguistic features, such as Part-Of-Speech (POS) and chunk tag",
              "series of features representing tokens' left and right context"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We define a set of features for characterizing the text at the token level. We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context. For extracting the POS and the chunk tag we use the Python library twitter_nlp, presented in BIBREF1 ."
            ],
            "highlighted_evidence": [
              "We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context."
            ]
          }
        ]
      },
      {
        "question": "Which machine learning algorithms did the explore?",
        "question_id": "689d1d0c4653a8fa87fd0e01fa7e12f75405cd38",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "biLSTM-networks"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments. In the first, we train the model using only the word embeddings as feature. In the second, together with the word embeddings we use the POS and chunk tag. In the third, all the features previously defined are included, in addition to the word embeddings. For every experiment, we use both the pre-trained embeddings and the ones that we created with our Twitter corpora. In section 4, results obtained from the several experiments are reported."
            ],
            "highlighted_evidence": [
              "However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments."
            ]
          }
        ]
      },
      {
        "question": "What language is the Twitter content in?",
        "question_id": "7920f228de6ef4c685f478bac4c7776443f19f39",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "English",
            "evidence": [
              "In this work, we analyze a set of tweets related to a specific classical music radio channel, BBC Radio 3, interested in detecting two types of musical named entities, Contributor and Musical Work."
            ],
            "highlighted_evidence": [
              "In this work, we analyze a set of tweets related to a specific classical music radio channel, BBC Radio 3, interested in detecting two types of musical named entities, Contributor and Musical Work."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/cys/PorcaroS19",
    "dblp_title": "Recognizing Musical Entities in User-generated Content.",
    "year": "2019"
  },
  {
    "id": "1709.00387",
    "title": "MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge",
    "qas": [
      {
        "question": "What is the architecture of the siamese neural network?",
        "question_id": "41844d1d1ee6d6d38f31b3a17a2398f87566ed92",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "two parallel convolutional networks, INLINEFORM0 , that share the same set of weights"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To further distinguish speech from different Arabic dialects, while making speech from the same dialect more similar, we adopted a Siamese neural network architecture BIBREF24 based on an i-vector feature space. The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a). Let INLINEFORM2 and INLINEFORM3 be a pair of i-vectors for which we wish to compute a distance. Let INLINEFORM4 be the label for the pair, where INLINEFORM5 = 1 if the i-vectors INLINEFORM6 and INLINEFORM7 belong to same dialect, and INLINEFORM8 otherwise. To optimize the network, we use a Euclidean distance loss function between the label and the cosine distance, INLINEFORM9 , where INLINEFORM10"
            ],
            "highlighted_evidence": [
              "The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a)."
            ]
          }
        ]
      },
      {
        "question": "How do they explore domain mismatch?",
        "question_id": "ae17066634bd2731a07cd60e9ca79fc171692585",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How do they explore dialect variability?",
        "question_id": "4fa2faa08eeabc09d78d89aaf0ea86bb36328172",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which are the four Arabic dialects?",
        "question_id": "e87f47a293e0b49ab8b15fc6633d9ca6dc9de071",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Egyptian (EGY)",
              "Levantine (LEV)",
              "Gulf (GLF)",
              "North African (NOR)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the MGB-3 ADI task, the challenge organizers provided 13,825 utterances (53.6 hours) for the training (TRN) set, 1,524 utterances (10 hours) for a development (DEV) set, and 1,492 utterances (10.1 hours) for a test (TST) set. Each dataset consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA). Detailed statistics of the ADI dataset can be found in BIBREF23 . Table TABREF3 shows some facts about the evaluation conditions and data properties. Note that the development set is relatively small compared to the training set. However, it is matched with the test set channel domain. Thus, the development set provides valuable information to adapt or compensate the channel (recording) domain mismatch between the train and test sets."
            ],
            "highlighted_evidence": [
              "Each dataset consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA)."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/asru/ShonAG17",
    "dblp_title": "MIT-QCRI Arabic dialect identification system for the 2017 multi-genre broadcast challenge.",
    "year": "2017"
  },
  {
    "id": "1806.11322",
    "title": "Bias in Semantic and Discourse Interpretation",
    "qas": [
      {
        "question": "What factors contribute to interpretive biases according to this research?",
        "question_id": "7426a6e800d6c11795941616fc4a243e75716a10",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While the choice of wording helps to convey bias, just as crucial is the way that the reporters portray the march as being related to other events. Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias. Townhall's bias against the March of Science expressed in the argument that it politicizes science cannot be traced back to negative opinion words; it relies on a comparison between the March for Science and the Women's March, which is portrayed as a political, anti-Trump event. Newsbusters takes a different track: the opening paragraph conveys an overall negative perspective on the March for Science, despite its neutral language, but it achieves this by contrasting general interest in the march with a claimed negative view of the march by many “actual scientists.” On the other hand, the New York Times points to an important and presumably positive outcome of the march, despite its controversiality: a renewed look into the role of science in public life and politics. Like Newsbusters, it lacks any explicit evaluative language and relies on the structural relations between events to convey an overall positive perspective; it contrasts the controversy surrounding the march with a claim that the march has triggered an important discussion, which is in turn buttressed by the reporter's mentioning of the responses of the Times' readership."
            ],
            "highlighted_evidence": [
              "Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias."
            ]
          }
        ]
      },
      {
        "question": "Which interpretative biases are analyzed in this paper?",
        "question_id": "da4535b75e360604e3ce4bb3631b0ba96f4dadd3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "An epistemic ME game is an ME game with a Harsanyi type space and a type/history correspondence as we've defined it. By adding types to an ME game, we provide the beginnings of a game theoretic model of interpretive bias that we believe is completely new. Our definition of bias is now: [Interpretive Bias] An interpretive bias in an epistemic ME game is the probability distribution over types given by the belief function of the conversationalists or players, or the Jury. Note that in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury."
            ],
            "highlighted_evidence": [
              "Our definition of bias is now: [Interpretive Bias] An interpretive bias in an epistemic ME game is the probability distribution over types given by the belief function of the conversationalists or players, or the Jury.",
              "Note that in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1806-11322",
    "dblp_title": "Bias in Semantic and Discourse Interpretation.",
    "year": "2018"
  },
  {
    "id": "2004.00139",
    "title": "A Swiss German Dictionary: Variation in Speech and Writing",
    "qas": [
      {
        "question": "How many words are coded in the dictionary?",
        "question_id": "4d30c2223939b31216f2e90ef33fe0db97e962ac",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "11'248"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We pair 11'248 standard German written words with their phonetical representations in six different Swiss dialects: Zürich, St. Gallen, Basel, Bern, Visp, and Stans (Figure FIGREF1). The phonetic words were written in a modified version of the Speech Assessment Methods Phonetic Alphabet (SAMPA). The Swiss German phonetic words are also paired with Swiss German writings in the latin alphabet. (From here onwards, a phonetic representation of a Swiss German word will be called a SAMPA and a written Swiss German word will be called a GSW.)"
            ],
            "highlighted_evidence": [
              "We pair 11'248 standard German written words with their phonetical representations in six different Swiss dialects: Zürich, St. Gallen, Basel, Bern, Visp, and Stans (Figure FIGREF1)."
            ]
          }
        ]
      },
      {
        "question": "Is the model evaluated on the graphemes-to-phonemes task?",
        "question_id": "7b47aa6ba247874eaa8ab74d7cb6205251c01eb5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To increase the benefits of our data for ASR systems, we also trained a grapheme-to-phoneme (g2p) model: Out-of-vocabulary words can be a problem for ASR system. For those out-of-vocabulary words we need a model that can generate pronunciations from a written form, in real time. This is why we train a grapheme-to-phoneme (g2p) model that generates a sequence of phonemes for a given word. We train the g2p model using our dictionary and compare its performance with a widely used joint-sequence g2p model, Sequitur BIBREF26. For the g2p model we are using the same architecture as for the p2g model. The only difference is input and output vocabulary. The Sequitur and our model are using the dictionary with the same train (19'898 samples), test (2'412 samples) and validation (2'212 samples) split. Additionally, we also test their performance only on the items from the Zurich and Visp dialect, because most of the samples are from this two dialects. In Table TABREF15 we show the result of the comparison of the two models. We compute the edit distance between the predicted and the true pronunciation and report the number of exact matches. In the first columns we have the result using the whole test set with all the dialects, and in the 2nd and 3rd columns we show the number of exact matches only on the samples from the test set that are from the Zurich and Visp dialect. For here we can clearly see that our model performs better than the Sequitur model. The reason why we have less matches in the Visp dialect compared to Zurich is because most of the our data is from the Zurich dialect."
            ],
            "highlighted_evidence": [
              "Additionally, we also test their performance only on the items from the Zurich and Visp dialect, because most of the samples are from this two dialects. In Table TABREF15 we show the result of the comparison of the two models."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/SchmidtLDLSM20",
    "dblp_title": "A Swiss German Dictionary: Variation in Speech and Writing.",
    "year": "2020"
  },
  {
    "id": "1811.08048",
    "title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships",
    "qas": [
      {
        "question": "How does the QuaSP+Zero model work?",
        "question_id": "ce14b87dacfd5206d2a5af7c0ed1cfeb7b181922",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "does not just consider the question tokens, but also the relationship between those tokens and the properties"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model. For example, a question token such as “longer” can act as a cue for (the property) length, even if unseen in the training data, because “longer” and a lexical form of length (e.g.,“length”) are similar. This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing. Here, we modify their entity linking score INLINEFORM1 , linking question tokens INLINEFORM2 and property “entities” INLINEFORM3 , to be: INLINEFORM4"
            ],
            "highlighted_evidence": [
              "We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model.",
              "This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing."
            ]
          }
        ]
      },
      {
        "question": "Which off-the-shelf tools do they use on QuaRel?",
        "question_id": "709a4993927187514701fe3cc491ac3030da1215",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "information retrieval system",
              "word-association method",
              " CCG-style rule-based semantic parser written specifically for friction questions",
              "state-of-the-art neural semantic parser"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use four systems to evaluate the difficulty of this dataset. (We subsequently present two new models, extending the baseline neural semantic parser, in Sections SECREF36 and SECREF44 ). The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn."
            ],
            "highlighted_evidence": [
              "We use four systems to evaluate the difficulty of this dataset.",
              " The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn."
            ]
          }
        ]
      },
      {
        "question": "How do they obtain the logical forms of their questions in their dataset?",
        "question_id": "a3c6acf900126bc9bd9c50ce99041ea00761da6a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " workers were given a seed qualitative relation",
              "asked to enter two objects, people, or situations to compare",
              "created a question, guided by a large number of examples",
              "LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 ).",
              "Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:",
              "From this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred). Three independent workers answer these follow-up questions to ensure reliable results."
            ],
            "highlighted_evidence": [
              "First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words.",
              "Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:\n\nFrom this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred)."
            ]
          }
        ]
      },
      {
        "question": "Do all questions in the dataset allow the answers to pick from 2 options?",
        "question_id": "31b631a8634f6180b20a72477040046d1e085494",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 )."
            ],
            "highlighted_evidence": [
              "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/TafjordC0YS19",
    "dblp_title": "QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships.",
    "year": "2019"
  },
  {
    "id": "1904.10500",
    "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
    "qas": [
      {
        "question": "What is shared in the joint model?",
        "question_id": "ab78f066144936444ecd164dc695bec1cb356762",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "jointly trained with slots"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Joint-1: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots)",
              "Joint-2: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots & intent keywords)"
            ],
            "highlighted_evidence": [
              "Using sequence-to-sequence networks, the approach here is jointly training annotated utterance-level intents and slots/intent keywords by adding / tokens to the beginning/end of each utterance, with utterance-level intent-type as labels of such tokens. Our approach is an extension of BIBREF2 , in which only an term is added with intent-type tags associated to this sentence final token, both for LSTM and Bi-LSTM cases. However, we experimented with adding both and terms as Bi-LSTMs will be used for seq2seq learning, and we observed that slightly better results can be achieved by doing so. The idea behind is that, since this is a seq2seq learning problem, at the last time step (i.e., prediction at ) the reverse pass in Bi-LSTM would be incomplete (refer to Fig. FIGREF24 (a) to observe the last Bi-LSTM cell). Therefore, adding token and leveraging the backward LSTM output at first time step (i.e., prediction at ) would potentially help for joint seq2seq learning. An overall network architecture can be found in Fig. FIGREF30 for our joint models. We will report the experimental results on two variations (with and without intent keywords) as follows:\n\nJoint-1: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots)\n\nJoint-2: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots & intent keywords)"
            ]
          }
        ]
      },
      {
        "question": "Are the intent labels imbalanced in the dataset?",
        "question_id": "e659ceb184777015c12db2da5ae396635192f0b0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "For in-cabin intent understanding, we described 4 groups of usages to support various natural commands for interacting with the vehicle: (1) Set/Change Destination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/off, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are identified and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropOff, OpenDoor, and Other. For slot filling task, relevant slots are identified and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., `this', `that', `over there', etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar tasks on Amazon Mechanical Turk BIBREF21 and reached 3418 utterances with intents in total. Intent and slot annotations are obtained on the transcribed utterances by majority voting of 3 annotators. Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",
              "FLOAT SELECTED: Table 2: AMIE Dataset Statistics: Slots and Intent Keywords"
            ],
            "highlighted_evidence": [
              " Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",
              "FLOAT SELECTED: Table 2: AMIE Dataset Statistics: Slots and Intent Keywords"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/cicling/OkurKSEN19",
    "dblp_title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances.",
    "year": "2019"
  },
  {
    "id": "1704.00177",
    "title": "Sentiment Analysis of Citations Using Word2vec",
    "qas": [
      {
        "question": "What kernels are used in the support vector machines?",
        "question_id": "b512ab8de26874ee240cffdb3c65d9ac8d6023d9",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What dataset is used?",
        "question_id": "4e4d377b140c149338446ba69737ea191c4328d9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "ACL Anthology Reference Corpus"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The ACL-Embeddings (300 and 100 dimensions) from ACL collection were trained . ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which I have generated 622,144 sentences after filtering out sentences with lower quality."
            ],
            "highlighted_evidence": [
              "ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which I have generated 622,144 sentences after filtering out sentences with lower quality."
            ]
          }
        ]
      },
      {
        "question": "What metrics are considered?",
        "question_id": "828ce5faed7783297cf9ce202364f999b8d4a1f6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "F-score",
              "micro-F",
              "macro-F",
              "weighted-F "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "One-Vs-The-Rest strategy was adopted for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores using 10-fold cross-validation. The F1 score is a weighted average of the precision and recall. In the multi-class case, this is the weighted average of the F1 score of each class. There are several types of averaging performed on the data: Micro-F calculates metrics globally by counting the total true positives, false negatives and false positives. Macro-F calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance."
            ],
            "highlighted_evidence": [
              "One-Vs-The-Rest strategy was adopted for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores using 10-fold cross-validation. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/Liu17b",
    "dblp_title": "Sentiment Analysis of Citations Using Word2vec.",
    "year": "2017"
  },
  {
    "id": "1711.11221",
    "title": "Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches",
    "qas": [
      {
        "question": "Did the authors evaluate their system output for coherence?",
        "question_id": "9d016eb3913b41f7a18c6fa865897c12b5fe0212",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We want to further study how the proposed cache-based neural model influence coherence in document translation. For this, we follow Lapata2005Automatic to measure coherence as sentence similarity. First, each sentence is represented by the mean of the distributed vectors of its words. Second, the similarity between two sentences is determined by the cosine of their means."
            ],
            "highlighted_evidence": [
              "we follow Lapata2005Automatic to measure coherence as sentence similarity"
            ]
          }
        ]
      },
      {
        "question": "What evaluations did the authors use on their system?",
        "question_id": "c1c611409b5659a1fd4a870b6cc41f042e2e9889",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.",
            "evidence": [
              "FLOAT SELECTED: Table 1: Experiment results on the NIST Chinese-English translation tasks. [+Cd] is the proposed model with the dynamic cache. [+Cd,Ct] is the proposed model with both the dynamic and topic cache. The BLEU scores are case-insensitive. Avg means the average BLEU score on all test sets.",
              "FLOAT SELECTED: Table 3: The average number of words in translations of beginning sentences of documents that are also in the topic cache. Reference represents the average number of words in four human translations that are also in the topic cache.",
              "FLOAT SELECTED: Table 6: The average cosine similarity of adjacent sentences (coherence) on all test sets."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Experiment results on the NIST Chinese-English translation tasks. [+Cd] is the proposed model with the dynamic cache. [+Cd,Ct] is the proposed model with both the dynamic and topic cache. The BLEU scores are case-insensitive. Avg means the average BLEU score on all test sets.",
              "FLOAT SELECTED: Table 3: The average number of words in translations of beginning sentences of documents that are also in the topic cache. Reference represents the average number of words in four human translations that are also in the topic cache.",
              "FLOAT SELECTED: Table 6: The average cosine similarity of adjacent sentences (coherence) on all test sets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/coling/KuangXLZ18",
    "dblp_title": "Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches.",
    "year": "2018"
  },
  {
    "id": "1912.07025",
    "title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts",
    "qas": [
      {
        "question": "What accuracy does CNN model achieve?",
        "question_id": "79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Combined per-pixel accuracy for character line segments is 74.79",
            "evidence": [
              "FLOAT SELECTED: TABLE IV: Class-wise average IoUs and per-pixel accuracies on the test set. Refer to Table I for full names of abbreviated region types listed at top of the table.",
              "FLOAT SELECTED: TABLE I: Counts for various annotated region types in INDISCAPES dataset. The abbreviations used for region types are given below each region type."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: TABLE IV: Class-wise average IoUs and per-pixel accuracies on the test set. Refer to Table I for full names of abbreviated region types listed at top of the table.",
              "FLOAT SELECTED: TABLE I: Counts for various annotated region types in INDISCAPES dataset. The abbreviations used for region types are given below each region type."
            ]
          }
        ]
      },
      {
        "question": "How many documents are in the Indiscapes dataset?",
        "question_id": "26faad6f42b6d628f341c8d4ce5a08a591eea8c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "508",
            "evidence": [
              "FLOAT SELECTED: TABLE III: Scripts in the INDISCAPES dataset."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: TABLE III: Scripts in the INDISCAPES dataset."
            ]
          }
        ]
      },
      {
        "question": "What language(s) are the manuscripts written in?",
        "question_id": "20be7a776dfda0d3c9dc10270699061cb9bc8297",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/icdar/PrustyATS19",
    "dblp_title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts.",
    "year": "2019"
  },
  {
    "id": "1709.01256",
    "title": "Semantic Document Distance Measures and Unsupervised Document Revision Detection",
    "qas": [
      {
        "question": "What metrics are used to evaluation revision detection?",
        "question_id": "3bfb8c12f151dada259fbd511358914c4b4e1b0e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "precision",
              "recall",
              "F-measure"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We use precision, recall and F-measure to evaluate the detected revisions. A true positive case is a correctly identified revision. A false positive case is an incorrectly identified revision. A false negative case is a missed revision record."
            ],
            "highlighted_evidence": [
              "We use precision, recall and F-measure to evaluate the detected revisions."
            ]
          }
        ]
      },
      {
        "question": "How large is the Wikipedia revision dump dataset?",
        "question_id": "3f85cc5be84479ba668db6d9f614fedbff6d77f1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "eight GB"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The Wikipedia revision dumps that were previously introduced by Leskovec et al. leskovec2010governance contain eight GB (compressed size) revision edits with meta data."
            ],
            "highlighted_evidence": [
              "The Wikipedia revision dumps that were previously introduced by Leskovec et al. leskovec2010governance contain eight GB (compressed size) revision edits with meta data."
            ]
          }
        ]
      },
      {
        "question": "What are simulated datasets collected?",
        "question_id": "126e8112e26ebf8c19ca7ff3dd06691732118e90",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents",
            "evidence": [
              "The generation process of the simulated data sets is designed to mimic the real world. Users open some existing documents in a file system, make some changes (e.g. addition, deletion or replacement), and save them as separate documents. These documents become revisions of the original documents. We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. Similar to a file system, at any moment new documents could be added and/or some of the current documents could be revised. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The addition of words, ..., section names, and new documents were pulled from the Wikipedia abstracts. This corpus generation process had five time periods INLINEFORM0 . Figure FIGREF42 illustrates this simulation. We set a Poisson distribution with rate INLINEFORM1 (the number of documents in the initial corpus) to control the number of new documents added in each time period, and a Poisson distribution with rate INLINEFORM2 to control the number of documents revised in each time period.",
              "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). Table TABREF48 summarizes the first data set. In each data set, we name the initial corpus Corpus 0, and define INLINEFORM0 as the timestamp when we started this simulation process. We set INLINEFORM1 , INLINEFORM2 . Corpus INLINEFORM3 corresponds to documents generated before timestamp INLINEFORM4 . We extracted document revisions from Corpus INLINEFORM5 and compared the revisions generated in (Corpus INLINEFORM6 - Corpus INLINEFORM7 ) with the ground truths in Table TABREF48 . Hence, we ran four experiments on this data set in total. In every experiment, INLINEFORM8 is calibrated based on Corpus INLINEFORM9 . For instance, the training set of the first experiment was Corpus 1. We trained INLINEFORM10 from Corpus 1. We extracted all revisions in Corpus 2, and compared revisions generated in the test set (Corpus 2 - Corpus 1) with the ground truth: 258 revised documents. The word2vec model shared in the four experiments was trained on Corpus 5."
            ],
            "highlighted_evidence": [
              "We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents.",
              "The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The addition of words, ..., section names, and new documents were pulled from the Wikipedia abstracts.",
              "We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5)."
            ]
          }
        ]
      },
      {
        "question": "Which are the state-of-the-art models?",
        "question_id": "be08ef81c3cfaaaf35c7414397a1871611f1a7fd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WMD",
              "VSM",
              "PV-DTW",
              "PV-TED"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods.",
              "We denote the following distance/similarity measures.",
              "WMD: The Word Mover's Distance introduced in Section SECREF1 . WMD adapts the earth mover's distance to the space of documents.",
              "VSM: The similarity measure introduced in Section UID12 .",
              "PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1 is the PV embedding of paragraph INLINEFORM2 .",
              "PV-TED: PV-TED is the same as Algorithm SECREF23 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 ."
            ],
            "highlighted_evidence": [
              "This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods.",
              "We denote the following distance/similarity measures.",
              "WMD: The Word Mover's Distance introduced in Section SECREF1 .",
              "VSM: The similarity measure introduced in Section UID12 .",
              "PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1 is the PV embedding of paragraph INLINEFORM2 .",
              "PV-TED: PV-TED is the same as Algorithm SECREF23 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ijcnlp/ZhuKB17",
    "dblp_title": "Semantic Document Distance Measures and Unsupervised Document Revision Detection.",
    "year": "2017"
  },
  {
    "id": "1909.03526",
    "title": "Multi-Task Bidirectional Transformer Representations for Irony Detection",
    "qas": [
      {
        "question": "Why is being feature-engineering free an advantage?",
        "question_id": "dc57ae854d78aa5d5e8c979826d3e2524d4e9165",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Where did this model place in the final evaluation of the shared task?",
        "question_id": "18412237f7faafc6befe975d5bcd348e2b499b55",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$4th$"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For the shared task submission, we use the predictions of BERT-1M-MT5 as our first submitted system. Then, we concatenate our DEV and TRAIN data to compose a new training set (thus using all the training data released by organizers) to re-train BERT-1M-MT5 and BERT-MT6 with the same parameters. We use the predictions of these two models as our second and third submissions. Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task."
            ],
            "highlighted_evidence": [
              "Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task."
            ]
          }
        ]
      },
      {
        "question": "What in-domain data is used to continue pre-training?",
        "question_id": "02945c85d6cc4cdd1757b2f2bfa5e92ee4ed14a0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "dialectal tweet data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data)."
            ],
            "highlighted_evidence": [
              "We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data)."
            ]
          }
        ]
      },
      {
        "question": "What dialect is used in the Google BERT model and what is used in the task data?",
        "question_id": "6e51af9088c390829703c6fa966e98c3a53114c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Modern Standard Arabic (MSA)",
              "MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA). This introduces a language variety mismatch due to the irony data involving a number of dialects that come from the Twitter domain. To mitigate this issue, we further pre-train BERT on an in-house dialectal Twitter dataset, showing the utility of this measure. To summarize, we make the following contributions:",
              "The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e. targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony\"). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine."
            ],
            "highlighted_evidence": [
              "Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA).",
              "The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018.",
              "Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine."
            ]
          }
        ]
      },
      {
        "question": "What are the tasks used in the mulit-task learning setup?",
        "question_id": "07ee4e0277ad1083270131d32a71c3fe062a916d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Author profiling and deception detection in Arabic",
              "LAMA+DINA Emotion detection",
              "Sentiment analysis in Arabic tweets"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our multi-task BERT models involve six different Arabic classification tasks. We briefly introduce the data for these tasks here:",
              "Author profiling and deception detection in Arabic (APDA). BIBREF9 . From APDA, we only use the corpus of author profiling (which includes the three profiling tasks of age, gender, and variety). The organizers of APDA provide 225,000 tweets as training data. Each tweet is labelled with three tags (one for each task). To develop our models, we split the training data into 90% training set ($n$=202,500 tweets) and 10% development set ($n$=22,500 tweets). With regard to age, authors consider tweets of three classes: {Under 25, Between 25 and 34, and Above 35}. For the Arabic varieties, they consider the following fifteen classes: {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. Gender is labeled as a binary task with {male,female} tags.",
              "LAMA+DINA Emotion detection. Alhuzali et al. BIBREF10 introduce LAMA, a dataset for Arabic emotion detection. They use a first-person seed phrase approach and extend work by Abdul-Mageed et al. BIBREF11 for emotion data collection from 6 to 8 emotion categories (i.e. anger, anticipation, disgust, fear, joy, sadness, surprise and trust). We use the combined LAMA+DINA corpus. It is split by the authors as 189,902 tweets training set, 910 as development, and 941 as test. In our experiment, we use only the training set for out MTL experiments.",
              "Sentiment analysis in Arabic tweets. This dataset is a shared task on Kaggle by Motaz Saad . The corpus contains 58,751 Arabic tweets (46,940 training, and 11,811 test). The tweets are annotated with positive and negative labels based on an emoji lexicon."
            ],
            "highlighted_evidence": [
              "Our multi-task BERT models involve six different Arabic classification tasks.",
              "Author profiling and deception detection in Arabic (APDA).",
              "LAMA+DINA Emotion detection.",
              "Sentiment analysis in Arabic tweets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/fire/ZhangA19a",
    "dblp_title": "Multi-Task Bidirectional Transformer Representations for Irony Detection.",
    "year": "2019"
  },
  {
    "id": "1902.11049",
    "title": "Evaluating Rewards for Question Generation Models",
    "qas": [
      {
        "question": "What human evaluation metrics were used in the paper?",
        "question_id": "bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context",
            "evidence": [
              "For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer."
            ],
            "highlighted_evidence": [
              "For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/Hosking019",
    "dblp_title": "Evaluating Rewards for Question Generation Models.",
    "year": "2019"
  },
  {
    "id": "1905.06906",
    "title": "Gated Convolutional Neural Networks for Domain Adaptation",
    "qas": [
      {
        "question": "For the purposes of this paper, how is something determined to be domain specific knowledge?",
        "question_id": "dfbab3cd991f86d998223726617d61113caa6193",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "reviews under distinct product categories are considered specific domain knowledge",
            "evidence": [
              "Amazon Reviews Dataset BIBREF24 is a large dataset with millions of reviews from different product categories. For our experiments, we consider a subset of 20000 reviews from the domains Cell Phones and Accessories(C), Clothing and Shoes(S), Home and Kitchen(H) and Tools and Home Improvement(T). Out of 20000 reviews, 10000 are positive and 10000 are negative. We use 12800 reviews for training, 3200 reviews for validation and 4000 reviews for testing from each domain."
            ],
            "highlighted_evidence": [
              "Amazon Reviews Dataset BIBREF24 is a large dataset with millions of reviews from different product categories. For our experiments, we consider a subset of 20000 reviews from the domains Cell Phones and Accessories(C), Clothing and Shoes(S), Home and Kitchen(H) and Tools and Home Improvement(T). Out of 20000 reviews, 10000 are positive and 10000 are negative. We use 12800 reviews for training, 3200 reviews for validation and 4000 reviews for testing from each domain."
            ]
          }
        ]
      },
      {
        "question": "Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?",
        "question_id": "df510c85c277afc67799abcb503caa248c448ad2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. This is depicted in Table 1 .",
              "We find that gated architectures vastly outperform non gated CNN model. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation."
            ],
            "highlighted_evidence": [
              "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models.",
              "The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation."
            ]
          }
        ]
      },
      {
        "question": "Are there conceptual benefits to using GCNs over more complex architectures like attention?",
        "question_id": "d95180d72d329a27ddf2fd5cc6919f469632a895",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "The proposed model architecture is shown in the Figure 1 . Recurrent Neural Networks like LSTM, GRU update their weights at every timestep sequentially and hence lack parallelization over inputs in training. In case of attention based models, the attention layer has to wait for outputs from all timesteps. Hence, these models fail to take the advantage of parallelism either. Since, proposed model is based on convolution layers and gated mechanism, it can be parallelized efficiently. The convolution layers learn higher level representations for the source domain. The gated mechanism learn the domain agnostic representations. They together control the information that has to flow through further fully connected output layer after max pooling.",
              "We find that gated architectures vastly outperform non gated CNN model. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation.",
              "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. This is depicted in Table 1 ."
            ],
            "highlighted_evidence": [
              "The gated mechanism learn the domain agnostic representations. They together control the information that has to flow through further fully connected output layer after max pooling.",
              "The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation.",
              "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/nldb/MadasuR19",
    "dblp_title": "Gated Convolutional Neural Networks for Domain Adaptation.",
    "year": "2019"
  },
  {
    "id": "1809.09795",
    "title": "Deep contextualized word representations for detecting sarcasm and irony",
    "qas": [
      {
        "question": "Do they evaluate only on English?",
        "question_id": "e196e2ce72eb8b2d50732c26e9bf346df6643f69",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .",
              "In Table TABREF1 , we see a notable difference in terms of size among the Twitter datasets. Given this circumstance, and in light of the findings by BIBREF18 , we are interested in studying how the addition of external soft-annotated data impacts on the performance. Thus, in addition to the datasets introduced before, we use two corpora for augmentation purposes. The first dataset was collected using the Twitter API, targeting tweets with the hashtags #sarcasm or #irony, resulting on a total of 180,000 and 45,000 tweets respectively. On the other hand, to obtain non-sarcastic and non-ironic tweets, we relied on the SemEval 2018 Task 1 dataset BIBREF25 . To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems. We later extract all the hashtags in each target dataset and proceed to augment only using those external tweets that contain any of these hashtags. This allows us to, for each class, add a total of 36,835 tweets for the Ptáček corpus, 8,095 for the Riloff corpus and 26,168 for the SemEval-2018 corpus."
            ],
            "highlighted_evidence": [
              "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 ",
              "Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .",
              " To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems."
            ]
          }
        ]
      },
      {
        "question": "What are the 7 different datasets?",
        "question_id": "46570c8faaeefecc8232cfc2faab0005faaba35f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "SemEval 2018 Task 3",
              "BIBREF20",
              "BIBREF4",
              "SARC 2.0",
              "SARC 2.0 pol",
              "Sarcasm Corpus V1 (SC-V1)",
              "Sarcasm Corpus V2 (SC-V2)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 ."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC)."
            ]
          }
        ]
      },
      {
        "question": "What are the three different sources of data?",
        "question_id": "982d375378238d0adbc9a4c987d633ed16b7f98f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Twitter",
              "Reddit",
              "Online Dialogues"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 ."
            ],
            "highlighted_evidence": [
              "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.",
              "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.",
              "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.",
              "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 ."
            ]
          }
        ]
      },
      {
        "question": "What type of model are the ELMo representations used in?",
        "question_id": "bbdb2942dc6de3d384e3a1b705af996a5341031b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A bi-LSTM with max-pooling on top of it",
            "evidence": [
              "The usage of a purely character-based input would allow us to directly recover and model these features. Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .",
              "Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification."
            ],
            "highlighted_evidence": [
              "Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10",
              "Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .",
              "Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification."
            ]
          }
        ]
      },
      {
        "question": "Which morphosyntactic features are thought to indicate irony or sarcasm?",
        "question_id": "4ec538e114356f72ef82f001549accefaf85e99c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "all caps",
              "quotation marks",
              "emoticons",
              "emojis",
              "hashtags"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "On the other hand, deep models for irony and sarcasm detection, which are currently offer state-of-the-art performance, have exploited sequential neural networks such as LSTMs and GRUs BIBREF15 , BIBREF23 on top of distributed word representations. Recently, in addition to using a sequential model, BIBREF14 proposed to use intra-attention to compare elements in a sequence against themselves. This allowed the model to better capture word-to-word level interactions that could also be useful for detecting sarcasm, such as the incongruity phenomenon BIBREF3 . Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags."
            ],
            "highlighted_evidence": [
              "Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/wassa/IlicMBM18",
    "dblp_title": "Deep contextualized word representations for detecting sarcasm and irony.",
    "year": "2018"
  },
  {
    "id": "1812.03593",
    "title": "SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering",
    "qas": [
      {
        "question": "Is the model evaluated on other datasets?",
        "question_id": "40a45d59a2ef7a67c8ab0f2b2d5b43fc85b85498",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "We evaluated SDNet on CoQA dataset, which improves the previous state-of-the-art model's result by 1.6% (from 75.0% to 76.6%) overall $F_1$ score. The ensemble model further increase the $F_1$ score to $79.3\\%$ . Moreover, SDNet is the first model ever to pass $80\\%$ on CoQA's in-domain dataset."
            ],
            "highlighted_evidence": [
              "We evaluated SDNet on CoQA dataset"
            ]
          }
        ]
      },
      {
        "question": "Does the model incorporate coreference and entailment?",
        "question_id": "b29b5c39575454da9566b3dd27707fced8c6f4a1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Self-Attention on Question. As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution. We thus employ self-attention on question. The formula is the same as word-level attention, except that we are attending a question to itself: $\\lbrace {u}_i^Q\\rbrace _{i=1}^n=\\mbox{Attn}(\\lbrace {h}_i^{Q,K+1}\\rbrace _{i=1}^n, \\lbrace {h}_i^{Q,K+1}\\rbrace _{i=1}^n, \\lbrace {h}_i^{Q,K+1}\\rbrace _{i=1}^n)$ . The final question representation is thus $\\lbrace {u}_i^Q\\rbrace _{i=1}^n$ ."
            ],
            "highlighted_evidence": [
              "Self-Attention on Question. As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution. We thus employ self-attention on question."
            ]
          }
        ]
      },
      {
        "question": "Is the incorporation of context separately evaluated?",
        "question_id": "4040f5c9f365f9bc80b56dce944ada85bb8b4ab4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1812-03593",
    "dblp_title": "SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering.",
    "year": "2018"
  },
  {
    "id": "2003.01769",
    "title": "Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data",
    "qas": [
      {
        "question": "Which frozen acoustic model do they use?",
        "question_id": "7dce1b64c0040500951c864fce93d1ad7a1809bc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics."
            ],
            "highlighted_evidence": [
              "Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics."
            ]
          }
        ]
      },
      {
        "question": "By how much does using phonetic feedback improve state-of-the-art systems?",
        "question_id": "e1b36927114969f3b759cba056cfb3756de474e4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9",
            "evidence": [
              "In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.",
              "FLOAT SELECTED: Table 2. Speech enhancement scores for the state-of-the-art system trained with the parallel data available in the CHiME4 corpus. Evaluation is done on channel 5 of the simulation et05 data. Mimic loss is applied to the AECNN model trained with time-domain mapping loss only, as well as time-domain and spectral magnitude mapping losses. The joint training system is done with an identical setup to the mimic system with all three losses."
            ],
            "highlighted_evidence": [
              "In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.",
              "FLOAT SELECTED: Table 2. Speech enhancement scores for the state-of-the-art system trained with the parallel data available in the CHiME4 corpus. Evaluation is done on channel 5 of the simulation et05 data. Mimic loss is applied to the AECNN model trained with time-domain mapping loss only, as well as time-domain and spectral magnitude mapping losses. The joint training system is done with an identical setup to the mimic system with all three losses."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/icassp/PlantingaBF20",
    "dblp_title": "Phonetic Feedback for Speech Enhancement with and Without Parallel Speech Data.",
    "year": "2020"
  },
  {
    "id": "1910.04006",
    "title": "Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction",
    "qas": [
      {
        "question": "What features are used?",
        "question_id": "186ccc18c6361904bee0d58196e341a719fb31c2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Sociodemographics: gender, age, marital status, etc.",
              "Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.",
              "Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):",
              "Sociodemographics: gender, age, marital status, etc.",
              "Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.",
              "Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.",
              "The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.",
              "Feature Extraction ::: Structured Features",
              "Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:",
              "Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.",
              "Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).",
              "Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).",
              "These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.",
              "Feature Extraction ::: Unstructured Features",
              "Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.",
              "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain."
            ],
            "highlighted_evidence": [
              "45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):\n\nSociodemographics: gender, age, marital status, etc.\n\nPast medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.\n\nInformation from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\nThe Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.\n\nFeature Extraction ::: Structured Features\nStructure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:\n\nGlobal Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.\n\nInsight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).\n\nCompliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).\n\nThese features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.\n\nFeature Extraction ::: Unstructured Features\nUnstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.\n\nThese unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain."
            ]
          }
        ]
      },
      {
        "question": "Do they compare to previous models?",
        "question_id": "fd5412e2784acefb50afc3bfae1e087580b90ab9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6. We then compare two models incorporating the unstructured features. In the \"Baseline+Domain Sentences\" model, we consider whether adding the counts of sentences per EHR that involve each of the seven risk factor domains as identified by our topic extraction model improved the model performance. In the \"Baseline+Clinical Sentiment\" model, we evaluate whether adding clinical sentiment scores for each risk factor domain improved the model performance. We also experimented with combining both sets of features and found no additional improvement."
            ],
            "highlighted_evidence": [
              "To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6."
            ]
          }
        ]
      },
      {
        "question": "How do they incorporate sentiment analysis?",
        "question_id": "c7f087c78768d5c6f3ff26921858186d627fd4fd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "features per admission were extracted as inputs to the readmission risk classifier"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.",
              "These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component.",
              "45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):"
            ],
            "highlighted_evidence": [
              "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.\n\nThese sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text.",
              "45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier."
            ]
          }
        ]
      },
      {
        "question": "What is the dataset used?",
        "question_id": "82596190560dc2e2ced2131779730f40a3f3eb8c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission."
            ],
            "highlighted_evidence": [
              "The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission."
            ]
          }
        ]
      },
      {
        "question": "How do they extract topics?",
        "question_id": "345f65eaff1610deecb02ff785198aa531648e75",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component."
            ],
            "highlighted_evidence": [
              "These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl-louhi/Alvarez-Mellado19",
    "dblp_title": "Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction.",
    "year": "2019"
  },
  {
    "id": "1611.04361",
    "title": "Attending to Characters in Neural Sequence Labeling Models",
    "qas": [
      {
        "question": "How does this compare to simple interpolation between a word-level and a character-level language model?",
        "question_id": "51d03f0741b72ae242c380266acd2321baf43444",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/coling/ReiCP16",
    "dblp_title": "Attending to Characters in Neural Sequence Labeling Models.",
    "year": "2016"
  },
  {
    "id": "1911.01188",
    "title": "Analysing Coreference in Transformer Outputs",
    "qas": [
      {
        "question": "What translationese effects are seen in the analysis?",
        "question_id": "96c20af8bbef435d0d534d10c42ae15ff2f926f8",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "potentially indicating a shining through effect",
              "explicitation effect"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set.",
              "We also characterised the originals and translations according to coreference features such as total number of chains and mentions, average chain length and size of the longest chain. We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do. As future work, we consider a more detailed comparison of the human and machine translations, and analyse the purpose of the additional mentions added by the NMT systems. It would be also interesting to evaluate of the quality of the automatically computed coreferences chains used for S3."
            ],
            "highlighted_evidence": [
              "Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set.",
              "We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do."
            ]
          }
        ]
      },
      {
        "question": "What languages are seen in the news and TED datasets?",
        "question_id": "9544cc0244db480217ce9174aa13f1bf09ba0d94",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "English",
              "German"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an English-German corpus annotated with full coreference chains BIBREF46. The corpus contains ca. 160.7 thousand tokens manually annotated with about 14.9 thousand mentions and 4.7 thousand coreference chains. For our analysis, we select a portion of English news texts and TED talks from ParCorFull and translate them with the three NMT systems described in SECREF4 above. As texts considerably differ in their length, we select 17 news texts (494 sentences) and four TED talks (518 sentences). The size (in tokens) of the total data set under analysis – source (src) and human translations (ref) from ParCorFull and the automatic translations produced within this study (S1, S2 and S3) are presented in Table TABREF20."
            ],
            "highlighted_evidence": [
              "As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an English-German corpus annotated with full coreference chains BIBREF46."
            ]
          }
        ]
      },
      {
        "question": "How are the coreference chain translations evaluated?",
        "question_id": "c97a4a1c0e3d00137a9ae8d6fbb809ba6492991d",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How are the (possibly incorrect) coreference chains in the MT outputs annotated?",
        "question_id": "3758669426e8fb55a4102564cf05f2864275041b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause)",
              "The mentions referring to the same discourse item are linked between each other.",
              "chain members are annotated for their correctness"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The English sources and their corresponding human translations into German were already manually annotated for coreference chains. We follow the same scheme as BIBREF47 to annotate the MT outputs with coreference chains. This scheme allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause). The mentions can be defined further in terms of their cohesive function (antecedent, anaphoric, cataphoric, comparative, substitution, ellipsis, apposition). Antecedents can either be marked as simple or split or as entity or event. The annotation scheme also includes pronoun type (personal, possessive, demonstrative, reflexive, relative) and modifier types of NPs (possessive, demonstrative, definite article, or none for proper names), see BIBREF46 for details. The mentions referring to the same discourse item are linked between each other. We use the annotation tool MMAX2 BIBREF48 which was also used for the annotation of ParCorFull.",
              "In the next step, chain members are annotated for their correctness. For the incorrect translations of mentions, we include the following error categories: gender, number, case, ambiguous and other. The latter category is open, which means that the annotators can add their own error types during the annotation process. With this, the final typology of errors also considered wrong named entity, wrong word, missing word, wrong syntactic structure, spelling error and addressee reference."
            ],
            "highlighted_evidence": [
              "We follow the same scheme as BIBREF47 to annotate the MT outputs with coreference chains. This scheme allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause). The mentions can be defined further in terms of their cohesive function (antecedent, anaphoric, cataphoric, comparative, substitution, ellipsis, apposition). Antecedents can either be marked as simple or split or as entity or event. The annotation scheme also includes pronoun type (personal, possessive, demonstrative, reflexive, relative) and modifier types of NPs (possessive, demonstrative, definite article, or none for proper names), see BIBREF46 for details. The mentions referring to the same discourse item are linked between each other. We use the annotation tool MMAX2 BIBREF48 which was also used for the annotation of ParCorFull.",
              "In the next step, chain members are annotated for their correctness. For the incorrect translations of mentions, we include the following error categories: gender, number, case, ambiguous and other. The latter category is open, which means that the annotators can add their own error types during the annotation process. With this, the final typology of errors also considered wrong named entity, wrong word, missing word, wrong syntactic structure, spelling error and addressee reference."
            ]
          }
        ]
      },
      {
        "question": "Which three neural machine translation systems are analyzed?",
        "question_id": "1ebd6f703458eb6690421398c79abf3fc114148f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "first two systems are transformer models trained on different amounts of data",
              "The third system includes a modification to consider the information of full coreference chains"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We train three systems (S1, S2 and S3) with the corpora summarised in Table TABREF5. The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sentences as seen in the Table). The third system includes a modification to consider the information of full coreference chains throughout a document augmenting the sentence to be translated with this information and it is trained with the same amount of sentence pairs as S1. A variant of the S3 system participated in the news machine translation of the shared task held at WMT 2019 BIBREF43."
            ],
            "highlighted_evidence": [
              "We train three systems (S1, S2 and S3) with the corpora summarised in Table TABREF5. The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sentences as seen in the Table). The third system includes a modification to consider the information of full coreference chains throughout a document augmenting the sentence to be translated with this information and it is trained with the same amount of sentence pairs as S1."
            ]
          }
        ]
      },
      {
        "question": "Which coreference phenomena are analyzed?",
        "question_id": "15a1df59ed20aa415a4daf0acb256747f6766f77",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "shining through",
              "explicitation"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Therefore, in our analysis, we focus on the chain features related to the phenomena of shining through and explicitation. These features include number of mentions, number of chains, average chain length and the longest chain size. Machine-translated texts are compared to their sources and the corresponding human translations in terms of these features. We expect to find shining through and explicitation effects in automatic translations."
            ],
            "highlighted_evidence": [
              "Therefore, in our analysis, we focus on the chain features related to the phenomena of shining through and explicitation."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/discomt/Lapshinova-Koltunski19",
    "dblp_title": "Analysing Coreference in Transformer Outputs.",
    "year": "2019"
  },
  {
    "id": "1909.08191",
    "title": "Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space",
    "qas": [
      {
        "question": "What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?",
        "question_id": "b124137e62178a2bd3b5570d73b1652dfefa2457",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " analogy query",
              "analogy browsing"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Based on our theoretical results, we design a general framework for data exploration on scholarly data by semantic queries on knowledge graph embedding space. The main component in this framework is the conversion between the data exploration tasks and the semantic queries. We first outline the semantic query solutions to some traditional data exploration tasks, such as similar paper prediction and similar author prediction. We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries."
            ],
            "highlighted_evidence": [
              "We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries."
            ]
          }
        ]
      },
      {
        "question": "What are the uncanny semantic structures of the embedding space?",
        "question_id": "c6aa8a02597fea802890945f0b4be8d631e4d5cd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Semantic similarity structure",
              "Semantic direction structure"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We mainly concern with the two following structures of the embedding space.",
              "Semantic similarity structure: Semantically similar entities are close to each other in the embedding space, and vice versa. This structure can be identified by a vector similarity measure, such as the dot product between two embedding vectors. The similarity between two embedding vectors is computed as:",
              "Semantic direction structure: There exist semantic directions in the embedding space, by which only one semantic aspect changes while all other aspects stay the same. It can be identified by a vector difference, such as the subtraction between two embedding vectors. The semantic direction between two embedding vectors is computed as:"
            ],
            "highlighted_evidence": [
              "We mainly concern with the two following structures of the embedding space.\n\nSemantic similarity structure: Semantically similar entities are close to each other in the embedding space, and vice versa.",
              "Semantic direction structure: There exist semantic directions in the embedding space, by which only one semantic aspect changes while all other aspects stay the same. It can be identified by a vector difference, such as the subtraction between two embedding vectors."
            ]
          }
        ]
      },
      {
        "question": "What is the general framework for data exploration by semantic queries?",
        "question_id": "bfad30f51ce3deea8a178944fa4c6e8acdd83a48",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "three main components, namely data processing, task processing, and query processing"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Given the theoretical results, here we design a general framework for scholarly data exploration by using semantic queries on knowledge graph embedding space. Figure FIGREF19 shows the architecture of the proposed framework. There are three main components, namely data processing, task processing, and query processing.",
              "FLOAT SELECTED: Fig. 1. Architecture of the semantic query framework. Eclipse denotes operation, parallelogram denotes resulting data."
            ],
            "highlighted_evidence": [
              "Figure FIGREF19 shows the architecture of the proposed framework. There are three main components, namely data processing, task processing, and query processing.",
              "FLOAT SELECTED: Fig. 1. Architecture of the semantic query framework. Eclipse denotes operation, parallelogram denotes resulting data."
            ]
          }
        ]
      },
      {
        "question": "What data exploration is supported by the analysis of these semantic structures?",
        "question_id": "dd9883f4adf7be072d314d7ed13fe4518c5500e0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Task processing: converting data exploration tasks to algebraic operations on the embedding space",
              "Query processing: executing semantic query on the embedding space and return results"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Task processing: converting data exploration tasks to algebraic operations on the embedding space by following task-specific conversion templates. Some important tasks and their conversion templates are discussed in Section SECREF5.",
              "Query processing: executing semantic query on the embedding space and return results. Note that the algebraic operations on embedding vectors are linear and can be performed in parallel. Therefore, the semantic query is efficient."
            ],
            "highlighted_evidence": [
              "Task processing: converting data exploration tasks to algebraic operations on the embedding space by following task-specific conversion templates. Some important tasks and their conversion templates are discussed in Section SECREF5.\n\nQuery processing: executing semantic query on the embedding space and return results. Note that the algebraic operations on embedding vectors are linear and can be performed in parallel. Therefore, the semantic query is efficient."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ercimdl/TranT19",
    "dblp_title": "Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space.",
    "year": "2019"
  },
  {
    "id": "1910.06701",
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "qas": [
      {
        "question": "what are the existing models they compared with?",
        "question_id": "81669c550d32d756f516dab5d2b76ff5f21c0f36",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Syn Dep",
              "OpenIE",
              "SRL",
              "BiDAF",
              "QANet",
              "BERT",
              "NAQANet",
              "NAQANet+"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Experiments ::: Baselines",
              "For comparison, we select several public models as baselines including semantic parsing models:",
              "BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;",
              "QANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;",
              "BERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;",
              "and numerical MRC models:",
              "NAQANet BIBREF6, a numerical version of QANet model.",
              "NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.",
              "Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;",
              "OpenIE BIBREF6, KDG with open information extraction based sentence representations;",
              "SRL BIBREF6, KDG with semantic role labeling based sentence representations;",
              "and traditional MRC models:"
            ],
            "highlighted_evidence": [
              "Experiments ::: Baselines\nFor comparison, we select several public models as baselines including semantic parsing models:",
              "BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;\n\nQANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;\n\nBERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;\n\nand numerical MRC models:",
              "NAQANet BIBREF6, a numerical version of QANet model.\n\nNAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc.",
              "Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;\n\nOpenIE BIBREF6, KDG with open information extraction based sentence representations;\n\nSRL BIBREF6, KDG with semantic role labeling based sentence representations;\n\nand traditional MRC models:"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/RanLLZL19",
    "dblp_title": "NumNet: Machine Reading Comprehension with Numerical Reasoning.",
    "year": "2019"
  },
  {
    "id": "1904.11942",
    "title": "Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding",
    "qas": [
      {
        "question": "Do they report results only on English data?",
        "question_id": "b0b1ff2d6515fb40d74a4538614a0db537e020ea",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What conclusions do the authors draw from their detailed analyses?",
        "question_id": "4266aacb575b4be7dbcdb8616766324f8790763c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "neural network-based models can outperform feature-based models with wide margins",
              "contextualized representation learning can boost performance of NN models"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We established strong baselines for two story narrative understanding datasets: CaTeRS and RED. We have shown that neural network-based models can outperform feature-based models with wide margins, and we conducted an ablation study to show that contextualized representation learning can boost performance of NN models. Further research can focus on more systematic study or build stronger NN models over the same datasets used in this work. Exploring possibilities to directly apply temporal relation extraction to enhance performance of story generation systems is another promising research direction."
            ],
            "highlighted_evidence": [
              "We have shown that neural network-based models can outperform feature-based models with wide margins, and we conducted an ablation study to show that contextualized representation learning can boost performance of NN models."
            ]
          }
        ]
      },
      {
        "question": "Do the BERT-based embeddings improve results?",
        "question_id": "191107cd112f7ee6d19c1dc43177e6899452a2c7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "Table TABREF25 contains the best hyper-parameters and Table TABREF26 contains micro-average F1 scores for both datasets on dev and test sets. We only consider positive pairs, i.e. correct predictions on NONE pairs are excluded for evaluation. In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance. We now provide more detailed analysis and discussion for each dataset."
            ],
            "highlighted_evidence": [
              "In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance."
            ]
          }
        ]
      },
      {
        "question": "What were the traditional linguistic feature-based models?",
        "question_id": "b0dca7b74934f51ff3da0c074ad659c25d84174d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CAEVO"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The series of TempEval competitions BIBREF21 , BIBREF22 , BIBREF23 have attracted many research interests in predicting event temporal relations. Early attempts by BIBREF24 , BIBREF21 , BIBREF25 , BIBREF26 only use pair-wise classification models. State-of-the-art local methods, such as ClearTK BIBREF27 , UTTime BIBREF28 , and NavyTime BIBREF29 improve on earlier work by feature engineering with linguistic and syntactic rules. As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. Additionally, several models BramsenDLB2006, ChambersJ2008, DoLuRo12, NingWuRo18, P18-1212 have successfully incorporated global inference to impose global prediction consistency such as temporal transitivity."
            ],
            "highlighted_evidence": [
              "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets."
            ]
          }
        ]
      },
      {
        "question": "What type of baseline are established for the two datasets?",
        "question_id": "601e58a3d2c03a0b4cd627c81c6228a714e43903",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CAEVO"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The series of TempEval competitions BIBREF21 , BIBREF22 , BIBREF23 have attracted many research interests in predicting event temporal relations. Early attempts by BIBREF24 , BIBREF21 , BIBREF25 , BIBREF26 only use pair-wise classification models. State-of-the-art local methods, such as ClearTK BIBREF27 , UTTime BIBREF28 , and NavyTime BIBREF29 improve on earlier work by feature engineering with linguistic and syntactic rules. As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. Additionally, several models BramsenDLB2006, ChambersJ2008, DoLuRo12, NingWuRo18, P18-1212 have successfully incorporated global inference to impose global prediction consistency such as temporal transitivity."
            ],
            "highlighted_evidence": [
              "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1904-11942",
    "dblp_title": "Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding.",
    "year": "2019"
  },
  {
    "id": "1705.02394",
    "title": "Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks",
    "qas": [
      {
        "question": "What model achieves state of the art performance on this task?",
        "question_id": "a0fbf90ceb520626b80ff0f9160b3cd5029585a5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BIBREF16"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Multitask learning, on the other hand, does not appear to have any positive impact on performance. Comparing the two CNN models, the addition of multitask learning actually appears to impair performance, with MultitaskCNN doing worse than BasicCNN in all three metrics. The difference is smaller when comparing BasicDCGAN and MultitaskDCGAN, and may not be enough to decidedly conclude that the use of multitask learning has a net negative impact there, but certainly there is no indication of a net positive impact. The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16 . It needs to be noted that in BIBREF16 data from the test speaker's session partner was utilized in the training of the model. Our models in contrast are trained on only four of the five sessions as discussed in SECREF5 . Further, the here presented models are trained on the raw spectrograms of the audio and no feature extraction was employed whatsoever. This representation learning approach is employed in order to allow the DCGAN component of the model to train on vast amounts of unsupervised speech data."
            ],
            "highlighted_evidence": [
              "The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16 "
            ]
          }
        ]
      },
      {
        "question": "Which multitask annotated corpus is used?",
        "question_id": "e8ca81d5b36952259ef3e0dbeac7b3a622eabe8e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "IEMOCAP"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Due to the semi-supervised nature of the proposed Multitask DCGAN model, we utilize both labeled and unlabeled data. For the unlabeled data, we use audio from the AMI BIBREF8 and IEMOCAP BIBREF7 datasets. For the labeled data, we use audio from the IEMOCAP dataset, which comes with labels for activation and valence, both measured on a 5-point Likert scale from three distinct annotators. Although IEMOCAP provides per-word activation and valence labels, in practice these labels do not generally change over time in a given audio file, and so for simplicity we label each audio clip with the average valence and activation. Since valence and activation are both measured on a 5-point scale, the labels are encoded in 5-element one-hot vectors. For instance, a valence of 5 is represented with the vector INLINEFORM0 . The one-hot encoding can be thought of as a probability distribution representing the likelihood of the correct label being some particular value. Thus, in cases where the annotators disagree on the valence or activation label, this can be represented by assigning probabilities to multiple positions in the label vector. For instance, a label of 4.5 conceptually means that the “correct” valence is either 4 or 5 with equal probability, so the corresponding vector would be INLINEFORM1 . These “fuzzy labels” have been shown to improve classification performance in a number of applications BIBREF14 , BIBREF15 . It should be noted here that we had generally greater success with this fuzzy label method than training the neural network model on the valence label directly, i.e. classification task vs. regression."
            ],
            "highlighted_evidence": [
              "For the labeled data, we use audio from the IEMOCAP dataset, which comes with labels for activation and valence, both measured on a 5-point Likert scale from three distinct annotators."
            ]
          }
        ]
      },
      {
        "question": "What are the tasks in the multitask learning setup?",
        "question_id": "e75685ef5f58027be44f42f30cb3988b509b2768",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "set of related tasks are learned (e.g., emotional activation)",
              "primary task (e.g., emotional valence)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Within this work, we particularly target emotional valence as the primary task, as it has been shown to be the most challenging emotional dimension for acoustic analyses in a number of studies BIBREF10 , BIBREF11 . Apart from solely targeting valence classification, we further investigate the principle of multitask learning. In multitask learning, a set of related tasks are learned (e.g., emotional activation), along with a primary task (e.g., emotional valence); both tasks share parts of the network topology and are hence jointly trained, as depicted in Figure FIGREF4 . It is expected that data for the secondary task models information, which would also be discriminative in learning the primary task. In fact, this approach has been shown to improve generalizability across corpora BIBREF12 ."
            ],
            "highlighted_evidence": [
              "In multitask learning, a set of related tasks are learned (e.g., emotional activation), along with a primary task (e.g., emotional valence); both tasks share parts of the network topology and are hence jointly trained, as depicted in Figure FIGREF4 ."
            ]
          }
        ]
      },
      {
        "question": "What are the subtle changes in voice which have been previously overshadowed?",
        "question_id": "1df24849e50fcf22f0855e0c0937c1288450ed5c",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/icassp/ChangS17",
    "dblp_title": "Learning representations of emotional speech with deep convolutional generative adversarial networks.",
    "year": "2017"
  },
  {
    "id": "1806.09103",
    "title": "Subword-augmented Embedding for Cloze Reading Comprehension",
    "qas": [
      {
        "question": "how are rare words defined?",
        "question_id": "859e0bed084f47796417656d7a68849eb9cb324f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "low-frequency words"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In our experiments, the short list is determined according to the word frequency. Concretely, we sort the vocabulary according to the word frequency from high to low. A frequency filter ratio INLINEFORM0 is set to filter out the low-frequency words (rare words) from the lookup table. For example, INLINEFORM1 =0.9 means the least frequent 10% words are replaced with the default UNK notation."
            ],
            "highlighted_evidence": [
              "A frequency filter ratio INLINEFORM0 is set to filter out the low-frequency words (rare words) from the lookup table"
            ]
          }
        ]
      },
      {
        "question": "which public datasets were used?",
        "question_id": "04e90c93d046cd89acef5a7c58952f54de689103",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "CMRC-2017",
              "People's Daily (PD)",
              "Children Fairy Tales (CFT) ",
              "Children's Book Test (CBT)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To verify the effectiveness of our proposed model, we conduct multiple experiments on three Chinese Machine Reading Comprehension datasets, namely CMRC-2017 BIBREF17 , People's Daily (PD) and Children Fairy Tales (CFT) BIBREF2 . In these datasets, a story containing consecutive sentences is formed as the Document and one of the sentences is either automatically or manually selected as the Query where one token is replaced by a placeholder to indicate the answer to fill in. Table TABREF8 gives data statistics. Different from the current cloze-style datasets for English reading comprehension, such as CBT, Daily Mail and CNN BIBREF0 , the three Chinese datasets do not provide candidate answers. Thus, the model has to find the correct answer from the entire document.",
              "Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case. We only focus on subsets where the answer is either a common noun (CN) or NE which is more challenging since the answer is likely to be rare words. We evaluate all the models in terms of accuracy, which is the standard evaluation metric for this task."
            ],
            "highlighted_evidence": [
              "To verify the effectiveness of our proposed model, we conduct multiple experiments on three Chinese Machine Reading Comprehension datasets, namely CMRC-2017 BIBREF17 , People's Daily (PD) and Children Fairy Tales (CFT) BIBREF2 ",
              "Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case."
            ]
          }
        ]
      },
      {
        "question": "what are the baselines?",
        "question_id": "f513e27db363c28d19a29e01f758437d7477eb24",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "AS Reader, GA Reader, CAS Reader",
            "evidence": [
              "Table TABREF17 shows our results on CMRC-2017 dataset, which shows that our SAW Reader (mul) outperforms all other single models on the test set, with 7.57% improvements compared with Attention Sum Reader (AS Reader) baseline. Although WHU's model achieves the best besides our model on the valid set with only 0.75% below ours, their result on the test set is lower than ours by 2.27%, indicating our model has a satisfactory generalization ability.",
              "FLOAT SELECTED: Table 2: Accuracy on CMRC-2017 dataset. Results marked with † are from the latest official CMRC2017 Leaderboard 7. The best results are in bold face.",
              "FLOAT SELECTED: Table 3: Case study on CMRC-2017."
            ],
            "highlighted_evidence": [
              "Table TABREF17 shows our results on CMRC-2017 dataset, which shows that our SAW Reader (mul) outperforms all other single models on the test set, with 7.57% improvements compared with Attention Sum Reader (AS Reader) baseline",
              "FLOAT SELECTED: Table 2: Accuracy on CMRC-2017 dataset. Results marked with † are from the latest official CMRC2017 Leaderboard 7. The best results are in bold face.",
              "FLOAT SELECTED: Table 3: Case study on CMRC-2017."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/coling/ZhangHZ18",
    "dblp_title": "Subword-augmented Embedding for Cloze Reading Comprehension.",
    "year": "2018"
  },
  {
    "id": "1911.13087",
    "title": "Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset",
    "qas": [
      {
        "question": "What are the results of the experiment?",
        "question_id": "eb5ed1dd26fd9adb587d29225c7951a476c6ec28",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They were able to create a language model from the dataset, but did not test.",
            "evidence": [
              "The BD-4SK-ASR Dataset ::: The Language Model",
              "We created the language from the transcriptions. The model was created using CMUSphinx in which (fixed) discount mass is 0.5, and backoffs are computed using the ratio method. The model includes 283 unigrams, 5337 bigrams, and 6935 trigrams."
            ],
            "highlighted_evidence": [
              "The BD-4SK-ASR Dataset ::: The Language Model\nWe created the language from the transcriptions. The model was created using CMUSphinx in which (fixed) discount mass is 0.5, and backoffs are computed using the ratio method. The model includes 283 unigrams, 5337 bigrams, and 6935 trigrams."
            ]
          }
        ]
      },
      {
        "question": "How was the dataset collected?",
        "question_id": "0828cfcf0e9e02834cc5f279a98e277d9138ffd9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "extracted text from Sorani Kurdish books of primary school and randomly created sentences",
            "evidence": [
              "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences."
            ],
            "highlighted_evidence": [
              "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences."
            ]
          }
        ]
      },
      {
        "question": "What is the size of the dataset?",
        "question_id": "7b2de0109b68f78afa9e6190c82ca9ffaf62f9bd",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2000 sentences"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences."
            ],
            "highlighted_evidence": [
              "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences."
            ]
          }
        ]
      },
      {
        "question": "How many different subjects does the dataset contain?",
        "question_id": "482ac96ff675975227b6d7058b9b87aeab6f81fe",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How many annotators participated?",
        "question_id": "3f3c09c1fd542c1d9acf197957c66b79ea1baf6e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "1",
            "evidence": [
              "Two thousand narration files were created. We used Audacity to record the narrations. We used a normal laptop in a quiet room and minimized the background noise. However, we could not manage to avoid the noise of the fan of the laptop. A single speaker narrated the 2000 sentences, which took several days. We set the Audacity software to have a sampling rate of 16, 16-bit bit rate, and a mono (single) channel. The noise reduction db was set to 6, the sensitivity to 4.00, and the frequency smoothing to 0."
            ],
            "highlighted_evidence": [
              "A single speaker narrated the 2000 sentences, which took several days. "
            ]
          }
        ]
      },
      {
        "question": "How long is the dataset?",
        "question_id": "0a82534ec6e294ab952103f11f56fd99137adc1f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "2000"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."
            ],
            "highlighted_evidence": [
              "The corpus includes 2000 sentences. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-13087",
    "dblp_title": "Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset.",
    "year": "2019"
  },
  {
    "id": "1608.04917",
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "qas": [
      {
        "question": "Do the authors mention any possible confounds in their study?",
        "question_id": "938688871913862c9f8a28b42165237b7324e0de",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": true,
            "free_form_answer": "",
            "evidence": [
              "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration. We observed the same phenomenon recently during the Brexit campaign BIBREF38 . Along our interpretation the Brexit was “won” to some extent due to these social media activities, which are practically non-existent among the pro-EU political groups. The fact that ENL and EFDD are the least cohesive groups in the European Parliament can be attributed to their political focus. It seems more important for the group to agree on its anti-EU stance and to call for independence and sovereignty, and much less important to agree on other issues put forward in the parliament."
            ],
            "highlighted_evidence": [
              "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration."
            ]
          }
        ]
      },
      {
        "question": "What is the relationship between the co-voting and retweeting patterns?",
        "question_id": "4170ed011b02663f5b1b1a3c1f0415b7abfaa85d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "we observe a positive correlation between retweeting and co-voting",
              "strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets",
              "Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union",
              "significantly negative coefficient, is the area Economic and monetary system"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns. Results from section “sec:coalitionpolicy”, confirm that this is indeed the case. Especially noteworthy are the coalitions between GUE-NGL and Greens-EFA on the left wing, and EFDD and ENL on the right wing. In the section “sec:coalitionpolicy” we interpret these results as a combination of Euroscepticism on both sides, motivated on the left by a skeptical attitude towards the market orientation of the EU, and on the right by a reluctance to give up national sovereignty."
            ],
            "highlighted_evidence": [
              "Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns."
            ]
          }
        ]
      },
      {
        "question": "Does the analysis find that coalitions are formed in the same way for different policy areas?",
        "question_id": "fd08dc218effecbe5137a7e3b73d9e5e37ace9c1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "In the area of Economic and monetary system we see a strong cooperation between EPP and S&D (Fig FIGREF42 A), which is on a par with the cohesion of the most cohesive groups (GUE-NGL, S&D, Greens-EFA, ALDE, and EPP), and is above the cohesion of the other groups. As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B. On one hand, we observe cooperation between S&D, ALDE, EPP, and ECR, and on the other, cooperation between GUE-NGL, Greens-EFA, EFDD, ENL, and NI. This division in blocks is seen again in Fig FIGREF42 C, which shows the strongest disagreements. Here, we observe two blocks composed of S&D, EPP, and ALDE on one hand, and GUE-NGL, EFDD, ENL, and NI on the other, which are in strong opposition to each other.",
              "In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.",
              "The patterns of coalitions forming on Twitter closely resemble those in the European Parliament. In Fig FIGREF42 J we see that the strongest degrees of cooperation on Twitter are within the groups. The only group with low cohesion is the NI, whose members have only seven retweets between them. The coalitions on Twitter follow the seating order in the European Parliament remarkably well (see Fig FIGREF42 K). What is striking is that the same blocks form on the left, center, and on the center-right, both in the European Parliament and on Twitter. The largest difference between the coalitions in the European Parliament and on Twitter is on the far-right, where we observe ENL and NI as isolated blocks."
            ],
            "highlighted_evidence": [
              "As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B.",
              "In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.\n\nThe patterns of coalitions forming on Twitter closely resemble those in the European Parliament."
            ]
          }
        ]
      },
      {
        "question": "What insights does the analysis give about the cohesion of political groups in the European parliament?",
        "question_id": "a85c2510f25c7152940b5ac4333a80e0f91ade6e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Greens-EFA, S&D, and EPP exhibit the highest cohesion",
              "non-aligned members NI have the lowest cohesion, followed by EFDD and ENL",
              "two methods disagree is the level of cohesion of GUE-NGL"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "As with INLINEFORM0 , Greens-EFA, S&D, and EPP exhibit the highest cohesion, even though their ranking is permuted when compared to the ranking obtained with INLINEFORM1 . At the other end of the scale, we observe the same situation as with INLINEFORM2 . The non-aligned members NI have the lowest cohesion, followed by EFDD and ENL.",
              "The only place where the two methods disagree is the level of cohesion of GUE-NGL. The Alpha attributes GUE-NGL a rather high level of cohesion, on a par with ALDE, whereas the ERGM attributes them a much lower cohesion. The reason for this difference is the relatively high abstention rate of GUE-NGL. Whereas the overall fraction of non-attending and abstaining MEPs across all RCVs and all political groups is 25%, the GUE-NGL abstention rate is 34%. This is reflected in an above average cohesion by INLINEFORM0 where only yes/no votes are considered, and in a relatively lower, below average cohesion by ERGM. In the later case, the non-attendance is interpreted as a non-cohesive voting of a political groups as a whole."
            ],
            "highlighted_evidence": [
              "As with INLINEFORM0 , Greens-EFA, S&D, and EPP exhibit the highest cohesion, even though their ranking is permuted when compared to the ranking obtained with INLINEFORM1 . At the other end of the scale, we observe the same situation as with INLINEFORM2 . The non-aligned members NI have the lowest cohesion, followed by EFDD and ENL.",
              "The only place where the two methods disagree is the level of cohesion of GUE-NGL. The Alpha attributes GUE-NGL a rather high level of cohesion, on a par with ALDE, whereas the ERGM attributes them a much lower cohesion."
            ]
          }
        ]
      },
      {
        "question": "Do they authors account for differences in usage of Twitter amongst MPs into their model?",
        "question_id": "fa572f1f3f3ce6e1f9f4c9530456329ffc2677ca",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs. The resulting retweet network is an undirected, weighted network.",
              "We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 . The higher the ratio, the more each MEP (on average) retweets the MEPs from the same political group, hence, the higher the cohesion of the political group. The definition of the average retweeets ( INLINEFORM3 ) of a group INLINEFORM4 is: INLINEFORM5"
            ],
            "highlighted_evidence": [
              "The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs",
              "We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2 "
            ]
          }
        ]
      },
      {
        "question": "Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?",
        "question_id": "5e057e115f8976bf9fe70ab5321af72eb4b4c0fc",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/CherepnalkoskiK16",
    "dblp_title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities.",
    "year": "2016"
  },
  {
    "id": "1711.02013",
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "qas": [
      {
        "question": "How do they show their model discovers underlying syntactic structure?",
        "question_id": "d824f837d8bc17f399e9b8ce8b30795944df0d51",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "By visualizing syntactic distance estimated by the parsing network",
            "evidence": [
              "In Figure FIGREF32 , we visualize the syntactic distance estimated by the Parsing Network, while reading three different sequences from the PTB test set. We observe that the syntactic distance tends to be higher between the last character of a word and a space, which is a reasonable breakpoint to separate between words. In other words, if the model sees a space, it will attend on all previous step. If the model sees a letter, it will attend no further then the last space step. The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data. As a result our model achieve state-of-the-art performance and significantly outperform baseline models. It is worth noting that HM-LSTM BIBREF6 also unsupervisedly induce similar structure from data. But discrete operations in HM-LSTM make their training procedure more complicated then ours."
            ],
            "highlighted_evidence": [
              "In Figure FIGREF32 , we visualize the syntactic distance estimated by the Parsing Network, while reading three different sequences from the PTB test set. We observe that the syntactic distance tends to be higher between the last character of a word and a space, which is a reasonable breakpoint to separate between words. ",
              "The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data."
            ]
          }
        ]
      },
      {
        "question": "Which dataset do they experiment with?",
        "question_id": "2ff3898fbb5954aa82dd2f60b37dd303449c81ba",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Penn Treebank",
              "Text8",
              "WSJ10"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "From a character-level view, natural language is a discrete sequence of data, where discrete symbols form a distinct and shallow tree structure: the sentence is the root, words are children of the root, and characters are leafs. However, compared to word-level language modeling, character-level language modeling requires the model to handle longer-term dependencies. We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.",
              "The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset. WSJ10 is the 7422 sentences in the Penn Treebank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although standard constituency parsing tree is not limited to binary tree. Previous unsupervised constituency parsing model also generate binary trees BIBREF11 , BIBREF13 . Our model is compared with the several baseline methods, that are explained in Appendix ."
            ],
            "highlighted_evidence": [
              "We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.",
              "The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset."
            ]
          }
        ]
      },
      {
        "question": "How do they measure performance of language model tasks?",
        "question_id": "3070d6d6a52aa070f0c0a7b4de8abddd3da4f056",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "BPC, Perplexity",
            "evidence": [
              "In Table TABREF39 , our results are comparable to the state-of-the-art methods. Since we do not have the same computational resource used in BIBREF50 to tune hyper-parameters at large scale, we expect that our model could achieve better performance after an aggressive hyperparameter tuning process. As shown in Table TABREF42 , our method outperform baseline methods. It is worth noticing that the continuous cache pointer can also be applied to output of our Predict Network without modification. Visualizations of tree structure generated from learned PTB language model are included in Appendix . In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. By removing Parsing Network, we observe a significant drop of performance. This stands as empirical evidence regarding the benefit of having structure information to control attention.",
              "FLOAT SELECTED: Table 1: BPC on the Penn Treebank test set",
              "Word-level Language Model"
            ],
            "highlighted_evidence": [
              "In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. ",
              "FLOAT SELECTED: Table 1: BPC on the Penn Treebank test set",
              "Word-level Language Model"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iclr/ShenLHC18",
    "dblp_title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon.",
    "year": "2018"
  },
  {
    "id": "1909.00183",
    "title": "Extracting information from free text through unsupervised graph-based clustering: an application to patient incident records",
    "qas": [
      {
        "question": "How are content clusters used to improve the prediction of incident severity?",
        "question_id": "ee9b95d773e060dced08705db8d79a0a6ef353da",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "they are used as additional features in a supervised classification task",
            "evidence": [
              "As a further application of our work, we have carried out a supervised classification task aimed at predicting the degree of harm of an incident directly from the text and the hand-coded features (e.g., external category, medical specialty, location). A one-hot encoding is applied to turn these categorical values into numerical ones. We also checked if using our unsupervised content-driven cluster labels as additional features can improve the performance of the supervised classification."
            ],
            "highlighted_evidence": [
              "As a further application of our work, we have carried out a supervised classification task aimed at predicting the degree of harm of an incident directly from the text and the hand-coded features (e.g., external category, medical specialty, location). ",
              "We also checked if using our unsupervised content-driven cluster labels as additional features can improve the performance of the supervised classification."
            ]
          }
        ]
      },
      {
        "question": "What cluster identification method is used in this paper?",
        "question_id": "dbdf13cb4faa1785bdee90734f6c16380459520b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18",
            "evidence": [
              "The trained Doc2Vec model is subsequently used to infer high-dimensional vector descriptions for the text of each document in our target analysis set. We then compute a matrix containing all the pairwise (cosine) similarities between the Doc2Vec document vectors. This similarity matrix can be thought of as the adjacency matrix of a full, weighted graph with documents as nodes and edges weighted by their similarity. We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF14, a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The MST-kNN graph is then analysed with Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18, a multi-resolution graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need to choose a priori the number or type of clusters."
            ],
            "highlighted_evidence": [
              "We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF14, a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The MST-kNN graph is then analysed with Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18, a multi-resolution graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1909-00183",
    "dblp_title": "Extracting information from free text through unsupervised graph-based clustering: an application to patient incident records.",
    "year": "2019"
  },
  {
    "id": "1703.08885",
    "title": "Question Answering from Unstructured Text by Retrieval and Comprehension",
    "qas": [
      {
        "question": "How can a neural model be used for a retrieval if the input is the entire Wikipedia?",
        "question_id": "73e715e485942859e1db75bfb5f35f1d5eb79d2e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question. For example, for x_to_movie question type, the answer movie articles are the correct articles to be retrieved. On the other hand, for questions in movie_to_x type, the movie in the question should be retrieved. Having collected the labels, we train a retrieval model for classifying a question and article pair as relevant or not relevant.",
              "Figure 5 gives an overview of the model, which uses a Word Level Attention (WLA) mechanism. First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs $v$ (for the question) and $H_c$ (for the document) similar to the previous section."
            ],
            "highlighted_evidence": [
              "The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question.",
              "First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs $v$ (for the question) and $H_c$ (for the document) similar to the previous section."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/WatanabeDS17",
    "dblp_title": "Question Answering from Unstructured Text by Retrieval and Comprehension.",
    "year": "2017"
  },
  {
    "id": "1908.06138",
    "title": "UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task",
    "qas": [
      {
        "question": "Which algorithm is used in the UDS-DFKI system?",
        "question_id": "12391aab31c899bac0ecd7238c111cb73723a6b7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. "
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. We use multi-head attention to jointly attend to information at different positions from different representation subspaces."
            ],
            "highlighted_evidence": [
              "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. We use multi-head attention to jointly attend to information at different positions from different representation subspaces."
            ]
          }
        ]
      },
      {
        "question": "Does the use of out-of-domain data improve the performance of the method?",
        "question_id": "8b43201e7e648c670c02e16ba189230820879228",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants.",
              "This paper presented the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. We presented the results obtained by our system in translating from Czech to Polish. Our system achieved competitive performance ranking second among ten teams in the competition in terms of BLEU score. The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants."
            ],
            "highlighted_evidence": [
              "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants.",
              "The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/wmt/PalZG19",
    "dblp_title": "UDS-DFKI Submission to the WMT2019 Czech-Polish Similar Language Translation Shared Task.",
    "year": "2019"
  },
  {
    "id": "1801.09030",
    "title": "Exploration on Generating Traditional Chinese Medicine Prescriptions from Symptoms with an End-to-End Approach",
    "qas": [
      {
        "question": "Do they impose any grammatical constraints over the generated output?",
        "question_id": "5d5a571ff04a5fdd656ca87f6525a60e917d6558",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "In the TCM prescription generation task, the textual symptom descriptions can be seen as the question and the aim of the task is to produce a set of TCM herbs that form a prescription as the answer to the question. However, the set of herbs is different from the textual answers to a question in the QA task. A difference that is most evident is that there will not be any duplication of herbs in the prescription. However, the basic seq2seq model sometimes produces the same herb tokens repeatedly when applied to the TCM prescription generation task. This phenomenon can hurt the performance of recall rate even after applying a post-process to eliminate repetitions. Because in a limited length of the prescription , the model would produce the same token over and over again, rather than real and novel ones. Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order. In this paper, we explore to automatically generate TCM prescriptions based on textual symptoms. We propose a soft seq2seq model with coverage mechanism and a novel soft loss function. The coverage mechanism is designed to make the model aware of the herbs that have already been generated while the soft loss function is to relieve the side effect of strict order assumption. In the experiment results, our proposed model beats all the baselines in professional evaluations, and we observe a large increase in both the recall rate and the F1 score compared with the basic seq2seq model."
            ],
            "highlighted_evidence": [
              "Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order."
            ]
          }
        ]
      },
      {
        "question": "Why did they think this was a good idea?",
        "question_id": "3c362bfa11c60bad6c7ea83f8753d427cda77de0",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They think it will help human TCM practitioners make prescriptions.",
            "evidence": [
              "During the long history of TCM, there has been a number of therapy records or treatment guidelines in the TCM classics composed by outstanding TCM researchers and practitioners. In real life, TCM practitioners often take these classical records for reference when prescribing for the patient, which inspires us to design a model that can automatically generate prescriptions by learning from these classics. It also needs to be noted that due to the issues in actual practice, the objective of this work is to generate candidate prescriptions to facilitate the prescribing procedure instead of substituting the human practitioners completely. An example of TCM prescription is shown in Table 1 . The herbs in the prescription are organized in a weak order. By “weak order”, we mean that the effect of the herbs are not influenced by the order. However, the order of the herbs reflects the way of thinking when constructing the prescription. Therefore, the herbs are connected to each other, and the most important ones are usually listed first."
            ],
            "highlighted_evidence": [
              "It also needs to be noted that due to the issues in actual practice, the objective of this work is to generate candidate prescriptions to facilitate the prescribing procedure instead of substituting the human practitioners completely."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/nlpcc/0101Y19",
    "dblp_title": "Exploration on Generating Traditional Chinese Medicine Prescriptions from Symptoms with an End-to-End Approach.",
    "year": "2019"
  },
  {
    "id": "1805.09055",
    "title": "Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter",
    "qas": [
      {
        "question": "How many languages are included in the tweets?",
        "question_id": "e78a47aec37d9a3bec5a18706b0a462c148c118b",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What languages are explored?",
        "question_id": "351510da69ab6879df5ff5c7c5f49a8a7aea4632",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Which countries did they look at?",
        "question_id": "d43e868cae91b3dc393c05c55da0754b0fb3a46a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/acl-peoples/VilaresG18",
    "dblp_title": "Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter.",
    "year": "2018"
  },
  {
    "id": "1804.03396",
    "title": "QA4IE: A Question Answering based Framework for Information Extraction",
    "qas": [
      {
        "question": "What QA models were used?",
        "question_id": "fd8b6723ad5f52770bec9009e45f860f4a8c4321",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.",
            "evidence": [
              "The input of our model are the words in the input text $x[1], ... , x[n]$ and query $q[1], ... , q[n]$ . We concatenate pre-trained word embeddings from GloVe BIBREF40 and character embeddings trained by CharCNN BIBREF41 to represent input words. The $2d$ -dimension embedding vectors of input text $x_1, ... , x_n$ and query $q_1, ... , q_n$ are then fed into a Highway Layer BIBREF42 to improve the capability of word embeddings and character embeddings as",
              "$$\\begin{split} g_t &= {\\rm sigmoid}(W_gx_t+b_g) \\\\ s_t &= {\\rm relu } (W_xx_t+b_x) \\\\ u_t &= g_t \\odot s_t + (1 - g_t) \\odot x_t~. \\end{split}$$ (Eq. 18)",
              "Here $W_g, W_x \\in \\mathbb {R}^{d \\times 2d}$ and $b_g, b_x \\in \\mathbb {R}^d$ are trainable weights, $u_t$ is a $d$ -dimension vector. The function relu is the rectified linear units BIBREF43 and $\\odot $ is element-wise multiply over two vectors. The same Highway Layer is applied to $q_t$ and produces $v_t$ .",
              "Next, $u_t$ and $v_t$ are fed into a Bi-Directional Long Short-Term Memory Network (BiLSTM) BIBREF44 respectively in order to model the temporal interactions between sequence words:",
              "Here we obtain $\\mathbf {U} = [u_1^{^{\\prime }}, ... , u_n^{^{\\prime }}] \\in \\mathbb {R}^{2d \\times n}$ and $\\mathbf {V} = [v_1^{^{\\prime }}, ... , v_m^{^{\\prime }}] \\in \\mathbb {R}^{2d \\times m}$ . Then we feed $\\mathbf {U}$ and $\\mathbf {V}$ into the attention flow layer BIBREF27 to model the interactions between the input text and query. We obtain the $8d$ -dimension query-aware context embedding vectors $h_1, ... , h_n$ as the result.",
              "After modeling interactions between the input text and queries, we need to enhance the interactions within the input text words themselves especially for the longer text in IE settings. Therefore, we introduce Self-Matching Layer BIBREF29 in our model as",
              "$$\\begin{split} o_t &= {\\rm BiLSTM}(o_{t-1}, [h_t, c_t]) \\\\ s_j^t &= w^T {\\rm tanh}(W_hh_j+\\tilde{W_h}h_t)\\\\ \\alpha _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\alpha _i^th_i ~. \\end{split}$$ (Eq. 20)",
              "Here $W_h, \\tilde{W_h} \\in \\mathbb {R}^{d \\times 8d}$ and $w \\in \\mathbb {R}^d$ are trainable weights, $[h, c]$ is vector concatenation across row. Besides, $\\alpha _i^t$ is the attention weight from the $t^{th}$ word to the $i^{th}$ word and $c_t$ is the enhanced contextual embeddings over the $t^{th}$ word in the input text. We obtain the $2d$ -dimension query-aware and self-enhanced embeddings of input text after this step. Finally we feed the embeddings $\\mathbf {O} = [o_1, ... , o_n]$ into a Pointer Network BIBREF39 to decode the answer sequence as",
              "$$\\begin{split} p_t &= {\\rm LSTM}(p_{t-1}, c_t) \\\\ s_j^t &= w^T {\\rm tanh}(W_oo_j+W_pp_{t-1})\\\\ \\beta _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\beta _i^to_i~. \\end{split}$$ (Eq. 21)",
              "Here $\\beta _{n+1}^t$ denotes the probability of generating the “ ${\\rm eos}$ ” symbol since the decoder also needs to determine when to stop. Therefore, the probability of generating the answer sequence $\\textbf {a}$ is as follows",
              "$${\\rm P}(\\textbf {a}|\\mathbf {O}) = \\prod _t {\\rm P}(a^t | a^1, ... , a^{t-1}, \\mathbf {O})~.$$ (Eq. 23)"
            ],
            "highlighted_evidence": [
              "The input of our model are the words in the input text $x[1], ... , x[n]$ and query $q[1], ... , q[n]$ . We concatenate pre-trained word embeddings from GloVe BIBREF40 and character embeddings trained by CharCNN BIBREF41 to represent input words. The $2d$ -dimension embedding vectors of input text $x_1, ... , x_n$ and query $q_1, ... , q_n$ are then fed into a Highway Layer BIBREF42 to improve the capability of word embeddings and character embeddings as\n\n$$\\begin{split} g_t &= {\\rm sigmoid}(W_gx_t+b_g) \\\\ s_t &= {\\rm relu } (W_xx_t+b_x) \\\\ u_t &= g_t \\odot s_t + (1 - g_t) \\odot x_t~. \\end{split}$$ (Eq. 18)",
              "The same Highway Layer is applied to $q_t$ and produces $v_t$ .",
              "Next, $u_t$ and $v_t$ are fed into a Bi-Directional Long Short-Term Memory Network (BiLSTM) BIBREF44 respectively in order to model the temporal interactions between sequence words:",
              "Then we feed $\\mathbf {U}$ and $\\mathbf {V}$ into the attention flow layer BIBREF27 to model the interactions between the input text and query.",
              "Therefore, we introduce Self-Matching Layer BIBREF29 in our model as\n\n$$\\begin{split} o_t &= {\\rm BiLSTM}(o_{t-1}, [h_t, c_t]) \\\\ s_j^t &= w^T {\\rm tanh}(W_hh_j+\\tilde{W_h}h_t)\\\\ \\alpha _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\alpha _i^th_i ~. \\end{split}$$ (Eq. 20)",
              "Finally we feed the embeddings $\\mathbf {O} = [o_1, ... , o_n]$ into a Pointer Network BIBREF39 to decode the answer sequence as\n\n$$\\begin{split} p_t &= {\\rm LSTM}(p_{t-1}, c_t) \\\\ s_j^t &= w^T {\\rm tanh}(W_oo_j+W_pp_{t-1})\\\\ \\beta _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\beta _i^to_i~. \\end{split}$$ (Eq. 21)",
              "Therefore, the probability of generating the answer sequence $\\textbf {a}$ is as follows\n\n$${\\rm P}(\\textbf {a}|\\mathbf {O}) = \\prod _t {\\rm P}(a^t | a^1, ... , a^{t-1}, \\mathbf {O})~.$$ (Eq. 23)"
            ]
          }
        ]
      },
      {
        "question": "Can this approach model n-ary relations?",
        "question_id": "4ce3a6632e7d86d29a42bd1fcf325114b3c11d46",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. Besides, processing longer documents and improving the quality of our benchmark are all challenging problems as we mentioned previously. We hope this work can provide new thoughts for the area of information extraction.",
              "The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. We ignore the adverbials and only consider the entity pairs and their relations as in standard RE settings. Note that we process the entire document as a whole instead of processing individual sentences separately as in previous systems. As shown in Figure 1 , our QA4IE framework consists of four key steps:"
            ],
            "highlighted_evidence": [
              "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper.",
              "The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation."
            ]
          }
        ]
      },
      {
        "question": "Was this benchmark automatically created from an existing dataset?",
        "question_id": "e7c0cdc05b48889905cc03215d1993ab94fb6eaa",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "As mentioned above, step 1, 2 and 4 in the QA4IE framework can be solved by existing work. Therefore, in this paper, we mainly focus on step 3. According to the recent progress in QA and MRC, deep neural networks are very good at solving this kind of problem with a large-scale dataset to train the network. However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark. Inspired by WikiReading BIBREF33 , a recent large-scale QA benchmark over Wikipedia, we find that the articles in Wikipedia together with the high quality triples in knowledge bases such as Wikidata BIBREF34 and DBpedia can form the supervision we need. Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.",
              "Incorporating DBpedia. Unlike WikiData, DBpedia is constructed automatically without human verification. Relations and properties in DBpedia are coarse and noisy. Thus we fix the existing 636 relation types in WikiData and build a projection from DBpedia relations to these 636 relation types. We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations. Then we gather all the DBpedia triples with the first entity is corresponding to one of the above 3.5M articles and the relation is one of the projected 148 relations. After the same clipping process as above and removing the repetitive triples, we obtain 394K additional triples in 302K existing Wikipedia articles."
            ],
            "highlighted_evidence": [
              "However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark.",
              "Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types.",
              "We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/semweb/QiuZQZLRRQTY18",
    "dblp_title": "QA4IE: A Question Answering Based Framework for Information Extraction.",
    "year": "2018"
  },
  {
    "id": "2004.02083",
    "title": "A Resource for Studying Chatino Verbal Morphology",
    "qas": [
      {
        "question": "How does morphological analysis differ from morphological inflection?",
        "question_id": "99760276cfd699e55b827ceeb653b31b043b9ceb",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Morphological analysis is the task of creating a morphosyntactic description for a given word",
              " inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply “morphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.",
              "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose."
            ],
            "highlighted_evidence": [
              "Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply “morphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.",
              "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11."
            ]
          }
        ]
      },
      {
        "question": "What was the criterion used for selecting the lemmata?",
        "question_id": "247e1fe052230458ce11b98e3637acf0b86795cd",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are the architectures used for the three tasks?",
        "question_id": "79cfd1b82c72d18e2279792c66a042c0e9dfa6b7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DyNet"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Morphological inflection has been thoroughly studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges BIBREF8, BIBREF9, BIBREF10, with the latest iteration focusing more on low-resource settings, utilizing cross-lingual transfer BIBREF11. We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
              "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose.",
              "Lemmatization is the task of retrieving the underlying lemma from which an inflected form was derived. Although in some languages the lemma is distinct from all forms, in SJQ Chatino the lemma is defined as the completive third-person singular form. As a computational task, lemmatization entails producing the lemma given an inflected form (and possibly, given a set of morphological tags describing the input form). Popular approaches tackle it as a character-level edit sequence generation task BIBREF15, or as a character-level sequence-to-sequence task BIBREF16. For our baseline lemmatization systems we follow the latter approach. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
            ],
            "highlighted_evidence": [
              "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
              "We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet.",
              "We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
            ]
          }
        ]
      },
      {
        "question": "Which language family does Chatino belong to?",
        "question_id": "9e1bf306658ef2972159643fdaf149c569db524b",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "the Otomanguean language family"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family. There are three main Chatino languages: Zenzontepec Chatino (ZEN, ISO 639-2 code czn), Tataltepec Chatino (TAT, cta), and Eastern Chatino (ISO 639-2 ctp, cya, ctz, and cly) (E.Cruz 2011 and Campbell 2011). San Juan Quiahije Chatino (SJQ), the language of the focus of this study, belongs to Eastern Chatino, and is used by about 3000 speakers."
            ],
            "highlighted_evidence": [
              "Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family. "
            ]
          }
        ]
      },
      {
        "question": "What system is used as baseline?",
        "question_id": "25b24ab1248f14a621686a57555189acc1afd49c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DyNet"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Morphological inflection has been thoroughly studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges BIBREF8, BIBREF9, BIBREF10, with the latest iteration focusing more on low-resource settings, utilizing cross-lingual transfer BIBREF11. We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
              "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose.",
              "Lemmatization is the task of retrieving the underlying lemma from which an inflected form was derived. Although in some languages the lemma is distinct from all forms, in SJQ Chatino the lemma is defined as the completive third-person singular form. As a computational task, lemmatization entails producing the lemma given an inflected form (and possibly, given a set of morphological tags describing the input form). Popular approaches tackle it as a character-level edit sequence generation task BIBREF15, or as a character-level sequence-to-sequence task BIBREF16. For our baseline lemmatization systems we follow the latter approach. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
            ],
            "highlighted_evidence": [
              "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
              "We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet.",
              "We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
            ]
          }
        ]
      },
      {
        "question": "How was annotation done?",
        "question_id": "8486e06c03f82ebd48c7cfbaffaa76e8b899eea5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              " hand-curated collection of complete inflection tables for 198 lemmata"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:",
              "Person: first (1), second (2), and third (3)",
              "Number: singular (SG) ad plural (PL)",
              "Inclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)",
              "Aspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB)."
            ],
            "highlighted_evidence": [
              "We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:\n\nPerson: first (1), second (2), and third (3)\n\nNumber: singular (SG) ad plural (PL)\n\nInclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)\n\nAspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB)."
            ]
          }
        ]
      },
      {
        "question": "How was the data collected?",
        "question_id": "27f575e90487ef68298cfb6452683bb977e39e43",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      }
    ],
    "dblp_id": "conf/lrec/CruzAS20",
    "dblp_title": "A Resource for Studying Chatino Verbal Morphology.",
    "year": "2020"
  },
  {
    "id": "1707.03764",
    "title": "N-GrAM: New Groningen Author-profiling Model",
    "qas": [
      {
        "question": "How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?",
        "question_id": "157b9f6f8fb5d370fa23df31de24ae7efb75d6f3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline",
            "evidence": [
              "FLOAT SELECTED: Table 8. Results (accuracy) on the test set for variety, gender and their joint prediction.",
              "For the final evaluation we submitted our system, N-GrAM, as described in Section 2. Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). For the global scores, all languages are combined. We present finer-grained scores showing the breakdown per language in Table TABREF24 . We compare our gender and variety accuracies against the LDR-baseline BIBREF10 , a low dimensionality representation especially tailored to language variety identification, provided by the organisers. The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline)."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 8. Results (accuracy) on the test set for variety, gender and their joint prediction.",
              "For the final evaluation we submitted our system, N-GrAM, as described in Section 2. Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). ",
              "We present finer-grained scores showing the breakdown per language in Table TABREF24 .",
              "The final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).\n\n"
            ]
          }
        ]
      },
      {
        "question": "On which task does do model do worst?",
        "question_id": "9bcc1df7ad103c7a21d69761c452ad3cd2951bda",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Gender prediction task",
            "evidence": [
              "N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task."
            ],
            "highlighted_evidence": [
              "N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task.\n\n"
            ]
          }
        ]
      },
      {
        "question": "On which task does do model do best?",
        "question_id": "8427988488b5ecdbe4b57b3813b3f981b07f53a5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Variety prediction task",
            "evidence": [
              "N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task."
            ],
            "highlighted_evidence": [
              "Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/clef/BasileDMRHN17",
    "dblp_title": "N-GrAM: New Groningen Author-profiling Model.",
    "year": "2017"
  },
  {
    "id": "2001.10179",
    "title": "Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device",
    "qas": [
      {
        "question": "Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed?",
        "question_id": "3604c4fba0a82d7139efd5ced47612c90bd10601",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "Does this implementation on CNN-DSA lead to diminishing of performance?",
        "question_id": "38e2f07ba965b676a99be06e8872dade7c04722a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is Super Character method modified to handle tabular data also?",
        "question_id": "931a2a13a1f6a8d9107d26811089bdccc39b0800",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "simply split the image into two parts. One for the text input, and the other for the tabular data"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "For multi-modal sentiment analysis, we can simply split the image into two parts. One for the text input, and the other for the tabular data. Such that both can be embedded into the Super Characters image. The CNN accelerator chip comes together with a Model Development Kit (MDK) for CNN model training, which feeds the two-dimensional Super Characters images into MDK and then obtain the fixed point model. Then, using the Software Development Kit (SDK) to load the model into the chip and send command to the CNN accelerator chip, such as to read an image, or to forward pass the image through the network to get the inference result. The advantage of using the CNN accelerator is low-power, it consumes only 300mw for an input of size 3x224x224 RGB image at the speed of 140fps. Compared with other models using GPU or FPGA, this solution implement the heavy-lifting DNN computations in the CNN accelerator chip, and the host computer is only responsible for memory read/write to generate the designed Super Character image. This has shown good result on system implementations for NLP applications BIBREF9."
            ],
            "highlighted_evidence": [
              "For multi-modal sentiment analysis, we can simply split the image into two parts. One for the text input, and the other for the tabular data."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/aaai/SunYSL20",
    "dblp_title": "Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device.",
    "year": "2020"
  },
  {
    "id": "1907.11907",
    "title": "Nefnir: A high accuracy lemmatizer for Icelandic",
    "qas": [
      {
        "question": "How are the substitution rules built?",
        "question_id": "8c981f8b992cb583e598f71741c322f522c6d2ad",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "from the Database of Modern Icelandic Inflection (DMII) BIBREF1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In this paper, we describe and evaluate Nefnir BIBREF0 , a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules derived (learned) from the Database of Modern Icelandic Inflection (DMII) BIBREF1 , which contains over 5.8 million inflectional forms."
            ],
            "highlighted_evidence": [
              "In this paper, we describe and evaluate Nefnir BIBREF0 , a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules derived (learned) from the Database of Modern Icelandic Inflection (DMII) BIBREF1 , which contains over 5.8 million inflectional forms."
            ]
          }
        ]
      },
      {
        "question": "Which dataset do they use?",
        "question_id": "16f33de90b76975a99572e0684632d5aedbd957c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "a reference corpus of 21,093 tokens and their correct lemmas"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We have evaluated the output of Nefnir against a reference corpus of 21,093 tokens and their correct lemmas.",
              "Samples for the reference corpus were extracted from two larger corpora, in order to obtain a diverse vocabulary:"
            ],
            "highlighted_evidence": [
              "We have evaluated the output of Nefnir against a reference corpus of 21,093 tokens and their correct lemmas.\n\nSamples for the reference corpus were extracted from two larger corpora, in order to obtain a diverse vocabulary:"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/nodalida/IngolfsdottirLD19",
    "dblp_title": "Nefnir: A high accuracy lemmatizer for Icelandic.",
    "year": "2019"
  },
  {
    "id": "1911.03842",
    "title": "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation",
    "qas": [
      {
        "question": "What baseline is used to compare the experimental results against?",
        "question_id": "d0b005cb7ed6d4c307745096b2ed8762612480d2",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Transformer generation model"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous. While ALL has more data than CDA and CT, more data alone is not enough — the Positive-Bias Data Collection model does not achieve as good results. Both the CT and ALL models benefit from knowing the data split ($\\text{F}^{0}\\text{M}^{0}$, for example), and both models yield a genderedness ratio closest to ground truth."
            ],
            "highlighted_evidence": [
              "Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous."
            ]
          }
        ]
      },
      {
        "question": "How does counterfactual data augmentation aim to tackle bias?",
        "question_id": "9d9b11f86a96c6d3dd862453bf240d6e018e75af",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "The training dataset is augmented by swapping all gendered words by their other gender counterparts",
            "evidence": [
              "One of the solutions that has been proposed for mitigating gender bias on the word embedding level is Counterfactual Data Augmentation (CDA) BIBREF25. We apply this method by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by BIBREF21. For example, all instances of grandmother are swapped with grandfather."
            ],
            "highlighted_evidence": [
              "One of the solutions that has been proposed for mitigating gender bias on the word embedding level is Counterfactual Data Augmentation (CDA) BIBREF25. We apply this method by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by BIBREF21."
            ]
          }
        ]
      },
      {
        "question": "In the targeted data collection approach, what type of data is targetted?",
        "question_id": "415f35adb0ef746883fb9c33aa53b79cc4e723c3",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Gendered characters in the dataset",
            "evidence": [
              "There are a larger number of male-gendered character personas than female-gendered character personas (see Section SECREF2), so we balance existing personas using gender-swapping. For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns. Additionally, we ask annotators to swap the gender of any characters that are referred to in the persona text for a given character."
            ],
            "highlighted_evidence": [
              "For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/emnlp/DinanFWUKW20",
    "dblp_title": "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation.",
    "year": "2020"
  },
  {
    "id": "1707.02377",
    "title": "Efficient Vector Representation for Documents through Corruption",
    "qas": [
      {
        "question": "Which language models do they compare against?",
        "question_id": "52f1a91f546b8a25a5d72325c503ec8f9c72de23",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "RNNLM BIBREF11"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) BIBREF14 , a representation learned from reconstructing original document INLINEFORM0 using corrupted one INLINEFORM1 . SDAs have been shown to be the state-of-the-art for sentiment analysis tasks BIBREF29 . We used Kullback-Liebler divergence as the reconstruction error and an affine encoder. To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of INLINEFORM2 in the reconstruction error and employed negative sampling for the remainings; Word2Vec BIBREF1 +IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec BIBREF2 ; Skip-thought Vectors BIBREF16 , a generic, distributed sentence encoder that extends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic sentence representations that apply to various natural language processing tasks. We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison. In the semantic relatedness task, we further compare to LSTM-based methods BIBREF18 that have been reported on this dataset."
            ],
            "highlighted_evidence": [
              "We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison."
            ]
          }
        ]
      },
      {
        "question": "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
        "question_id": "bb5697cf352dd608edf119ca9b82a6b7e51c8d21",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Similar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer as well as an output layer to predict the target word, “ceremony” in this example. The embeddings of neighboring words (“opening”, “for”, “the”) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (“performance” at position INLINEFORM0 , “praised” at position INLINEFORM1 , and “brazil” at position INLINEFORM2 ). BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. This corruption mechanism offers us great speedup during training as it significantly reduces the number of parameters to update in back propagation. At the same time, as we are going to detail in the next section, it introduces a special form of regularization, which brings great performance improvement."
            ],
            "highlighted_evidence": [
              "BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained."
            ]
          }
        ]
      },
      {
        "question": "How do they determine which words are informative?",
        "question_id": "98785bf06e60fcf0a6fe8921edab6190d0c2cec1",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Informative are those that will not be suppressed by regularization performed.",
            "evidence": [
              "Data dependent regularization. As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100 in this experiment. Table TABREF24 lists the words having the smallest INLINEFORM0 norm of embeddings found by different algorithms. The number inside the parenthesis after each word is the number of times this word appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have embeddings that are close to zero, despite some of them being indicative of sentiment such as debacle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words."
            ],
            "highlighted_evidence": [
              "As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words.",
              "In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iclr/Chen17",
    "dblp_title": "Efficient Vector Representation for Documents through Corruption.",
    "year": "2017"
  },
  {
    "id": "1911.06191",
    "title": "Microsoft Research Asia's Systems for WMT19",
    "qas": [
      {
        "question": "What is their best performance on the largest language direction dataset?",
        "question_id": "9846f84747b89f5c692665c4ea7111671ad9839a",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How does soft contextual data augmentation work?",
        "question_id": "eecf62e18a790bcfdd8a56f0c4f498927ff2fb47",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words",
              "replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is relatively limited. SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary. It was applied in Russian$\\rightarrow $English translation in our submitted systems."
            ],
            "highlighted_evidence": [
              "SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary."
            ]
          }
        ]
      },
      {
        "question": "How does muli-agent dual learning work?",
        "question_id": "acda028a21a465c984036dcbb124b7f03c490b41",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The core idea of dual learning is to leverage the duality between the primal task (mapping from domain $\\mathcal {X}$ to domain $\\mathcal {Y}$) and dual task (mapping from domain $\\mathcal {Y}$ to $\\mathcal {X}$ ) to boost the performances of both tasks. MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models. It was integrated into our submitted systems for German$\\leftrightarrow $English and German$\\leftrightarrow $French translations."
            ],
            "highlighted_evidence": [
              "The core idea of dual learning is to leverage the duality between the primal task (mapping from domain $\\mathcal {X}$ to domain $\\mathcal {Y}$) and dual task (mapping from domain $\\mathcal {Y}$ to $\\mathcal {X}$ ) to boost the performances of both tasks. MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models."
            ]
          }
        ]
      },
      {
        "question": "Which language directions are machine translation systems of WMT evaluated on?",
        "question_id": "42af0472e6895eaf7b9392674b0d956e64e86b03",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English",
              "Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We participated in the WMT19 shared news translation task in 11 translation directions. We achieved first place for 8 directions: German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, and three other directions were placed second (ranked by teams), which included Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh."
            ],
            "highlighted_evidence": [
              "We achieved first place for 8 directions: German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, and three other directions were placed second (ranked by teams), which included Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/wmt/XiaTTGHCFGLLWWZ19",
    "dblp_title": "Microsoft Research Asia&apos;s Systems for WMT19.",
    "year": "2019"
  },
  {
    "id": "1701.06538",
    "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
    "qas": [
      {
        "question": "Approximately how much computational cost is saved by using this model?",
        "question_id": "a85698f19a91ecd3cd3a90a93a453d2acebae1b7",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What improvement does the MOE model make over the SOTA on machine translation?",
        "question_id": "af073d84b8a7c968e5822c79bef34a28655886de",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3",
              "perplexity scores are also better",
              "On the Google Production dataset, our model achieved 1.01 higher test BLEU score"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Tables TABREF42 , TABREF43 , and TABREF44 show the results of our largest models, compared with published results. Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT'14 En INLINEFORM0 Fr and En INLINEFORM1 De benchmarks. As our models did not use RL refinement, these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3 . The perplexity scores are also better. On the Google Production dataset, our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time."
            ],
            "highlighted_evidence": [
              "As our models did not use RL refinement, these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3 . The perplexity scores are also better. On the Google Production dataset, our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time."
            ]
          }
        ]
      },
      {
        "question": "What improvement does the MOE model make over the SOTA on language modelling?",
        "question_id": "e8fcfb1412c3b30da6cbc0766152b6e11e17196c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Perpexity is improved from 34.7 to 28.0.",
            "evidence": [
              "The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 .",
              "In addition to the largest model from the previous section, we trained two more MoE models with similarly high capacity (4 billion parameters), but higher computation budgets. These models had larger LSTMs, and fewer but larger and experts. Details can be found in Appendix UID77 . Results of these three models form the bottom line of Figure FIGREF32 -right. Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation.",
              "FLOAT SELECTED: Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets, vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C."
            ],
            "highlighted_evidence": [
              "The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 .",
              " Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation.",
              "FLOAT SELECTED: Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets, vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C."
            ]
          }
        ]
      },
      {
        "question": "How is the correct number of experts to use decided?",
        "question_id": "0cd90e5b79ea426ada0203177c28812a7fc86be5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "varied the number of experts between models"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Each expert in the MoE layer is a feed forward network with one ReLU-activated hidden layer of size 1024 and an output layer of size 512. Thus, each expert contains INLINEFORM0 parameters. The output of the MoE layer is passed through a sigmoid function before dropout. We varied the number of experts between models, using ordinary MoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts. We call the resulting models MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h and MoE-4096-h. For the hierarchical MoE layers, the first level branching factor was 16, corresponding to the number of GPUs in our cluster. We use Noisy-Top-K Gating (see Section UID14 ) with INLINEFORM1 for the ordinary MoE layers and INLINEFORM2 at each level of the hierarchical MoE layers. Thus, each example is processed by exactly 4 experts for a total of 4M ops/timestep. The two LSTM layers contribute 2M ops/timestep each for the desired total of 8M."
            ],
            "highlighted_evidence": [
              "We varied the number of experts between models, using ordinary MoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts."
            ]
          }
        ]
      },
      {
        "question": "What equations are used for the trainable gating network?",
        "question_id": "f01a88e15ef518a68d8ca2bec992f27e7a3a6add",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "DISPLAYFORM0",
              "DISPLAYFORM0 DISPLAYFORM1"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "A simple choice of non-sparse gating function BIBREF17 is to multiply the input by a trainable weight matrix INLINEFORM0 and then apply the INLINEFORM1 function. DISPLAYFORM0",
              "We add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to INLINEFORM0 (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix SECREF51 . The amount of noise per component is controlled by a second trainable weight matrix INLINEFORM1 . DISPLAYFORM0 DISPLAYFORM1"
            ],
            "highlighted_evidence": [
              "A simple choice of non-sparse gating function BIBREF17 is to multiply the input by a trainable weight matrix INLINEFORM0 and then apply the INLINEFORM1 function. DISPLAYFORM0\n\nWe add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to INLINEFORM0 (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix SECREF51 . The amount of noise per component is controlled by a second trainable weight matrix INLINEFORM1 . DISPLAYFORM0 DISPLAYFORM1",
              "A simple choice of non-sparse gating function BIBREF17 is to multiply the input by a trainable weight matrix INLINEFORM0 and then apply the INLINEFORM1 function. DISPLAYFORM0\n\nWe add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to INLINEFORM0 (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix SECREF51 . The amount of noise per component is controlled by a second trainable weight matrix INLINEFORM1 . DISPLAYFORM0 DISPLAYFORM1"
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iclr/ShazeerMMDLHD17",
    "dblp_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.",
    "year": "2017"
  },
  {
    "id": "1905.10810",
    "title": "Evaluation of basic modules for isolated spelling error correction in Polish texts",
    "qas": [
      {
        "question": "What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?",
        "question_id": "44104668796a6ca10e2ea3ecf706541da1cec2cf",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.",
            "evidence": [
              "The experimental results are presented in Table TABREF4 . Diacritic swapping showed a remarkably poor performance, despite promising mentions in existing literature. This might be explained by the already mentioned feature of Wikipedia edits, which can be expected to be to some degree self-reviewed before submission. This can very well limit the number of most trivial mistakes.",
              "FLOAT SELECTED: Table 1: Test results for all the methods used. The loss measure is cross-entropy."
            ],
            "highlighted_evidence": [
              "The experimental results are presented in Table TABREF4 .",
              "FLOAT SELECTED: Table 1: Test results for all the methods used. The loss measure is cross-entropy."
            ]
          }
        ]
      },
      {
        "question": "What solutions are proposed for error detection and context awareness?",
        "question_id": "bbcd77aac74989f820e84488c52f3767d0405d51",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How is PIEWi annotated?",
        "question_id": "6a31bd676054222faf46229fc1d283322478a020",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "[error, correction] pairs"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "PlEWi BIBREF20 is an early version of WikEd BIBREF21 error corpus, containing error type annotations allowing us to select only non-word errors for evaluation. Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. The corpus contains data extracted from histories of page versions of Polish Wikipedia. An algorithm designed by the corpus author determined where the changes were correcting spelling errors, as opposed to expanding content and disagreements among Wikipedia editors."
            ],
            "highlighted_evidence": [
              "Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique."
            ]
          }
        ]
      },
      {
        "question": "What methods are tested in PIEWi?",
        "question_id": "e4d16050f0b457c93e590261732a20401def9cde",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Levenshtein distance metric BIBREF8",
              "diacritical swapping",
              "Levenshtein distance is used in a weighted sum to cosine distance between word vectors",
              "ELMo-augmented LSTM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The methods that we evaluated are baselines are the ones we consider to be basic and with moderate potential of yielding particularly good results. Probably the most straightforward approach to error correction is selecting known words from a dictionary that are within the smallest edit distance from the error. We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . It is a version of edit distance that treats deletions, insertions and replacements as adding one unit distance, without giving a special treatment to character swaps. The SGJP – Grammatical Dictionary of Polish BIBREF10 was used as the reference vocabulary.",
              "Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . Namely, from the incorrect form we try to produce all strings obtainable by either adding or removing diacritical marks from characters. We then exclude options that are not present in SGJP, and select as the correction the one within the smallest edit distance from the error. It is possible for the number of such diacritically-swapped options to become very big. For example, the token Modlin-Zegrze-Pultusk-Różan-Ostrołęka-Łomża-Osowiec (taken from PlEWi corpus of spelling errors, see below) can yield over INLINEFORM0 states with this method, such as Módłiń-Żęgrzę-Pułtuśk-Roźąń-Óśtróleką-Lómzą-Óśówięć. The actual correction here is just fixing the ł in Pułtusk. Hence we only try to correct in this way tokens that are shorter than 17 characters.",
              "A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. This is based on the observation that trained vectors models of distributional semantics contain also representations of spelling errors, if they were not pruned. Their representations tend to be similar to those of their correct counterparts. For example, the token enginir will appear in similar contexts as engineer, and therefore will be assigned a similar vector embedding.",
              "(applied cellwise) in order to obtain the initial setting of parameters for the main LSTM. Our ELMo-augmented LSTM is bidirectional."
            ],
            "highlighted_evidence": [
              "We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 .",
              "Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 .",
              "A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors.",
              "Our ELMo-augmented LSTM is bidirectional."
            ]
          }
        ]
      },
      {
        "question": "Which specific error correction solutions have been proposed for specialized corpora in the past?",
        "question_id": "b25e7137f49f77e7e67ee2f40ca585d3a377f8b5",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "spellchecking mammography reports and tweets BIBREF7 , BIBREF4"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein distance solutions were used for cleaning mainframe inputs BIBREF5 , BIBREF6 . Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 ."
            ],
            "highlighted_evidence": [
              "Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/ltconf/Rutkowski19",
    "dblp_title": "Evaluation of Basic Modules for Isolated Spelling Error Correction in Polish Texts.",
    "year": "2019"
  },
  {
    "id": "2002.12328",
    "title": "Few-shot Natural Language Generation for Task-Oriented Dialog",
    "qas": [
      {
        "question": "What was the criteria for human evaluation?",
        "question_id": "d803b782023553bbf9b36551fbc888ad189b1f29",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges."
            ],
            "highlighted_evidence": [
              "We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges."
            ]
          }
        ]
      },
      {
        "question": "What automatic metrics are used to measure performance of the system?",
        "question_id": "fc5f9604c74c9bb804064f315676520937131e17",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BLEU scores and the slot error rate (ERR)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Following BIBREF3, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. $\\text{ERR}=(p+q)/M$, where $M$ is the total number of slots in the dialog act, and $p$, $q$ is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output."
            ],
            "highlighted_evidence": [
              "Following BIBREF3, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. $\\text{ERR}=(p+q)/M$, where $M$ is the total number of slots in the dialog act, and $p$, $q$ is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output."
            ]
          }
        ]
      },
      {
        "question": "What existing methods is SC-GPT compared to?",
        "question_id": "b37fd665dfa5fad43977069d5623f4490a979305",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$({1})$ SC-LSTM BIBREF3",
              "$({2})$ GPT-2 BIBREF6 ",
              "$({3})$ HDSA BIBREF7"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We compare with three baseline methods. $({1})$ SC-LSTM BIBREF3 is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. $({2})$ GPT-2 BIBREF6 is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. $({3})$ HDSA BIBREF7 is a state-of-the-art model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting, showing superior performance than SC-LSTM."
            ],
            "highlighted_evidence": [
              "We compare with three baseline methods. $({1})$ SC-LSTM BIBREF3 is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. $({2})$ GPT-2 BIBREF6 is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. $({3})$ HDSA BIBREF7 is a state-of-the-art model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting, showing superior performance than SC-LSTM."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/acl/WangZFAH23",
    "dblp_title": "DSPM-NLG: A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System.",
    "year": "2023"
  },
  {
    "id": "1910.07481",
    "title": "Using Whole Document Context in Neural Machine Translation",
    "qas": [
      {
        "question": "Which language-pair had the better performance?",
        "question_id": "c1f4d632da78714308dc502fe4e7b16ea6f76f81",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "French-English",
            "evidence": [
              "FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001."
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001."
            ]
          }
        ]
      },
      {
        "question": "Which datasets were used in the experiment?",
        "question_id": "749a307c3736c5b06d7b605dc228d80de36cbabe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "WMT 2019 parallel dataset",
              "a restricted dataset containing the full TED corpus from MUST-C BIBREF10",
              "sampled sentences from WMT 2019 dataset"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities."
            ],
            "highlighted_evidence": [
              "For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. "
            ]
          }
        ]
      },
      {
        "question": "What evaluation metrics did they use?",
        "question_id": "102de97c123bb1e247efec0f1d958f8a3a86e2f6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "BLEU and TER scores"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment."
            ],
            "highlighted_evidence": [
              "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/iwslt/MaceS19",
    "dblp_title": "Using Whole Document Context in Neural Machine Translation.",
    "year": "2019"
  },
  {
    "id": "1610.09516",
    "title": "Finding Street Gang Members on Twitter",
    "qas": [
      {
        "question": "Do they evaluate only on English datasets?",
        "question_id": "3460393d6888dd34113fa0813a1b3a1514c66aa6",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What are the differences in the use of emojis between gang member and the rest of the Twitter population?",
        "question_id": "d491ee69db39ec65f0f6da9ec03450520389699a",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members",
              "only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them",
              "gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Motivated by recent work involving the use of emojis by gang members BIBREF22 , we also studied if and how gang and non-gang members use emoji symbols in their tweets. Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets. The frequency of each emoji symbol used across the set of user's tweets are thus considered as features for our classifier."
            ],
            "highlighted_evidence": [
              "Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets."
            ]
          }
        ]
      },
      {
        "question": "What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?",
        "question_id": "d3839c7acee4f9c8db0a4a475214a8dcbd0bc26f",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "It has been recognized that music is a key cultural component in an urban lifestyle and that gang members often want to emulate the scenarios and activities the music conveys BIBREF7 . Our analysis confirms that the influence of gangster rap is expressed in gang members' Twitter posts. We found that 51.25% of the gang members collected have a tweet that links to a YouTube video. Following these links, a simple keyword search for the terms gangsta and hip-hop in the YouTube video description found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre. Moreover, this high proportion is not driven by a small number of profiles that prolifically share YouTube links; eight YouTube links are shared on average by a gang member."
            ],
            "highlighted_evidence": [
              "We found that 51.25% of the gang members collected have a tweet that links to a YouTube video. Following these links, a simple keyword search for the terms gangsta and hip-hop in the YouTube video description found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre."
            ]
          }
        ]
      },
      {
        "question": "What are the differences in the use of images between gang member and the rest of the Twitter population?",
        "question_id": "a6d00f44ff8f83b6c1787e39333e759b0c3daf15",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash. Descriptions of these images may thus empower our classifier. Thus, we translated profile images into features with the Clarifai web service. Clarifai offers a free API to query a deep learning system that tags images with a set of scored keywords that reflect what is seen in the image. We tagged the profile image and cover image for each profile using 20 tags identified by Clarifai. Figure FIGREF36 offers the 20 most often used tags applied to gang and non-gang member profiles. Since we take all the tags returned for an image, we see common words such as people and adult coming up in the top 20 tag set. However, gang member profile images were assigned unique tags such as trigger, bullet, worship while non-gang images were uniquely tagged with beach, seashore, dawn, wildlife, sand, pet. The set of tags returned by Clarifai were thus considered as features for the classifier."
            ],
            "highlighted_evidence": [
              "In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash."
            ]
          }
        ]
      },
      {
        "question": "What are the differences in language use between gang member and the rest of the Twitter population?",
        "question_id": "0d4aa05eb00d9dee74000ea5b21b08f693ba1e62",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word",
              "gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Figure FIGREF14 summarizes the words seen most often in the gang and non-gang members' tweets as clouds. They show a clear difference in language. For example, we note that gang members more frequently use curse words in comparison to ordinary users. Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, which is nearly five times more than the average curse word usage on Twitter. The clouds also reflect the fact that gang members often talk about drugs and money with terms such as smoke, high, hit, and money, while ordinary users hardly speak about finances and drugs. We also noticed that gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us. These differences make it clear that the individual words used by gang and non-gang members will be relevant features for gang profile classification."
            ],
            "highlighted_evidence": [
              "Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, which is nearly five times more than the average curse word usage on Twitter.",
              "The clouds also reflect the fact that gang members often talk about drugs and money with terms such as smoke, high, hit, and money, while ordinary users hardly speak about finances and drugs. We also noticed that gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us."
            ]
          }
        ]
      },
      {
        "question": "How is gang membership verified?",
        "question_id": "382bef47d316d7c12ea190ae160bf0912a0f4ffe",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Manual verification"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "3. Manual verification of Twitter profiles: We verified each profile discovered manually by examining the profile picture, profile background image, recent tweets, and recent pictures posted by a user. During these checks, we searched for terms, activities, and symbols that we believed could be associated with a gang. For example, profiles whose image or background included guns in a threatening way, stacks of money, showing gang hand signs and gestures, and humans holding or posing with a gun, appeared likely to be from a gang member. Such images were often identified in profiles of users who submitted tweets that contain messages of support or sadness for prisoners or recently fallen gang members, or used a high volume of threatening and intimidating slang language. Only profiles where the images, words, and tweets all suggested gang affiliation were labeled as gang affiliates and added to our dataset. Although this manual verification does have a degree of subjectivity, in practice, the images and words used by gang members on social media are so pronounced that we believe any reasonable analyst would agree that they are gang members. We found that not all the profiles collected belonged to gang members; we observed relatives and followers of gang members posting the same hashtags as in Step 1 to convey similar feelings in their profile descriptions."
            ],
            "highlighted_evidence": [
              "Manual verification of Twitter profiles: We verified each profile discovered manually by examining the profile picture, profile background image, recent tweets, and recent pictures posted by a user."
            ]
          }
        ]
      },
      {
        "question": "Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?",
        "question_id": "32a232310babb92991c4b1b75f7aa6b4670ec447",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": false,
            "free_form_answer": "",
            "evidence": [
              "Street gang members have established online presences coinciding with their physical occupation of neighborhoods. The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . Gang members typically use social networking sites and social media to develop online respect for their street gang BIBREF5 and to post intimidating, threatening images or videos BIBREF6 . This “Cyber-” or “Internet banging” BIBREF7 behavior is precipitated by the fact that an increasing number of young members of the society are joining gangs BIBREF8 , and these young members have become enamored with technology and with the notion of sharing information quickly and publicly through social media. Stronger police surveillance in the physical spaces where gangs congregate further encourages gang members to seek out virtual spaces such as social media to express their affiliation, to sell drugs, and to celebrate their illegal activities BIBREF9 ."
            ],
            "highlighted_evidence": [
              "The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 ."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/asunam/BalasuriyaWDS16",
    "dblp_title": "Finding street gang members on Twitter.",
    "year": "2016"
  },
  {
    "id": "2001.05493",
    "title": "A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts",
    "qas": [
      {
        "question": "What is English mixed with in the TRAC dataset?",
        "question_id": "5845d1db7f819dbadb72e7df69d49c3f424b5730",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Hindi"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "In future, we will explore other methods to increase the understanding of deep learning models on group targeted text, although the categories are well defined we will look after if we further fine-tune the categories with more data. In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).",
              "The block diagram of the proposed system is shown in Figure FIGREF22. The proposed system does not use any data augmentation techniques like BIBREF14, which is the top performer in TRAC (in English code-mixed Facebook data). This means the performance achieved by our system totally depends on the training dataset provided by TRAC. This also proves the effectiveness of our approach. Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set. The remaining part of this paper is organized as follows: Section SECREF2 is an overview of related work. Section SECREF3 presents the methodology and algorithmic details. Section SECREF4 discusses the experimental evaluation of the system, and Section SECREF5 concludes this paper.",
              "The fine-grained definition of the aggressiveness/aggression identification is provided by the organizers of TRAC-2018 BIBREF0, BIBREF2. They have classified the aggressiveness into three labels (Overtly aggressive(OAG), Covertly aggressive(CAG), Non-aggressive(NAG)). The detailed description for each of the three labels is described as follows:",
              "Overtly Aggressive(OAG) - This type of aggression shows direct verbal attack pointing to the particular individual or group. For example, \"Well said sonu..you have courage to stand against dadagiri of Muslims\".",
              "Covertly Aggressive(CAG) - This type of aggression the attack is not direct but hidden, subtle and more indirect while being stated politely most of the times. For example, \"Dear India, stop playing with the emotions of your people for votes.\"",
              "Non-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive."
            ],
            "highlighted_evidence": [
              " In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources).",
              "Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set.",
              "The fine-grained definition of the aggressiveness/aggression identification is provided by the organizers of TRAC-2018 BIBREF0, BIBREF2. They have classified the aggressiveness into three labels (Overtly aggressive(OAG), Covertly aggressive(CAG), Non-aggressive(NAG)). The detailed description for each of the three labels is described as follows:\n\nOvertly Aggressive(OAG) - This type of aggression shows direct verbal attack pointing to the particular individual or group. For example, \"Well said sonu..you have courage to stand against dadagiri of Muslims\".\n\nCovertly Aggressive(CAG) - This type of aggression the attack is not direct but hidden, subtle and more indirect while being stated politely most of the times. For example, \"Dear India, stop playing with the emotions of your people for votes.\"\n\nNon-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive."
            ]
          }
        ]
      },
      {
        "question": "Which psycholinguistic and basic linguistic features are used?",
        "question_id": "e829f008d62312357e0354a9ed3b0827c91c9401",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features",
            "evidence": [
              "Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper.",
              "We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.).",
              "FLOAT SELECTED: Table 1: Details of NLP features"
            ],
            "highlighted_evidence": [
              "Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper.",
              "We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.).",
              "FLOAT SELECTED: Table 1: Details of NLP features"
            ]
          }
        ]
      },
      {
        "question": "How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?",
        "question_id": "54fe8f05595f2d1d4a4fd77f4562eac519711fa6",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "Systems do not perform well both in Facebook and Twitter texts",
            "evidence": [
              "Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. So, we concentrated to develop a robust system for English code-mixed texts, and uni-lingual texts, which can also handle different writing styles. Our approach is based on three main ideas:"
            ],
            "highlighted_evidence": [
              "Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets."
            ]
          }
        ]
      },
      {
        "question": "What are the key differences in communication styles between Twitter and Facebook?",
        "question_id": "61404466cf86a21f0c1783ce535eb39a01528ce8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?",
        "question_id": "fbe5e513745d723aad711ceb91ce0c3c2ceb669e",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "None",
            "evidence": [
              "The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset."
            ],
            "highlighted_evidence": [
              "The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/comad/KhandelwalK20",
    "dblp_title": "A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts.",
    "year": "2020"
  },
  {
    "id": "1908.09951",
    "title": "An Emotional Analysis of False Information in Social Media and News Articles",
    "qas": [
      {
        "question": "What is the baseline?",
        "question_id": "1571e16063b53409f2d1bd6ec143fccc5b29ebb9",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "Majority Class baseline (MC) ",
              "Random selection baseline (RAN)"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Emotions have been used in many natural language processing tasks and they showed their efficiency BIBREF35. We aim at investigating their efficiency to detect false information. In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN)."
            ],
            "highlighted_evidence": [
              " In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN)."
            ]
          }
        ]
      },
      {
        "question": "What datasets did they use?",
        "question_id": "d71937fa5da853f7529f767730547ccfb70e5908",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "News Articles",
              "Twitter"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Evaluation Framework ::: Datasets ::: News Articles",
              "Our dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites. These news articles include satires, hoaxes, and propagandas but not clickbaits. Since we are interested also in analyzing clickbaits, we slice a sample from an available clickbait dataset BIBREF33 that was originally collected from two sources: Wikinews articles' headlines and other online sites that are known to publish clickbaits. The satire, hoax, and propaganda news articles are considerably long (some of them reach the length of 5,000 words). This length could affect the quality of the analysis as we mentioned before. We focus on analyzing the initial part of the article. Our intuition is that it is where emotion-bearing words will be more frequent. Therefore, we shorten long news articles into a maximum length of N words (N=300). We choose the value of N based on the length of the shortest articles. Moreover, we process the dataset by removing very short articles, redundant articles or articles that do not have a textual content.",
              "With the complicated political and economic situations in many countries, some agendas are publishing suspicious news to affect public opinions regarding specific issues BIBREF0. The spreading of this phenomenon is increasing recently with the large usage of social media and online news sources. Many anonymous accounts in social media platforms start to appear, as well as new online news agencies without presenting a clear identity of the owner. Twitter has recently detected a campaign organized by agencies from two different countries to affect the results of the last U.S. presidential elections of 2016. The initial disclosures by Twitter have included 3,841 accounts. A similar attempt was done by Facebook, as they detected coordinated efforts to influence U.S. politics ahead of the 2018 midterm elections.",
              "For this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. For the real news, we merge this list with another 32 Twitter accounts from BIBREF34. In this work we could not use the previous dataset and we decide to collect tweets again. For each of these accounts, we collected the last M tweets posted (M=1000). By investigating these accounts manually, we found that many tweets just contain links without textual news. Therefore, to ensure of the quality of the crawled data, we chose a high value for M (also to have enough data). After the collecting process, we processed these tweets by removing duplicated, very short tweets, and tweets without textual content. Table TABREF35 shows a summary for both datasets."
            ],
            "highlighted_evidence": [
              " News Articles\nOur dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites.",
              "Twitter\nFor this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. "
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/toit/GhanemRP20",
    "dblp_title": "An Emotional Analysis of False Information in Social Media and News Articles.",
    "year": "2020"
  },
  {
    "id": "1606.08140",
    "title": "STransE: a novel embedding model of entities and relationships in knowledge bases",
    "qas": [
      {
        "question": "What scoring function does the model use to score triples?",
        "question_id": "8d258899e36326183899ebc67aeb4188a86f682c",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Let $\\mathcal {E}$ denote the set of entities and $\\mathcal {R}$ the set of relation types. For each triple $(h, r, t)$ , where $h, t \\in \\mathcal {E}$ and $r \\in \\mathcal {R}$ , the STransE model defines a score function $f_r(h, t)$ of its implausibility. Our goal is to choose $f$ such that the score $f_r(h,t)$ of a plausible triple $(h,r,t)$ is smaller than the score $f_{r^{\\prime }}(h^{\\prime },t^{\\prime })$ of an implausible triple $\\mathcal {R}$0 . We define the STransE score function $\\mathcal {R}$1 as follows:",
              "$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $",
              "using either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results). To learn the vectors and matrices we minimize the following margin-based objective function: $ \\mathcal {L} & = & \\sum _{\\begin{array}{c}(h,r,t) \\in \\mathcal {G} \\\\ (h^{\\prime },r,t^{\\prime }) \\in \\mathcal {G}^{\\prime }_{(h, r, t)}\\end{array}} [\\gamma + f_r(h, t) - f_r(h^{\\prime }, t^{\\prime })]_+ $"
            ],
            "highlighted_evidence": [
              "We define the STransE score function $\\mathcal {R}$1 as follows:\n\n$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $\n\nusing either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results)."
            ]
          }
        ]
      },
      {
        "question": "What datasets are used to evaluate the model?",
        "question_id": "955ca31999309685c1daa5cb03867971ca99ec52",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "WN18, FB15k",
            "evidence": [
              "As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. We expect that the STransE will also be able to serve as the basis for extended models that exploit a wider variety of information sources, just as TransE does."
            ],
            "highlighted_evidence": [
              "As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/naacl/NguyenSQJ16",
    "dblp_title": "STransE: a novel embedding model of entities and relationships in knowledge bases.",
    "year": "2016"
  },
  {
    "id": "1911.11698",
    "title": "Doc2Vec on the PubMed corpus: study of a new approach to generate related articles",
    "qas": [
      {
        "question": "How long it took for each Doc2Vec model to be trained?",
        "question_id": "9b2b063e8a9938da195c9c0d6caa3e37a4a615a8",
        "answers": [
          {
            "unanswerable": true,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [],
            "highlighted_evidence": []
          }
        ]
      },
      {
        "question": "How better are results for pmra algorithm  than Doc2Vec in human evaluation? ",
        "question_id": "ac3c88ace59bf75788370062db139f60499c2056",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents."
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Regarding the evaluation itself, based on the three-modality scale (bad, partial or full relevance), models are clearly not equivalents (Figure FIGREF26). The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents. By looking at the results ranking, the mean position for D2V was 14.09 (ranging from 13.98 for JPL to 14.20 for EL). Regarding the pmra, this average position was equal to 6.89 (ranging from 6.47 for EL to 7.23 for SJD)."
            ],
            "highlighted_evidence": [
              "The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents. "
            ]
          }
        ]
      },
      {
        "question": "What Doc2Vec architectures other than PV-DBOW have been tried?",
        "question_id": "26012f57cba21ba44b9a9f7ed8b1ed9e8ee7625d",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "PV-DM"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). The hs option defines whether hierarchical softmax or negative sampling is used during the training. Finally, the vector_size parameter affects the number of dimensions composing the resulting vector."
            ],
            "highlighted_evidence": [
              "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). "
            ]
          }
        ]
      },
      {
        "question": "What four evaluation tasks are defined to determine what influences proximity?",
        "question_id": "bd26a6d5d8b68d62e1b6eaf974796f3c34a839c4",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "String length",
              "Words co-occurrences",
              "Stems co-occurrences",
              "MeSH similarity"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "The goal here being to assess if D2V could effectively replace the related-document function on PubMed, five different document similarity evaluations were designed as seen on figure FIGREF9. These tasks were designed to cover every similarities, from the most general (the context) to the character-level similarity.",
              "Methods ::: Evaluation ::: String length",
              "To assess whether a similar length could lead to convergence of two documents, the size of the query document $D_{x}$ has been compared with the top-close document $C_{x}$ for 10,000 document randomly selected from the TeS after some pre-processing steps (stopwords and spaces were removed from both documents).",
              "Methods ::: Evaluation ::: Words co-occurrences",
              "A matrix of words co-occurrence was constructed on the total corpus from PubMed. Briefly, each document was lowered and tokenized. A matrix was filled with the number of times that two words co-occur in a single document. Then, for 5,000 documents $D_{x}$ from the TeS, all models were queried for the top-close document $C_{x}$. All possible combinations between all words $WD_{x} \\in D_{x}$ and all words $WC_{x} \\in C_{x}$ (excluding stopwords) were extracted, 500 couples were randomly selected and the number of times each of them was co-occurring was extracted from the matrix. The average value of this list was calculated, reflecting the proximity between D and C regarding their words content. This score was also calculated between each $D_{x}$ and the top-close document $C_{x}$ returned by the pmra algorithm.",
              "Methods ::: Evaluation ::: Stems co-occurrences",
              "The evaluation task explained above was also applied on 10,000 stemmed texts (using the Gensim’s PorterStemmer to only keep word’s roots). The influence of the conjugation form or other suffixes can be assessed.",
              "Methods ::: Evaluation ::: MeSH similarity",
              "It is possible to compare the ability of both pmra and D2V to bring closer articles which were indexed with common labels. To do so, 5,000 documents $D_{x}$ randomly selected from the TeS were sent to both pmra and D2V architectures, and the top-five closer articles $C_{x}$ were extracted. The following rules were then applied to each MeSH found associated with $D_{x}$ for each document $C_{x_i}$ : add 1 to the score if this MeSH term is found in both $D_{x}$ and $C_{x_i}$, add 3 if this MeSH is defined as major topic and add 1 for each qualifier in common between $D_{x}$ and Cxi regarding this particular MeSH term. Then, the mean of these five scores was calculated for both pmra and D2V."
            ],
            "highlighted_evidence": [
              "The goal here being to assess if D2V could effectively replace the related-document function on PubMed, five different document similarity evaluations were designed as seen on figure FIGREF9. ",
              "Methods ::: Evaluation ::: String length\nTo assess whether a similar length could lead to convergence of two documents, the size of the query document $D_{x}$ has been compared with the top-close document $C_{x}$ for 10,000 document randomly selected from the TeS after some pre-processing steps (stopwords and spaces were removed from both documents).",
              "Methods ::: Evaluation ::: Words co-occurrences\nA matrix of words co-occurrence was constructed on the total corpus from PubMed. Briefly, each document was lowered and tokenized. A matrix was filled with the number of times that two words co-occur in a single document. Then, for 5,000 documents $D_{x}$ from the TeS, all models were queried for the top-close document $C_{x}$. All possible combinations between all words $WD_{x} \\in D_{x}$ and all words $WC_{x} \\in C_{x}$ (excluding stopwords) were extracted, 500 couples were randomly selected and the number of times each of them was co-occurring was extracted from the matrix. The average value of this list was calculated, reflecting the proximity between D and C regarding their words content. This score was also calculated between each $D_{x}$ and the top-close document $C_{x}$ returned by the pmra algorithm.",
              "Methods ::: Evaluation ::: Stems co-occurrences\nThe evaluation task explained above was also applied on 10,000 stemmed texts (using the Gensim’s PorterStemmer to only keep word’s roots). The influence of the conjugation form or other suffixes can be assessed.",
              "Methods ::: Evaluation ::: MeSH similarity\nIt is possible to compare the ability of both pmra and D2V to bring closer articles which were indexed with common labels. To do so, 5,000 documents $D_{x}$ randomly selected from the TeS were sent to both pmra and D2V architectures, and the top-five closer articles $C_{x}$ were extracted. The following rules were then applied to each MeSH found associated with $D_{x}$ for each document $C_{x_i}$ : add 1 to the score if this MeSH term is found in both $D_{x}$ and $C_{x_i}$, add 3 if this MeSH is defined as major topic and add 1 for each qualifier in common between $D_{x}$ and Cxi regarding this particular MeSH term. Then, the mean of these five scores was calculated for both pmra and D2V."
            ]
          }
        ]
      },
      {
        "question": "What six parameters were optimized with grid search?",
        "question_id": "7d4fad6367f28c67ad22487094489680c45f5062",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [
              "window_size",
              "alpha",
              "sample",
              "dm",
              "hs",
              "vector_size"
            ],
            "yes_no": null,
            "free_form_answer": "",
            "evidence": [
              "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). The hs option defines whether hierarchical softmax or negative sampling is used during the training. Finally, the vector_size parameter affects the number of dimensions composing the resulting vector."
            ],
            "highlighted_evidence": [
              "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). The hs option defines whether hierarchical softmax or negative sampling is used during the training. Finally, the vector_size parameter affects the number of dimensions composing the resulting vector."
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1911-11698",
    "dblp_title": "Doc2Vec on the PubMed corpus: study of a new approach to generate related articles.",
    "year": "2019"
  },
  {
    "id": "1901.02257",
    "title": "Multi-Perspective Fusion Network for Commonsense Reading Comprehension",
    "qas": [
      {
        "question": "What baseline models do they compare against?",
        "question_id": "3aa7173612995223a904cc0f8eef4ff203cbb860",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)",
            "evidence": [
              "FLOAT SELECTED: Table 2: Experimental Results of Models"
            ],
            "highlighted_evidence": [
              "FLOAT SELECTED: Table 2: Experimental Results of Models"
            ]
          }
        ]
      }
    ],
    "dblp_id": "journals/corr/abs-1901-02257",
    "dblp_title": "Multi-Perspective Fusion Network for Commonsense Reading Comprehension.",
    "year": "2019"
  },
  {
    "id": "1710.01507",
    "title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks",
    "qas": [
      {
        "question": "What are the differences with previous applications of neural networks for this task?",
        "question_id": "acc8d9918d19c212ec256181e51292f2957b37d7",
        "answers": [
          {
            "unanswerable": false,
            "extractive_spans": [],
            "yes_no": null,
            "free_form_answer": "This approach considers related images",
            "evidence": [
              "One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task."
            ],
            "highlighted_evidence": [
              "One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task."
            ]
          }
        ]
      }
    ],
    "dblp_id": "conf/sigir/KumarKGLV18",
    "dblp_title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks.",
    "year": "2018"
  }
]